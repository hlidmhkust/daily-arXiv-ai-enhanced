{"id": "2510.20857", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20857", "abs": "https://arxiv.org/abs/2510.20857", "authors": ["Phat Tran", "Enbai Kuang", "Fred Xu"], "title": "Lightweight Classifier for Detecting Intracranial Hemorrhage in Ultrasound Data", "comment": null, "summary": "Intracranial hemorrhage (ICH) secondary to Traumatic Brain Injury (TBI)\nrepresents a critical diagnostic challenge, with approximately 64,000\nTBI-related deaths annually in the United States. Current diagnostic modalities\nincluding Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) have\nsignificant limitations: high cost, limited availability, and infrastructure\ndependence, particularly in resource-constrained environments. This study\ninvestigates machine learning approaches for automated ICH detection using\nUltrasound Tissue Pulsatility Imaging (TPI), a portable technique measuring\ntissue displacement from hemodynamic forces during cardiac cycles. We analyze\nultrasound TPI signals comprising 30 temporal frames per cardiac cycle with\nrecording angle information, collected from TBI patients with CT-confirmed\nground truth labels. Our preprocessing pipeline employs z-score normalization\nand Principal Component Analysis (PCA) for dimensionality reduction, retaining\ncomponents explaining 95% of cumulative variance. We systematically evaluate\nmultiple classification algorithms spanning probabilistic, kernel-based, neural\nnetwork, and ensemble learning approaches across three feature representations:\noriginal 31-dimensional space, reduced subset, and PCA-transformed space.\nResults demonstrate that PCA transformation substantially improves classifier\nperformance, with ensemble methods achieving 98.0% accuracy and F1-score of\n0.890, effectively balancing precision and recall despite class imbalance.\nThese findings establish the feasibility of machine learning-based ICH\ndetection in TBI patients using portable ultrasound devices, with applications\nin emergency medicine, rural healthcare, and military settings where\ntraditional imaging is unavailable."}
{"id": "2510.20864", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20864", "abs": "https://arxiv.org/abs/2510.20864", "authors": ["Daisuke Matsumoto", "Tomohiro Kikuchi", "Yusuke Takagi", "Soichiro Kojima", "Ryoma Kobayashi", "Daiju Ueda", "Kohei Yamamoto", "Sho Kawabe", "Harushi Mori"], "title": "Eye-Tracking as a Tool to Quantify the Effects of CAD Display on Radiologists' Interpretation of Chest Radiographs", "comment": null, "summary": "Rationale and Objectives: Computer-aided detection systems for chest\nradiographs are widely used, and concurrent reader displays, such as\nbounding-box (BB) highlights, may influence the reading process. This pilot\nstudy used eye tracking to conduct a preliminary experiment to quantify which\naspects of visual search were affected. Materials and Methods: We sampled 180\nchest radiographs from the VinDR-CXR dataset: 120 with solitary pulmonary\nnodules or masses and 60 without. The BBs were configured to yield an overall\ndisplay sensitivity and specificity of 80%. Three radiologists (with 11, 5, and\n1 years of experience, respectively) interpreted each case twice - once with\nBBs visible and once without - after a washout of >= 2 weeks. Eye movements\nwere recorded using an EyeTech VT3 Mini. Metrics included interpretation time,\ntime to first fixation on the lesion, lesion dwell time, total gaze-path\nlength, and lung-field coverage ratio. Outcomes were modeled using a linear\nmixed model, with reading condition as a fixed effect and case and reader as\nrandom intercepts. The primary analysis was restricted to true positives\n(n=96). Results: Concurrent BB display prolonged interpretation time by 4.9 s\n(p<0.001) and increased lesion dwell time by 1.3 s (p<0.001). Total gaze-path\nlength increased by 2,076 pixels (p<0.001), and lung-field coverage ratio\nincreased by 10.5% (p<0.001). Time to first fixation on the lesion was reduced\nby 1.3 s (p<0.001). Conclusion: Eye tracking captured measurable alterations in\nsearch behavior associated with concurrent BB displays during chest radiograph\ninterpretation. These findings support the feasibility of this approach and\nhighlight the need for larger studies to confirm effects and explore\nimplications across modalities and clinical contexts."}
{"id": "2510.21040", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21040", "abs": "https://arxiv.org/abs/2510.21040", "authors": ["Mohammad Mahdi Danesh Pajouh", "Sara Saeedi"], "title": "Efficient Meningioma Tumor Segmentation Using Ensemble Learning", "comment": "2nd Place Winner in the BraTS 2025 MICCAI Challenge (Task 2:\n  Meningioma Tumor Segmentation)", "summary": "Meningiomas represent the most prevalent form of primary brain tumors,\ncomprising nearly one-third of all diagnosed cases. Accurate delineation of\nthese tumors from MRI scans is crucial for guiding treatment strategies, yet\nremains a challenging and time-consuming task in clinical practice. Recent\ndevelopments in deep learning have accelerated progress in automated tumor\nsegmentation; however, many advanced techniques are hindered by heavy\ncomputational demands and long training schedules, making them less accessible\nfor researchers and clinicians working with limited hardware. In this work, we\npropose a novel ensemble-based segmentation approach that combines three\ndistinct architectures: (1) a baseline SegResNet model, (2) an\nattention-augmented SegResNet with concatenative skip connections, and (3) a\ndual-decoder U-Net enhanced with attention-gated skip connections (DDUNet). The\nensemble aims to leverage architectural diversity to improve robustness and\naccuracy while significantly reducing training demands. Each baseline model was\ntrained for only 20 epochs and Evaluated on the BraTS-MEN 2025 dataset. The\nproposed ensemble model achieved competitive performance, with average\nLesion-Wise Dice scores of 77.30%, 76.37% and 73.9% on test dataset for\nEnhancing Tumor (ET), Tumor Core (TC) and Whole Tumor (WT) respectively. These\nresults highlight the effectiveness of ensemble learning for brain tumor\nsegmentation, even under limited hardware constraints. Our proposed method\nprovides a practical and accessible tool for aiding the diagnosis of\nmeningioma, with potential impact in both clinical and research settings."}
{"id": "2510.20903", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.20903", "abs": "https://arxiv.org/abs/2510.20903", "authors": ["Yirong Shen", "Lu Gan", "Cong Ling"], "title": "Information Theoretic Learning for Diffusion Models with Warm Start", "comment": "NeurIPS 2025", "summary": "Generative models that maximize model likelihood have gained traction in many\npractical settings. Among them, perturbation based approaches underpin many\nstrong likelihood estimation models, yet they often face slow convergence and\nlimited theoretical understanding. In this paper, we derive a tighter\nlikelihood bound for noise driven models to improve both the accuracy and\nefficiency of maximum likelihood learning. Our key insight extends the\nclassical KL divergence Fisher information relationship to arbitrary noise\nperturbations, going beyond the Gaussian assumption and enabling structured\nnoise distributions. This formulation allows flexible use of randomized noise\ndistributions that naturally account for sensor artifacts, quantization\neffects, and data distribution smoothing, while remaining compatible with\nstandard diffusion training. Treating the diffusion process as a Gaussian\nchannel, we further express the mismatched entropy between data and model,\nshowing that the proposed objective upper bounds the negative log-likelihood\n(NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and SOTA\nresults on ImageNet across multiple resolutions, all without data augmentation,\nand the framework extends naturally to discrete data."}
{"id": "2510.20828", "categories": ["eess.SP", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20828", "abs": "https://arxiv.org/abs/2510.20828", "authors": ["Dixon Vimalajeewa", "Ursula U. Muller", "Brani Vidakovic"], "title": "A Multiscale Approach for Enhancing Weak Signal Detection", "comment": null, "summary": "Stochastic resonance (SR), a phenomenon originally introduced in climate\nmodeling, enhances signal detection by leveraging optimal noise levels within\nnon-linear systems. Traditional SR techniques, mainly based on single-threshold\ndetectors, are limited to signals whose behavior does not depend on time. Often\nlarge amounts of noise are needed to detect weak signals, which can distort\ncomplex signal characteristics. To address these limitations, this study\nexplores multi-threshold systems and the application of SR in multiscale\napplications using wavelet transforms. In the multiscale domain signals can be\nanalyzed at different levels of resolution to better understand the underlying\ndynamics.\n  We propose a double-threshold detection system that integrates two\nsingle-threshold detectors to enhance weak signal detection. We evaluate it\nboth in the original data domain and in the multiscale domain using simulated\nand real-world signals and compare its performance with existing methods.\n  Experimental results demonstrate that, in the original data domain, the\nproposed double-threshold detector significantly improves weak signal detection\ncompared to conventional single-threshold approaches. Its performance is\nfurther improved in the frequency domain, requiring lower noise levels while\noutperforming existing detection systems. This study advances SR-based\ndetection methodologies by introducing a robust approach to weak signal\nidentification, with potential applications in various disciplines."}
{"id": "2510.21030", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.21030", "abs": "https://arxiv.org/abs/2510.21030", "authors": ["En-Jui Chang"], "title": "Overlapped-repetition Shor codes achieving fourfold asymptotic rate", "comment": "4 pages", "summary": "The standard Shor code employs two repetition codes as inner and outer codes,\nyielding a simple structure but a relatively low code rate. By overlapping a\nsmall number of repetition codes, we enhance the asymptotic code rate fourfold.\nIn the minimal-distance case $d = 3$, this construction reduces the overhead\nfrom $[[9,1,3]]$ to the more efficient $[[7,1,3]]$ configuration."}
{"id": "2510.20998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.20998", "abs": "https://arxiv.org/abs/2510.20998", "authors": ["Martin Andersson", "Anubhab Chowdhury", "Erik G. Larsson"], "title": "Is Repeater-Assisted Massive MIMO Compatible with Dynamic TDD?", "comment": "Submitted to 2026 IEEE ICASSP", "summary": "We present a framework for joint amplification and phase shift optimization\nof the repeater gain in dynamic time-division duplex (TDD) repeater-assisted\nmassive MIMO networks. Repeaters, being active scatterers with amplification\nand phase shift, enhance the received signal strengths for users. However, they\ninevitably also amplify undesired noise and interference signals, which become\nparticularly prominent in dynamic TDD systems due to the concurrent downlink\n(DL) and uplink (UL) transmissions, introducing cross-link interference among\naccess points and users operating in opposite transmit directions. This causes\na non-trivial trade-off between amplification of desired and undesired signals.\nTo underpin the conditions under which such a trade-off can improve\nperformance, we first derive DL and UL spectral efficiencies (SEs), and then\ndevelop a repeater gain optimization algorithm for SE maximization.\nNumerically, we show that our proposed algorithm successfully calibrates the\nrepeater gain to amplify the desired signal while limiting the interference."}
{"id": "2510.21253", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21253", "abs": "https://arxiv.org/abs/2510.21253", "authors": ["Boaz Moav", "Ryan Gabrys", "Eitan Yaakobi"], "title": "Complex DNA Synthesis Sequences", "comment": null, "summary": "DNA-based storage offers unprecedented density and durability, but its\nscalability is fundamentally limited by the efficiency of parallel strand\nsynthesis. Existing methods either allow unconstrained nucleotide additions to\nindividual strands, such as enzymatic synthesis, or enforce identical additions\nacross many strands, such as photolithographic synthesis. We introduce and\nanalyze a hybrid synthesis framework that generalizes both approaches: in each\ncycle, a nucleotide is selected from a restricted subset and incorporated in\nparallel. This model gives rise to a new notion of a complex synthesis\nsequence. Building on this framework, we extend the information rate definition\nof Lenz et al. and analyze an analog of the deletion ball, defined and studied\nin this setting, deriving tight expressions for the maximal information rate\nand its asymptotic behavior. These results bridge the theoretical gap between\nconstrained models and the idealized setting in which every nucleotide is\nalways available. For the case of known strands, we design a dynamic\nprogramming algorithm that computes an optimal complex synthesis sequence,\nhighlighting structural similarities to the shortest common supersequence\nproblem. We also define a distinct two-dimensional array model with synthesis\nconstraints over the rows, which extends previous synthesis models in the\nliterature and captures new structural limitations in large-scale strand\narrays. Additionally, we develop a dynamic programming algorithm for this\nproblem as well. Our results establish a new and comprehensive theoretical\nframework for constrained DNA, subsuming prior models and setting the stage for\nfuture advances in the field."}
{"id": "2510.21137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21137", "abs": "https://arxiv.org/abs/2510.21137", "authors": ["Zhonglun Wang", "Yizhe Zhao", "Gangming Hu", "Yali Zheng", "Kun Yang"], "title": "6D Movable Holographic Surface Assisted Integrated Data and Energy Transfer: A Sensing Enhanced Approach", "comment": null, "summary": "Reconfigurable holographic surface (RHS) enables cost-effective large-scale\narrays with high spatial gain. However, its amplitude-controlled holographic\nbeamforming suffers from directional fluctuations, making it difficult to fully\nexploit the spatial gain of RHS. Fortunately, the promising 6D movable antenna\n(6DMA) provides a potential solution to this problem. In this paper, we study a\n6D movable holographic surface (6DMHS) integrated data and energy transfer\n(IDET) system, where a three-stage protocol is proposed, consisting of an\nuplink sensing stage, an orientation adjustment stage and a downlink\ntransmission stage, to coordinate the 6DMHS and effectively serve the IDET\nreceivers. Firstly, the holographic-based sensing technology is proposed and\nthe sensing information of the IDET receivers is exploited. Secondly, by fixing\nthe rotations with the sensing information, the orientation optimization\nproblem is formulated for designing the holographic beamforming of the RHS and\nadjusting the translations of the 6DMHS. As a result, the directions with\nmaximum beamforming gain are aligned with each IDET receiver. Thirdly, by\nfixing the orientation of the 6DMHS and the holographic beamforming, the\nequivalent wireless channel is obtained. The IDET performance optimization\nproblem is formulated for obtaining the optimal digital beamforming, power\nsplitting factor and energy harvesting (EH) power. Simulation results\ndemonstrate that the proposed scheme is capable of improving the IDET\nperformance compared to the benchmarks."}
{"id": "2510.21299", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21299", "abs": "https://arxiv.org/abs/2510.21299", "authors": ["Shengkang Chen", "Tong Wu", "Zhiyong Chen", "Feng Yang", "Meixia Tao", "Wenjun Zhang"], "title": "Text-Guided Diffusion Model-based Generative Communication for Wireless Image Transmission", "comment": "submitted to IEEE journal", "summary": "Reliable image transmission over wireless channels is particularly\nchallenging at extremely low transmission rates, where conventional compression\nand channel coding schemes fail to preserve adequate visual quality. To address\nthis issue, we propose a generative communication framework based on diffusion\nmodels, which integrates joint source channel coding (JSCC) with\nsemantic-guided reconstruction leveraging a pre-trained generative model.\nUnlike conventional architectures that aim to recover exact pixel values of the\noriginal image, the proposed method focuses on preserving and reconstructing\nsemantically meaningful visual content under severely constrained rates,\nensuring perceptual plausibility and faithfulness to the scene intent.\nSpecifically, the transmitter encodes the source image via JSCC and jointly\ntransmits it with a textual prompt over the wireless channel. At the receiver,\nthe corrupted low-rate representation is fused with the prompt and\nreconstructed through a Stable Diffusion model with ControlNet, enabling\nhigh-quality visual recovery. Leveraging both generative priors and semantic\nguidance, the proposed framework produces perceptually convincing images even\nunder extreme bandwidth limitations. Experimental results demonstrate that the\nproposed method consistently outperforms conventional coding-based schemes and\ndeep learning baselines, achieving superior perceptual quality and robustness\nacross various channel conditions."}
{"id": "2510.21278", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21278", "abs": "https://arxiv.org/abs/2510.21278", "authors": ["Laura M. Wolf", "Vincent Albert Wolff", "Simon Steuernagel", "Kolja Thormann", "Marcus Baum"], "title": "Track-to-Track Association for Collective Perception based on Stochastic Optimization", "comment": null, "summary": "Collective perception is a key aspect for autonomous driving in smart cities\nas it aims to combine the local environment models of multiple intelligent\nvehicles in order to overcome sensor limitations. A crucial part of\nmulti-sensor fusion is track-to-track association. Previous works often suffer\nfrom high computational complexity or are based on heuristics. We propose an\nassociation algorithms based on stochastic optimization, which leverages a\nmultidimensional likelihood incorporating the number of tracks and their\nspatial distribution and furthermore computes several association hypotheses.\nWe demonstrate the effectiveness of our approach in Monte Carlo simulations and\na realistic collective perception scenario computing high-likelihood\nassociations in ambiguous settings."}
{"id": "2510.21386", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21386", "abs": "https://arxiv.org/abs/2510.21386", "authors": ["Xiaotian Fan", "Xingyu Zhou", "Le Liang", "Shi Jin"], "title": "Low-Complexity MIMO Channel Estimation with Latent Diffusion Models", "comment": null, "summary": "Deep generative models offer a powerful alternative to conventional channel\nestimation by learning the complex prior distribution of wireless channels.\nCapitalizing on this potential, this paper proposes a novel channel estimation\nalgorithm based on latent diffusion models (LDMs), termed posterior sampling\nwith latent diffusion for channel estimation (PSLD-CE). The core of our\napproach is a lightweight LDM architecture specifically designed for channel\nestimation, which serves as a powerful generative prior to capture the\nintricate channel distribution. Furthermore, we enhance the diffusion posterior\nsampling process by introducing an effective approximation for the likelihood\nterm and a tailored self-consistency constraint on the variational autoencoder\nlatent space. Extensive experimental results demonstrate that PSLD-CE\nconsistently outperforms a wide range of existing methods. Notably, these\nsignificant performance gains are achieved while maintaining low computational\ncomplexity and fast inference speed, establishing our method as a highly\npromising and practical solution for next-generation wireless systems."}
{"id": "2510.21378", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21378", "abs": "https://arxiv.org/abs/2510.21378", "authors": ["Biao Dong", "Bin Cao"], "title": "Optimized Power Control for Multi-User Integrated Sensing and Edge AI", "comment": null, "summary": "This work investigates an integrated sensing and edge artificial intelligence\n(ISEA) system, where multiple devices first transmit probing signals for target\nsensing and then offload locally extracted features to the access point (AP)\nvia analog over-the-air computation (AirComp) for collaborative inference. To\ncharacterize the relationship between AirComp error and inference performance,\ntwo proxies are established: the \\emph{computation-optimal} proxy that\nminimizes the aggregation distortion, and the \\emph{decision-optimal} proxy\nthat maximizes the inter-class separability, respectively. Optimal transceiver\ndesigns in terms of closed-form power allocation are derived for both\ntime-division multiplexing (TDM) and frequency-division multiplexing (FDM)\nsettings, revealing threshold-based and dual-decomposition structures,\nrespectively. Experimental results validate the theoretical findings."}
{"id": "2510.21414", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21414", "abs": "https://arxiv.org/abs/2510.21414", "authors": ["Hoang Ly", "Emina Soljanin"], "title": "Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication", "comment": null, "summary": "Maximum-likelihood (ML) decoding for arbitrary block codes remains\nfundamentally hard, with worst-case time complexity-measured by the total\nnumber of multiplications-being no better than straightforward exhaustive\nsearch, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper\nintroduces a simple, code-agnostic framework that reduces the worst-case\ncomplexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable\nreduction in practice. The result holds for both linear and nonlinear block\ncodes over general memoryless channels and under both hard-decision and\nsoft-decision decoding. It naturally extends to intersymbol-interference (ISI)\nchannels and ML list decoding with only a negligible increase in complexity.\nOur core insight is that, upon receipt of each sequence at the receiver, the\nconditional probability of that sequence for each codeword in the codebook\n(i.e., the \\emph{likelihood}) can be expressed as the inner product of two\ncarefully constructed vectors -- the first depending on the received sequence,\nand the second on that codeword itself. As a result, evaluating the likelihoods\nfor all codewords in the codebook reduces to a single vector-matrix\nmultiplication, and ML decoding (MLD) becomes the simple task of picking the\nmaximum entry in the resulting vector. The only non-trivial cost lies in the\nvector-matrix product. However, our matrix construction allows the use of the\nMailman algorithm to reduce this cost. This time reduction is achieved at the\ncost of high space complexity, requiring $\\mathcal{O}(q^{k+1} n)$ space to\nstore the pre-computed codebook matrix."}
{"id": "2510.21509", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21509", "abs": "https://arxiv.org/abs/2510.21509", "authors": ["Carmen Álvarez Roa", "Yunus Can Gültekin", "Vincent van Vliet", "Menno van den Hout", "Chigo Okonkwo", "Alex Alvarado"], "title": "On Irradiance Distributions for Weakly Turbulent FSO Links: Log-Normal vs. Gamma-Gamma", "comment": null, "summary": "Weak turbulence is commonly modeled using the log-normal distribution. Our\nexperimental results show that this distribution fails to capture irradiance\nfluctuations in this regime. The Gamma-Gamma model is shown to be more\naccurate."}
{"id": "2510.21587", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21587", "abs": "https://arxiv.org/abs/2510.21587", "authors": ["Bho Matthiesen", "Armin Dekorsy", "Petar Popovski"], "title": "Resilient Radio Access Networks: AI and the Unknown Unknowns", "comment": "Accepted for presentation at 2025 IEEE Globecom Workshop on\n  Resilience in Next-Generation Wireless Communication Networks", "summary": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference."}
