<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 22]
- [cs.IT](#cs.IT) [Total: 50]
- [eess.IV](#eess.IV) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Dual radar-guided glide path error correction based on the Izhikevich neuron model](https://arxiv.org/abs/2601.06068)
*Yuan Gao,Xinyu Wang,Yifan Ren,Yuning Zhou,Ziwei Wang*

Main category: eess.SP

TL;DR: 提出基于Izhikevich神经模型的双雷达航迹误差校正方法，通过脉冲频率表示误差补偿量，利用STDP动态调整权重，抑制系统噪声和雷达测距测角误差


<details>
  <summary>Details</summary>
Motivation: 双雷达跟踪中目标反射特性和系统噪声会导致测距和测角误差，影响航迹精度，需要有效的误差校正方法

Method: 基于Izhikevich神经模型构建网络，输入层整合双雷达坐标测量数据，输出层通过脉冲发射频率表示误差补偿量，利用STDP动态调整神经元连接权重

Result: 能够有效抑制系统噪声和雷达测距测角误差引起的轨迹失真

Conclusion: 基于Izhikevich神经模型的双雷达航迹误差校正方法能有效提高跟踪精度，抑制误差影响

Abstract: Aiming at the ranging and angle measurement errors caused by target reflection characteristics and system noise in dual radar tracking, this paper proposes a dual radar track error correction method based on the Izhikevich neural model. The network uses the dynamic differential equation of the Izhikevich model to simulate the discharge characteristics of biological neurons. Its input layer integrates the coordinate measurement data of the dual radar, and the output layer represents the error compensation amount through the pulse emission frequency. The spike-timing-dependent plasticity (STDP) is used to adjust the neuron connection weights dynamically, and the trajectory distortion caused by system noise and radar ranging and angle measurement errors can be effectively suppressed.

</details>


### [2] [Optimizing the 4G--5G Migration: A Simulation-Driven Roadmap for Emerging Markets](https://arxiv.org/abs/2601.06076)
*Desire Guel,Justin Pegd-Windé Kouraogo,Kouka Kouakou Nakoulma*

Main category: eess.SP

TL;DR: 通过MATLAB仿真分析5G在发展中市场的部署策略，量化MIMO、载波聚合、频谱重耕、毫米波传播等关键技术对容量、覆盖、时延和干扰鲁棒性的影响，提出从NSA开始、优先载波聚合、选择性密集化、逐步迁移到SA的实用路线图。


<details>
  <summary>Details</summary>
Motivation: 在发展中市场部署5G网络需要在性能目标与预算、频谱和基础设施限制之间取得平衡。需要量化各种无线和架构技术对网络性能的影响，为实际部署提供指导。

Method: 使用MATLAB仿真量化分析多种技术：MIMO（波束赋形、分集、空间复用）、载波聚合、频谱重耕到NR、毫米波传播（遮挡/降雨影响）、NSA与SA核心网架构。评估这些技术对容量、覆盖、时延和干扰鲁棒性的影响。

Result: 波束赋形提升小区边缘SNR约3-6 dB；空间复用在中等/高SNR下通过多流增益占主导；载波聚合显著提升吞吐量（1到5个20MHz载波使峰值速率从200 Mb/s提升到1 Gb/s）；中频段重耕到NR提升城市中值吞吐量60-90%、农村40-70%；28GHz毫米波受降雨和人体遮挡增加8-30 dB额外损耗；NSA比SA提供更广泛的初始覆盖。

Conclusion: 提出实用部署路线图：从NSA开始部署NR，优先采用以载波聚合为中心的频谱策略和重点频谱重耕，在需求热点区域选择性密集化，随着回传和设备生态系统成熟逐步迁移到SA架构。

Abstract: Deploying fifth-generation (5G) networks in emerging markets demands a balance between performance targets and constraints in budget, spectrum, and infrastructure. We use MATLAB simulations to quantify how radio and architectural levers - MIMO (beamforming, diversity, spatial multiplexing), carrier aggregation (CA), targeted spectrum refarming to New Radio (NR), mmWave propagation with blockage/rain, and Non-Standalone (NSA) versus Standalone (SA) cores - affect capacity, coverage, latency, and interference robustness, with D2D and M2M as complements to wide-area access. Beamforming improves cell-edge SNR by about 3-6 dB, while spatial multiplexing dominates at moderate/high SNR via multi-stream gains. Throughput scales strongly with CA: increasing from 1 to 5x20-MHz carriers raises peak rate from about 200 Mb/s to about 1 Gb/s at 30 dB SNR; water-filling adds 5-12% over equal power at mid-SNR. Targeted mid-band refarming to NR increases median throughput by 60-90% in urban and 40-70% in rural scenarios when sub-1-GHz layers preserve coverage. At 28 GHz, rain and human blockage add about 8-30 dB excess loss, so viable mmWave deployment concentrates in LOS hot zones with narrow-beam arrays and short inter-site distances. NSA delivers broader initial coverage than SA by reusing LTE/EPC, while SA becomes attractive as transport improves (e.g., >= 10 Gb/s and < 5 ms RTT) and site density grows. We synthesize these results into a practical roadmap: start NR on NSA, prioritize CA-centric spectrum strategies with focused refarming, densify selectively in demand hotspots, and migrate to SA as backhaul and device ecosystems mature.

</details>


### [3] [Timing Fragility Aware Selective Hardening of RISCV Soft Processors on SRAM Based FPGAs](https://arxiv.org/abs/2601.06308)
*Mostafa Darvishi*

Main category: eess.SP

TL;DR: 提出一种基于时序脆弱性感知的选择性加固方法，用于SRAM FPGA上的RISC-V软处理器，通过量化流水线组件的时序敏感性来指导加固决策，在保持可靠性的同时显著减少开销。


<details>
  <summary>Details</summary>
Motivation: 现有选择性加固方法主要依赖架构关键性或功能故障分析，忽略了路由相关的时序敏感性对处理器鲁棒性的影响。需要一种考虑时序脆弱性的方法来更有效地指导加固决策。

Method: 基于原位时序可观测性的最新进展，通过受控路由扰动量化流水线组件的统计时序敏感性，利用这些信息指导加固决策。该方法识别时序脆弱的组件，并针对性地进行加固。

Result: 实验结果显示，时序脆弱性更高的组件对路由引起的延迟效应也表现出更高的脆弱性。利用这种相关性，提出的选择性加固策略在保持与完全加固相当的鲁棒性的同时，显著减少了面积和时序开销。

Conclusion: 时序脆弱性为FPGA处理器架构的可靠性感知设计优化提供了一个实用且有效的度量标准，能够实现可靠性与开销的良好平衡。

Abstract: Selective hardening is widely employed to improve the reliability of FPGA based soft processors while limiting the overhead of full redundancy. However, existing approaches primarily rely on architectural criticality or functional fault analysis, overlooking the impact of routing dependent timing sensitivity on processor robustness. This paper introduces a timing fragility aware selective hardening methodology for RISCV soft processors implemented on SRAM based FPGAs. Building on recent advances in in situ timing observability, the proposed approach quantifies the statistical timing sensitivity of pipeline components under controlled routing perturbations and uses this information to guide hardening decisions. Experimental results on a RISCV processor implemented on a commercial FPGA platform show that components exhibiting higher timing fragility also demonstrate increased vulnerability to routing induced delay effects. Leveraging this correlation, the proposed selective hardening strategy achieves robustness comparable to full hardening while significantly reducing area and timing overhead. These results demonstrate that timing fragility provides a practical and effective metric for reliability aware design optimization in FPGA based processor architectures.

</details>


### [4] [Building Envelope Inversion by Data-driven Interpretation of Ground Penetrating Radar](https://arxiv.org/abs/2601.06333)
*Ahmed Nirjhar Alam,Wesley Reinhart,Rebecca Napolitano*

Main category: eess.SP

TL;DR: 开发基于GPR的墙体诊断框架，将墙体诊断分解为垂直（立柱存在）和水平（墙体类型）变化的分类任务，使用稀疏神经网络实现高精度且可物理解释的特征提取。


<details>
  <summary>Details</summary>
Motivation: 探地雷达（GPR）具有深度分辨率、非破坏性操作和广泛的材料敏感性，但在建筑围护结构诊断中应用有限。墙体组件的紧凑几何结构使得来自紧密排列的立柱、护板和覆层的反射信号强烈重叠，导致系统反演困难。数据驱动解释的最新进展为重新审视这一挑战提供了机会，以评估机器学习是否能从这种复杂信号中可靠提取结构信息。

Method: 开发GPR反演框架，将墙体诊断分解为处理垂直（立柱存在）和水平（墙体类型）变化的分类任务。实施多种特征最小化策略，包括递归消除、凝聚聚类和基于L0的稀疏性，以提高保真度和可解释性。其中基于L0的稀疏神经网络（SparseNN）表现尤为突出。

Result: 基于L0的稀疏神经网络（SparseNN）效果最好：它超过了随机森林的准确性，同时仅依赖于一小部分输入特征，每个特征都与可识别的介电界面相关联。SHAP分析进一步证实SparseNN学习的反射模式与物理层边界一致。

Conclusion: 该框架为使用GPR雷达图进行物理可解释且数据高效的墙体组件反演奠定了基础。虽然未解决缺陷检测问题，但重建完整围护结构并隔离与关键元素相关的特征的能力为未来的反演和异常分析任务提供了必要的基准。

Abstract: Ground-penetrating radar (GPR) combines depth resolution, non-destructive operation, and broad material sensitivity, yet it has seen limited use in diagnosing building envelopes. The compact geometry of wall assemblies, where reflections from closely spaced studs, sheathing, and cladding strongly overlap, has made systematic inversion difficult. Recent advances in data-driven interpretation provide an opportunity to revisit this challenge and assess whether machine learning can reliably extract structural information from such complex signals. Here, we develop a GPR-based inversion framework that decomposes wall diagnostics into classification tasks addressing vertical (stud presence) and lateral (wall-type) variations. Alongside model development, we implement multiple feature minimization strategies - including recursive elimination, agglomerative clustering, and L0-based sparsity - to promote fidelity and interpretability. Among these approaches, the L0-based sparse neural network (SparseNN) emerges as particularly effective: it exceeds Random Forest accuracy while relying on only a fraction of the input features, each linked to identifiable dielectric interfaces. SHAP analysis further confirms that the SparseNN learns reflection patterns consistent with physical layer boundaries. In summary, this framework establishes a foundation for physically interpretable and data-efficient inversion of wall assemblies using GPR radargrams. Although defect detection is not addressed here, the ability to reconstruct intact envelope structure and isolate features tied to key elements provides a necessary baseline for future inversion and anomaly-analysis tasks.

</details>


### [5] [Performance Analysis for Wireless Localization with Random Sensor Network](https://arxiv.org/abs/2601.06396)
*Mengqi Ma,Aihua Xia*

Main category: eess.SP

TL;DR: 论文研究了无线定位中基于RSS和AOA融合估计器的性能，证明了在高噪声条件下，平稳各向同性传感器网络的观测分布与齐次泊松点过程无法区分，并推导了MSE和CMSE的可解析上界。


<details>
  <summary>Details</summary>
Motivation: 准确的无线定位对于自动驾驶系统和智能基础设施等应用至关重要。现有研究需要更好地理解部署参数（如传感器密度、观测半径）与定位精度之间的定量关系，特别是在实际非理想部署场景下。

Method: 首先建立了近似定理：当测量噪声足够大时，广泛类别的平稳各向同性部署下RSS和AOA观测的联合分布与齐次泊松点过程（PPP）诱导的分布无法区分。然后基于PPP部署，提出融合估计器，推导了有限观测区域内MSE和CMSE的可解析上界。

Result: 推导出了关于传感器密度、观测半径和噪声方差的显式缩放规律。PPP推导的上界可作为非泊松部署在噪声较大时的合理代理。结果为部署和感知参数转化为可实现的精度目标提供了量化依据。

Conclusion: 该研究为下一代位置感知无线网络的设计提供了稳健、成本感知的指导，将部署和感知参数转化为可实现的精度目标，特别是在噪声较大的实际场景中。

Abstract: Accurate wireless localization underpins applications from autonomous systems to smart infrastructure. We study the mean-squared error (MSE) and conditional MSE (CMSE) of a practical fusion-based estimator in d-dimensional, stationary isotropic (translation- and rotation-invariant) random sensor networks, where a central processor combines received-signal-strength (RSS) and angle-of-arrival (AOA) measurements to infer a target's position. Our contributions are twofold. First, we establish an approximation theorem: when measurement noise is sufficiently large, the joint law of RSS and AOA observations under a broad class of stationary isotropic deployments is, in distribution, indistinguishable from that induced by a homogeneous Poisson point process (PPP). Second, leveraging this equivalence, we investigate a homogeneous PPP-based sensor network. We propose a fusion-based estimator in which a central processor aggregates RSS and AOA measurements from a set of spatially distributed sensors to infer the target position. For this PPP deployment within a finite observation region, we derive tractable analytical upper bounds for both the MSE and CMSE, establishing explicit scaling laws with respect to sensor density, observation radius, and noise variance. The approximation theorem then certifies these PPP-based bounds as reasonable proxies for non-Poisson deployments in noisy regimes. Overall, the results translate deployment and sensing parameters into achievable accuracy targets and provide robust, cost-aware guidance for the design of next-generation location-aware wireless networks.

</details>


### [6] [Neuro-Wideband WiFi Sensing via Self-Conditioned CSI Extrapolation](https://arxiv.org/abs/2601.06467)
*Sijie Ji,Weiying Hou,Chenshu Wu*

Main category: eess.SP

TL;DR: Neuro-Wideband (NWB) 提出了一种无需额外硬件或信道测量的宽带WiFi感知新范式，通过深度学习将有限带宽的CSI转换为近似宽带测量的eCSI。


<details>
  <summary>Details</summary>
Motivation: 传统WiFi感知受限于通信设计的有限带宽，导致多径分辨率和多用户感知能力受限。商用WiFi难以获得大带宽，因为频谱有限且网络拥挤。

Method: 提出Neuro-Wideband (NWB) 范式，核心洞察是CSI测量固有地封装了多径参数。开发WUKONG框架，将NWB作为自条件学习问题处理，集成Transformer和Diffusion模型，从现有CSI数据中学习样本特定的多径参数并转换为扩展CSI (eCSI)。

Result: 在真实世界实验中，WUKONG在不同协议和带宽的WiFi信号上表现出色。通过定位和多人员呼吸监测的案例研究进一步证明了eCSI的有效性。

Conclusion: NWB为在商用硬件上实现宽带WiFi感知提供了实用途径，扩展了无线感知系统的设计空间。

Abstract: WiFi sensing has suffered from the limited bandwidths designated for its original communication purpose, leading to fundamental limits in multipath resolution and thus multi-user sensing. Unfortunately, it is practically prohibitive to obtain large bandwidths on commercial WiFi, considering the conflict between the limited spectrum and the crowded networks. In this paper, we present Neuro-Wideband (NWB), a completely different paradigm that enables wideband WiFi sensing without specialized hardware or extra channel measurements. Our key insight is that any physical measurement of channel state information (CSI) inherently encapsulates multipath parameters, which, while unsolvable in isolation, can be transformed into an expanded form of CSI (eCSI) approximating measurements over a broader bandwidth. To ground this insight, we propose WUKONG to address NWB as a unique self-conditioned learning problem that can be trained by using any existing CSI data as self-labeled samples. WUKONG introduces a novel deep learning framework by integrating Transformer and Diffusion models, which captures sample-specific multipath parameters and transfers this sample-level knowledge to the outcome eCSI. We conduct real-world experiments to evaluate WUKONG on diverse WiFi signals across protocols and bandwidths. The results show the promising effectiveness of NWB, which is further demonstrated through case studies on localization and multi-person breathing monitoring using eCSI. Overall, the proposed NWB promises a practical pathway toward realizing wideband WiFi sensing on commodity hardware, expanding the design space of wireless sensing systems.

</details>


### [7] [Joint Impact of ADC and Fronthaul Quantization in Cell-Free Massive MIMO-OFDM Uplink](https://arxiv.org/abs/2601.06483)
*Özlem Tuğfe Demir,Emil Björnson*

Main category: eess.SP

TL;DR: 论文研究宽带OFDM无蜂窝大规模MIMO上行链路中ADC和fronthaul双重量化的联合影响，提出一种在fronthaul传输前处理接收信号以减少量化失真的策略。


<details>
  <summary>Details</summary>
Motivation: 在无蜂窝大规模MIMO系统的上行链路中，量化影响表现在两个关键领域：AP端ADC的时域失真和fronthaul传输到CPU的量化。虽然窄带系统中这两种量化效应已被广泛研究，但在实际宽带OFDM系统中的联合影响尚未充分探索。

Method: 建模联合失真并提出一种fronthaul策略：每个AP在处理接收信号以减少量化伪影后再传输。开发了一种高效估计算法，在fronthaul传输前重建未量化的时域信号。

Result: 提出的设计为在宽带无蜂窝架构中实现高效、量化感知的上行链路传输提供了新的见解。通过处理接收信号减少量化伪影，能够降低fronthaul负载并避免不必要的失真。

Conclusion: 该研究填补了宽带OFDM无蜂窝大规模MIMO系统中ADC和fronthaul双重量化联合影响的空白，提出的量化感知传输策略为实现高效上行链路传输提供了有价值的解决方案。

Abstract: In the uplink of a cell-free massive MIMO system, quantization affects performance in two key domains: the time-domain distortion introduced by finite-resolution analog-to-digital converters (ADCs) at the access points (APs), and the fronthaul quantization of signals sent to the central processing unit (CPU). Although quantizing twice may seem redundant, the ADC quantization in orthogonal frequency-division duplex (OFDM) systems appears in the time domain, and one must then convert to the frequency domain, where quantization can be applied only to the signals at active subcarriers. This reduces fronthaul load and avoids unnecessary distortion, since the ADC output spans all OFDM samples while only a subset of subcarriers carries useful information.
  While both quantization effects have been extensively studied in narrowband systems, their joint impact in practical wideband OFDM-based cell-free massive MIMO remains largely unexplored. This paper addresses the gap by modeling the joint distortion and proposing a fronthaul strategy in which each AP processes the received signal to reduce quantization artifacts before transmission. We develop an efficient estimation algorithm that reconstructs the unquantized time-domain signal prior to fronthaul transmission and evaluate its effectiveness. The proposed design offers new insights for implementing efficient, quantization-aware uplink transmission in wideband cell-free architectures.

</details>


### [8] [Cell-Free Massive MIMO with Hardware-Impaired Wireless Fronthaul](https://arxiv.org/abs/2601.06486)
*Özlem Tuğfe Demir,Emil Björnson*

Main category: eess.SP

TL;DR: 提出了一种针对无蜂窝大规模MIMO的新型无线前传方案，考虑了AP和前传收发器的硬件损伤，设计了损伤感知的线性组合器以提升性能。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO通过分布式AP的联合传输和接收提高了频谱和能量效率，但低成本密集部署的AP存在不可避免的硬件损伤。现有研究主要关注接入侧的硬件损伤，而无线前传链路的硬件损伤影响尚未充分探索。

Method: 提出了一种基于放大转发的新型无线前传方案，开发了分析框架联合建模AP和前传收发器的硬件损伤，推导端到端失真信号表达式，量化各损伤对频谱效率的贡献，并设计了损伤感知的线性组合器来优化缓解这些影响。

Result: 数值结果表明，损伤感知处理带来了显著的性能增益，证明了所提出的AF前传方案作为未来无蜂窝架构成本效益高的使能器的潜力。

Conclusion: 该工作填补了无线前传链路硬件损伤研究的重要空白，提出的损伤感知处理方案能有效提升无蜂窝大规模MIMO系统的性能，为未来低成本部署提供了可行的技术方案。

Abstract: Cell-free massive MIMO (multiple-input multiple-output) enhances spectral and energy efficiency compared to conventional cellular networks by enabling joint transmission and reception across a large number of distributed access points (APs). Since these APs are envisioned to be low-cost and densely deployed, hardware impairments, stemming from non-ideal radio-frequency (RF) chains, are unavoidable. While existing studies primarily address hardware impairments on the access side, the impact of hardware impairments on the wireless fronthaul link has remained largely unexplored. In this work, we fill this important gap by introducing a novel amplify-and-forward (AF) based wireless fronthauling scheme tailored for cell-free massive MIMO. Focusing on the uplink, we develop an analytical framework that jointly models the hardware impairments at both the APs and the fronthaul transceivers, derives the resulting end-to-end distorted signal expression, and quantifies the individual contribution of each impairment to the spectral efficiency. Furthermore, we design distortion-aware linear combiners that optimally mitigate these effects. Numerical results demonstrate significant performance gains from distortion-aware processing and illustrate the potential of the proposed AF fronthauling scheme as a cost-effective enabler for future cell-free architectures.

</details>


### [9] [A Multimodal Deep Learning Framework for Predicting ICU Deterioration: Integrating ECG Waveforms with Clinical Data and Clinician Benchmarking](https://arxiv.org/abs/2601.06645)
*Juan Miguel López Alcaraz,Xicoténcatl López Moran,Erick Dávila Zaragoza,Claas Händel,Richard Koebe,Wilhelm Haverkamp,Nils Strodthoff*

Main category: eess.SP

TL;DR: MDS ICU是一个统一的多模态机器学习框架，融合了ICU常规收集的多种数据（人口统计学、生命体征、实验室值、ECG波形等），用于预测33种临床相关结局，在多项预测任务中表现优异，并能增强临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有ICU风险预测模型通常关注孤立结局或有限数据类型，而临床医生需要整合纵向历史、实时生理数据和异质临床信息。需要开发能够融合多模态数据、提供连续预测支持的统一框架。

Method: 开发了MDS ICU框架，融合了人口统计学、生物特征、生命体征、实验室值、ECG波形、手术过程和医疗设备使用等常规数据。使用MIMIC IV数据库中63001个样本（来自27062名患者），采用深度学习架构：ECG波形使用结构化状态空间S4编码器，表格数据使用多层感知机RealMLP编码器，共同预测33种临床相关结局。

Result: 模型表现出强大的区分能力：24小时死亡率AUROC 0.90，镇静剂使用0.92，有创机械通气0.97，凝血功能障碍0.93。校准分析显示预测风险与观察风险高度一致。ECG波形整合带来一致增益。模型预测单独优于临床医生和大型语言模型，提供模型输出作为决策支持能进一步提升临床医生表现。

Conclusion: 多模态AI能够在ICU中提供具有临床意义的跨结局风险分层，增强而非替代临床专业知识，为精准重症监护决策支持建立了可扩展的基础。

Abstract: Artificial intelligence holds strong potential to support clinical decision making in intensive care units where timely and accurate risk assessment is critical. However, many existing models focus on isolated outcomes or limited data types, while clinicians integrate longitudinal history, real time physiology, and heterogeneous clinical information. To address this gap, we developed MDS ICU, a unified multimodal machine learning framework that fuses routinely collected data including demographics, biometrics, vital signs, laboratory values, ECG waveforms, surgical procedures, and medical device usage to provide continuous predictive support during ICU stays. Using 63001 samples from 27062 patients in MIMIC IV, we trained a deep learning architecture that combines structured state space S4 encoders for ECG waveforms with multilayer perceptron RealMLP encoders for tabular data to jointly predict 33 clinically relevant outcomes spanning mortality, organ dysfunction, medication needs, and acute deterioration. The model achieved strong discrimination with AUROCs of 0.90 for 24 hour mortality, 0.92 for sedative administration, 0.97 for invasive mechanical ventilation, and 0.93 for coagulation dysfunction. Calibration analysis showed close agreement between predicted and observed risks, with consistent gains from ECG waveform integration. Comparisons with clinicians and large language models showed that model predictions alone outperformed both, and that providing model outputs as decision support further improved their performance. These results demonstrate that multimodal AI can deliver clinically meaningful risk stratification across diverse ICU outcomes while augmenting rather than replacing clinical expertise, establishing a scalable foundation for precision critical care decision support.

</details>


### [10] [Artificial Intelligence Driven Channel Coding and Resource Optimization for Wireless Networks](https://arxiv.org/abs/2601.06796)
*Yasir Ali,Tayyab Manzoor,Huan Yang,Chenhang Yan,Yuanqing Xia*

Main category: eess.SP

TL;DR: 本文探讨AI在5G/5G+网络中的关键作用，特别是AI驱动的编码理论创新如何解决干扰缓解、动态资源分配等挑战，提升错误校正性能和传输效率。


<details>
  <summary>Details</summary>
Motivation: 5G/5G+网络对超高速数据传输、超低延迟和弹性连接的需求急剧增长，以支持物联网、自动驾驶和智慧城市等关键应用。传统错误校正和调制技术存在局限性，需要AI来解决网络面临的干扰缓解、动态资源分配和网络无缝运行等挑战。

Method: 研究采用深度学习、强化学习和神经网络方法，专注于AI驱动的编码理论创新。同时探讨AI与大规模MIMO、智能反射面和隐私增强机制等新兴技术的集成。

Result: AI方法在错误校正性能、解码效率和自适应传输策略方面取得显著进展，能够推动下一代无线网络的发展。

Conclusion: AI对现代无线通信具有变革性影响，为可扩展、自适应和更高效的网络架构奠定了基础，特别是在编码理论方面的创新为解决5G/5G+网络挑战提供了先进解决方案。

Abstract: The ongoing evolution of 5G and its enhanced version, 5G+, has significantly transformed the telecommunications landscape, driving an unprecedented demand for ultra-high-speed data transmission, ultra-low latency, and resilient connectivity. These capabilities are essential for enabling mission-critical applications such as the Internet of Things, autonomous vehicles, and smart city infrastructures. This paper investigates the important role of Artificial Intelligence (AI) in addressing the key challenges faced by 5G/5G+ networks, including interference mitigation, dynamic resource allocation, and maintaining seamless network operation. The study particularly focuses on AI-driven innovations in coding theory, which offer advanced solutions to the limitations of conventional error correction and modulation techniques. By employing deep learning, reinforcement learning, and neural network-based approaches, this research demonstrates significant advancements in error correction performance, decoding efficiency, and adaptive transmission strategies. Additionally, the integration of AI with emerging technologies, such as massive multiple-input and multiple-output, intelligent reflecting surfaces, and privacy-enhancing mechanisms, is discussed, highlighting their potential to propel the next generation of wireless networks. This paper also provides insights into the transformative impact of AI on modern wireless communication, establishing a foundation for scalable, adaptive, and more efficient network architectures.

</details>


### [11] [RIS-aided ISAC with $K$-Rydberg Atomic Receivers](https://arxiv.org/abs/2601.06809)
*Hong-Bae Jeon,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 该论文提出了一种基于可重构智能表面辅助的集成感知与通信框架，采用多用户Rydberg原子接收器，通过联合优化方法平衡通信性能与感知精度。


<details>
  <summary>Details</summary>
Motivation: 为了在6G网络中实现高效的集成感知与通信，需要解决通信性能与感知精度之间的平衡问题。Rydberg原子接收器具有参考辅助接收机制，但需要专门的优化框架来充分发挥其潜力。

Method: 提出RIS辅助的ISAC框架，结合Rydberg原子接收器用户。开发统一信号模型，建立CRB约束的效用最大化问题，采用分数规划、主化最小化和交替方向乘子法的联合优化框架。

Result: 仿真结果表明，所提出的框架在各种系统环境下始终优于传统方法，验证了该框架在释放Rydberg原子接收器潜力方面的重要性。

Conclusion: 该研究为6G网络中的集成感知与通信系统提供了有效的优化框架，通过Rydberg原子接收器和可重构智能表面的协同设计，实现了通信与感知性能的良好平衡。

Abstract: In this paper, we investigate a reconfigurable intelligent surface (RIS)-assisted integrated sensing and communications (ISAC) framework equipped with multiple Rydberg atomic receiver (RAR)-aided users. By leveraging the reference-assisted reception mechanism of RARs, we develop a unified signal model that jointly captures downlink multi-user communication with RARs and monostatic radar sensing. To explicitly balance communication performance and sensing accuracy, we formulate a Cramer-Rao bound (CRB)-constrained utility maximization problem. To address these challenges, we propose a joint optimization framework that combines fractional programming (FP), majorization-minimization (MM), and the alternating direction method of multipliers (ADMM). Simulation results demonstrate that the proposed framework consistently outperforms the conventional approach over a wide range of system environments, thereby highlighting the importance of the proposed framework in unlocking the potential of RARs for 6G.

</details>


### [12] [Radar-Based Identification of Individuals Using Heartbeat Features Extracted from Signal Amplitude and Phase](https://arxiv.org/abs/2601.06824)
*Haruto Kobayashi,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出基于毫米波雷达心跳特征的非接触式身份识别方法，通过分析幅度、相位和复数信号三种谱图的贡献，并融合这些特征来提高识别准确率


<details>
  <summary>Details</summary>
Motivation: 虽然复数值雷达信号谱图常用于心跳身份识别，但对信号分量（幅度、相位或复数信号本身）的选择关注不足，这些分量对识别准确率的贡献尚不明确

Method: 首先分别评估幅度、相位和复数信号谱图的识别性能，然后提出特征融合方法整合这三种表示，使用79GHz雷达系统进行实验

Result: 在6名参与者的实验中实现了97.67%的识别准确率，证明了分量分析和融合方法的有效性

Conclusion: 通过分量分析和特征融合显著提高了基于心跳的身份识别性能，为毫米波雷达生物识别提供了有效方法

Abstract: This study proposes a non-contact method for identifying individuals through the use of heartbeat features measured with millimeter-wave radar. Although complex-valued radar signal spectrograms are commonly used for this task, little attention has been paid to the choice of signal components, namely, whether to use amplitude, phase, or the complex signal itself. Although spectrograms can be constructed independently from amplitude or phase information, their respective contributions to identification accuracy remain unclear. To address this issue, we first evaluate identification performance using spectrograms derived separately from amplitude, phase, and complex signals. We then propose a feature fusion method that integrates these three representations to enhance identification accuracy. Experiments conducted with a 79-GHz radar system and involving six participants achieved an identification accuracy of 97.67%, demonstrating the effectiveness of the proposed component-wise analysis and integration approach.

</details>


### [13] [Movable Beyond-Diagonal Reconfigurable Intelligent Surfaces: Moving, Interconnecting, or Both?](https://arxiv.org/abs/2601.06837)
*Shuyue Xu,Matteo Nerini,Bruno Clerckx*

Main category: eess.SP

TL;DR: 提出了一种可移动超对角可重构智能表面(MA-BD-RIS)设计，结合了单元间连接性和可移动性来增强信道性能，通过联合优化波束成形、BD-RIS配置和单元位置来最大化多用户MISO系统的和速率。


<details>
  <summary>Details</summary>
Motivation: 传统RIS设计中，单元间连接性和单元位置通常是分开考虑的。本文旨在结合这两种特性，通过同时优化连接性和可移动性来获得更好的信道增强效果，特别是在不同场景下（小规模RIS与大规模RIS）实现性能优势。

Method: 开发了一个高效算法，包含：1) 闭式波束成形；2) 用于BD-RIS设计的低复杂度部分近端交替方向乘子法；3) 用于单元位置优化的逐次凸逼近方法。系统模型为MA-BD-RIS辅助的多用户MISO系统。

Result: 仿真表明：1) 高可移动性结构在小规模RIS和丰富散射场景中表现优异；2) 高连接性结构在大规模RIS和大型发射阵列配置中占主导地位；3) 提出的联合优化方法能有效提升系统性能。

Conclusion: MA-BD-RIS设计成功结合了单元间连接性和可移动性，在不同场景下展现出互补优势。提出的联合优化算法有效解决了波束成形、BD-RIS配置和单元位置的三重优化问题，为未来RIS设计提供了新思路。

Abstract: This letter proposes a movable beyond-diagonal reconfigurable intelligent surfaces (MA-BD-RIS) design, combining inter-element connectivity and movability for channel enhancement. We study a MA-BD-RIS assisted multi-user multiple input single output system where beamforming, BD-RIS configuration, and elements positions are jointly optimized to maximize the sum-rate. An efficient algorithm is developed, incorporating closed-form beamforming, a low-complexity partially proximal alternating direction method of multipliers for BD-RIS design, and successive convex approximation for element placement. Simulations show that the high-movability structure yields superior performance in small-scale RIS and rich scattering scenarios, while the high-connectivity structure dominates in large-scale RIS and massive transmit array configurations.

</details>


### [14] [Deep Learning Based Channel Extrapolation for Dual-Band Massive MIMO Systems](https://arxiv.org/abs/2601.06858)
*Qikai Xiao,Kehui Li,Binggui Zhou,Shaodan Ma*

Main category: eess.SP

TL;DR: 提出MDFCE方法，通过融合多域特征将sub-6 GHz CSI外推到mmWave CSI，减少毫米波信道估计的导频开销


<details>
  <summary>Details</summary>
Motivation: 毫米波大规模MIMO系统需要高精度CSI，但直接估计毫米波信道需要大量导频开销，因为CSI维度大且信噪比低。双频段系统中，可以利用sub-6 GHz CSI来外推毫米波CSI以减少开销。

Method: 提出多域融合信道外推器(MDFCE)，结合专家混合框架和多头自注意力机制，融合sub-6 GHz CSI的多域特征，有效表征从sub-6 GHz CSI到mmWave CSI的映射关系。

Result: 仿真结果表明，MDFCE在各种天线阵列规模和信噪比水平下，相比现有方法能以更少的训练导频获得更优性能，同时计算效率更高。

Conclusion: MDFCE为双频段大规模MIMO系统提供了一种高效的毫米波CSI获取方法，显著减少了导频开销，同时保持了高性能和高计算效率。

Abstract: Future wireless communication systems will increasingly rely on the integration of millimeter wave (mmWave) and sub-6 GHz bands to meet heterogeneous demands on high-speed data transmission and extensive coverage. To fully exploit the benefits of mmWave bands in massive multiple-input multiple-output (MIMO) systems, highly accurate channel state information (CSI) is required. However, directly estimating the mmWave channel demands substantial pilot overhead due to the large CSI dimension and low signal-to-noise ratio (SNR) led by severe path loss and blockage attenuation. In this paper, we propose an efficient \textbf{M}ulti-\textbf{D}omain \textbf{F}usion \textbf{C}hannel \textbf{E}xtrapolator (MDFCE) to extrapolate sub-6 GHz band CSI to mmWave band CSI, so as to reduce the pilot overhead for mmWave CSI acquisition in dual band massive MIMO systems. Unlike traditional channel extrapolation methods based on mathematical modeling, the proposed MDFCE combines the mixture-of-experts framework and the multi-head self-attention mechanism to fuse multi-domain features of sub-6 GHz CSI, aiming to characterize the mapping from sub-6 GHz CSI to mmWave CSI effectively and efficiently. The simulation results demonstrate that MDFCE can achieve superior performance with less training pilots compared with existing methods across various antenna array scales and signal-to-noise ratio levels while showing a much higher computational efficiency.

</details>


### [15] [Continuous Energy Landscape Model for Analyzing Brain State Transitions](https://arxiv.org/abs/2601.06991)
*Triet M. Tran,Seyed Majid Razavi,Dee H. Wu,Sina Khanmohammadi*

Main category: eess.SP

TL;DR: 提出基于图神经网络的连续能量景观框架，直接学习fMRI信号的连续精度矩阵，避免传统二值化方法的信息损失，在合成和真实fMRI数据上验证了优越性。


<details>
  <summary>Details</summary>
Motivation: 传统能量景观模型使用二值化脑状态表示（激活/非激活），导致显著信息损失，且随着脑区数量增加，可能状态数指数增长，使能量计算不可行。

Method: 提出连续能量景观框架，使用图神经网络直接从fMRI信号学习连续精度矩阵，在能量景观计算中保留完整的信号值范围。

Result: 在合成数据（SLDS和Kuramoto模型）上，连续能量模型比传统二值模型获得更高似然度，更准确恢复盆地几何、状态占据和转移动态。在真实fMRI数据上，预测工作记忆和执行功能的AUC提高0.27，预测反应时间的解释方差（R2）提高0.35。

Conclusion: 利用完整信号值的连续能量景观模型能更好捕捉神经元动态，对神经疾病的诊断和监测具有重要应用价值。

Abstract: Energy landscape models characterize neural dynamics by assigning energy values to each brain state that reflect their stability or probability of occurrence. The conventional energy landscape models rely on binary brain state representation, where each region is considered either active or inactive based on some signal threshold. However, this binarization leads to significant information loss and an exponential increase in the number of possible brain states, making the calculation of energy values infeasible for large numbers of brain regions. To overcome these limitations, we propose a novel continuous energy landscape framework that employs Graph Neural Networks (GNNs) to learn a continuous precision matrix directly from functional MRI (fMRI) signals, preserving the full range of signal values during energy landscape computation. We validated our approach using both synthetic data and real-world fMRI datasets from brain tumor patients. Our results on synthetic data generated from a switching linear dynamical system (SLDS) and a Kuramoto model show that the continuous energy model achieved higher likelihood and more accurate recovery of basin geometry, state occupancy, and transition dynamics than conventional binary energy landscape models. In addition, results from the fMRI dataset indicate a 0.27 increase in AUC for predicting working memory and executive function, along with a 0.35 improvement in explained variance (R2) for predicting reaction time. These findings highlight the advantages of utilizing the full signal values in energy landscape models for capturing neuronal dynamics, with strong implications for diagnosing and monitoring neurological disorders.

</details>


### [16] [Autofocus Method for Human-Body Imaging under Respiratory Motion Using Synthetic Aperture Radar](https://arxiv.org/abs/2601.07099)
*Masaya Kato,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出一种针对呼吸运动条件下人体合成孔径雷达成像的有效自聚焦方法，通过分离雷达回波并分别估计相位误差，改善图像质量


<details>
  <summary>Details</summary>
Motivation: 在呼吸运动条件下，人体不同部位会产生不同的运动，导致传统合成孔径雷达成像方法产生相位误差，影响图像聚焦质量

Method: 在空间-时间频率域分离雷达回波，为每个分离的回波单独估计相位误差，然后补偿这些误差来生成聚焦良好的合成孔径雷达图像

Result: 与常规方法相比，Muller-Buffington锐度指标提高5.1倍，相对于参考点云的均方根误差从34mm降低到20mm

Conclusion: 该方法能有效抑制呼吸运动引起的相位误差，即使人体多个部位因呼吸产生不同运动，也能生成聚焦良好的合成孔径雷达图像

Abstract: This study presents an effective autofocusing approach for synthetic aperture radar imaging of the human body under conditions of respiratory motion. The proposed method suppresses respiratory-motion-induced phase errors by separating radar echoes in the spatial- and time-frequency domains and estimating phase errors individually for each separated echo. By compensating for the estimated phase errors, synthetic aperture radar images focused on all scattering points are generated, even when multiple body parts exhibit different motions due to respiration. The performance of the proposed method is evaluated through experiments with four participants in the supine position. Compared with a conventional method, the proposed approach improves image quality by a factor of 5.1 in terms of Muller-Buffington sharpness, and reduces the root-mean-square error with respect to a reference point cloud from 34 mm to 20 mm.

</details>


### [17] [Antenna Coding Optimization for Pixel Antenna Empowered MIMO Wireless Power Transfer](https://arxiv.org/abs/2601.07324)
*Yijun Chen,Shanpu Shen,Tianrui Qiao,Hongyu Li,Kai-Kit Wong,Ross Murch*

Main category: eess.SP

TL;DR: 本文研究利用像素天线作为天线编码的新自由度来增强MIMO无线功率传输系统，通过联合优化天线编码、波束成形和整流器非线性，显著提升输出直流功率。


<details>
  <summary>Details</summary>
Motivation: 传统MIMO WPT系统天线配置固定，限制了性能提升。像素天线提供天线编码自由度，可联合优化波束成形和整流器非线性，有望显著提升输出直流功率。

Method: 提出基于波束空间信道模型的MIMO WPT系统模型，支持二进制和连续天线编码；设计联合天线编码和波束成形优化框架；提出两种高效的闭式逐次凸逼近算法优化波束成形；为降低复杂度，提出基于K-means聚类的码本天线编码设计。

Result: 二进制天线编码相比传统固定天线配置提升输出直流功率超过15dB；连续天线编码可再提升6dB；提出的码本设计比先前设计性能提升达40%，且计算复杂度降低。

Conclusion: 像素天线赋能的天线编码技术可显著提升MIMO WPT系统性能，验证了利用天线编码增强WPT系统的潜力，为高效无线功率传输提供了新思路。

Abstract: We investigate antenna coding utilizing pixel antennas as a new degree of freedom for enhancing multiple-input multiple-output (MIMO) wireless power transfer (WPT) systems. The objective is to enhance the output direct current (DC) power under RF combining and DC combining schemes by jointly exploiting gains from antenna coding, beamforming, and rectenna nonlinearity. We first propose the MIMO WPT system model with binary and continuous antenna coding using the beamspace channel model and formulate the joint antenna coding and beamforming optimization using a nonlinear rectenna model. We propose two efficient closed-form successive convex approximation algorithms to efficiently optimize the beamforming. To further reduce the computational complexity, we propose codebook-based antenna coding designs for output DC power maximization based on K-means clustering. Results show that the proposed pixel antenna empowered MIMO WPT system with binary antenna coding increases output DC power by more than 15 dB compared with conventional systems with fixed antenna configuration. With continuous antenna coding, the performance improves another 6 dB. Moreover, the proposed codebook design outperforms previous designs by up to 40% and shows good performance with reduced computational complexity. Overall, the significant improvement in output DC power verifies the potential of leveraging antenna coding utilizing pixel antennas to enhance WPT systems.

</details>


### [18] [PIDT: Physics-Informed Digital Twin for Optical Fiber Parameter Estimation](https://arxiv.org/abs/2601.07436)
*Zicong Jiang,Magnus Karlsson,Erik Agrell,Christian Häger*

Main category: eess.SP

TL;DR: 提出物理信息数字孪生(PIDT)：一种结合参数化分步方法与物理信息损失的光纤参数估计方法，相比先前神经算子具有更高精度、更快收敛速度和更低复杂度


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在光纤参数估计中存在精度不足、收敛慢、复杂度高等问题，需要一种更高效准确的参数估计方法

Method: 结合参数化分步方法与物理信息损失函数，构建物理信息数字孪生框架，利用物理约束提升参数估计性能

Result: PIDT相比先前神经算子方法，在光纤参数估计中实现了更高的精度、更快的收敛速度，同时降低了计算复杂度

Conclusion: 物理信息数字孪生为光纤参数估计提供了一种高效准确的新方法，通过结合物理约束与参数化模型显著提升了性能

Abstract: We propose physics-informed digital twin (PIDT): a fiber parameter estimation approach that combines a parameterized split-step method with a physics-informed loss. PIDT improves accuracy and convergence speed with lower complexity compared to previous neural operators.

</details>


### [19] [Vector Quantized-Aided XL-MIMO CSI Feedback with Channel Adaptive Transmission](https://arxiv.org/abs/2601.07584)
*Yuhang Ma,Nan Ma,Jianqiao Chen,Wenkai Liu*

Main category: eess.SP

TL;DR: 提出VQ-DJSCC-F方案，结合向量量化与深度联合信源信道编码，用于6G XL-MIMO系统的CSI反馈，在考虑近场效应下降低反馈开销并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 6G XL-MIMO系统天线规模巨大导致CSI反馈开销严重，现有量化方法面临量化精度有限和信道鲁棒性不足的双重挑战，特别是在压缩高维信道特征时。

Method: 1) 利用近场信道在极延迟域的稀疏性提取能量集中特征降维；2) 同时设计Transformer和CNN架构分层提取CSI特征；3) 使用VQ模块将特征投影到离散潜在空间；4) 引入熵损失正则化和EMA更新策略最大化量化精度；5) 开发注意力机制驱动的信道自适应模块减轻无线信道衰落对索引序列传输的影响。

Result: 仿真结果表明，所提方案在不同信道条件下以更低的反馈开销实现了优越的CSI重构精度。

Conclusion: VQ-DJSCC-F方案有效解决了XL-MIMO系统CSI反馈的高开销和鲁棒性问题，通过结合向量量化、深度联合信源信道编码和近场信道特性，为6G大规模MIMO系统提供了高效的CSI反馈解决方案。

Abstract: Efficient channel state information (CSI) feedback is critical for 6G extremely large-scale multiple-input multiple-output (XL-MIMO) systems to mitigate channel interference. However, the massive antenna scale imposes a severe burden on feedback overhead. Meanwhile, existing quantized feedback methods face dual challenges of limited quantization precision and insufficient channel robustness when compressing high-dimensional channel features into discrete symbols. To reduce these gaps, guided by the deep joint source-channel coding (DJSCC) framework, we propose a vector quantized (VQ)-aided scheme for CSI feedback in XL-MIMO systems considering the near-field effect, named VQ-DJSCC-F. Firstly, taking advantage of the sparsity of near-field channels in the polar-delay domain, we extract energy-concentrated features to reduce dimensionality. Then, we simultaneously design the Transformer and CNN (convolutional neural network) architectures as the backbones to hierarchically extract CSI features, followed by VQ modules projecting features into a discrete latent space. The entropy loss regularization in synergy with an exponential moving average (EMA) update strategy is introduced to maximize quantization precision. Furthermore, we develop an attention mechanism-driven channel adaptation module to mitigate the impact of wireless channel fading on the transmission of index sequences. Simulation results demonstrate that the proposed scheme achieves superior CSI reconstruction accuracy with lower feedback overheads under varying channel conditions.

</details>


### [20] [Learning to Unfold Fractional Programming for Multi-Cell MU-MIMO Beamforming with Graph Neural Networks](https://arxiv.org/abs/2601.07630)
*Zihan Jiao,Xinping Yi,Shi Jin*

Main category: eess.SP

TL;DR: 论文提出改进多小区MU-MIMO系统中波束赋形优化的分数规划方法，通过避免大维度矩阵求逆降低计算复杂度，并利用深度学习展开算法加速收敛


<details>
  <summary>Details</summary>
Motivation: 在多小区多用户MIMO系统中，分数规划方法虽然能有效优化波束赋形向量，但存在计算复杂度高的问题，需要更高效的算法

Method: 提出两种改进方法：1) FastFP - 避免大维度矩阵求逆降低复杂度；2) DeepFP - 通过深度学习展开FastFP算法加速收敛

Result: FastFP显著降低了计算复杂度，DeepFP进一步提高了收敛速度，两者结合实现了高效的多小区MU-MIMO波束赋形优化

Conclusion: 通过避免矩阵求逆和深度学习展开，成功解决了分数规划在多小区MU-MIMO系统中的高复杂度问题，为实际系统部署提供了可行方案

Abstract: In the multi-cell multiuser multi-input multi-output (MU-MIMO) systems, fractional programming (FP) has demonstrated considerable effectiveness in optimizing beamforming vectors, yet it suffers from high computational complexity. Recent improvements demonstrate reduced complexity by avoiding large-dimension matrix inversions (i.e., FastFP) and faster convergence by learning to unfold the FastFP algorithm (i.e., DeepFP).

</details>


### [21] [Lagrangian Grid-based Estimation of Nonlinear Systems with Invertible Dynamics](https://arxiv.org/abs/2601.07721)
*Jindřich Duník,Jan Krejčí,Jakub Matoušek,Marek Brandner,Yeongkwon Choe*

Main category: eess.SP

TL;DR: 本文提出了一种用于非线性可逆系统的拉格朗日网格滤波器，将计算复杂度从二次降低到对数线性，同时保持了原始网格滤波器的鲁棒性、准确性和确定性行为。


<details>
  <summary>Details</summary>
Motivation: 针对非线性非高斯系统的状态估计问题，需要解决贝叶斯递归关系的数值解。现有的线性系统拉格朗日网格滤波器需要扩展到非线性系统，同时降低计算复杂度。

Method: 基于最近为线性系统开发的拉格朗日网格滤波器，将其扩展到具有可逆非线性动力学的系统。通过拉格朗日方法将计算复杂度从二次降低到对数线性。

Result: 提出的非线性拉格朗日网格滤波器在保持原始网格滤波器所有优点的同时，显著降低了计算复杂度。通过多个数值研究，与粒子滤波器进行了比较验证。

Conclusion: 该方法为非线性可逆系统提供了一种高效、鲁棒且准确的状态估计解决方案，计算复杂度显著降低，具有实际应用价值。

Abstract: This paper deals with the state estimation of non-linear and non-Gaussian systems with an emphasis on the numerical solution to the Bayesian recursive relations. In particular, this paper builds upon the Lagrangian grid-based filter (GbF) recently-developed for linear systems and extends it for systems with nonlinear dynamics that are invertible. The proposed nonlinear Lagrangian GbF reduces the computational complexity of the standard GbFs from quadratic to log-linear, while preserving all the strengths of the original GbF such as robustness, accuracy, and deterministic behaviour. The proposed filter is compared with the particle filter in several numerical studies using the publicly available MATLAB\textregistered\ implementation\footnote{https://github.com/pesslovany/Matlab-LagrangianPMF}.

</details>


### [22] [Tensor Decompositions for Online Grid-Based Terrain-Aided Navigation](https://arxiv.org/abs/2601.07728)
*J. Matoušek,J. Krejčí,J. Duník,R. Zanetti*

Main category: eess.SP

TL;DR: 提出一种适用于高维可逆线性动态和非线性测量模型的实用网格状态估计方法，通过利用可分解模型结构（块对角动态和稀疏耦合测量维度）实现实时估计


<details>
  <summary>Details</summary>
Motivation: 传统基于张量分解的方法大多停留在概念验证阶段，无法处理高维模型。需要开发一种实用、可扩展的网格状态估计方法，能够处理具有可逆线性动态和高度非线性测量的高维模型

Method: 利用可分解模型结构（块对角动态和稀疏耦合测量维度），结合拉格朗日公式进行时间更新，并利用低秩张量分解紧凑表示和有效传播状态密度

Result: 实现了对高维模型的实时状态估计，显著扩展了网格滤波器的实用范围，超越了传统的低维应用。在基于地形辅助导航的实验中验证了方法的有效性

Conclusion: 该方法为具有可分解结构的高维模型提供了实用解决方案，虽然计算复杂度和估计精度取决于具体模型结构，但已成功将网格滤波扩展到高维实时应用领域

Abstract: This paper presents a practical and scalable grid-based state estimation method for high-dimensional models with invertible linear dynamics and with highly non-linear measurements, such as the nearly constant velocity model with measurements of e.g. altitude, bearing, and/or range. Unlike previous tensor decomposition-based approaches, which have largely remained at the proof-of-concept stage, the proposed method delivers an efficient and practical solution by exploiting decomposable model structure-specifically, block-diagonal dynamics and sparsely coupled measurement dimensions. The algorithm integrates a Lagrangian formulation for the time update and leverages low-rank tensor decompositions to compactly represent and effectively propagate state densities. This enables real-time estimation for models with large state dimension, significantly extending the practical reach of grid-based filters beyond their traditional low-dimensional use. Although demonstrated in the context of terrain-aided navigation, the method is applicable to a wide range of models with decomposable structure. The computational complexity and estimation accuracy depend on the specific structure of the model. All experiments are fully reproducible, with source code provided alongside this paper (GitHub link: https://github.com/pesslovany/Matlab-LagrangianPMF).

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Context Video Semantic Transmission with Variable Length and Rate Coding over MIMO Channels](https://arxiv.org/abs/2601.06059)
*Bingyan Xie,Yongpeng Wu,Wenjun Zhang,Derrick Wing Kwan Ng,Merouane Debbah*

Main category: cs.IT

TL;DR: 提出CVST框架用于MIMO信道下的视频语义传输，通过上下文-信道关联图、多参考熵编码和棋盘特征调制实现高效传输


<details>
  <summary>Details</summary>
Motivation: 现有语义通信方案主要针对简单信道（AWGN或Rayleigh衰落）优化，忽略了实际部署中普遍存在的MIMO环境，这严重阻碍了实际应用部署

Method: 提出CVST框架：1) 学习上下文-信道关联图来明确特征组与MIMO子信道的关系；2) 设计多参考熵编码机制实现信道状态感知的变长编码；3) 采用棋盘式特征调制策略在单一训练模型中实现多速率点

Result: CVST在性能上显著优于各种标准分离编码方法和最近的无线视频语义通信方法

Conclusion: CVST框架成功解决了MIMO环境下的视频语义传输问题，通过创新的MR-VLRC方案实现了高效灵活的视频传输，为实际部署提供了可行方案

Abstract: The evolution of semantic communications has profoundly impacted wireless video transmission, whose applications dominate driver of modern bandwidth consumption. However, most existing schemes are predominantly optimized for simple additive white Gaussian noise or Rayleigh fading channels, neglecting the ubiquitous multiple-input multiple-output (MIMO) environments that critically hinder practical deployment. To bridge this gap, we propose the context video semantic transmission (CVST) framework under MIMO channels. Building upon an efficient contextual video transmission backbone, CVST effectively learns a context-channel correlation map to explicitly formulate the relationships between feature groups and MIMO subchannels. Leveraging these channel-aware features, we design a multi-reference entropy coding mechanism, enabling channel state-aware variable length coding. Furthermore, CVST incorporates a checkerboard-based feature modulation strategy to achieve multiple rate points within a single trained model, thereby enhancing deployment flexibility. These innovations constitute our multi-reference variable length and rate coding (MR-VLRC) scheme. By integrating contextual transmission with MR-VLRC, CVST demonstrates substantial performance gains over various standardized separated coding methods and recent wireless video semantic communication approaches. The code is available at https://github.com/xie233333/CVST.

</details>


### [24] [Jamming Detection in Cell-Free MIMO with Dynamic Graphs](https://arxiv.org/abs/2601.06075)
*Ali Hossary,Laura Crosara,Stefano Tomasin*

Main category: cs.IT

TL;DR: 提出基于动态图和图卷积神经网络的干扰检测框架，用于应对无蜂窝大规模MIMO系统中的干扰攻击威胁


<details>
  <summary>Details</summary>
Motivation: 干扰攻击对无线网络构成严重威胁，特别是在无蜂窝大规模MIMO系统中，分布式接入点和用户设备的复杂时变拓扑增加了检测难度

Method: 将网络建模为动态图以捕捉通信链路的演化，采用GCN-Transformer模型学习图嵌入，通过监督学习识别恶意干扰

Result: 在移动用户、不同干扰条件和信道衰落的模拟场景中，通过准确率和F1分数评估，该方法表现出有效的干扰检测性能

Conclusion: 基于动态图和GCN的框架为无蜂窝大规模MIMO系统中的干扰检测提供了有前景的解决方案

Abstract: Jamming attacks pose a critical threat to wireless networks, particularly in cell-free massive MIMO systems, where distributed access points and user equipment (UE) create complex, time-varying topologies. This paper proposes a novel jamming detection framework leveraging dynamic graphs and graph convolutional neural networks (GCN) to address this challenge. By modeling the network as a dynamic graph, we capture evolving communication links and detect jamming attacks as anomalies in the graph evolution. A GCN-Transformer-based model, trained with supervised learning, learns graph embeddings to identify malicious interference. Performance evaluation in simulated scenarios with moving UEs, varying jamming conditions and channel fadings, demonstrates the method's effectiveness, which is assessed through accuracy and F1 score metrics, achieving promising results for effective jamming detection.

</details>


### [25] [One if by Land, Two if by Sea, Three if by Four Seas, and More to Come -- Values of Perception, Prediction, Communication, and Common Sense in Decision Making](https://arxiv.org/abs/2601.06077)
*Aolin Xu*

Main category: cs.IT

TL;DR: 该论文从决策理论角度严格定义了感知、预测、通信和常识在决策中的价值，这些量具有信息论类比，能回答自主决策系统设计中的实际问题。


<details>
  <summary>Details</summary>
Motivation: 论文旨在为决策中的感知、预测、通信和常识等概念提供严格的数学定义，这些定义具有决策理论性质，同时与信息论概念（如香农熵和互信息）有相似性。动机包括为自主决策系统设计提供理论指导，并帮助理解自然决策者如何利用不同来源的信息。

Method: 采用决策理论框架，定义感知、预测、通信和常识的量化价值。这些定义具有信息论类比，共享香农熵和互信息的关键数学性质，在特定设置下可简化为这些信息论量。通过理论分析探索这些量之间的关系和性质。

Result: 发现感知价值在没有预测的情况下可能为负值，而感知与预测结合的价值以及单独预测的价值总是非负的。定义的量能够回答自主决策系统设计中的实际问题，如是否需要观察和预测特定代理行为、其重要性如何、以及观察和预测的最佳顺序等。

Conclusion: 该研究为决策中的信息处理操作提供了严格的量化框架，不仅对自主决策系统设计有实用价值，还能为认知科学和神经科学提供见解，帮助理解自然决策者如何整合不同来源的信息进行决策。

Abstract: This work aims to rigorously define the values of perception, prediction, communication, and common sense in decision making. The defined quantities are decision-theoretic, but have information-theoretic analogues, e.g., they share some simple but key mathematical properties with Shannon entropy and mutual information, and can reduce to these quantities in particular settings. One interesting observation is that, the value of perception without prediction can be negative, while the value of perception together with prediction and the value of prediction alone are always nonnegative. The defined quantities suggest answers to practical questions arising in the design of autonomous decision-making systems. Example questions include: Do we need to observe and predict the behavior of a particular agent? How important is it? What is the best order to observe and predict the agents? The defined quantities may also provide insights to cognitive science and neural science, toward the understanding of how natural decision makers make use of information gained from different sources and operations.

</details>


### [26] [Deep Q-Network Based Resilient Drone Communication:Neutralizing First-Order Markov Jammers](https://arxiv.org/abs/2601.06095)
*Andrii Grekhov,Volodymyr Kharchenko,Vasyl Kondratiuk*

Main category: cs.IT

TL;DR: 基于深度强化学习的抗干扰通信方案，使用跳频扩频技术，在16信道环境中对抗一阶反应式干扰，通过自学习实现均匀随机跳频策略，有效抵消干扰的预测优势。


<details>
  <summary>Details</summary>
Motivation: 现代电子战场景中，通信系统面临智能干扰威胁，特别是能够预测传输模式的一阶反应式干扰。需要开发自主、弹性的通信策略来对抗这类智能干扰。

Method: 采用深度Q网络（DQN）作为发射机代理，在16信道跳频环境中进行连续信道选择决策。面对一阶反应式干扰（利用观测到的转移统计信息预测和中断传输），通过自训练学习最优跳频策略。同时评估了瑞利衰落和加性噪声环境下，BCH前向纠错码的影响。

Result: 代理成功学习到均匀随机跳频策略，有效中和了干扰的预测优势。即使中等冗余度的BCH纠错码也能显著减少丢包率。学习动态、信道利用率分布、ε-贪婪衰减、累积奖励、BER和SNR演化等可视化结果证实了收敛到接近最优的抗干扰策略。

Conclusion: 该研究为现代电子战场景中的自主弹性通信提供了实用框架，证明了深度强化学习在对抗智能干扰方面的有效性，结合前向纠错码可以显著提高通信系统的抗干扰能力。

Abstract: Deep Reinforcement Learning based solution for jamming communications using Frequency Hopping Spread Spectrum technology in a 16 channel radio environment is presented. Deep Q Network based transmitter continuously selects the next frequency hopping channel while facing first order reactive jamming, which uses observed transition statistics to predict and interrupt transmissions. Through self training, the proposed agent learns a uniform random frequency hopping policy that effectively neutralizes the predictive advantage of the jamming. In the presence of Rayleigh fading and additive noise, the impact of forward error correction Bose Chaudhuri Hocquenghem type codes is systematically evaluated, demonstrating that even moderate redundancy significantly reduces packet loss. Extensive visualization of the learning dynamics, channel utilization distribution, epsilon greedy decay, cumulative reward, BER and SNR evolution, and detailed packet loss tables confirms convergence to a near optimal jamming strategy. The results provide a practical framework for autonomous resilient communications in modern electronic warfare scenarios.

</details>


### [27] [Optimal Beamforming for Uplink Covert Communication in MIMO GEO Satellite-Terrestrial Systems](https://arxiv.org/abs/2601.06110)
*Zewei Guo,Ranran Sun,Yulong Shen,Xiaohong Jiang*

Main category: cs.IT

TL;DR: 该论文研究了MIMO卫星-地面系统中上行链路隐蔽通信，通过波束成形和天线方向优化设计来最大化隐蔽速率，考虑了完美和不完美信道估计两种场景。


<details>
  <summary>Details</summary>
Motivation: 在MIMO卫星-地面系统中实现上行链路隐蔽通信面临挑战，需要防止多个卫星监视器检测到通信活动，同时保证通信质量和隐蔽性。

Method: 提出基于波束成形和天线方向设置的隐蔽传输方案，建立检测错误概率、传输中断概率和隐蔽速率的理论模型，开发最优波束成形设计以及联合波束成形和天线方向优化设计。

Result: 通过半定松弛、交替优化、罗德里格斯旋转公式和一维搜索算法开发了高效优化算法，数值结果验证了理论分析并展示了波束成形和天线方向设计对提升MIMO GEO卫星-地面系统上行隐蔽通信效率的有效性。

Conclusion: 波束成形和天线方向联合优化设计能有效支持MIMO GEO卫星-地面系统的上行隐蔽通信，在完美和不完美信道估计场景下都能显著提升隐蔽通信性能。

Abstract: This paper investigates the uplink covert communication in a multiple-input multiple-output (MIMO) satellite-terrestrial system consisting of an Earth station transmitter Alice, a geosynchronous Earth orbit (GEO) satellite receiver Bob, and multiple GEO satellite wardens around Bob, where each node in the system is equipped with an array of directional antennas. Based on beamforming and the default antenna orientation setting, we first propose a scheme for covert Alice-Bob uplink transmission. Under the perfect channel estimation scenario, we provide theoretical modeling for the system performance in terms of detection error probability (DEP), transmission outage probability (TOP) and covert rate (CR), and then explore the optimal beamforming (OB) design as well as the joint optimal beamforming and antenna orientation (JO-BA) design for CR maximization. We then extend our study to the imperfect channel estimation scenario, and conduct related performance modeling and OB/JO-BA designs for CR maximization. We also apply the techniques of semidefinite relaxation, alternating optimization, Rodrigues' rotation formula and 1-D search algorithm to develop efficient algorithms to solve the above optimization problems. Finally, extensive numerical results are presented to verify our theoretical results and to illustrate the efficiency of beamforming and antenna orientation design for supporting the uplink covert communication in MIMO GEO satellite-terrestrial systems.

</details>


### [28] [Range-Coder with fast Adaptation and Table-Based Decoding](https://arxiv.org/abs/2601.06120)
*Tilo Strutz,Roman Rischke*

Main category: cs.IT

TL;DR: 提出一种基于环形缓冲区的自适应表解码方法，通过用位移操作替代除法来加速算术编码，在静态模式下编码时间减少约40%，在自适应模式下对12-64符号字母表比现有方法更快。


<details>
  <summary>Details</summary>
Motivation: 传统基于区间的熵编码方法（如算术编码、范围编码、ANS）在解码时需要搜索过程来确定符号，虽然可以用O(1)复杂度的表方法替代搜索，但符号统计的自适应更新会因表更新耗时过长而不可行。

Method: 使用环形缓冲区技术实现自适应表解码过程，同时在编码器和解码器核心例程中用位移操作替代除法运算，显著加速编码过程。

Result: 在静态（非自适应）模式下，编码时间可减少约40%；在自适应模式下，对于12-64个不同符号的字母表，整体编码+解码时间比替代方法更快。

Conclusion: 提出的环形缓冲区自适应表解码方法有效解决了传统表方法无法适应符号统计变化的问题，同时通过位移操作替代除法显著提升了编码效率，在静态和自适应模式下都实现了性能提升。

Abstract: The transmission or storage of signals typically involves data compression. The final processing step in compression systems is generally an entropy coding stage, which converts symbols into a bit stream based on their probability distribution. A distinct class of entropy coding methods operates not by mapping input symbols to discrete codewords but by operating on intervals or ranges. This approach enables a more accurate approximation of the source entropy, particularly for sources with highly skewed or varying symbol distributions. Representative techniques in this category include traditional arithmetic coding, range coding, and methods based on asymmetric numeral systems (ANS). The complexity of these methods depends mainly on three processing steps: the core routines of encoding and decoding doing the calculations, the interval-based determination of the correct symbol at decoder, and the efforts of keeping updated with respect to the varying symbol distribution.
  The interval-based symbol determination at decoder typically demands for a searching procedure. In previous literature, it could be shown that the search can be replaced by a table-based approach with only O(1)-complexity but having the side-effect that the adaptation of the symbols statistic becomes infeasible because of the high time-consumption of adapting the table.
  We propose an adaptation process using a ring-buffer technique enabling the adaptive table-based decoding procedure as well as the replacement of a division by a bit-shift operation at encoder and decoder core routines. This accelerates the coding process significantly. In static (non-adaptive) mode, the coding time can be reduced by about 40 percent. In adaptive mode, the proposed technique is faster than alternative approaches for alphabets from about 12 to 64 different symbol when comparing the overall encoder+decoder time.

</details>


### [29] [Extended Target Adaptive Beamforming for ISAC:A Perspective of Predictive Error Ellipse](https://arxiv.org/abs/2601.06125)
*Shengcai Zhou,Luping Xiang,Yi Wang,Kun Yang,Kai Kit Wong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文推导了OFDM波形和UPA配置下雷达参数估计的CRB，并针对V2X网络中车辆作为扩展目标的情况，提出了两种NR-V2X兼容的波束成形方案，分别用于波束建立和调整阶段，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 在V2X网络中，利用通信信号提取运动参数已成为重要方向。准确建模通信信号与感知性能之间的关系对系统发展至关重要。现有研究主要依赖定性分析，缺乏定量理论支撑，且车辆作为扩展目标的特性未被充分考虑。

Method: 1. 推导OFDM波形和UPA配置下雷达参数估计的Cramér-Rao界(CRB)；2. 提出两种NR-V2X兼容的波束成形方案：a) 波束建立阶段：基于预测误差椭圆并集的波束成形方法，通过时间辅助的波束训练增强散射体定位；b) 波束调整阶段：自适应最窄波束策略，利用散射体和通信接收机位置进行有效跟踪；3. 使用最小包围椭圆算法和定制天线控制方法解决波束设计问题。

Result: 仿真结果表明，所提方法相比传统波束扫描，在相同SNR条件下，32*32发射天线阵列可实现32.4%的速率提升，8*8阵列可实现5.2%的性能增益。

Conclusion: 本文为V2X网络中通信感知一体化提供了理论框架和实用方案，通过定量CRB分析和针对扩展目标的波束成形设计，显著提升了系统性能，为未来智能交通系统发展提供了重要技术支撑。

Abstract: Utilizing communication signals to extract motion parameters has emerged as a key direction in Vehicle-to- Everything (V2X) networks. Accurately modeling the relationship between communication signals and sensing performance is critical for the advancement of such systems. Unlike prior work that relies primarily on qualitative analysis, this paper derives the Cramér-Rao Bound (CRB) for radar parameter estimation in the context of Orthogonal Frequency Division Multiplexing (OFDM) waveforms and Uniform Planar Array (UPA) configurations. Recognizing that vehicles may act as extended targets, we propose two New Radio (NR)-V2X-compatible beamforming schemes tailored to different phases of the communication process. During the initial beam establishment phase, we develop a beamforming approach based on the union of predictive error ellipses, which enhances scatterer localization through temporally assisted beam training. In the beam adjustment phase, we introduce an adaptive narrowest-beam strategy that leverages the positions of scatterers and the communication receiver (CR), enabling effective tracking with reduced complexity. The beam design problem is addressed using the minimum enclosing ellipse algorithm and tailored antenna control methods. Simulation results validate the proposed approach, showing up to a 32.4% improvement in achievable rate with a 32*32 transmit antenna array and a 5.2% gain with an 8*8 array, compared to conventional beam sweeping under identical SNR conditions.

</details>


### [30] [Channel Knowledge Map Construction via Guided Flow Matching](https://arxiv.org/abs/2601.06156)
*Ziyu Huang,Yong Zeng,Shen Fu,Xiaoli Xu,Hongyang Du*

Main category: cs.IT

TL;DR: 提出基于线性传输引导流匹配（LT-GFM）的CKM构建框架，相比DDPM方法在保持高保真度的同时将推理速度提升25倍


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型（如DDPM）的CKM构建方法依赖迭代随机采样，推理速度过慢，无法满足实时无线应用需求，需要在保持高保真度的同时提升效率

Method: 提出LT-GFM框架，将CKM生成建模为遵循线性最优传输路径的确定性ODE，而非传统的噪声去除范式；采用统一架构支持CGM和SCM构建，并集成环境语义（如建筑掩码）进行边缘恢复，对SCM强制实施厄米对称性

Result: LT-GFM在Fréchet Inception Distance（FID）指标上表现更优，分布保真度更高，推理速度比DDPM快25倍

Conclusion: LT-GFM成功解决了CKM构建中高保真度与高效率之间的平衡问题，为实时无线应用提供了可行的解决方案

Abstract: The efficient construction of accurate channel knowledge maps (CKMs) is crucial for unleashing the full potential of environment-aware wireless networks, yet it remains a difficult ill-posed problem due to the sparsity of available location-specific channel knowledge data. Although diffusion-based methods such as denoising diffusion probabilistic models (DDPMs) have been exploited for CKM construction, they rely on iterative stochastic sampling, rendering them too slow for real-time wireless applications. To bridge the gap between high fidelity and efficient CKM construction, this letter introduces a novel framework based on linear transport guided flow matching (LT-GFM). Deviating from the noise-removal paradigm of diffusion models, our approach models the CKM generation process as a deterministic ordinary differential equation (ODE) that follows linear optimal transport paths, thereby drastically reducing the number of required inference steps. We propose a unified architecture that is applicable to not only the conventional channel gain map (CGM) construction, but also the more challenging spatial correlation map (SCM) construction. To achieve physics-informed CKM constructions, we integrate environmental semantics (e.g., building masks) for edge recovery and enforce Hermitian symmetry for property of the SCM. Simulation results verify that LT-GFM achieves superior distributional fidelity with significantly lower Fréchet Inception Distance (FID) and accelerates inference speed by a factor of 25 compared to DDPMs.

</details>


### [31] [Visible Light Communication using Led-Based AR Markers for Robot Localization](https://arxiv.org/abs/2601.06527)
*Wataru Uemura,Shogo Kawasaki*

Main category: cs.IT

TL;DR: 提出一种将ArUco标记实现为照明形式的方法，通过LED阵列按标记网格排列，利用不同闪烁频率区分黑白单元格，使人眼看到均匀照明而相机能识别标记信息。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人在日常环境中的广泛应用，需要设计既自然又不显眼的视觉标记，特别是在人机协作环境中（如工厂单元制造系统或家庭伴侣机器人）。

Method: 将LED按ArUco标记的网格模式排列，根据对应单元格的黑白状态设置每个LED的闪烁频率。人眼看到均匀亮光，而相机能捕捉闪烁频率差异，从而重建黑白图案并识别标记信息。

Result: 开发了原型系统，并通过实验评估了在不同距离和视角下对ArUco标记的识别准确率。

Conclusion: 该方法成功实现了照明形式的ArUco标记，既能满足机器人定位需求，又能在人机协作环境中保持自然不显眼的外观。

Abstract: A method of information transmission using visual markers has been widely studied. In this approach, information or identifiers (IDs) are encoded in the black-and-white pattern of each marker. By analyzing the geometric properties of the marker frame - such as its size, distortion, and coordinates - the relative position and orientation between the camera and the marker can be estimated. Furthermore, by associating the positional information of each marker with its corresponding ID, the position of the camera that takes the image picture can be calculated. In the field of mobile robotics, such markers are commonly utilized for robot localization. As mobile robots become more widely used in everyday environments, such visual markers are expected to be utilized across various contexts. In environments where robots collaborate with humans - such as in cell-based manufacturing systems in factories or in domestic settings with partner robots - it is desirable for such markers to be designed in a manner that appears natural and unobtrusive to humans. In this paper, we propose a method for implementing an ArUco marker in the form of illumination. In the proposed method, LEDs are arranged in accordance with the grid pattern of the marker, and the blinking frequency of each LED is determined based on the corresponding black or white cell. As a result, the illumination appears uniformly bright to the human eye, while the camera can capture variations in the blinking frequency. From these differences, the black-and-white pattern can be reconstructed, enabling the identification of the marker's tag information. We develop a prototype system, and conduct experiments which are conducted to evaluate its performance in terms of recognition accuracy under varying distances and viewing angles with respect to the ArUco marker.

</details>


### [32] [Large Multimodal Model-Aided Scheduling for 6G Autonomous Communications](https://arxiv.org/abs/2601.06211)
*Sunwoo Kim,Byonghyo Shim*

Main category: cs.IT

TL;DR: 提出基于大型多模态模型（LMM）的6G网络调度技术，通过分析视觉感知信息和导频信号预测未来信道参数，实现预判性调度决策，相比传统方法提升30%以上吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着AI功能在自主设备中的指数级增长，中央处理单元需要处理大型多模态模型来控制这些设备。在6G实际环境中，即使用户微小移动也会导致信道突变，传统调度技术难以应对这种挑战。

Method: 提出基于LMM的调度技术：1）利用LMM分析视觉感知信息预测可靠路径的存在和用户几何信息；2）结合导频信号的过去信道状态；3）准确预测未来信道参数（距离、角度、路径增益等）；4）基于预测结果进行预判性的信道感知调度决策。

Result: 数值评估显示，所提出的技术相比传统调度技术实现了超过30%的吞吐量增益。

Conclusion: LMM能够有效预测未来信道参数，通过结合视觉感知信息和导频信号，可以实现更准确的信道感知调度，显著提升6G网络性能。

Abstract: Recently, large language models (LLMs) have gained significant attention for their ability to generate fast and accurate answer to the given query. These models have evolved into large multimodal models (LMMs), which can interpret and analyze multimodal inputs such as images and text. With the exponential growth of AI functionalities in autonomous devices, the central unit (CU), a digital processing unit performing AI inference, needs to handle LMMs to effectively control these devices. To ensure seamless command delivery to devices, the CU must perform the scheduling, which involves resource block (RB) allocation for data transmission and modulation and coding scheme (MCS) index selection based on the channel conditions. This task is challenging in many practical environments in 6G, where even small user movement can cause abrupt channel changes. In this paper, we propose a novel LMM-based scheduling technique to address this challenge. Our key idea is to leverage LMM to predict future channel parameters (e.g., distance, angles, and path gain) by analyzing the visual sensing information as well as pilot signals. By exploiting LMMs to predict the presence of reliable path and geometric information of users from the visual sensing information, and then combining these with past channel states from pilot signals, we can accurately predict future channel parameters. Using these predictions, we can preemptively make channel-aware scheduling decisions. From the numerical evaluations, we show that the proposed technique achieves more than 30% throughput gain over the conventional scheduling techniques.

</details>


### [33] [Robust and Secure Blockage-Aware Pinching Antenna-assisted Wireless Communication](https://arxiv.org/abs/2601.06430)
*Ruotong Zhao,Shaokang Hu,Deepak Mishra,Derrick Wing Kwan Ng*

Main category: cs.IT

TL;DR: 提出一种抗阻塞的夹持天线系统，用于存在窃听者时的安全无线通信，通过联合优化波束成形、人工噪声、天线位置等参数，在信道状态信息不完美条件下最大化系统总速率。


<details>
  <summary>Details</summary>
Motivation: 传统线性CSI误差边界对空间分布式天线架构过于保守，需要开发几何感知的不确定性集合来联合表征窃听者位置和阵列方向误差，以应对阻塞效应并确保安全通信。

Method: 开发几何感知不确定性集合，构建鲁棒联合优化问题，采用块坐标下降、惩罚方法、MM算法、S-过程和Lipschitz代理函数等低复杂度迭代算法求解。

Result: 所提算法比传统固定天线系统性能提升4.7dB，自适应天线定位能保持与合法用户的视距连接，同时利用波导几何破坏窃听者信道。

Conclusion: 阻塞效应对夹持天线系统设计有显著影响，忽略阻塞会导致性能下降和安全保障不足；所提方法能有效提升系统速率和安全性能。

Abstract: In this work, we investigate a blockage-aware pinching antenna (PA) system designed for secure and robust wireless communication. The considered system comprises a base station equipped with multiple waveguides, each hosting multiple PAs, and serves multiple single-antenna legitimate users in the presence of multi-antenna eavesdroppers under imperfect channel state information (CSI). To safeguard confidential transmissions, artificial noise (AN) is deliberately injected to degrade the eavesdropping channels. Recognizing that conventional linear CSI-error bounds become overly conservative for spatially distributed PA architectures, we develop new geometry-aware uncertainty sets that jointly characterize eavesdroppers position and array-orientation errors. Building upon these sets, we formulate a robust joint optimization problem that determines per-waveguide beamforming and AN covariance, individual PA power-ratio allocation, and PA positions to maximize the system sum rate subject to secrecy constraints. The highly non-convex design problem is efficiently addressed via a low computational complexity iterative algorithm that capitalizes on block coordinate descent, penalty-based methods, majorization-minimization, the S-procedure, and Lipschitz-based surrogate functions. Simulation results demonstrate that sum rates for the proposed algorithm outperforms conventional fixed antenna systems by 4.7 dB, offering substantially improved rate and secrecy performance. In particular, (i) adaptive PA positioning preserves LoS to legitimate users while effectively exploiting waveguide geometry to disrupt eavesdropper channels, and (ii) neglecting blockage effects in the PA system significantly impacts the system design, leading to performance degradation and inadequate secrecy guarantees.

</details>


### [34] [Error correction methods based on two-faced processes](https://arxiv.org/abs/2601.06447)
*Boris Ryabko*

Main category: cs.IT

TL;DR: 提出一种通过增加符号间相互依赖性来增强信道纠错能力的新方法，编码解码复杂度为线性


<details>
  <summary>Details</summary>
Motivation: 传统信道纠错方法存在局限性，需要更高效且复杂度低的纠错方案来降低误码率

Method: 通过变换输入序列显著增加符号间的相互依赖性，利用这种特性在传输后进行纠错

Result: 显著降低剩余误码率，同时保持编码和解码的线性复杂度

Conclusion: 该方法为信道纠错提供了新的有效途径，在保持低复杂度的同时显著提升纠错性能

Abstract: A new approach to the problem of error correction in communication channels is proposed, in which the input sequence is transformed in such a way that the interdependence of symbols is significantly increased. Then, after the sequence is transmitted over the channel, this property is used for error correction so that the remaining error rate is significantly reduced. The complexity of encoding and decoding is linear.

</details>


### [35] [Function-Correcting Partition codes](https://arxiv.org/abs/2601.06450)
*Charul Rajput,B. Sundar Rajan,Ragnar Freij-Hollanti,Camilla Hollanti*

Main category: cs.IT

TL;DR: 函数校正分区码(FCPC)是函数校正码(FCC)的自然推广，通过分区编码保护多个函数，可节省带宽并提供部分隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有函数校正码(FCC)只能保护单个函数，需要为每个函数单独构造编码，效率低下。本文旨在开发一种能同时保护多个函数的统一编码方案，以节省带宽并提高效率。

Method: 引入函数校正分区码(FCPC)，定义在F_q^k的分区上。使用分区连接构造同时保护多个函数的单一编码。定义分区冗余增益和分区率增益衡量带宽节省。针对线性函数，通过核的交集的陪集分区进行特化。通过分区图和团结构寻找最优冗余。引入分区保持收缩简化问题。

Result: 证明FCC是FCPC在函数诱导分区下的特例。展示了在权重分区和支持分区图中存在全尺寸团，可获得最优冗余。FCPC能同时保护多个函数，节省带宽，并提供部分隐私保护。

Conclusion: FCPC是FCC的自然推广，能有效保护多个函数，节省带宽，并提供部分隐私保护。分区图、团结构和分区保持收缩为寻找最优冗余提供了有效工具。

Abstract: We introduce function-correcting partition codes (FCPCs) that are a natural generalization of function-correcting codes (FCCs). A $t$-error function-correcting partition code is an $(\mathcal{P},t)$-encoding defined directly on a partition $\mathcal{P}$ of $\mathbb{F}_q^k$. For a partition $\mathcal{P}=\{P_1,P_2,\ldots,P_E\}$ a systematic mapping $\mathcal{C}_{\mathcal{P}} : \mathbb{F}_q^k \rightarrow \mathbb{F}_q^{k+r}$ is called a \emph{$(\mathcal{P},t)$-encoding} if for all $u\in P_i$ and $v\in P_j$ with $i\neq j$, $d\big(\mathcal{C}_{\mathcal{P}}(u), \mathcal{C}_{\mathcal{P}}(v)\big)\ge 2t+1.$ We show that any $t$-error correcting code for a function $f$, denoted by $(f,t)$-FCC is exactly an FCPC with respect to the domain partition induced by $f$, which makes these codes a natural generalization of FCCs. We use the join of domain partitions to construct a single code that protects multiple functions simultaneously. We define the notion of partition redundancy gain and partition rate gain to measure the bandwidth saved by using a single FCPC for multiple functions instead of constructing separate FCCs for each function. We specialize this to linear functions via coset partition of the intersection of their kernels. Then, we associate a partition graph to any given partition of $\mathbb{F}_q^k$, and show that the existence of a suitable clique in this graph yields a set of representative information vectors that achieves the optimal redundancy. We showed the existence of a full-size clique in the partition graphs of weight partition and support partition. Finally, we introduce the notion of a block-preserving contraction for a partition, which helps reduce the problem of finding optimal redundancy for an FCPC. We observe that FCPCs naturally provide a form of partial privacy, in the sense that only the domain partition of the function needs to be revealed to the transmitter.

</details>


### [36] [Algorithms for Computing the Petz-Augustin Capacity](https://arxiv.org/abs/2601.06492)
*Chun-Neng Chu,Wei-Fu Tseng,Yen-Huan Li*

Main category: cs.IT

TL;DR: 提出了首个具有非渐近收敛保证的算法来计算Petz-Augustin容量，该容量推广了信道容量并刻画了经典-量子信道编码中的最优误差指数。


<details>
  <summary>Details</summary>
Motivation: Petz-Augustin容量是信道容量的推广，描述了经典-量子信道编码的最优误差指数，但之前缺乏具有非渐近收敛保证的高效计算方法。

Method: 采用双层方法：1) 对于Petz-Rényi信息最大化，利用其凸Hölder光滑特性，应用Nesterov的通用快速梯度法；2) 对于Petz-Augustin信息最大化，采用熵镜像下降法，并设计了基于Thompson度量的收缩性定点算法来计算每次迭代的Petz-Augustin信息。

Result: 提出了首个具有非渐近收敛保证的算法来计算Petz-Augustin容量，将Blahut-Arimoto算法的镜像下降解释推广到量子信息论领域。

Conclusion: 该工作为计算Petz-Augustin容量提供了高效且理论保证的算法框架，推广了经典信道容量计算技术到量子信息领域。

Abstract: We propose the first algorithms with non-asymptotic convergence guarantees for computing the Petz-Augustin capacity, which generalizes the channel capacity and characterizes the optimal error exponent in classical-quantum channel coding. This capacity can be equivalently expressed as the maximization of two generalizations of mutual information: the Petz-Rényi information and the Petz-Augustin information. To maximize the Petz-Rényi information, we show that it corresponds to a convex Hölder-smooth optimization problem, and hence the universal fast gradient method of Nesterov (2015), along with its convergence guarantees, readily applies. Regarding the maximization of the Petz-Augustin information, we adopt a two-layered approach: we show that the objective function is smooth relative to the negative Shannon entropy and can be efficiently optimized by entropic mirror descent; each iteration of entropic mirror descent requires computing the Petz-Augustin information, for which we propose a novel fixed-point algorithm and establish its contractivity with respect to the Thompson metric. Notably, this two-layered approach can be viewed as a generalization of the mirror-descent interpretation of the Blahut-Arimoto algorithm due to He et al. (2024).

</details>


### [37] [On the Number of Subsequences in the Nonbinary Deletion Channel](https://arxiv.org/abs/2601.06493)
*Han Li,Xiang Wang,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 研究删除信道中非二进制字符串的子序列数量，针对r-run字符串提出改进的上下界，并找到在任意t次删除下具有最大子序列数量的字符串族。


<details>
  <summary>Details</summary>
Motivation: 在删除信道中，确定长度为n的字符串U经过t次删除后产生的子序列数量是一个重要问题。已知子序列数量与字符串的run数（连续相同字符的最大子串）密切相关。本文研究非二进制字符串在此场景下的子序列数量。

Method: 研究非二进制字符串在删除信道中的子序列数量，提出r-run非二进制字符串子序列数量的改进上下界。特别地，刻画了在任意t次删除下具有最大子序列数量的r-run非二进制字符串族。

Result: 证明了该最大子序列数量可以在多项式时间内计算，为r-run非二进制字符串的子序列数量提供了更精确的界限。

Conclusion: 本文对非二进制字符串在删除信道中的子序列数量问题做出了理论贡献，提供了改进的界限并找到了具有最大子序列数量的字符串族，且相关计算具有多项式时间复杂度。

Abstract: In the deletion channel, an important problem is to determine the number of subsequences derived from a string $U$ of length $n$ when subjected to $t$ deletions. It is well-known that the number of subsequences in the setting exhibits a strong dependence on the number of runs in the string $U$, where a run is defined as a maximal substring of identical characters. In this paper we study the number of subsequences of a non-binary string in this scenario, and propose some improved bounds on the number of subsequences of $r$-run non-binary strings. Specifically, we characterize a family of $r$-run non-binary strings with the maximum number of subsequences under any $t$ deletions, and show that this number can be computed in polynomial time.

</details>


### [38] [Coding for Fading Channels with Imperfect CSI at the Transmitter and Quantized Feedback](https://arxiv.org/abs/2601.06501)
*Yuhan Yang,Haoheng Yuan,Chao Qi,Fan Cheng,Bin Dai*

Main category: cs.IT

TL;DR: 本文提出了针对多径衰落信道（含反馈）的Schalkwijk-Kailath型编码方案，通过将第二路径信号视为中继并采用放大转发策略，或将时域信道转换为频域MIMO信道，提高了传输速率。


<details>
  <summary>Details</summary>
Motivation: 经典的Schalkwijk-Kailath方案在高斯噪声信道中具有极低的编码复杂度和双指数衰减的译码错误率，但如何将其扩展到具有记忆性的信道模型尚未解决。本文旨在设计适用于多径衰落信道（含反馈）的SK型方案。

Method: 针对2路径准静态衰落信道，将第二路径信号视为中继并采用放大转发策略；针对任意多径衰落信道，提出将时域信道转换为频域MIMO信道的SK型方案。

Result: 研究表明，第二路径信号（作为中继）有助于提高传输速率；提出的方案成功将SK方案扩展到多径衰落信道模型。

Conclusion: 本文成功设计了适用于多径衰落信道（含反馈）的Schalkwijk-Kailath型编码方案，通过中继策略和信道转换方法，扩展了经典SK方案的应用范围并提高了传输性能。

Abstract: The classical Schalkwijk-Kailath (SK) scheme for the additive Gaussian noise channel with noiseless feedback is highly efficient since its coding complexity is extremely low and the decoding error doubly exponentially decays as the coding blocklength tends to infinity. However, how to extend the SK scheme to channel models with memory has yet to be solved. In this paper, we first investigate how to design SK-type scheme for the 2-path quasi-static fading channel with noiseless feedback. By viewing the signal of the second path as a relay and adopting an amplify-and-forward (AF) relay strategy, we show that the interference path signal can help to enhance the transmission rate. Besides this, for arbitrary multi-path fading channel with feedback, we also present an SK-type scheme for such a model, which
  transforms the time domain channel into a frequency domain MIMO channel.

</details>


### [39] [Some New Results on Sequence Reconstruction Problem for Deletion Channels](https://arxiv.org/abs/2601.06503)
*Xiang Wang,Weijun Fang,Han Li,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 论文解决了序列重构问题中N(n,3,4)的确切值，证明当n≥13时，N(n,3,4)=20n-166。


<details>
  <summary>Details</summary>
Motivation: Levenshtein在2001年提出的序列重构问题在组合学中等价于确定N(n,d,t)的值。Pham、Goyal和Kiah提出了一个开放性问题：确定N(n,3,4)的确切值。

Method: 提出了N(n,3,t)在n≥13且t≥4时的下界，并特别针对t=4的情况证明该下界是紧的。

Result: 证明了对于所有n≥13，N(n,3,4)=20n-166，解决了Pham等人提出的开放性问题。

Conclusion: 该研究完全确定了N(n,3,4)的值，为序列重构问题中这一特定参数组合提供了精确解。

Abstract: Levenshtein first introduced the sequence reconstruction problem in $2001$. In the realm of combinatorics, the sequence reconstruction problem is equivalent to determining the value of $N(n,d,t)$, which represents the maximum size of the intersection of two metric balls of radius $t$, given that the distance between their centers is at least $d$ and the sequence length is $n$. In this paper, We present a lower bound on $N(n,3,t)$ for $n\geq 13$ and $t \geq 4$. For $t=4$, we prove that this lower bound is tight. This settles an open question posed by Pham, Goyal, and Kiah, confirming that $N(n,3,4)=20n-166$ for all $n \geq 13$.

</details>


### [40] [Hard Thresholding Pursuit Algorithms for Least Absolute Deviations Problem](https://arxiv.org/abs/2601.06558)
*Jiao Xu,Peng Li,Bing Zheng*

Main category: cs.IT

TL;DR: GFHTP₁算法在LAD准则下对异常值具有鲁棒性，无需信号稀疏度先验信息且无参数设计，在计算效率和鲁棒性方面优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 研究自适应迭代硬阈值算法的鲁棒性，特别是在少数测量值被任意幅度异常值污染的场景下。现有算法通常需要信号稀疏度的先验信息且涉及参数优化，这限制了实际应用。

Method: 提出graded fast hard thresholding pursuit (GFHTP₁)算法，基于最小绝对偏差(LAD)准则，采用自适应迭代硬阈值方法。该算法无需信号稀疏度先验信息，且设计为无参数算法，简化了实现过程。

Result: 数值实验表明，GFHTP₁算法在鲁棒性和计算效率方面持续优于竞争算法。特别是在存在异常值的情况下，该算法表现出更好的性能。

Conclusion: GFHTP₁算法是一种有效的无参数稀疏信号恢复方法，在存在异常值的情况下具有优越的鲁棒性和计算效率，无需稀疏度先验信息，简化了实际应用。

Abstract: Least absolute deviations (LAD) is a statistical optimality criterion widely utilized in scenarios where a minority of measurements are contaminated by outliers of arbitrary magnitudes. In this paper, we delve into the robustness of the variant of adaptive iterative hard thresholding to outliers, known as graded fast hard thresholding pursuit (GFHTP$_1$) algorithm. Unlike the majority of the state-of-the-art algorithms in this field, GFHTP$_1$ does not require prior information about the signal's sparsity. Moreover, its design is parameterless, which not only simplifies the implementation process but also removes the intricacies of parameter optimization. Numerical experiments reveal that the GFHTP$_1$ algorithm consistently outperforms competing algorithms in terms of both robustness and computational efficiency.

</details>


### [41] [TCLNet: A Hybrid Transformer-CNN Framework Leveraging Language Models as Lossless Compressors for CSI Feedback](https://arxiv.org/abs/2601.06588)
*Zijiu Yang,Qianqian Yang,Shunpu Tang,Tingting Yang,Zhiguo Shi*

Main category: cs.IT

TL;DR: TCLNet是一个用于FDD大规模MIMO系统中CSI压缩的统一框架，结合了Transformer-CNN混合架构进行有损压缩，以及语言模型和因子化模型进行无损压缩，在重建精度和传输效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在FDD大规模MIMO系统中，下行链路CSI对实现高频谱和能量效率至关重要，但随着天线数量增加，CSI反馈开销成为主要瓶颈。现有的深度学习CSI压缩方法在捕捉CSI的局部和全局特征方面存在局限，限制了压缩效率。

Method: 提出TCLNet统一CSI压缩框架：1）有损压缩模块采用混合Transformer-CNN架构，联合利用局部特征和全局上下文；2）无损压缩模块采用混合语言模型和因子化模型设计，自适应切换上下文感知编码和并行编码以优化率失真复杂度权衡。

Result: 在真实世界和模拟数据集上的实验表明，TCLNet在重建精度和传输效率方面优于现有方法，在不同场景下实现了高达5dB的性能增益。此外，研究还展示了通过精心设计的提示，大型语言模型可以作为零样本CSI无损压缩器。

Conclusion: TCLNet通过集成有损和无损压缩的混合架构，有效解决了FDD大规模MIMO系统中CSI压缩的挑战，显著提升了压缩效率和性能。同时，探索了大型语言模型在CSI无损压缩中的潜在应用。

Abstract: In frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems, downlink channel state information (CSI) plays a crucial role in achieving high spectrum and energy efficiency. However, the CSI feedback overhead becomes a major bottleneck as the number of antennas increases. Although existing deep learning-based CSI compression methods have shown great potential, they still face limitations in capturing both local and global features of CSI, thereby limiting achievable compression efficiency. To address these issues, we propose TCLNet, a unified CSI compression framework that integrates a hybrid Transformer-CNN architecture for lossy compression with a hybrid language model (LM) and factorized model (FM) design for lossless compression. The lossy module jointly exploits local features and global context, while the lossless module adaptively switches between context-aware coding and parallel coding to optimize the rate-distortion-complexity (RDC) trade-off. Extensive experiments on both real-world and simulated datasets demonstrate that the proposed TCLNet outperforms existing approaches in terms of reconstruction accuracy and transmission efficiency, achieving up to a 5 dB performance gain across diverse scenarios. Moreover, we show that large language models (LLMs) can be leveraged as zero-shot CSI lossless compressors via carefully designed prompts.

</details>


### [42] [Symplectic Hulls over a Non-Unital Ring](https://arxiv.org/abs/2601.06609)
*Anup Kushwaha,Om Prakash*

Main category: cs.IT

TL;DR: 研究非幺环E上辛壳的结构与性质，包括自由E线性码的辛壳生成矩阵、两个码的和的辛壳、构建技术、置换等价性，并应用于小长度最优码的分类


<details>
  <summary>Details</summary>
Motivation: 研究非幺环E上辛壳的性质，该环具有特殊代数结构(2κ=2τ=0, κ²=κ, τ²=τ, κτ=κ, τκ=τ)，旨在理解自由E线性码的辛壳结构并应用于编码理论

Method: 1. 识别左、右和双边辛壳的剩余码和挠码；2. 刻画自由E线性码双边辛壳的生成矩阵；3. 探索两个自由E线性码和的辛壳；4. 提出两种构建技术扩展码长和辛壳秩；5. 讨论置换等价性和辛壳变化问题

Result: 建立了自由E线性码辛壳的完整理论框架，包括生成矩阵刻画、构建技术、置换等价性分析，并成功应用于小长度最优E线性码的分类

Conclusion: 该研究为非幺环E上辛壳理论提供了系统分析，发展了构建技术和等价性理论，为编码理论中自由E线性码的优化设计提供了理论基础

Abstract: This paper presents the study of the symplectic hulls over a non-unital ring $ E= \langle κ,τ\mid 2 κ=2 τ=0,~ κ^2=κ,~ τ^2=τ,~ κτ=κ,~ τκ=τ\rangle$. We first identify the residue and torsion codes of the left, right, and two-sided symplectic hulls, and characterize the generator matrix of the two-sided symplectic hull of a free $E$-linear code. Then, we explore the symplectic hull of the sum of two free $E$-linear codes. Subsequently, we provide two build-up techniques that extend a free $E$-linear code of smaller length and symplectic hull-rank to one of larger length and symplectic hull-rank. Further, for free $E$-linear codes, we discuss the permutation equivalence and investigate the symplectic hull-variation problem. An application of this study is given by classifying the free $E$-linear optimal codes for smaller lengths.

</details>


### [43] [Large Artificial Intelligence Models for Future Wireless Communications](https://arxiv.org/abs/2601.06906)
*Chong Huang,Gaojie Chen,Pei Xiao,Zhu Han,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 本文探讨了大型AI模型与无线通信的融合潜力，分析了其在数据管理、资源分配和实时适应方面的优势，同时讨论了能源、架构设计、隐私安全等挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络日益复杂，传统优化管理方法面临挑战，而大型AI模型凭借其广泛的参数空间和强大的学习能力，有望为无线通信提供创新解决方案。

Method: 提出面向未来无线通信的大型AI模型架构，分析其在数据分析、资源分配和实时适应方面的应用优势，并探讨能源、架构设计、隐私安全等挑战的解决方案。

Result: 建立了大型AI模型在无线通信中的集成框架，识别了关键应用场景和潜在挑战，为未来研究奠定了基础。

Conclusion: 大型AI模型与无线通信的融合将带来变革性影响，虽然面临能源、隐私、安全等多重挑战，但通过适当的架构设计和解决方案，有望推动无线通信进入新的发展阶段。

Abstract: The anticipated integration of large artificial intelligence (AI) models with wireless communications is estimated to usher a transformative wave in the forthcoming information age. As wireless networks grow in complexity, the traditional methodologies employed for optimization and management face increasingly challenges. Large AI models have extensive parameter spaces and enhanced learning capabilities and can offer innovative solutions to these challenges. They are also capable of learning, adapting and optimizing in real-time. We introduce the potential and challenges of integrating large AI models into wireless communications, highlighting existing AIdriven applications and inherent challenges for future large AI models. In this paper, we propose the architecture of large AI models for future wireless communications, introduce their advantages in data analysis, resource allocation and real-time adaptation, discuss the potential challenges and corresponding solutions of energy, architecture design, privacy, security, ethical and regulatory. In addition, we explore the potential future directions of large AI models in wireless communications, laying the groundwork for forthcoming research in this area.

</details>


### [44] [The Sample Complexity of Lossless Data Compression](https://arxiv.org/abs/2601.06688)
*Terence Viaud,Ioannis Kontoyiannis*

Main category: cs.IT

TL;DR: 本文提出了一种新的无损数据压缩基本极限分析框架，强调真正的非渐近结果，定义了压缩的样本复杂度，并建立了与Rényi熵的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 现有数据压缩理论主要关注渐近性能，缺乏对有限块长下压缩性能的系统分析。本文旨在建立非渐近框架，研究在指定速率和超额概率下压缩特定源所需的最小块长。

Method: 引入"样本复杂度"概念，定义为在指定速率和超额概率下压缩源所需的最小块长。通过将压缩问题与假设检验问题联系起来，利用现有假设检验的样本复杂度结果。分析不同类型编码器（变长码、前缀码、定长码）的样本复杂度关系。

Result: 1) 任意源的变长码、前缀码和定长码样本复杂度紧密耦合；2) 无记忆源的样本复杂度由1/2阶Rényi熵而非香农熵决定；3) 建立了马尔可夫源的样本复杂度由1/2阶Rényi熵率决定；4) 发展了通用数据压缩的样本复杂度界限，由源族与均匀分布的最小1/2阶Rényi散度表征。

Conclusion: 本文建立了数据压缩的非渐近理论框架，揭示了Rényi熵在有限块长压缩中的核心作用，为理解压缩的基本极限提供了新视角，并将压缩问题与假设检验问题联系起来。

Abstract: A new framework is introduced for examining and evaluating the fundamental limits of lossless data compression, that emphasizes genuinely non-asymptotic results. The {\em sample complexity} of compressing a given source is defined as the smallest blocklength at which it is possible to compress that source at a specified rate and to within a specified excess-rate probability. This formulation parallels corresponding developments in statistics and computer science, and it facilitates the use of existing results on the sample complexity of various hypothesis testing problems. For arbitrary sources, the sample complexity of general variable-length compressors is shown to be tightly coupled with the sample complexity of prefix-free codes and fixed-length codes. For memoryless sources, it is shown that the sample complexity is characterized not by the source entropy, but by its Rényi entropy of order~$1/2$. Nonasymptotic bounds on the sample complexity are obtained, with explicit constants. Generalizations to Markov sources are established, showing that the sample complexity is determined by the source's Rényi entropy rate of order~$1/2$. Finally, bounds on the sample complexity of universal data compression are developed for arbitrary families of memoryless sources. There, the sample complexity is characterized by the minimum Rényi divergence of order~$1/2$ between elements of the family and the uniform distribution. The connection of this problem with identity testing and with the associated separation rates is explored and discussed.

</details>


### [45] [Study of Adaptive Reliability-Driven Conditional Innovation Decoding for LDPC Codes](https://arxiv.org/abs/2601.06732)
*Hassan Touati,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出自适应可靠性驱动的条件创新(AR-CID)解码算法，用于LDPC码解码，通过消息质量检查和消息传递优化，在低延迟应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 针对低密度奇偶校验(LDPC)码解码，需要提高解码性能同时降低延迟，特别是在短码和中等长度码的应用场景中

Method: 提出AR-CID解码算法，包含两个阶段：消息质量检查阶段和消息传递优化阶段，基于残差置信传播解码策略

Result: AR-CID算法在多种LDPC码（包括短码和中等长度码）和广泛信道条件下，性能优于现有解码技术，收敛速度极快

Conclusion: AR-CID解码算法特别适合低延迟应用，在性能和收敛速度方面都有显著优势

Abstract: In this work, we present an adaptive reliability-driven conditional innovation (AR-CID) decoding algorithm for low-density parity check (LDPC) codes. The proposed AR-CID decoding algorithm consists of one stage of message quality checking and another stage of message passing refinement, which are incorporated into a residual belief propagation decoding strategy. An analysis of the AR-CID decoding algorithm is carried out along with a study of its computational complexity and latency characteristics. Simulation results for several examples of LDPC codes, including short and medium-length codes over an extended range of channel conditions, indicate that the proposed AR-CID decoding algorithm outperforms competing decoding techniques and has an extremely fast convergence, making it particularly suitable for low-delay applications.

</details>


### [46] [Optimal Rate Region for Multi-server Secure Aggregation with User Collusion](https://arxiv.org/abs/2601.06836)
*Zhou Li,Xiang Zhang,Kai Wan,Hua Sun,Mingyue Ji,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了多服务器安全聚合问题，在存在用户合谋的情况下，完整刻画了最优速率区域，发现最小通信和个体密钥速率均为每输入符号1个符号，最优源密钥速率由min{U+V+T-2, UV-1}给出。


<details>
  <summary>Details</summary>
Motivation: 研究隐私保护分布式学习系统中的安全聚合问题，特别是在多服务器两跳网络架构下，考虑用户合谋威胁时的安全聚合方案设计。

Method: 采用信息论安全框架，通过线性密钥构造实现方案，同时利用正确性和安全性约束推导紧密的熵界进行逆定理证明。

Result: 完整刻画了最优速率区域：最小用户-服务器和服务器-服务器通信速率均为每输入符号1个符号，最小个体密钥速率也为1，最优源密钥速率为min{U+V+T-2, UV-1}。

Conclusion: 多服务器架构相比单服务器能显著减少所需密钥随机性，揭示了安全性与密钥效率之间的基本权衡，为多服务器系统中用户合谋下的安全聚合提供了完整的信息论表征。

Abstract: Secure aggregation is a fundamental primitive in privacy-preserving distributed learning systems, where an aggregator aims to compute the sum of users' inputs without revealing individual data. In this paper, we study a multi-server secure aggregation problem in a two-hop network consisting of multiple aggregation servers and multiple users per server, under the presence of user collusion. Each user communicates only with its associated server, while the servers exchange messages to jointly recover the global sum. We adopt an information-theoretic security framework, allowing up to $T$ users to collude with any server.
  We characterize the complete optimal rate region in terms of user-to-server communication rate, server-to-server communication rate, individual key rate, and source key rate. Our main result shows that the minimum communication and individual key rates are all one symbol per input symbol, while the optimal source key rate is given by $\min\{U+V+T-2,\, UV-1\}$, where $U$ denotes the number of servers and $V$ the number of users per server. The achievability is established via a linear key construction that ensures correctness and security against colluding users, while the converse proof relies on tight entropy bounds derived from correctness and security constraints.
  The results reveal a fundamental tradeoff between security and key efficiency and demonstrate that the multi-server architecture can significantly reduce the required key randomness compared to single-server secure aggregation. Our findings provide a complete information-theoretic characterization of secure aggregation in multi-server systems with user collusion.

</details>


### [47] [Caching Yields up to 5x Spectral Efficiency in Multi-Beam Satellite Communications](https://arxiv.org/abs/2601.06925)
*Hui Zhao,Dirk Slock,Petros Elia*

Main category: cs.IT

TL;DR: 将向量编码缓存(VCC)集成到多波束卫星通信系统中，即使有限的接收端缓存也能显著提升频谱效率，实现300-550%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 卫星通信系统需要提高频谱效率以缩小与有线网络的性能差距，传统方法如多播、预取等存在局限性，需要一种纯物理层的解决方案。

Method: 将向量编码缓存(VCC)集成到多波束卫星通信中，利用缓存内容抑制干扰，使多个预编码信号向量能够并发传输。采用Rician-shadowed衰落信道模型，考虑匹配滤波预编码、CSI获取开销和CSI不完美等实际因素，推导出平均总速率和频谱效率增益的闭式表达式。

Result: VCC相比传统多用户MISO卫星通信，在相同资源下可获得300%到550%的频谱效率增益。这些增益与多播、预取或文件流行度无关，验证了VCC作为纯物理层解决方案的有效性。

Conclusion: VCC是未来高吞吐量卫星通信系统的有效物理层解决方案，能显著缩小卫星网络与有线网络之间的性能差距，即使有限的接收端缓存也能带来显著的频谱效率提升。

Abstract: This paper examines the integration of vector coded caching (VCC) into multi-beam satellite communications (SATCOM) systems and demonstrates that even limited receiver-side caching can substantially enhance spectral efficiency. By leveraging cached content to suppress interference, VCC enables the concurrent transmission of multiple precoded signal vectors that would otherwise require separate transmission resources. This leads to a multiplicative improvement in resource utilization in SATCOM. To characterize this performance, we model the satellite-to-ground channel using Rician-shadowed fading and after incorporating practical considerations such as matched-filter precoding, channel state information (CSI) acquisition overhead as well as CSI imperfections at the transmitter, we here derive closed-form expressions for the average sum rate and spectral efficiency gain of VCC in SATCOM. Our analysis, tightly validated through numerical simulations, reveals that VCC can yield spectral efficiency gains of 300% to 550% over traditional multi-user MISO SATCOM with the same resources. These gains -- which have nothing to do with multicasting, prefetching gains nor file popularity -- highlight VCC as a pure physical-layer solution for future high-throughput SATCOM systems, significantly narrowing the performance gap between satellite and wired networks.

</details>


### [48] [Generalization Bounds for Transformer Channel Decoders](https://arxiv.org/abs/2601.06969)
*Qinshan Zhang,Bin Chen,Yong Jiang,Shu-Tao Xia*

Main category: cs.IT

TL;DR: 首次为Transformer信道解码器（ECCT）提供理论泛化保证，通过比特级Rademacher复杂度推导泛化上界，分析码长、模型参数、训练集大小的影响，并证明奇偶校验掩码注意力能通过稀疏性收紧泛化界。


<details>
  <summary>Details</summary>
Motivation: Transformer信道解码器（如ECCT）在信道解码中表现出强大性能，但其泛化行为缺乏理论理解。本文旨在从学习理论角度研究ECCT的泛化性能，填补这一理论空白。

Method: 通过建立乘性噪声估计误差与误码率（BER）的联系，利用比特级Rademacher复杂度推导泛化上界。分析奇偶校验掩码注意力如何通过诱导稀疏性减少覆盖数，从而收紧泛化界。

Result: 推导出ECCT的泛化上界，揭示了泛化性能与码长、模型参数、训练集大小的依赖关系。证明奇偶校验掩码注意力能通过稀疏性降低覆盖数，获得更紧的泛化界。这是首次为该类解码器提供理论泛化保证。

Conclusion: 本文首次为Transformer信道解码器提供了理论泛化保证，建立了乘性噪声估计误差与误码率的理论联系，推导了泛化上界，并证明了奇偶校验掩码注意力的稀疏性优势，为这类模型的可靠应用奠定了理论基础。

Abstract: Transformer channel decoders, such as the Error Correction Code Transformer (ECCT), have shown strong empirical performance in channel decoding, yet their generalization behavior remains theoretically unclear. This paper studies the generalization performance of ECCT from a learning-theoretic perspective. By establishing a connection between multiplicative noise estimation errors and bit-error-rate (BER), we derive an upper bound on the generalization gap via bit-wise Rademacher complexity. The resulting bound characterizes the dependence on code length, model parameters, and training set size, and applies to both single-layer and multi-layer ECCTs. We further show that parity-check-based masked attention induces sparsity that reduces the covering number, leading to a tighter generalization bound. To the best of our knowledge, this work provides the first theoretical generalization guarantees for this class of decoders.

</details>


### [49] [Quantum Optical Integrated Sensing and Communication with Homodyne BPSK Detection](https://arxiv.org/abs/2601.07034)
*Ioannis Krikidis*

Main category: cs.IT

TL;DR: 提出一种量子集成传感与通信方案，使用BPSK调制和零差检测，在未知相位旋转的高斯信道中实现联合符号检测和相位估计。


<details>
  <summary>Details</summary>
Motivation: 在量子光学链路中，传统方法通常将传感和通信分开处理，无法充分利用量子资源。需要一种集成方案，在未知相位旋转的信道中同时实现可靠通信和精确传感。

Method: 采用二进制相移键控调制和零差检测，设计最小化误码率同时满足费舍尔信息约束的优化问题。开发迭代算法，包含用于联合检测和估计的内层期望最大化循环，以及自适应调整本地振荡器相位的外层循环。

Result: 数值结果验证了所提方法的有效性，并展示了通信可靠性和传感精度之间的基本权衡关系。

Conclusion: 该方案成功实现了量子集成传感与通信，在未知相位旋转的量子光学链路中同时优化了通信性能和传感能力。

Abstract: In this letter, we propose a quantum integrated sensing and communication scheme for a quantum optical link using binary phase-shift keying modulation and homodyne detection. The link operates over a phase-insensitive Gaussian channel with an unknown deterministic phase rotation, where the homodyne receiver jointly carries out symbol detection and phase estimation. We formulate a design problem that minimizes the bit-error rate subject to a Fisher information-based constraint on estimation accuracy. To solve it, we develop an iterative algorithm composed of an inner expectation-maximization loop for joint detection and estimation and an outer loop that adaptively retunes the local oscillator phase. Numerical results confirm the effectiveness of the proposed approach and demonstrate a fundamental trade-off between communication reliability and sensing accuracy.

</details>


### [50] [Random Access in DNA Storage: Algorithms, Constructions, and Bounds](https://arxiv.org/abs/2601.07053)
*Chen Wang,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出计算DNA数据存储随机访问问题中期望读取次数的O(n)算法，推导显式公式，改进码构造，提升上界和下界，证明n=k+1时简单奇偶码的最优性。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储走向实际应用，需要最小化测序覆盖深度以降低操作成本和检索延迟。随机访问问题评估从n个编码链中恢复特定信息链所需的期望读取次数。

Method: 提出计算期望读取次数精确值的O(n)算法（固定域大小q和信息长度k）。推导平均和最大期望读取次数的显式公式，在小参数下高效搜索最优生成矩阵。提出新的码构造方法。

Result: 改进已知上界：k=3时从0.8815k提升到0.8811k；k=4时对足够大的q达到0.8629k。建立更紧的理论下界，证明n=k+1时简单奇偶码在任何字母表q下的最优性。

Conclusion: 该工作为DNA数据存储随机访问问题提供了高效算法和理论分析，改进了码构造的性能边界，证明了特定情况下的最优码结构，对实际DNA存储系统设计有重要意义。

Abstract: As DNA data storage moves closer to practical deployment, minimizing sequencing coverage depth is essential to reduce both operational costs and retrieval latency. This paper addresses the recently studied Random Access Problem, which evaluates the expected number of read samples required to recover a specific information strand from $n$ encoded strands. We propose a novel algorithm to compute the exact expected number of reads, achieving a computational complexity of $O(n)$ for fixed field size $q$ and information length $k$. Furthermore, we derive explicit formulas for the average and maximum expected number of reads, enabling an efficient search for optimal generator matrices under small parameters. Beyond theoretical analysis, we present new code constructions that improve the best-known upper bound from $0.8815k$ to $0.8811k$ for $k=3$, and achieve an upper bound of $0.8629k$ for $k=4$ for sufficiently large $q$. We also establish a tighter theoretical lower bound on the expected number of reads that improves upon state-of-the-art bounds. In particular, this bound establishes the optimality of the simple parity code for the case of $n=k+1$ across any alphabet $q$.

</details>


### [51] [Score-Based VAMP with Fisher-Information-Based Onsager Correction](https://arxiv.org/abs/2601.07095)
*Tadashi Wadayama,Takumi Takahashi*

Main category: cs.IT

TL;DR: 提出SC-VAMP方法，通过条件Fisher信息表达Onsager修正，实现无需雅可比矩阵的非线性MMSE估计，扩展VAMP到复杂黑盒推理问题


<details>
  <summary>Details</summary>
Motivation: 传统VAMP需要计算雅可比矩阵，限制了在复杂黑盒推理问题中的应用。需要一种方法能够处理无法显式建模的复杂先验和似然函数

Method: 使用学习到的分数函数，通过Tweedie公式构建非线性MMSE估计器，从分数-范数统计推导Onsager项，避免解析导数计算。结合随机正交/酉混合处理结构化相关感知设置

Result: SC-VAMP实现了无需雅可比矩阵的VAMP变体，能够处理复杂黑盒推理问题。通过信息论视角为状态演化提供理论解释，超越了理想i.i.d.设置

Conclusion: SC-VAMP成功扩展了VAMP框架到显式建模不可行的复杂推理问题，为非线性机制下的解耦原理提供了新的理论见解

Abstract: We propose score-based VAMP (SC-VAMP), a variant of vector approximate message passing (VAMP) in which the Onsager correction is expressed and computed via conditional Fisher information, thereby enabling a Jacobian-free implementation. Using learned score functions, SC-VAMP constructs nonlinear MMSE estimators through Tweedie's formula and derives the corresponding Onsager terms from the score-norm statistics, avoiding the need for analytical derivatives of the prior or likelihood. When combined with random orthogonal/unitary mixing to mitigate non-ideal, structured or correlated sensing settings, the proposed framework extends VAMP to complex black-box inference problems where explicit modeling is intractable. Finally, by leveraging the entropic CLT, we provide an information-theoretic perspective on the Gaussian approximation underlying SE, offering insight into the decoupling principle beyond idealized i.i.d. settings, including nonlinear regimes.

</details>


### [52] [PASS-Enabled Covert Communications With Distributed Cooperative Wardens](https://arxiv.org/abs/2601.07147)
*Ji He*

Main category: cs.IT

TL;DR: 该论文研究了在分布式监控下的PASS使能下行隐蔽通信，通过双波导架构同时传输隐蔽信息和随机干扰，分析了三种PASS功率辐射规律，并开发了优化算法来最大化隐蔽速率。


<details>
  <summary>Details</summary>
Motivation: 在分布式监控环境下，多个看守者通过多数投票规则进行信号检测，对隐蔽通信构成严重威胁。需要设计能够有效隐藏传输足迹的系统架构和优化策略。

Method: 采用双波导架构同时传输隐蔽信息和随机干扰，考虑三种PASS功率辐射规律（通用、比例、相等）。通过PGF-ESP框架和断点阈值域划分，推导系统级检测错误概率的闭式表达式。开发MM-BCD-SCA算法进行功率/辐射变量和PA位置的联合优化。

Result: 推导了局部虚警和漏检概率的闭式表达式，获得了非独立同分布多数投票融合下的系统级检测错误概率的显式闭式表征。数值结果验证了理论分析，展示了协作监控和PASS辐射规律对隐蔽性-速率权衡的影响。

Conclusion: 提出的双波导PASS架构和优化算法能够有效对抗分布式监控，在满足隐蔽性约束的同时最大化隐蔽通信速率，为实际隐蔽通信系统设计提供了理论依据和实用工具。

Abstract: This paper investigates PASS-enabled downlink covert communication in the presence of distributed surveillance, where multiple wardens perform signal detection and fuse their local binary decisions via majority-voting rule. We consider a dual-waveguide architecture that simultaneously delivers covert information and randomized jamming to hide the transmission footprint, incorporating three representative PASS power-radiation laws-general, proportional, and equal. To characterize the system-level detectability, we derive closed-form expressions for local false-alarm and miss-detection probabilities. By leveraging a probability-generating-function (PGF) and elementary-symmetric-polynomial (ESP) framework, combined with a breakpoint-based partition of the threshold domain, we obtain explicit closed-form characterizations of the system-level detection error probability (DEP) under non-i.i.d. majority-voting fusion. Building on this analytical framework, we formulate a robust optimization problem to maximize the average covert rate subject to covertness constraint. To solve the resulting nonconvex design, we develop an MM-BCD-SCA algorithm that produces tractable alternating updates for power/radiation variables and PA positions via convex surrogates and inner approximations of the DEP value function. Numerical results validate the theoretical analysis and demonstrate the impact of cooperative monitoring and PASS radiation laws on the covertness-rate tradeoff.

</details>


### [53] [Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges](https://arxiv.org/abs/2601.07235)
*Agnivo Gosai,Shuvodeep De,Karun Thankachan*

Main category: cs.IT

TL;DR: 该论文对电影评论情感分析方法进行了全面综述，涵盖了从早期词典方法到现代深度学习和大语言模型的技术演进，重点关注领域特定挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 电影评论情感分析作为自然语言处理的基准任务，在推动技术进步方面发挥了核心作用。然而，现有综述主要关注文本处理流程，缺乏对领域特定问题（如讽刺、否定、上下文歧义等）的比较分析，也较少涉及多模态方法、可解释性、公平性等新兴问题。

Method: 采用系统性文献综述方法，对电影评论情感分析的技术演进进行全面梳理。方法包括：1）按时间顺序回顾从词典方法、经典机器学习到深度学习和LLM的技术发展；2）分析常用数据集（IMDb、Rotten Tomatoes、SST-2）和模型（朴素贝叶斯、SVM、LSTM、BERT、Transformer）；3）进行挑战驱动的比较分析，重点关注领域特定问题；4）综合多模态方法的最新进展；5）探讨可解释性、公平性等新兴问题。

Result: 论文提供了电影评论情感分析领域的全面技术路线图，系统总结了不同建模范式如何应对领域特定挑战。研究发现：1）虽然深度学习模型在准确率上有显著提升，但讽刺、否定、上下文歧义等问题仍是开放挑战；2）多模态方法通过整合文本、音频和视觉线索展现出潜力；3）可解释性、公平性和鲁棒性等新兴问题在现有研究中关注不足；4）领域迁移问题仍然存在。

Conclusion: 该综述为电影评论情感分析领域提供了既总结现有解决方案又突出未解决挑战的领域聚焦路线图。未来研究方向包括：零样本/少样本学习、混合符号-神经模型、实时部署考虑等，目标是构建更准确、可泛化和可解释的情感分析系统。

Abstract: This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.

</details>


### [54] [Bias-Aware BP Decoding of Quantum Codes via Directional Degeneracy](https://arxiv.org/abs/2601.07240)
*Mohammad Rowshan*

Main category: cs.IT

TL;DR: 该论文研究了针对量子CSS码的方向性置信传播解码，通过引入方向权重和偏置参数来利用Tanner图各向异性和噪声偏置，显著降低了逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 量子CSS码的Tanner图通常具有各向异性结构，且物理噪声可能存在方向性偏置。传统解码方法未充分利用这些方向性信息，导致解码性能受限。本文旨在开发一种能利用方向性信息来提升解码性能的硬件感知解码方法。

Method: 1. 在Tanner图边上分配方向权重，聚合成每个量子位的方向权重；2. 定义方向性简并枚举器来量化简并沿特定方向的集中程度；3. 引入偏置参数β将方向权重映射为位置相关的对数似然比，作为各向异性先验直接集成到标准BP→OSD解码器中；4. 推导方向距离与汉明距离的关系界限，并给出方向枚举器的MacWilliams型表达式。

Result: 在有限长度模拟中，该方法在码容量噪声下显著降低了逻辑错误率，通常在中度物理错误率下能降低一个数量级。理论分析表明，该方法能有效利用各向异性结构，通过方向性简并枚举器量化解码性能提升。

Conclusion: 适度的各向异性是获得硬件感知解码增益的简单有效途径。该方法无需改变码构造，可直接集成到现有BP→OSD解码器中，为量子纠错码的解码提供了新的方向性优化框架。

Abstract: We study directionally informed belief propagation (BP) decoding for quantum CSS codes, where anisotropic Tanner-graph structure and biased noise concentrate degeneracy along preferred directions. We formalize this by placing orientation weights on Tanner-graph edges, aggregating them into per-qubit directional weights, and defining a \emph{directional degeneracy enumerator} that summarizes how degeneracy concentrates along those directions. A single bias parameter~$β$ maps these weights into site-dependent log-likelihood ratios (LLRs), yielding anisotropic priors that plug directly into standard BP$\rightarrow$OSD decoders without changing the code construction. We derive bounds relating directional and Hamming distances, upper bound the number of degenerate error classes per syndrome as a function of distance, rate, and directional bias, and give a MacWilliams-type expression for the directional enumerator. Finite-length simulations under code-capacity noise show significant logical error-rate reductions -- often an order of magnitude at moderate physical error rates -- confirming that modest anisotropy is a simple and effective route to hardware-aware decoding gains.

</details>


### [55] [Rate-distortion Theory on Non-compact Spaces: A Concentration-compactness Approach](https://arxiv.org/abs/2601.07246)
*Jiayang Zou,Luyao Fan,Jiayang Gao,Jia Wang*

Main category: cs.IT

TL;DR: 将集中紧性原理引入率失真泛函分析，在非紧空间中建立最优重构分布的存在性


<details>
  <summary>Details</summary>
Motivation: 经典率失真理论的存在性结果依赖于紧性假设，这在非紧空间中经常被违反，需要新的理论框架来处理一般非紧空间上的率失真问题

Method: 引入集中紧性原理来分析率失真泛函，在失真函数的温和强制性条件下建立最优重构的存在性

Result: 建立了非紧空间上率失真问题的统一透明存在定理，放宽了对紧性的要求

Conclusion: 集中紧性原理为一般非紧空间上的率失真理论提供了有效的分析工具，扩展了经典存在性结果的应用范围

Abstract: In this paper, we study rate-distortion theory for general sources with an emphasis on the existence of optimal reconstruction distributions. Classical existence results rely on compactness assumptions that are often violated in non-compact settings. By introducing the concentration-compactness principle into the analysis of the rate-distortion functional, we establish the existence of optimal reconstructions under mild coercivity conditions on the distortion function. Our results provide a unified and transparent existence theorem for rate-distortion problems on general non-compact spaces.

</details>


### [56] [Engineering Favorable Propagation: Near-Field IRS Deployment for Spatial Multiplexing](https://arxiv.org/abs/2601.07317)
*Yuxuan Chen,Qingqing Wu,Guangji Chen,Qiaoyan Peng,Wen Chen*

Main category: cs.IT

TL;DR: IRS辅助MIMO系统中，通过稀疏阵列大孔径利用近场球面波前，建立确定性部署准则，将IRS战略性地放置在基站近场，以解决远场级联信道秩不足问题，提升空间复用能力。


<details>
  <summary>Details</summary>
Motivation: IRS辅助MIMO系统需要强视距链路补偿级联路径损耗，但这会导致有效信道秩不足，限制空间复用能力。现有远场级联信道存在固有秩缺陷问题。

Method: 利用稀疏阵列大孔径捕获近场球面波前，建立确定性部署准则，将IRS战略性地放置在基站近场。基于稀疏BS阵列和IRS的物理信道模型，分析级联信道秩特性和用户间相关性，推导闭式有利传播度量，设计几何驱动的部署规则。

Result: 提出的部署准则能有效降低用户间信道相关性，增强有效自由度。基于统计CSI的低复杂度MRT预编码方案和联合优化IRS相移与功率分配的算法，相比基准方案获得显著性能增益。

Conclusion: 通过近场球面波前利用和几何驱动部署，从根本上解决了IRS辅助MIMO系统中的秩不足问题，为创建有利传播环境提供了简单有效的指导原则。

Abstract: In intelligent reflecting surface IRS assisted multiple input multiple output MIMO systems, a strong line of sight LoS link is required to compensate for the severe cascaded path loss. However, such a link renders the effective channel highly rank deficient and fundamentally limits spatial multiplexing. To overcome this limitation, this paper leverages the large aperture of sparse arrays to harness near field spherical wavefronts, and establishes a deterministic deployment criterion that strategically positions the IRS in the near field of a base station BS. This placement exploits the spherical wavefronts of the BS IRS link to engineer decorrelated channels, thereby fundamentally overcoming the rank deficiency issue in far field cascaded channels. Based on a physical channel model for the sparse BS array and the IRS, we characterize the rank properties and inter user correlation of the cascaded BS IRS user channel. We further derive a closed form favorable propagation metric that reveals how the sparse array geometry and the IRS position can be tuned to reduce inter user channel correlation. The resulting geometry driven deployment rule provides a simple guideline for creating a favorable propagation environment with enhanced effective degrees of freedom. The favorable channel statistics induced by our deployment criterion enable a low complexity maximum ratio transmission MRT precoding scheme. This serves as the foundation for an efficient algorithm that jointly optimizes the IRS phase shifts and power allocation based solely on long term statistical channel state information CSI. Simulation results validate the effectiveness of our deployment criterion and demonstrate that our optimization framework achieves significant performance gains over benchmark schemes.

</details>


### [57] [Performance Bounds of Joint Detection with Kalman Filtering and Channel Decoding for Wireless Networked Control Systems](https://arxiv.org/abs/2601.07322)
*Jinnan Piao,Dong Li,Zhibo Li,Ming Yang,Xueting Yu,Jincheng Dai*

Main category: cs.IT

TL;DR: 该论文将联合检测视为MAP解码，推导了考虑系统干扰、量化间隔和权重分布的成对错误概率上下界，并通过无限状态马尔可夫链分析控制系统的连续丢包，最终获得MAP性能界限。


<details>
  <summary>Details</summary>
Motivation: 传统联合检测使用卡尔曼滤波估计控制输出的先验概率来辅助信道解码，但需要更精确的性能界限分析。本文旨在为联合检测建立理论性能界限，特别是在考虑系统干扰、量化间隔和权重分布的情况下。

Method: 1. 将联合检测视为最大后验概率解码；2. 基于成对错误概率推导上下界，考虑系统干扰、量化间隔和权重分布；3. 推导SNR趋于无穷大且系统干扰趋于零时的极限界；4. 构建无限状态马尔可夫链描述控制系统的连续丢包；5. 将MAP界近似为从无丢包状态到连续单丢包状态的转移概率界限。

Result: 仿真结果显示：(64,16)极化码和16位CRC的MAP性能随着SNR增加与极限上界一致；在块错误率10^-3时，相比有限块速率的正态近似有3.0dB的性能增益。

Conclusion: 本文成功为联合检测建立了理论性能界限框架，通过马尔可夫链建模和极限分析，证明了所提方法在控制系统中具有显著性能优势，为实际系统设计提供了理论指导。

Abstract: The joint detection uses Kalman filtering (KF) to estimate the prior probability of control outputs to assist channel decoding. In this paper, we regard the joint detection as maximum a posteriori (MAP) decoding and derive the lower and upper bounds based on the pairwise error probability considering system interference, quantization interval, and weight distribution. We first derive the limiting bounds as the signal-to-noise ratio (SNR) goes to infinity and the system interference goes to zero. Then, we construct an infinite-state Markov chain to describe the consecutive packet losses of the control systems to derive the MAP bounds. Finally, the MAP bounds are approximated as the bounds of the transition probability from the state with no packet loss to the state with consecutive single packet loss. The simulation results show that the MAP performance of $\left(64,16\right)$ polar code and 16-bit CRC coincides with the limiting upper bound as the SNR increases and has $3.0$dB performance gain compared with the normal approximation of the finite block rate at block error rate $10^{-3}$.

</details>


### [58] [On the Extremal Source Key Rates for Secure Storage over Graphs](https://arxiv.org/abs/2601.07340)
*Zhou Li*

Main category: cs.IT

TL;DR: 论文研究了图上的安全存储编码，其中多个独立源符号根据边级正确性和安全性约束在图的节点上编码存储。对于每条边，其两个相邻节点必须能够恢复指定的源符号子集，同时不泄露其余源符号的任何信息。为了满足安全性要求，可以使用共享源密钥。源符号大小与源密钥大小的比率定义了源密钥率，所有可实现速率的上确界称为源密钥容量。


<details>
  <summary>Details</summary>
Motivation: 研究图上的安全存储编码问题，其中源符号在图的节点上编码存储，需要满足边级的正确性（特定源符号可恢复）和安全性（其他源符号保密）约束。这种模型适用于分布式存储系统，其中数据需要在多个节点间安全存储和共享。

Method: 采用图论方法分析安全存储编码问题。首先研究每条边关联单个源符号的情况，刻画源密钥容量等于1的所有图。然后将结果推广到每条边关联多个源符号的情况，识别在温和结构条件下达到相应极值容量的一大类图。此外，还刻画了无需使用任何源密钥即可实现安全存储的所有图。

Result: 1. 对于每条边关联单个源符号的情况，完全刻画了源密钥容量等于1的所有图。
2. 对于每条边关联多个源符号的情况，识别了一大类在温和结构条件下达到相应极值容量的图。
3. 刻画了无需使用任何源密钥即可实现安全存储的所有图。

Conclusion: 论文通过图论方法系统研究了安全存储编码中的源密钥容量极值问题，为不同设置下的图结构提供了完整的刻画。这些结果为分布式安全存储系统的设计提供了理论基础，特别是关于何时需要以及需要多少密钥资源来实现所需的安全存储功能。

Abstract: This paper investigates secure storage codes over graphs, where multiple independent source symbols are encoded and stored at graph nodes subject to edge-wise correctness and security constraints. For each edge, a specified subset of source symbols must be recoverable from its two incident nodes, while no information about the remaining sources is revealed. To meet the security requirement, a shared source key may be employed. The ratio between the source symbol size and the source key size defines the source key rate, and the supremum of all achievable rates is referred to as the source key capacity.
  We study extremal values of the source key capacity in secure storage systems and provide complete graph characterizations for several fundamental settings. For the case where each edge is associated with a single source symbol, we characterize all graphs whose source key capacity equals one. We then generalize this result to the case where each edge is associated with multiple source symbols and identify a broad class of graphs that achieve the corresponding extremal capacity under a mild structural condition. In addition, we characterize all graphs for which secure storage can be achieved without using any source key.

</details>


### [59] [Fast and Provable Nonconvex Robust Matrix Completion](https://arxiv.org/abs/2601.07355)
*Yichen Fu,Tianming Wang,Ke Wei*

Main category: cs.IT

TL;DR: 本文提出了一种名为ARMC的高效非凸鲁棒矩阵补全方法，通过引入子空间投影改进奇异值阈值方法，在理论和实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 鲁棒矩阵补全问题在实际应用中非常重要，但现有方法在同时处理稀疏异常值和随机噪声时存在理论保证不足或计算效率低的问题。

Method: 提出ARMC方法，在更新低秩部分时引入子空间投影到奇异值阈值方法中，形成高效的非凸优化算法。

Result: 在合成数据和真实数据上的数值实验表明ARMC优于现有非凸RMC方法；理论分析基于留一法技术，建立了对稀疏异常值和随机噪声的理论保证。

Conclusion: ARMC方法在样本复杂度和异常值稀疏性方面建立了比凸方法更好的理论界限，同时保持了计算效率，为鲁棒矩阵补全提供了有效的解决方案。

Abstract: This paper studies the robust matrix completion problem and a computationally efficient non-convex method called ARMC has been proposed. This method is developed by introducing subspace projection to a singular value thresholding based method when updating the low rank part. Numerical experiments on synthetic and real data show that ARMC is superior to existing non-convex RMC methods. Through a refined analysis based on the leave-one-out technique, we have established the theoretical guarantee for ARMC subject to both sparse outliers and stochastic noise. The established bounds for the sample complexity and outlier sparsity are better than those established for a convex approach that also considers both outliers and stochastic noise.

</details>


### [60] [Novel Decoding Algorithm for Noiseless Non-Adaptive Group Testing](https://arxiv.org/abs/2601.07388)
*Manuel Franco-Vivo*

Main category: cs.IT

TL;DR: 提出W-SCOMP算法，提升无噪声非自适应群组检测性能，理论证明和实验验证其优于现有方法


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了非自适应群组检测的时间效率优势，需要提升现有方案的性能，特别是在无噪声条件下最大化成功识别缺陷项的概率

Method: 提出加权顺序组合正交匹配追踪算法（W-SCOMP），改进现有检测程序效率；建立模拟框架进行对比评估

Result: 理论证明W-SCOMP在无噪声非自适应群组检测中优于其他算法；实验验证与理论结果一致

Conclusion: 扩展了解码算法范围，增进了对无噪声非自适应群组检测的理解，为实际应用提供了更有效的工具

Abstract: Group testing enables the identification of a small subset of defective items within a larger population by performing tests on pools of items rather than on each item individually. Over the years, it has not only attracted attention from the academic community, but has also demonstrated its potential in addressing real-world problems such as infectious disease screening, drug discovery and manufacturing quality control. With the emergence of the COVID-19 pandemic, interest in group testing has grown further, particularly in non-adaptive testing, due to its time efficiency compared to adaptive approaches. This highlights the importance of improving the performance currently achievable in such a scheme. This article focuses on advancing the field of noiseless non-adaptive group testing. The main objective of this work is to study and maximize the probability of successfully identifying the subset of defective items while performing as few tests as possible. To this end, we first note current well-known decoding algorithms, as well as established test design strategies for assigning items to pools. From this review, we identify key opportunities for improvement that inform the development of new decoding algorithms. Specifically, we propose a novel method, Weighted Sequential Combinatorial Orthogonal Matching Pursuit (W-SCOMP), to enhance the efficiency of existing detection procedures. Theoretical results demonstrate that W-SCOMP outperforms other algorithms in noiseless non-adaptive group testing. Furthermore, we develop a simulation framework to model the group testing process and conduct comparative evaluations between the proposed and existing algorithms. The empirical results are consistent with the theoretical findings. Overall, our work expands the range of available decoding algorithms and contributes to the broader understanding of noiseless non-adaptive group testing.

</details>


### [61] [Center-Fed Pinching Antenna System (C-PASS) Aided Wireless Communications](https://arxiv.org/abs/2601.07424)
*Xu Gan,Yuanwei Liu*

Main category: cs.IT

TL;DR: 中心馈电夹持天线系统（C-PASS）通过可控功率分裂实现双自由度，提出PS、DS、TS三种协议，分别优化波束成形，在低功率时TS更优，高功率时PS和DS因增强自由度而表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统PASS系统的自由度有限，需要设计新的天线架构来提升性能。C-PASS通过将波导馈电信号分成两个传播方向，实现了双倍自由度，为通信系统提供更强的灵活性。

Method: 提出C-PASS基本信号模型，设计三种操作协议：功率分裂（PS）、方向切换（DS）和时间切换（TS）。针对每种协议，建立和速率最大化问题，联合优化发射和夹持波束成形：1）PS采用加权最小均方误差重构和交替优化；2）DS扩展上述方法，使用基于惩罚的算法处理混合整数约束；3）TS将问题分解为两个子问题，采用迭代技术求解，并推导出最优时间分配比的闭式解。

Result: 数值结果表明：在低功率区域，TS协议表现更优；在高功率区域，PS和DS协议由于增强的自由度而实现显著更高的速率。三种协议在不同功率条件下各有优势。

Conclusion: C-PASS架构通过双自由度设计显著提升了系统性能。提出的三种协议在不同功率条件下各有优势，为实际应用提供了灵活的选择方案。该研究为下一代无线通信系统的天线设计提供了新思路。

Abstract: The novel architecture of the center-fed pinching antenna system (C-PASS) is investigated, where the waveguide-fed signal is divided into two propagation directions through controllable power splitting. By doing so, a doubled degree of freedom (DoF) is achieved compared to conventional PASS. Based on the new designed basic signal model of C-PASS, three practical operating protocols for C-PASS are proposed, namely power splitting (PS), direction switching (DS), and time switching (TS). Then, the sum-rate maximization problem for the joint optimization of transmit and pinching beamforming is formulated for each of the proposed protocols. 1) For PS, the highly coupled non-convex problem is first transformed into a tractable form via the weighted minimum mean square error reformulation and solved using the alternating optimization framework; 2) For DS, the above approach is subsequently extended to solve the mixed-integer constraints inherent for DS via the penalty-based algorithm; 3) For TS, the optimization problem can be decomposed into two subproblems and solved using the similar iterative techniques, while its optimal time allocation ratio is derived in closed form. Finally, numerical results reveal that TS is superior in the low-power regime, while PS and DS achieve significantly higher rates in the high-power regime due to the enhanced DoF.

</details>


### [62] [Secure Joint Source-Channel Coding for the AWGN Channel with Feedback: A Finite Blocklength Analysis](https://arxiv.org/abs/2601.07472)
*Sheng Su,Yuhan Yang,Chao Qi,Xuan He,Bin Dai,Xiaohu Tang*

Main category: cs.IT

TL;DR: 本文研究AWGN窃听信道在有限码长下的反馈方案，证明经典SK方案非最优，提出改进方案并建立有限码长逆定理


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，在无限码长下，AWGN窃听信道在无噪声反馈下的保密容量等于无保密约束时的容量，且经典SK方案能达到保密容量。但在实际有限码长场景中，经典SK方案是否最优尚不清楚，需要研究有限码长下的优化方案和性能界限。

Method: 1) 证明经典SK方案在有限码长下非最优；2) 提出改进的SK方案；3) 建立AWGN窃听信道反馈模型的有限码长逆定理，该逆定理也可应用于无保密约束的情况

Result: 1) 经典SK方案在有限码长下非最优；2) 提出的改进SK方案性能优于经典方案；3) 建立了首个AWGN窃听信道反馈模型的有限码长逆定理；4) 通过数值算例验证了结果

Conclusion: 本文首次解决了AWGN窃听信道在有限码长下的反馈方案优化问题，证明了经典SK方案的局限性，提出了改进方案并建立了理论界限，为实际有限码长通信系统的设计提供了理论指导。

Abstract: In the literature, it has been shown that the secrecy capacity of the additive white Gaussian noise (AWGN) wiretap channel with noise-free feedback equals the capacity of the same model without secrecy constraint, and the classical Schalkwijk-Kailath (SK) scheme achieves the secrecy capacity. In this paper, we show that in finite blocklength regime, the SK scheme is not optimal, and propose a modified SK scheme which may perform better than the classical one. Besides this, this paper establishes a finite blocklength converse for the AWGN wiretap channel with feedback, which can also be viewed as a converse for the same model without secrecy constraint. To the best of the authors' knowledge, this is the first paper to address such a problem, and the results of this paper are further explained via numerical examples.

</details>


### [63] [Frequency-Adaptive Multi-Band Architecture for Upper Mid-Band MIMO Systems](https://arxiv.org/abs/2601.07489)
*Emiel Vanspranghels,Zhuangzhuang Cui,Sofie Pollin*

Main category: cs.IT

TL;DR: 本文评估了FR3频段（7-24GHz）在6G中的传播和MIMO特性，并提出了一种频率自适应多频段MIMO架构，通过动态资源重分配在频谱增益和MIMO增益之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: FR3频段（上中频段）作为6G的潜在频谱，其传播和MIMO特性随频率和环境变化显著，且频谱可用性可能因现有用户而间歇中断，需要深入研究和适应性解决方案。

Method: 使用特定场景的射线追踪（Sionna RT）在典型室内外场景中评估7、10、14、20和24GHz频段的SISO和MIMO配置，并提出基于开关切换的全数字频率自适应多频段MIMO架构。

Result: FR3表现出介于sub-6GHz和毫米波之间的传播特性，支持有意义的空间复用但具有强烈的场景依赖性；自适应架构在频谱不可用或复用增益集中在特定频率时特别有益。

Conclusion: FR3频段是6G的有前景频谱，提出的频率自适应多频段MIMO架构能够动态权衡带宽和天线整合，在频谱可用性和信道约束下优化系统性能。

Abstract: FR3 ($\approx$7-24 GHz), also referred to as the upper mid-band, has recently emerged as promising spectrum for 6G; however, its propagation and MIMO characteristics vary significantly with frequency and environment, and spectrum availability may be intermittent due to incumbents. Using site-specific ray tracing (Sionna RT) in representative indoor and outdoor scenarios, we evaluate 7, 10, 14, 20, and 24 GHz under SISO and MIMO configurations. The results show that FR3 exhibits propagation characteristics intermediate between sub-6 GHz and mmWave bands while supporting meaningful spatial multiplexing, albeit with strong site dependence. Motivated by these findings, we propose a fully digital frequency-adaptive multi-band MIMO architecture that repurposes ADCs/DACs and baseband processing resources across FR3 subbands via switching, enabling dynamic trade-offs between bandwidth (spectrum gain) and antenna consolidation (MIMO gain) under availability and channel constraints. Simulation results demonstrate that exploiting additional spectrum is often optimal, while adaptive resource repurposing becomes beneficial when subbands are unavailable or when multiplexing gains are concentrated at specific frequencies.

</details>


### [64] [A Parity-Consistent Decomposition Method for the Weight Distribution of Pre-Transformed Polar Codes](https://arxiv.org/abs/2601.07515)
*Yang Liu,Bolin Wu,Yuxin Han,Kai Niu*

Main category: cs.IT

TL;DR: 提出基于奇偶一致分解(PCD)的高效算法，用于计算预变换极化码的汉明重量分布，通过构建扩展信息集和等价类理论降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 预变换极化码中预变换矩阵引入了比特依赖性，使得传统的重量分布计算方法复杂度高，需要更高效的算法。

Method: 1) 提出迭代算法构建扩展信息集，通过将信息比特扩展为0和1消除相关性，使用PCD方法递归计算重量分布；2) 建立预变换极化码的等价类理论，选择最小扩展信息集大小的预变换矩阵优化计算。

Result: 数值结果表明，相比现有确定性算法，所提方法显著降低了计算复杂度。

Conclusion: 该算法通过扩展信息集和等价类理论，有效解决了预变换极化码重量分布计算的高复杂度问题，为极化码设计提供了高效分析工具。

Abstract: This paper introduces an efficient algorithm based on the Parity-Consistent Decomposition (PCD) method to determine the WD of pre-transformed polar codes. First, to address the bit dependencies introduced by the pre-transformation matrix, we propose an iterative algorithm to construct an \emph{Expanded Information Set}. By expanding the information bits within this set into 0s and 1s, we eliminate the correlations among information bits, thereby enabling the recursive calculation of the Hamming weight distribution using the \emph{PCD method}. Second, to further reduce computational complexity, we establish the theory of equivalence classes for pre-transformed polar codes. Codes within the same equivalence class share an identical weight distribution but correspond to different \emph{Expanded Information Set} sizes. By selecting the pre-transformation matrix that minimizes the \emph{Expanded Information Set} size within an equivalence class, we optimize the computation process. Numerical results demonstrate that the proposed method significantly reduces computational complexity compared to existing deterministic algorithms.

</details>


### [65] [Sparse Point-wise Privacy Leakage: Mechanism Design and Fundamental Limits](https://arxiv.org/abs/2601.07523)
*Amirreza Zamani,Sajad Daei,Parastoo Sadeghi,Mikael Skoglund*

Main category: cs.IT

TL;DR: 论文研究信息论隐私机制设计，提出稀疏点式隐私泄露准则，在高隐私场景下将设计问题转化为带ℓ₀约束的稀疏二次优化，并通过SDP松弛提供多项式时间可解方案。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保护敏感数据X的同时，从与之相关的有用数据Y中生成披露数据U。现有方法可能无法有效控制最坏情况下的隐私泄露，需要设计既能保护隐私又能保持数据效用的机制。

Method: 引入稀疏点式隐私泄露准则，包含两个约束：每个披露符号u最多与N个X实现相关，且总泄露量有界。在高隐私场景下，使用信息几何概念获得互信息的局部二次近似，将设计问题转化为带ℓ₀约束的稀疏二次最大化问题。提出SDP松弛和舍入算法提供多项式时间可解方案。

Result: 证明了对于近似问题，可以无损地将最优解限制为均匀分布的二元释放变量U。对于小字母表，可以通过组合枚举计算精确最优解；对于高维情况，提出的SDP松弛在多项式时间内可解，并识别了稀疏阈值，超过该阈值稀疏最优解饱和于无约束谱值且SDP松弛变紧。

Conclusion: 该研究为信息论隐私机制设计提供了新的稀疏隐私准则和高效算法框架，通过SDP松弛解决了NP-hard的稀疏优化问题，在高隐私场景下实现了隐私保护和数据效用的平衡。

Abstract: We study an information-theoretic privacy mechanism design problem, where an agent observes useful data $Y$ that is arbitrarily correlated with sensitive data $X$, and design disclosed data $U$ generated from $Y$ (the agent has no direct access to $X$). We introduce \emph{sparse point-wise privacy leakage}, a worst-case privacy criterion that enforces two simultaneous constraints for every disclosed symbol $u\in\mathcal{U}$: (i) $u$ may be correlated with at most $N$ realizations of $X$, and (ii) the total leakage toward those realizations is bounded. In the high-privacy regime, we use concepts from information geometry to obtain a local quadratic approximation of mutual information which measures utility between $U$ and $Y$. When the leakage matrix $P_{X|Y}$ is invertible, this approximation reduces the design problem to a sparse quadratic maximization, known as the Rayleigh-quotient problem, with an $\ell_0$ constraint. We further show that, for the approximated problem, one can without loss of optimality restrict attention to a binary released variable $U$ with a uniform distribution. For small alphabet sizes, the exact sparsity-constrained optimum can be computed via combinatorial support enumeration, which quickly becomes intractable as the dimension grows. For general dimensions, the resulting sparse Rayleigh-quotient maximization is NP-hard and closely related to sparse principal component analysis (PCA). We propose a convex semidefinite programming (SDP) relaxation that is solvable in polynomial time and provides a tractable surrogate for the NP-hard design, together with a simple rounding procedure to recover a feasible leakage direction. We also identify a sparsity threshold beyond which the sparse optimum saturates at the unconstrained spectral value and the SDP relaxation becomes tight.

</details>


### [66] [Estimators for Substitution Rates in Genomes from Read Data](https://arxiv.org/abs/2601.07546)
*Shiv Pratap Singh Rathore,Navin Kashyap*

Main category: cs.IT

TL;DR: 提出从噪声测序读数中估计序列间突变率的方法，扩展了现有仅适用于完整序列的无比对方法


<details>
  <summary>Details</summary>
Motivation: 现有无比对方法通常假设能直接访问完整序列，但在实际测序场景中只能观察到噪声读数，需要解决这一差距

Method: 使用简单模型（突变和测序错误均为替换），提出多个估计器，为其中一个提供理论保证，其他通过模拟评估

Result: 通过模拟评估验证了所提估计器的性能

Conclusion: 成功将突变率估计方法扩展到实际测序框架，为从噪声读数中估计序列差异提供了实用工具

Abstract: We study the problem of estimating the mutation rate between two sequences from noisy sequencing reads. Existing alignment-free methods typically assume direct access to the full sequences. We extend these methods to the sequencing framework, where only noisy reads from the sequences are observed. We use a simple model in which both mutations and sequencing errors are substitutions. We propose multiple estimators, provide theoretical guarantees for one of them, and evaluate the others through simulations.

</details>


### [67] [On the Sequence Reconstruction Problem for the Single-Deletion Two-Substitution Channel](https://arxiv.org/abs/2601.07547)
*Wentu Song,Kui Cai,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 研究了单删除双替换信道下的序列重建问题，证明了当两个q元长度为n的序列汉明距离d≥2时，其错误球交集大小的上界为(q²-1)n²-(3q²+5q-5)n+O_q(1)，且该上界在常数项内是紧的。


<details>
  <summary>Details</summary>
Motivation: 现有序列重建研究主要关注单一错误类型（插入、删除或替换），而对混合错误类型信道（如同时允许删除和替换）的研究相对较少。本文旨在填补这一空白，研究单删除双替换信道下的序列重建问题。

Method: 研究单删除双替换信道（允许一个删除和最多两个替换）下的序列重建问题。通过分析两个q元长度n序列的汉明距离d≥2时，其错误球交集的大小，推导出上界表达式。

Result: 证明了当两个q元长度n序列的汉明距离d≥2时，其错误球交集大小的上界为(q²-1)n²-(3q²+5q-5)n+O_q(1)，其中O_q(1)是与n无关但依赖于q的常数。同时证明该上界在常数项内是紧的。

Conclusion: 本文为混合错误类型信道（单删除双替换）下的序列重建问题提供了理论分析，确定了错误球交集大小的精确上界，这对于理解序列重建所需的最小错误副本数量具有重要意义。

Abstract: The Levenshtein sequence reconstruction problem studies the reconstruction of a transmitted sequence from multiple erroneous copies of it. A fundamental question in this field is to determine the minimum number of erroneous copies required to guarantee correct reconstruction of the original sequence. This problem is equivalent to determining the maximum possible intersection size of two error balls associated with the underlying channel. Existing research on the sequence reconstruction problem has largely focused on channels with a single type of error, such as insertions, deletions, or substitutions alone. However, relatively little is known for channels that involve a mixture of error types, for instance, channels allowing both deletions and substitutions. In this work, we study the sequence reconstruction problem for the single-deletion two-substitution channel, which allows one deletion and at most two substitutions applied to the transmitted sequence. Specifically, we prove that if two $q$-ary length-$n$ sequences have the Hamming distance $d\geq 2$, where $q\geq 2$ is any fixed integer, then the intersection size of their error balls under the single-deletion two-substitution channel is upper bounded by $(q^2-1)n^2-(3q^2+5q-5)n+O_q(1)$, where $O_q(1)$ is a constant independent from $n$ but dependent on $q$. Moreover, we show that this upper bound is tight up to an additive constant.

</details>


### [68] [A $q$-Polymatroid Framework for Information Leakage in Secure Linear Network Coding](https://arxiv.org/abs/2601.07567)
*Eimear Byrne,Johan Vester Dinesen,Ragnar Freij-Hollanti,Camilla Hollanti*

Main category: cs.IT

TL;DR: 研究基于嵌套秩度量码的安全线性网络编码方案中的信息泄露问题，揭示了信息泄露量与关联q-多拟阵的条件秩函数之间的关系，并推广了经典编码理论中的一些结果到秩度量设置。


<details>
  <summary>Details</summary>
Motivation: 研究安全线性网络编码方案中的信息泄露问题，特别是当攻击者观察到网络链路子集时，需要量化信息泄露量并建立理论框架来分析安全性能。

Method: 使用嵌套秩度量码构建安全线性网络编码方案，通过关联的q-多拟阵的条件秩函数来表征信息泄露量，引入q-多拟阵端口和q-访问结构的概念，并推广Massey对应和Brickell-Davenport定理到秩度量设置。

Result: 证明了信息泄露量由底层秩度量码对所关联的可表示q-多拟阵的条件秩函数刻画，建立了q-多拟阵端口和q-访问结构的理论框架，并证明了秩度量设置下的q-模拟Brickell-Davenport定理。

Conclusion: 该研究为安全线性网络编码方案的信息泄露分析提供了理论框架，将经典编码理论中的关键结果推广到秩度量设置，为设计更安全的网络编码方案奠定了基础。

Abstract: We study information leakage in secure linear network coding schemes based on nested rank-metric codes. We show that the amount of information leaked to an adversary that observes a subset of network links is characterized by the conditional rank function of a representable $q$-polymatroid associated with the underlying rank-metric code pair. Building on this connection, we introduce the notions of $q$-polymatroid ports and $q$-access structures and describe their structural properties. Moreover, we extend Massey's correspondence between minimal codewords and minimal access sets to the rank-metric setting and prove a $q$-analogue of the Brickell--Davenport theorem.

</details>


### [69] [Clipped Affine Policy: Low-Complexity Near-Optimal Online Power Control for Energy Harvesting Communications over Fading Channels](https://arxiv.org/abs/2601.07622)
*Hao Wu,Shengtian Yang,Huiguo Gao,Diao Wang,Jun Chen,Guanding Yu*

Main category: cs.IT

TL;DR: 该论文研究了无线衰落信道中能量收集通信的在线功率控制，提出了基于线性策略的相对值函数近似方法，得到了乐观和鲁棒两种裁剪仿射功率控制策略，并设计了结合领域知识的强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究能量收集通信系统中的在线功率控制问题，传统方法计算复杂度高，难以实时实现。需要开发既保持接近最优性能又具有低计算复杂度的在线功率控制策略。

Method: 1) 推导贝尔曼方程中相对值函数的线性策略近似；2) 提出两种裁剪仿射功率控制策略（乐观和鲁棒），形式为电池水平和信道信噪比系数倒数的裁剪仿射函数；3) 设计结合领域知识的强化学习算法；4) 扩展到具有能量和/或信道前瞻的场景。

Result: 提出的方法在计算复杂度和最优性之间取得了良好平衡。鲁棒裁剪仿射策略（结合强化学习，最多使用五个参数）在各种场景下优于所有现有方法，相对于最优策略的性能损失小于2%。

Conclusion: 该研究为能量收集通信系统提供了有效的在线功率控制解决方案，提出的裁剪仿射策略和领域知识增强的强化学习算法能够实现接近最优的性能，同时保持较低的计算复杂度，适用于实际系统实现。

Abstract: This paper investigates online power control for point-to-point energy harvesting communications over wireless fading channels. A linear-policy-based approximation is derived for the relative-value function in the Bellman equation of the power control problem. This approximation leads to two fundamental power control policies: optimistic and robust clipped affine policies, both taking the form of a clipped affine function of the battery level and the reciprocal of channel signal-to-noise ratio coefficient. They are essentially battery-limited weighted directional waterfilling policies operating between adjacent time slots. By leveraging the relative-value approximation and derived policies, a domain-knowledge-enhanced reinforcement learning (RL) algorithm is proposed for online power control. The proposed approach is further extended to scenarios with energy and/or channel lookahead. Comprehensive simulation results demonstrate that the proposed methods achieve a good balance between computational complexity and optimality. In particular, the robust clipped affine policy (combined with RL, using at most five parameters) outperforms all existing approaches across various scenarios, with less than 2\% performance loss relative to the optimal policy.

</details>


### [70] [New $X$-Secure $T$-Private Information Retrieval Schemes via Rational Curves and Hermitian Curves](https://arxiv.org/abs/2601.07676)
*Yuan Gao,Weijun Fang,Jingke Xu,Jiejing Wen*

Main category: cs.IT

TL;DR: 本文提出了一种新的XSTPIR方案构建方法，通过提高已有曲线上有理点的利用效率，而非追求更高亏格和更多有理点的曲线，从而获得更高的最大PIR率。


<details>
  <summary>Details</summary>
Motivation: 现有XSTPIR方案主要关注寻找具有更高亏格和更多有理点的曲线（如Hermitian曲线），但本文作者认为可以通过提高已有曲线上有理点的利用效率来获得更好的性能。

Method: 引入多项式空间span{F_q}{1,x,...,x^{k-1}}的一组新基替代拉格朗日插值基，基于有理曲线和Hermitian曲线分别构建了两个新的XSTPIR方案族。

Result: 新方案在参数比较中表现更优：基于Hermitian曲线的方案在q^2≥14^2且X+T≥4q时提供已知最大PIR率；在q^2≥28^2且X+T≥4时，两个方案共同提供已知最大PIR率。

Conclusion: 通过提高有理点利用效率而非追求更高亏格曲线，可以构建性能更优的XSTPIR方案，为XSTPIR方案设计提供了新的思路。

Abstract: $X$-secure and $T$-private information retrieval (XSTPIR) is a variant of private information retrieval where data security is guaranteed against collusion among up to $X$ servers and the user's retrieval privacy is guaranteed against collusion among up to $T$ servers. Recently, researchers have constructed XSTPIR schemes through the theory of algebraic geometry codes and algebraic curves, with the aim of obtaining XSTPIR schemes that have higher maximum PIR rates for fixed field size and $X,T$ (the number of servers $N$ is not restricted). The mainstream approach is to employ curves of higher genus that have more rational points, evolving from rational curves to elliptic curves to hyperelliptic curves and, most recently, to Hermitian curves.
  In this paper, we propose a different perspective: with the shared goal of constructing XSTPIR schemes with higher maximum PIR rates, we move beyond the mainstream approach of seeking curves with higher genus and more rational points. Instead, we aim to achieve this goal by enhancing the utilization efficiency of rational points on curves that have already been considered in previous work. By introducing a family of bases for the polynomial space $\text{span}_{\mathbb{F}_q}\{1,x,\dots,x^{k-1}\}$ as an alternative to the Lagrange interpolation basis, we develop two new families of XSTPIR schemes based on rational curves and Hermitian curves, respectively. Parameter comparisons demonstrate that our schemes achieve superior performance. Specifically, our Hermitian-curve-based XSTPIR scheme provides the largest known maximum PIR rates when the field size $q^2\geq 14^2$ and $X+T\geq 4q$. Moreover, for any field size $q^2\geq 28^2$ and $X+T\geq 4$, our two XSTPIR schemes collectively provide the largest known maximum PIR rates.

</details>


### [71] [Weak Composition Lattices and Ring-Linear Anticodes](https://arxiv.org/abs/2601.07725)
*Jessica Bariffi,Drisana Bhatia,Giuseppe Cotardo,Violetta Weger*

Main category: cs.IT

TL;DR: 研究环Z/p^sZ上的Lee度量最优反码，建立了反码格与弱组合格之间的双射，并应用于Lee度量码的新不变量


<details>
  <summary>Details</summary>
Motivation: 格论和偏序集在编码理论中日益重要，为研究纠错码的结构和代数性质提供了组合框架。受近期连接格论、反码和编码理论不变量的研究启发，研究具有Lee度量的环线性码

Method: 引入并刻画环Z/p^sZ上的最优Lee度量反码，证明这类反码族可自然划分为子类型并形成包含格，建立该格与按支配序的弱组合格之间的双射

Result: 建立了Lee度量反码格与弱组合格之间的对应关系，通过反码方法为Lee度量码引入了新的不变量

Conclusion: 成功将格论和反码理论应用于环上的Lee度量码研究，建立了新的组合对应关系，为Lee度量码的分析提供了新的工具和不变量的框架

Abstract: Lattices and partially ordered sets have played an increasingly important role in coding theory, providing combinatorial frameworks for studying structural and algebraic properties of error-correcting codes. Motivated by recent works connecting lattice theory, anticodes, and coding-theoretic invariants, we investigate ring-linear codes endowed with the Lee metric. We introduce and characterize optimal Lee-metric anticodes over the ring $\mathbb{Z}/p^s\mathbb{Z}$. We show that the family of such anticodes admits a natural partition into subtypes and forms a lattice under inclusion. We establish a bijection between this lattice and a lattice of weak compositions ordered by dominance. As an application, we use this correspondence to introduce new invariants for Lee-metric codes via an anticode approach.

</details>


### [72] [Lossy Source Coding with Broadcast Side Information](https://arxiv.org/abs/2601.07797)
*Yiqi Chen,Holger Boche,Marc Geitz*

Main category: cs.IT

TL;DR: 论文研究了带有广播边信息的信源编码问题，边信息通过有噪广播信道发送给两个接收器，给出了速率-失真-带宽四元组的外界和分离式方案的可达区域，并在二次高斯情况下比较了分离式方案与未编码方案。


<details>
  <summary>Details</summary>
Motivation: 研究广播边信息场景下的信源编码问题，其中边信息通过有噪广播信道传输，这种设置在实际通信系统中很常见，需要分析不同编码方案在速率、失真和带宽约束下的性能。

Method: 提出了速率-失真-带宽四元组的外界，给出了分离式方案的可达区域，并针对一些特殊情况提供了完整表征，最后在二次高斯情况下比较了分离式方案与未编码方案的性能。

Result: 建立了广播边信息信源编码问题的理论框架，给出了外界和可达区域，在特殊情况下获得了完整表征，并在二次高斯场景下展示了不同方案的性能比较。

Conclusion: 论文为广播边信息信源编码问题提供了理论基础和性能分析，分离式方案在某些情况下表现良好，但在二次高斯场景中与未编码方案相比存在性能差异，为实际系统设计提供了指导。

Abstract: This paper considers the source coding problem with broadcast side information. The side information is sent to two receivers through a noisy broadcast channel. We provide an outer bound of the rate--distortion--bandwidth (RDB) quadruples and achievable RDB quadruples when the helper uses a separation-based scheme. Some special cases with full characterization are also provided. We then compare the separation-based scheme with the uncoded scheme in the quadratic Gaussian case.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [73] [Deep Joint Source-Channel Coding for Wireless Video Transmission with Asymmetric Context](https://arxiv.org/abs/2601.06170)
*Xuechen Chen,Junting Li,Chuang Chen,Hairong Lin,Yishen Li*

Main category: eess.IV

TL;DR: 提出基于条件编码和不对称上下文的高效深度联合信源信道编码方法，用于视频传输，通过特征传播和内容自适应编码提升性能并减少误差累积。


<details>
  <summary>Details</summary>
Motivation: 传统条件编码的神经视频压缩需要从相同上下文预测编码和解码条件，但在JSCC伪模拟传输方案中，编码器无法推断与解码器相同的重建帧，即使编码器构建了模拟传输管道。需要解决这种不对称性问题。

Method: 1) 设计神经网络从不对称上下文学习编码和解码条件；2) 引入特征传播，让中间特征在编码器和解码器独立传播，帮助生成条件；3) 实现内容自适应编码，使用熵模型和掩码机制实现可变带宽传输。

Result: 实验结果表明，该方法在性能上优于现有的深度视频传输框架，有效缓解了误差累积问题。通过减少误差累积，可以降低插入帧内编码模式的频率，进一步提升性能。

Conclusion: 提出的基于条件编码和不对称上下文的高效深度JSCC方法成功解决了视频传输中的不对称上下文问题，通过特征传播和内容自适应编码显著提升了传输性能并减少了误差累积。

Abstract: In this paper, we propose a high-efficiency deep joint source-channel coding (JSCC) method for video transmission based on conditional coding with asymmetric context. The conditional coding-based neural video compression requires to predict the encoding and decoding conditions from the same context which includes the same reconstructed frames. However in JSCC schemes which fall into pseudo-analog transmission, the encoder cannot infer the same reconstructed frames as the decoder even a pipeline of the simulated transmission is constructed at the encoder. In the proposed method, without such a pipeline, we guide and design neural networks to learn encoding and decoding conditions from asymmetric contexts. Additionally, we introduce feature propagation, which allows intermediate features to be independently propagated at the encoder and decoder and help to generate conditions, enabling the framework to greatly leverage temporal correlation while mitigating the problem of error accumulation. To further exploit the performance of the proposed transmission framework, we implement content-adaptive coding which achieves variable bandwidth transmission using entropy models and masking mechanisms. Experimental results demonstrate that our method outperforms existing deep video transmission frameworks in terms of performance and effectively mitigates the error accumulation. By mitigating the error accumulation, our schemes can reduce the frequency of inserting intra-frame coding modes, further enhancing performance.

</details>


### [74] [Real-Time Image Processing Algorithms for Embedded Systems](https://arxiv.org/abs/2601.06243)
*Soundes Oumaima Boufaida,Abdemadjid Benmachiche,Majda Maatallah*

Main category: eess.IV

TL;DR: 该研究针对嵌入式视觉系统，优化了边缘检测、角点检测和斑点检测算法，通过硬件架构协同设计和量化技术，在DSP和FPGA上实现了更快速、更节能的图像处理。


<details>
  <summary>Details</summary>
Motivation: 嵌入式视觉系统需要在资源受限的硬件上实现实时图像处理，但现有算法在延迟、精度和功耗方面存在挑战，需要优化以满足汽车、监控和机器人等应用的需求。

Method: 采用优化的算法架构和量化技术，结合帧间冗余消除和自适应帧平均方法，在DSP和FPGA等嵌入式处理器上实现边缘检测、角点检测和斑点检测算法。

Result: 仿真和硬件试验表明，相比传统实现，所提方法在处理速度和能效方面有显著提升，为嵌入式成像系统提供了可扩展且经济的解决方案。

Conclusion: 该研究展示了算法与硬件架构协同设计对实际实时嵌入式视觉应用的重要性，为汽车、监控和机器人领域的嵌入式成像系统发展提供了可行路径。

Abstract: Embedded vision systems need efficient and robust image processing algorithms to perform real-time, with resource-constrained hardware. This research investigates image processing algorithms, specifically edge detection, corner detection, and blob detection, that are implemented on embedded processors, including DSPs and FPGAs. To address latency, accuracy and power consumption noted in the image processing literature, optimized algorithm architectures and quantization techniques are employed. In addition, optimal techniques for inter-frame redundancy removal and adaptive frame averaging are used to improve throughput with reasonable image quality. Simulations and hardware trials of the proposed approaches show marked improvements in the speed and energy efficiency of processing as compared to conventional implementations. The advances of this research facilitate a path for scalable and inexpensive embedded imaging systems for the automotive, surveillance, and robotics sectors, and underscore the benefit of co-designing algorithms and hardware architectures for practical real-time embedded vision applications.

</details>


### [75] [Performance Analysis of DCT, Hadamard, and PCA in Block-Based Image Compression](https://arxiv.org/abs/2601.06273)
*Yashika Ahlawat*

Main category: eess.IV

TL;DR: PCA在图像压缩中理论上最优，但实验显示DCT在标准块大小和低码率下仍接近最优，解释了DCT在实际编解码器中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究数据驱动的PCA变换与固定变换（DCT、Hadamard）在图像压缩中的性能差异，解释为何实际编解码器仍广泛使用DCT而非理论上最优的PCA。

Method: 通过实验比较DCT、Hadamard和PCA在不同块大小和压缩率下的性能，使用率失真分析和能量压缩分析进行评估。

Result: PCA仅在块维度足够大时才优于固定变换，而DCT在标准块大小（如8×8）和低码率下仍接近最优性能。

Conclusion: DCT在实际编解码器中表现出鲁棒性，而基于块的学得变换存在局限性，解释了为何传统DCT仍在实际应用中占主导地位。

Abstract: Block based image compression relies on transform coding to concentrate signal energy into a small number of coefficients. While classical codecs use fixed transforms such as the Discrete Cosine Transform (DCT), data driven methods such as Principal Component Analysis (PCA) are theoretically optimal for decorrelation. This paper presents an experimental comparison of DCT, Hadamard, and PCA across multiple block sizes and compression rates. Using rate distortion and energy compaction analysis, we show that PCA outperforms fixed transforms only when block dimensionality is sufficiently large, while DCT remains near optimal for standard block sizes such as $8\times8$ and at low bit rates. These results explain the robustness of DCT in practical codecs and highlight the limitations of block wise learned transforms.

</details>


### [76] [R$^3$D: Regional-guided Residual Radar Diffusion](https://arxiv.org/abs/2601.06465)
*Hao Li,Xinqi Liu,Yaoqing Jin*

Main category: eess.IV

TL;DR: R3D提出区域引导的残差雷达扩散框架，通过残差扩散建模和sigma自适应区域引导，提升毫米波雷达点云质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在恶劣条件下能实现鲁棒的环境感知，但存在点云稀疏、噪声大、角分辨率低的问题。现有基于扩散的雷达增强方法要么建模完整LiDAR分布导致学习复杂度高，要么均匀区域处理无法优先处理关键结构。

Method: 提出R3D框架：1) 残差扩散建模，专注于LiDAR-雷达残差编码互补高频细节以降低学习难度；2) sigma自适应区域引导，利用雷达特定信号特性生成注意力图，仅在低噪声阶段应用轻量级引导以避免梯度不平衡，同时细化关键区域。

Result: 在ColoRadar数据集上的大量实验表明，R3D优于最先进的方法，为雷达感知增强提供了实用解决方案。

Conclusion: R3D通过结合残差扩散建模和自适应区域引导，有效解决了现有雷达增强方法的局限性，在保持低学习复杂度的同时优先处理关键结构，显著提升了毫米波雷达感知性能。

Abstract: Millimeter-wave radar enables robust environment perception in autonomous systems under adverse conditions yet suffers from sparse, noisy point clouds with low angular resolution. Existing diffusion-based radar enhancement methods either incur high learning complexity by modeling full LiDAR distributions or fail to prioritize critical structures due to uniform regional processing. To address these issues, we propose R3D, a regional-guided residual radar diffusion framework that integrates residual diffusion modeling-focusing on the concentrated LiDAR-radar residual encoding complementary high-frequency details to reduce learning difficulty-and sigma-adaptive regional guidance-leveraging radar-specific signal properties to generate attention maps and applying lightweight guidance only in low-noise stages to avoid gradient imbalance while refining key regions. Extensive experiments on the ColoRadar dataset demonstrate that R3D outperforms state-of-the-art methods, providing a practical solution for radar perception enhancement. Our anonymous code and pretrained models are released here: https://anonymous.4open.science/r/r3d-F836

</details>


### [77] [USFetal: Tools for Fetal Brain Ultrasound Compounding](https://arxiv.org/abs/2601.06726)
*Mohammad Khateri,Morteza Ghahremani,Sergio Valencia,Camilo Jaimes,Alejandra Sierra,Jussi Tohka,P. Ellen Grant,Davood Karimi*

Main category: eess.IV

TL;DR: 该论文系统综述了胎儿脑超声复合技术，提出了首个分类框架，比较了多种方法，并引入了两种新的深度学习策略，在10个数据集上进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 超声是胎儿脑成像的安全、经济、易获取的技术，但存在视角依赖伪影、操作者变异性、视野有限等问题，使得解释和定量评估困难。超声复合技术旨在通过整合多个3D采集的互补信息来克服这些限制。

Method: 1) 首次系统分类胎儿脑超声复合的计算策略，包括经典技术和现代学习框架；2) 在四个关键类别（多尺度、基于变换、变分、深度学习）中实现和比较代表性方法；3) 针对缺乏监督学习所需的全视角无伪影真实数据，专注于无监督和自监督策略，引入两种新深度学习方法：自监督复合框架和无监督深度即插即用先验的复合适应；4) 在10个多视角胎儿脑超声数据集上进行全面评估。

Result: 进行了全面的评估，包括专家放射科医生评分和标准定量图像质量指标。发布了公开可用的USFetal复合工具箱，支持基准测试和未来研究。

Conclusion: 该研究为胎儿脑超声复合提供了系统的方法论框架和评估工具，特别关注无监督和自监督学习策略，以应对缺乏真实数据的挑战，推动了该领域的研究发展。

Abstract: Ultrasound offers a safe, cost-effective, and widely accessible technology for fetal brain imaging, making it especially suitable for routine clinical use. However, it suffers from view-dependent artifacts, operator variability, and a limited field of view, which make interpretation and quantitative evaluation challenging. Ultrasound compounding aims to overcome these limitations by integrating complementary information from multiple 3D acquisitions into a single, coherent volumetric representation. This work provides four main contributions: (1) We present the first systematic categorization of computational strategies for fetal brain ultrasound compounding, including both classical techniques and modern learning-based frameworks. (2) We implement and compare representative methods across four key categories - multi-scale, transformation-based, variational, and deep learning approaches - emphasizing their core principles and practical advantages. (3) Motivated by the lack of full-view, artifact-free ground truth required for supervised learning, we focus on unsupervised and self-supervised strategies and introduce two new deep learning based approaches: a self-supervised compounding framework and an adaptation of unsupervised deep plug-and-play priors for compounding. (4) We conduct a comprehensive evaluation on ten multi-view fetal brain ultrasound datasets, using both expert radiologist scoring and standard quantitative image-quality metrics. We also release the USFetal Compounding Toolbox, publicly available to support benchmarking and future research. Keywords: Ultrasound compounding, fetal brain, deep learning, self-supervised, unsupervised.

</details>


### [78] [LaminoDiff: Artifact-Free Computed Laminography in Non-Destructive Testing via Diffusion Model](https://arxiv.org/abs/2601.07254)
*Tan Liu,Liu Shi,Binghuang Peng,Tong Jia,Xiaoling Xu,Baodong Liu,Qiegen Liu*

Main category: eess.IV

TL;DR: LaminoDiff：一种结合扩散模型与高保真先验表示的计算层析成像框架，用于抑制层间混叠伪影并提高电路结构重建精度


<details>
  <summary>Details</summary>
Motivation: 计算层析成像（CL）在大尺寸平面物体内部结构可视化中至关重要，但其扫描几何结构不可避免地产生层间混叠伪影，限制了实际应用（尤其在电子元件检测中）。虽然深度学习为伪影去除提供了强大范式，但其效果常受合成数据与真实数据之间的领域差距限制。

Method: 提出LaminoDiff框架，将扩散模型与高保真先验表示相结合，通过双模态CT-CL融合策略生成先验，并将其作为条件约束集成到网络中，确保电路结构的高精度保存和几何保真度，同时抑制伪影。

Result: 在模拟和真实PCB数据集上的大量实验表明，LaminoDiff实现了高保真重建，在伪影抑制和细节恢复方面具有竞争力。更重要的是，结果有助于实现可靠的自动缺陷识别。

Conclusion: LaminoDiff通过整合扩散模型与高保真先验表示，有效弥合了CL成像中的领域差距，为电子元件检测等应用提供了高质量的重建结果，支持可靠的自动化缺陷识别。

Abstract: Computed Laminography (CL) is a key non-destructive testing technology for the visualization of internal structures in large planar objects. The inherent scanning geometry of CL inevitably results in inter-layer aliasing artifacts, limiting its practical application, particularly in electronic component inspection. While deep learning (DL) provides a powerful paradigm for artifact removal, its effectiveness is often limited by the domain gap between synthetic data and real-world data. In this work, we present LaminoDiff, a framework to integrate a diffusion model with a high-fidelity prior representation to bridge the domain gap in CL imaging. This prior, generated via a dual-modal CT-CL fusion strategy, is integrated into the proposed network as a conditional constraint. This integration ensures high-precision preservation of circuit structures and geometric fidelity while suppressing artifacts. Extensive experiments on both simulated and real PCB datasets demonstrate that LaminoDiff achieves high-fidelity reconstruction with competitive performance in artifact suppression and detail recovery. More importantly, the results facilitate reliable automated defect recognition.

</details>


### [79] [Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring](https://arxiv.org/abs/2601.07356)
*Tatiana Gelvez-Barrera,Barbara Nicolas,Bruno Gilles,Adrian Basarab,Denis Kouamé*

Main category: eess.IV

TL;DR: 提出基于时域卷积公式的被动声学映射波束形成框架，提高时间分辨率并降低计算负担


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的波束形成算法在治疗超声应用中虽然重建质量高、物理可解释性强，但计算负担大且时间分辨率有限，难以处理随时间演变的空化活动

Method: 提出时域卷积公式的PAM波束形成框架，将PAM表述为逆问题，前向算子将空化活动的时空分布映射到记录的射频信号，考虑采集几何定义的飞行时间延迟，并开发包含空化活动先验知识的正则化反演算法

Result: 实验结果表明，该框架优于经典波束形成方法，比频域技术提供更高的时间分辨率，同时相比迭代时域公式显著降低计算负担

Conclusion: 提出的时域卷积框架为治疗超声中的空化活动表征提供了高效、高时间分辨率的PAM波束形成解决方案

Abstract: Passive acoustic mapping (PAM) is a key imaging technique for characterizing cavitation activity in therapeutic ultrasound applications. Recent model-based beamforming algorithms offer high reconstruction quality and strong physical interpretability. However, their computational burden and limited temporal resolution restrict their use in applications with time-evolving cavitation. To address these challenges, we introduce a PAM beamforming framework based on a novel convolutional formulation in the time domain, which enables efficient computation. In this framework, PAM is formulated as an inverse problem in which the forward operator maps spatiotemporal cavitation activity to recorded radio-frequency signals accounting for time-of-flight delays defined by the acquisition geometry. We then formulate a regularized inversion algorithm that incorporates prior knowledge on cavitation activity. Experimental results demonstrate that our framework outperforms classical beamforming methods, providing higher temporal resolution than frequency-domain techniques while substantially reducing computational burden compared with iterative time-domain formulations.

</details>


### [80] [Fast Multi-Stack Slice-to-Volume Reconstruction via Multi-Scale Unrolled Optimization](https://arxiv.org/abs/2601.07519)
*Margherita Firenze,Sean I. Young,Clinton J. Wang,Hyuk Jin Yun,Elfar Adalsteinsson,Kiho Im,P. Ellen Grant,Polina Golland*

Main category: eess.IV

TL;DR: 提出快速卷积框架，用于从错位的2D切片堆栈中联合重建3D解剖结构和优化切片对齐，应用于胎儿脑MRI，在10秒内完成高质量3D重建，比传统迭代方法快100倍。


<details>
  <summary>Details</summary>
Motivation: 虽然全卷积网络在医学影像中广泛应用，但在切片到体积重建（SVR）任务中潜力未充分挖掘。SVR需要从错位的2D采集图像中联合估计3D解剖结构和切片姿态，传统方法计算量大。

Method: 提出快速卷积框架，融合多个正交2D切片堆栈来恢复连贯的3D结构，并通过轻量级基于模型的优化来细化切片对齐。使用非刚性位移场表示变换，可推广到其他SVR问题。

Result: 应用于胎儿脑MRI时，在10秒内重建高质量3D体积，其中切片配准仅需1秒，精度与最先进的迭代SVR流程相当，速度提升超过100倍。框架还可应用于胎儿身体和胎盘MRI。

Conclusion: 该框架实现了快速、准确的切片到体积重建，为实时、扫描器端的体积反馈开辟了道路，具有广泛的临床应用潜力。

Abstract: Fully convolutional networks have become the backbone of modern medical imaging due to their ability to learn multi-scale representations and perform end-to-end inference. Yet their potential for slice-to-volume reconstruction (SVR), the task of jointly estimating 3D anatomy and slice poses from misaligned 2D acquisitions, remains underexplored. We introduce a fast convolutional framework that fuses multiple orthogonal 2D slice stacks to recover coherent 3D structure and refines slice alignment through lightweight model-based optimization. Applied to fetal brain MRI, our approach reconstructs high-quality 3D volumes in under 10s, with 1s slice registration and accuracy on par with state-of-the-art iterative SVR pipelines, offering more than speedup. The framework uses non-rigid displacement fields to represent transformations, generalizing to other SVR problems like fetal body and placental MRI. Additionally, the fast inference time paves the way for real-time, scanner-side volumetric feedback during MRI acquisition.

</details>
