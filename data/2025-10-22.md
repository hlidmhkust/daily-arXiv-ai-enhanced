<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 3]
- [eess.SP](#eess.SP) [Total: 22]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information](https://arxiv.org/abs/2510.17841)
*Ishir Rao*

Main category: cs.IT

TL;DR: EEG的信息容量有限，每样本仅能传递数十比特关于低维神经活动的信息，信息量在64-128个电极时饱和，且随信噪比呈对数增长。


<details>
  <summary>Details</summary>
Motivation: 量化脑电图(EEG)的信息容量，了解其从皮层源到头皮记录的信息传递能力。

Method: 结合信息论和合成前向建模，使用高斯通道理论和经验模拟来估计皮层源与EEG记录之间的互信息。

Result: 头皮EEG仅能传递数十比特/样本的信息，信息在64-128个电极时饱和，线性解码器几乎能捕获所有可线性恢复的方差，但远低于分析通道容量。

Conclusion: 测量物理特性而非算法复杂性是主要限制因素，这界定了从EEG推断大脑状态或思维内容结构的内在上限。

Abstract: Electroencephalography (EEG) is widely used to study human brain dynamics,
yet its quantitative information capacity remains unclear. Here, we combine
information theory and synthetic forward modeling to estimate the mutual
information between latent cortical sources and EEG recordings. Using
Gaussian-channel theory and empirical simulations, we find that scalp EEG
conveys only tens of bits per sample about low-dimensional neural activity.
Information saturates with approximately 64-128 electrodes and scales
logarithmically with signal-to-noise ratio (SNR). Linear decoders capture
nearly all variance that is linearly recoverable, but the mutual information
they recover remains far below the analytic channel capacity, indicating that
measurement physics - not algorithmic complexity - is the dominant limitation.
These results outline the intrinsic ceiling on how much structure about brain
state or thought content can be inferred from EEG.

</details>


### [2] [Performance of Modified Fractional Frequency Reuse Algorithm in Random Ultra Dense Networks](https://arxiv.org/abs/2510.18440)
*Bach Hung Luu,Samuel Harry Gardner,Sinh Cong Lam,Trong Minh Hoang*

Main category: cs.IT

TL;DR: 提出了一种改进的分数频率复用算法，使用服务基站与第二近基站信号功率比来分类小区边缘用户和中心用户，以减轻5G及超5G蜂窝网络中的小区间干扰。


<details>
  <summary>Details</summary>
Motivation: 在5G及超5G高密度基站网络中，传统基于下行SINR或距离的用户分类方法存在局限性，需要更有效的干扰管理方法来提升用户性能。

Method: 采用服务基站与第二近基站信号功率比作为用户分类标准，当功率比低于预设阈值时，用户被分类为小区边缘用户并获得更高的传输功率。

Result: 仿真结果表明，增加传输功率能有效提升小区边缘用户性能，但会降低典型用户性能；在障碍物密集环境中，频率复用算法能有效抑制小区间干扰。

Conclusion: 基于功率比的分数频率复用算法在密集障碍物环境中具有可行性，能有效管理小区间干扰，但需要在边缘用户和典型用户性能之间进行权衡。

Abstract: Mitigating intercell interference by employing fractional frequency reuse
algorithms is one of the important approaches to improving user performance in
5G and Beyond 5G cellular network systems, which typically have a high density
of Base Stations (BSs). While most frequency reuse algorithms are based on the
downlink Signal-to-Interference-plus-Noise Ratio (SINR) or the distance between
the user and its serving BS to classify Cell-Edge Users (CEUs) and Cell-Center
Users (CCUs), this paper discusses a modified algorithm that uses the power
ratio between the signal strengths from the serving BS and the second nearest
BS for user classification. Specifically, if the power ratio is below a
predefined threshold, the user is classified as a CEU and is served with higher
transmission power. Simulation results show that increasing transmission power
is necessary to enhance CEU performance, but it also degrades the performance
of typical users. The use of frequency reuse algorithms is particularly
feasible in environments with a high density of obstacles, where intercell
interference can be effectively suppressed.

</details>


### [3] [A Markov-Chain Characterization of Finite-State Dimension and a Generalization of Agafonov's Theorem](https://arxiv.org/abs/2510.18736)
*Laurent Bienvenu,Hugo Gimbert,Subin Pulari*

Main category: cs.IT

TL;DR: 本文扩展了有限状态维度的信息论特征，建立了序列的有限状态维度与马尔可夫链模拟中条件KL散度的关系，并推广了Agafonov定理。


<details>
  <summary>Details</summary>
Motivation: 扩展Schnorr和Stimm(1972)关于Borel正规数的对应关系，为有限状态维度提供新的信息论特征，并推广Agafonov定理到任意序列。

Method: 通过马尔可夫链模拟，使用给定序列的二进制展开，分析经验分布与平稳分布之间的条件Kullback-Leibler散度来表征有限状态维度。

Result: 建立了序列有限状态维度与马尔可夫链模拟中条件KL散度的定量关系，推广了Schnorr-Stimm结果，并证明了Agafonov定理的广义版本。

Conclusion: 为有限状态维度提供了新的信息论特征，建立了序列维度与其自动子序列维度之间的紧密定量关系，扩展了正规数理论到更一般的序列分析。

Abstract: Finite-state dimension quantifies the asymptotic rate of information in an
infinite sequence as perceived by finite automata. For a fixed alphabet, the
infinite sequences that have maximal finite-state dimension are exactly those
that are Borel normal, i.e., in which all words of any given length appear with
the same frequency. A theorem of Schnorr and Stimm (1972) shows that a real
number is Borel normal if and only if, for every finite-state irreducible
Markov chain with fair transitions, when the chain is simulated using the
binary expansion of the given number, the empirical distribution of states
converges to its stationary distribution. In this paper we extend this
correspondence beyond normal numbers. We show that the finite-state dimension
of a sequence can be characterized in terms of the conditional Kullback-Leibler
divergence between the limiting distributions arising from the simulation of
Markov chains using the given sequence and their stationary distributions. This
provides a new information-theoretic characterization of finite-state dimension
which generalizes the Schnorr-Stimm result.
  As an application, we prove a generalization of Agafonov's theorem for normal
numbers. Agafonov's theorem states that a sequence is normal if and only if
every subsequence selected by a finite automaton is also normal. We extend this
to arbitrary sequences by establishing a tight quantitative relationship
between the finite-state dimension of a sequence and the finite-state
dimensions of its automatic subsequences.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [4] [Machine Learning-Based Performance Evaluation of a Solar-Powered Hydrogen Fuel Cell Hybrid in a Radio-Controlled Electric Vehicle](https://arxiv.org/abs/2510.17808)
*Amirhesam Aghanouri,Mohamed Sabry,Joshua Cherian Varughese,Cristina Olaverri-Monreal*

Main category: eess.SP

TL;DR: 该论文研究了镍氢电池与质子交换膜燃料电池混合动力系统在遥控车上的性能表现，通过机器学习技术进行数据分析和异常检测，证明了混合系统相比纯电池系统具有更好的电压稳定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究混合动力系统在小规模电动汽车上的性能，为未来大型车辆的扩展提供基准，同时探索可再生能源在交通领域的应用潜力。

Method: 采用实验测试和数据分析相结合的方法，包括信号处理和机器学习技术（如时间卷积网络），在不同负载和环境条件下评估系统性能，并与太阳能电解槽集成验证离网可再生氢能应用。

Result: 混合系统相比纯电池系统电压稳定性更好，关键故障更少；机器学习能高精度识别油门水平；时间卷积网络成功预测电压行为；系统展示了离网可再生氢能应用的可行性。

Conclusion: 质子交换膜燃料电池与镍氢电池的集成显著改善了小型电动汽车的电气性能和可靠性，这些发现为扩展到大型车辆提供了潜在基准，并展示了可再生氢能在交通领域的应用前景。

Abstract: This paper presents an experimental investigation and performance evaluation
of a hybrid electric radio-controlled car powered by a Nickel-Metal Hydride
battery combined with a renewable Proton Exchange Membrane Fuel Cell system.
The study evaluates the performance of the system under various load-carrying
scenarios and varying environmental conditions, simulating real-world operating
conditions including throttle operation. In order to build a predictive model,
gather operational insights, and detect anomalies, data-driven analyses using
signal processing and modern machine learning techniques were employed.
Specifically, machine learning techniques were used to distinguish throttle
levels with high precision based on the operational data. Anomaly and change
point detection methods enhanced voltage stability, resulting in fewer critical
faults in the hybrid system compared to battery-only operation. Temporal
Convolutional Networks were effectively employed to predict voltage behavior,
demonstrating potential for use in planning the locations of fueling or
charging stations. Moreover, integration with a solar-powered electrolyzer
confirmed the system's potential for off-grid, renewable hydrogen use. The
results indicate that integrating a Proton Exchange Membrane Fuel Cell with
Nickel-Metal Hydride batteries significantly improves electrical performance
and reliability for small electric vehicles, and these findings can be a
potential baseline for scaling up to larger vehicles.

</details>


### [5] [In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning](https://arxiv.org/abs/2510.17809)
*Massimo Capurso,Luciano Afferrante*

Main category: eess.SP

TL;DR: 提出了一种基于振动信号分析和机器学习的齿轮强力珩磨在线监测框架，使用三种子空间学习方法进行特征提取，结合SVM分类器实现高达100%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统质量控制策略依赖后处理检测和统计过程控制，无法捕捉瞬时加工异常和实现实时缺陷检测，难以满足现代齿轮制造对NVH性能的严格要求。

Method: 通过加速度计连续采集数据，使用时频信号分析，比较三种子空间学习方法（PCA、PCA+LDA、R-UMLDA）进行特征提取，然后使用SVM分类器预测四个齿轮质量类别。

Result: 在工业环境中收集的实验数据集上，该框架实现了高分类准确率（最高达100%），提供了与工艺动态相关的可解释频谱特征。

Conclusion: 该方法可实现实时监测和预测性维护系统的实际集成，为齿轮强力珩磨过程提供有效的在线质量控制解决方案。

Abstract: In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)
requirements demand high-precision finishing operations such as power honing.
Conventional quality control strategies rely on post-process inspections and
Statistical Process Control (SPC), which fail to capture transient machining
anomalies and cannot ensure real-time defect detection. This study proposes a
novel, data-driven framework for in-process monitoring of gear power honing
using vibration signal analysis and machine learning. Our proposed methodology
involves continuous data acquisition via accelerometers, followed by
time-frequency signal analysis. We investigate and compare the efficacy of
three subspace learning methods for features extraction: (1) Principal
Component Analysis (PCA) for dimensionality reduction; (2) a two-stage
framework combining PCA with Linear Discriminant Analysis (LDA) for enhanced
class separation; and (3) Uncorrelated Multilinear Discriminant Analysis with
Regularization (R-UMLDA), adapted for tensor data, which enforces feature
decorrelation and includes regularization for small sample sizes. These
extracted features are then fed into a Support Vector Machine (SVM) classifier
to predict four distinct gear quality categories, established through rigorous
geometrical inspections and test bench results of assembled gearboxes. The
models are trained and validated on an experimental dataset collected in an
industrial context during gear power-honing operations, with gears classified
into four different quality categories. The proposed framework achieves high
classification accuracy (up to 100%) in an industrial setting. The approach
offers interpretable spectral features that correlate with process dynamics,
enabling practical integration into real-time monitoring and predictive
maintenance systems.

</details>


### [6] [Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification](https://arxiv.org/abs/2510.17810)
*Camilo Quiceno Quintero,Sandip Varkey George*

Main category: eess.SP

TL;DR: 使用非线性时间序列分析研究心电图复杂性如何随心脏病理变化，发现非线性测量能显著区分健康和患病个体，并提升机器学习分类准确率


<details>
  <summary>Details</summary>
Motivation: 心脏的复杂动态反映在心电图中，本研究旨在通过非线性时间序列分析理解心电图复杂性如何随心脏病理变化

Method: 使用PTB-XL数据集，从导联II ECG提取非线性测量，并使用Spearman相关性和互信息计算跨通道指标（导联II、V2、AVL）

Result: 几乎所有测量都显示健康和患病个体之间存在显著差异（p<.001），将复杂性量化指标加入机器学习模型后，AUC从0.86提升至0.87（非线性测量）和0.90（包括跨时间序列指标）

Conclusion: 非线性复杂性测量能有效区分心脏病理状态，并显著提升机器学习分类性能

Abstract: The complex dynamics of the heart are reflected in its electrical activity,
captured through electrocardiograms (ECGs). In this study we use nonlinear time
series analysis to understand how ECG complexity varies with cardiac pathology.
Using the large PTB-XL dataset, we extracted nonlinear measures from lead II
ECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations
and mutual information. Significant differences between diseased and healthy
individuals were found in almost all measures between healthy and diseased
classes, and between 5 diagnostic superclasses ($p<.001$). Moreover,
incorporating these complexity quantifiers into machine learning models
substantially improved classification accuracy measured using area under the
ROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90
(including cross-time series metrics).

</details>


### [7] [Channel Modeling of Satellite-to-Underwater Laser Communication Links: An Analytical-Monte Carlo Hybrid Approach](https://arxiv.org/abs/2510.17811)
*Zhixing Wang,Renzhi Yuan,Haifeng Yao,Chuang Yang,Mugen Peng*

Main category: eess.SP

TL;DR: 提出了一种卫星到水下激光通信的完整信道模型，采用解析-蒙特卡洛混合方法，同时考虑粒子和湍流效应，分析了不同环境条件下的通信性能。


<details>
  <summary>Details</summary>
Motivation: 现有卫星到水下激光通信信道模型要么关注分离信道，要么忽略粒子和湍流对激光传播的综合影响，需要建立更全面的信道模型。

Method: 采用解析-蒙特卡洛混合方法：基于扩展惠更斯-菲涅耳原理获得大气湍流后的激光强度分布；推导空气-水界面后光子传播方向的闭式概率密度函数；使用蒙特卡洛方法模拟水下链路并获得接收面功率分布。

Result: 数值结果表明，水下粒子浓度对通信性能的影响远大于大气湍流和水下湍流；增加空气-水界面风速不会显著恶化通信性能。

Conclusion: 建立了全面的卫星到水下激光通信信道模型，揭示了水下粒子浓度是影响通信性能的主要因素，为系统设计提供了重要指导。

Abstract: Channel modeling for satellite-to-underwater laser communication (StULC)
links remains challenging due to long distances and the diversity of the
channel constituents. The StULC channel is typically segmented into three
isolated channels: the atmospheric channel, the air-water interface channel,
and the underwater channel. Previous studies involving StULC channel modeling
either focused on separated channels or neglected the combined effects of
particles and turbulence on laser propagation. In this paper, we established a
comprehensive StULC channel model by an analytical-Monte Carlo hybrid approach,
taking into account the effects of both particles and turbulence. We first
obtained the intensity distribution of the transmitted laser beam after passing
through the turbulent atmosphere based on the extended Huygens-Fresnel
principle. Then we derived a closed-form probability density function of the
photon propagating direction after passing through the air-water interface,
which greatly simplified the modeling of StULC links. At last, we employed a
Monte Carlo method to model the underwater links and obtained the power
distribution at the receiving plane. Based on the proposed StULC channel model,
we analyzed the bit error rate and the outage probability under different
environmental conditions. Numerical results demonstrated that, the influence of
underwater particle concentration on the communication performance is much
pronounced than those of both the atmospheric turbulence and the underwater
turbulence. Notably, increasing the wind speed at the air-water interface does
not significantly worsen the communication performance of the StULC links.

</details>


### [8] [Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing](https://arxiv.org/abs/2510.17816)
*Xin Li,Jingzhi Hu,Yinghui He,Hongbo Wang,Jin Gan,Jun Luo*

Main category: eess.SP

TL;DR: WiAnchor是一个用于Wi-Fi多人员活动识别的跨域适应训练框架，通过锚点匹配机制处理不完整活动类别，在缺少某些类别的情况下仍能实现90%以上的跨域准确率。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi活动识别在多人员场景下受限于空间分辨率，而利用近场效应建立个人设备感知链路时，由于近场信号的特异性和不规则模式，神经网络模型需要跨域微调，这在某些活动类别不可用时尤为困难。

Method: 框架分三步：预训练阶段扩大类间特征边界增强活动可分性；微调阶段创新锚点匹配机制进行跨域适应，基于不完整活动类别过滤主体特异性干扰；最后基于输入样本与锚点的特征相似性进一步改进识别。

Result: 构建了综合数据集进行充分评估，在活动类别缺失的情况下实现了超过90%的跨域准确率。

Conclusion: WiAnchor通过创新的锚点匹配机制有效解决了Wi-Fi多人员活动识别中跨域适应和不完整类别的问题，显著提升了识别性能。

Abstract: Wi-Fi-based human activity recognition (HAR) provides substantial convenience
and has emerged as a thriving research field, yet the coarse spatial resolution
inherent to Wi-Fi significantly hinders its ability to distinguish multiple
subjects. By exploiting the near-field domination effect, establishing a
dedicated sensing link for each subject through their personal Wi-Fi device
offers a promising solution for multi-person HAR under native traffic. However,
due to the subject-specific characteristics and irregular patterns of
near-field signals, HAR neural network models require fine-tuning (FT) for
cross-domain adaptation, which becomes particularly challenging with certain
categories unavailable. In this paper, we propose WiAnchor, a novel training
framework for efficient cross-domain adaptation in the presence of incomplete
activity categories. This framework processes Wi-Fi signals embedded with
irregular time information in three steps: during pre-training, we enlarge
inter-class feature margins to enhance the separability of activities; in the
FT stage, we innovate an anchor matching mechanism for cross-domain adaptation,
filtering subject-specific interference informed by incomplete activity
categories, rather than attempting to extract complete features from them;
finally, the recognition of input samples is further improved based on their
feature-level similarity with anchors. We construct a comprehensive dataset to
thoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with
absent activity categories.

</details>


### [9] [Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels](https://arxiv.org/abs/2510.18604)
*Zian Meng,Qiang Li,Wenqian Tang,Mingdie Yan,Xiaohu Ge*

Main category: eess.SP

TL;DR: 提出了一种通道感知向量量化算法(CAVQ)，在联合源信道编码框架中实现离散语义传输，通过整合信道状态信息提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的语义通信多采用模拟或半数字传输，与现代数字通信基础设施兼容性差；现有向量量化方法在码本优化时忽略信道状态信息，导致鲁棒性不足。

Method: 在离散无记忆信道中建立VQJSCC框架，将语义特征离散化并直接映射到调制星座符号；CAVQ将信道转移概率整合到量化过程中，使易混淆符号与语义相似码字对齐；引入多码本对齐机制处理码本顺序与调制顺序不匹配问题。

Result: 实验结果表明VQJSCC有效缓解了数字悬崖效应，在各种调制方案下实现优越的重建质量，在鲁棒性和效率方面均优于现有数字语义通信基准方法。

Conclusion: 所提出的通道感知向量量化方法能够显著提升数字语义通信的性能，为现代数字通信基础设施提供了更好的兼容性和鲁棒性。

Abstract: Deep learning-based semantic communication has largely relied on analog or
semi-digital transmission, which limits compatibility with modern digital
communication infrastructures. Recent studies have employed vector quantization
(VQ) to enable discrete semantic transmission, yet existing methods neglect
channel state information during codebook optimization, leading to suboptimal
robustness. To bridge this gap, we propose a channel-aware vector quantization
(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed
VQJSCC, established on a discrete memoryless channel. In this framework,
semantic features are discretized and directly mapped to modulation
constellation symbols, while CAVQ integrates channel transition probabilities
into the quantization process, aligning easily confused symbols with
semantically similar codewords. A multi-codebook alignment mechanism is further
introduced to handle mismatches between codebook order and modulation order by
decomposing the transmission stream into multiple independently optimized
subchannels. Experimental results demonstrate that VQJSCC effectively mitigates
the digital cliff effect, achieves superior reconstruction quality across
various modulation schemes, and outperforms state-of-the-art digital semantic
communication baselines in both robustness and efficiency.

</details>


### [10] [Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach](https://arxiv.org/abs/2510.17818)
*Salar Nouri*

Main category: eess.SP

TL;DR: 提出了一种用于均匀圆阵列单快拍数据的二维无网格DOA估计新框架，通过联合估计流形变换矩阵和源方位-仰角对，使用不精确增广拉格朗日方法高效求解，避免了半定规划。


<details>
  <summary>Details</summary>
Motivation: 传统无网格方法在单快拍场景下由于计算成本过高或缺乏鲁棒性而失效，需要解决这一挑战性问题。

Method: 提出统一优化问题联合估计流形变换矩阵和源方位-仰角对，采用不精确增广拉格朗日方法高效求解，避免半定规划。

Result: 仿真结果表明该iALM框架能够提供鲁棒且高分辨率的无网格2D-DOA估计。

Conclusion: 该方法特别适用于具有挑战性的单快拍阵列信号处理应用，建立了其有效性。

Abstract: This paper tackles the challenging problem of gridless two-dimensional (2D)
direction-of-arrival (DOA) estimation for a uniform circular array (UCA) from a
single snapshot of data. Conventional gridless methods often fail in this
scenario due to prohibitive computational costs or a lack of robustness. We
propose a novel framework that overcomes these limitations by jointly
estimating a manifold transformation matrix and the source azimuth-elevation
pairs within a single, unified optimization problem. This problem is solved
efficiently using an inexact Augmented Lagrangian Method (iALM), which
completely circumvents the need for semidefinite programming. By unifying the
objectives of data fidelity and transformation robustness, our approach is
uniquely suited for the demanding single-snapshot case. Simulation results
confirm that the proposed iALM framework provides robust and high-resolution,
gridless 2D-DOA estimates, establishing its efficacy for challenging array
signal processing applications.

</details>


### [11] [CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms](https://arxiv.org/abs/2510.17821)
*Long Lin,Pablo Peiro-Corbacho,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Gonzalo R. Ríos-Muñoz,Carlos Sevilla-Salcedo*

Main category: eess.SP

TL;DR: CLARAE是一种用于心房内电图的自动编码器，通过保持波形形态、减少重建伪影和生成可解释嵌入，实现高保真重建和紧凑的64维潜在表示。


<details>
  <summary>Details</summary>
Motivation: 心房内电图常受噪声污染且维度高，限制了实时分析。需要一种既能降噪又能生成紧凑表示的解决方案。

Method: 使用一维编码器-解码器架构，采用三个原则：池化下采样、混合插值-卷积上采样路径和有界潜在空间。

Result: 在495,731个心电图段上评估，CLARAE在节律分类中F1分数超过0.97，潜在空间显示清晰的节律聚类，在降噪任务中表现优异。

Conclusion: CLARAE结合了鲁棒降噪和紧凑判别表示，为节律鉴别、信号质量评估和实时映射等临床工作流程提供了实用基础。

Abstract: Intracavitary atrial electrograms (EGMs) provide high-resolution insights
into cardiac electrophysiology but are often contaminated by noise and remain
high-dimensional, limiting real-time analysis. We introduce CLARAE
(CLArity-preserving Reconstruction AutoEncoder), a one-dimensional
encoder--decoder designed for atrial EGMs, which achieves both high-fidelity
reconstruction and a compact 64-dimensional latent representation. CLARAE is
designed to preserve waveform morphology, mitigate reconstruction artifacts,
and produce interpretable embeddings through three principles: downsampling
with pooling, a hybrid interpolation--convolution upsampling path, and a
bounded latent space.
  We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29
patients across three rhythm types (AF, SR300, SR600). Performance was
benchmarked against six state-of-the-art autoencoders using reconstruction
metrics, rhythm classification, and robustness across signal-to-noise ratios
from -5 to 15 dB. In downstream rhythm classification, CLARAE achieved
F1-scores above 0.97 for all rhythm types, and its latent space showed clear
clustering by rhythm. In denoising tasks, it consistently ranked among the top
performers for both unipolar and bipolar signals.
  In order to promote reproducibility and enhance accessibility, we offer an
interactive web-based application. This platform enables users to explore
pre-trained CLARAE models, visualize the reconstructions, and compute metrics
in real time. Overall, CLARAE combines robust denoising with compact,
discriminative representations, offering a practical foundation for clinical
workflows such as rhythm discrimination, signal quality assessment, and
real-time mapping.

</details>


### [12] [Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming](https://arxiv.org/abs/2510.17823)
*Saeed Mohammadzadeh,Rodrigo C. de Lamare,Yuriy Zakharov*

Main category: eess.SP

TL;DR: 提出一种高效的鲁棒自适应波束形成技术，通过方向估计和协方差矩阵重构来处理导向矢量失配和数据协方差矩阵重建问题。


<details>
  <summary>Details</summary>
Motivation: 解决导向矢量估计失配和数据协方差矩阵重建问题，提高自适应波束形成在干扰环境下的鲁棒性和性能。

Method: 利用可用快照估计干扰源方向，采用通用线性组合算法和预处理空间采样重构干扰加噪声协方差矩阵，使用功率谱采样策略和功率方法计算信号导向矢量。

Result: 仿真结果表明该方法相比现有方法具有更好的性能表现。

Conclusion: 提出的PPBSS技术能够有效处理导向矢量失配和协方差矩阵重建问题，在计算成本和性能方面都表现出优势。

Abstract: This work proposes an efficient, robust adaptive beamforming technique to
deal with steering vector (SV) estimation mismatches and data covariance matrix
reconstruction problems. In particular, the direction-of-arrival(DoA) of
interfering sources is estimated with available snapshots in which the angular
sectors of the interfering signals are computed adaptively. Then, we utilize
the well-known general linear combination algorithm to reconstruct the
interference-plus-noise covariance (IPNC) matrix using preprocessing-based
spatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be
replaced by the sample covariance matrix (SCM) in the shrinkage method. A power
spectrum sampling strategy is then devised based on a preprocessing matrix
computed with the estimated angular sectors' information. Moreover, the
covariance matrix for the signal is formed for the angular sector of the
signal-of-interest (SOI), which allows for calculating an SV for the SOI using
the power method. An analysis of the array beampattern in the proposed PPBSS
technique is carried out, and a study of the computational cost of competing
approaches is conducted. Simulation results show the proposed method's
effectiveness compared to existing approaches.

</details>


### [13] [Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin](https://arxiv.org/abs/2510.17825)
*Shumaila Javaid,Nasir Saeed*

Main category: eess.SP

TL;DR: 提出基于数字孪生的碳感知编排框架，用于集成卫星-空中-地面网络，通过多时间尺度PDCA循环和碳感知控制策略，在保证服务质量的同时降低碳排放。


<details>
  <summary>Details</summary>
Motivation: ISATN大规模部署面临能源消耗和碳排放的可持续性问题，需要超越传统能耗感知的方法，实现碳感知的网络编排。

Method: 采用数字孪生技术，以gCO2/bit为主要可持续性指标，实现结合日前预测和实时自适应优化的多时间尺度PDCA循环，利用碳感知切换、无人机占空比控制和可再生能源感知边缘放置等控制策略。

Result: 使用真实碳强度数据的仿真结果显示，相比仅考虑QoS的编排，gCO2/bit降低高达29%，同时提高了可再生能源利用率和在不利事件下的弹性。

Conclusion: 该碳感知编排框架能有效降低ISATN的碳排放，提高可持续性，同时保持网络性能，为6G网络的绿色部署提供解决方案。

Abstract: Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as
key enablers of 6G, providing global connectivity for applications such as
autonomous transportation, Industrial IoT, and disaster response. Their
large-scale deployment, however, risks unsustainable energy use and carbon
emissions. This work advances prior energy-aware studies by proposing a
carbon-aware orchestration framework for ISATNs that leverages Digital Twin
(DT) technology. The framework adopts grams of CO$_2$-equivalent per bit
(gCO$_2$/bit) as a primary sustainability metric and implements a multi
timescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting
with real-time adaptive optimization. ISATN-specific control knobs, including
carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement,
are exploited to reduce emissions. Simulation results with real carbon
intensity data show up to 29\% lower gCO$_2$/bit than QoS-only orchestration,
while improving renewable utilization and resilience under adverse events.

</details>


### [14] [Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks](https://arxiv.org/abs/2510.17832)
*Henrique de Lima Alexandre,Clodoaldo Aparecido de Moraes Lima*

Main category: eess.SP

TL;DR: 该研究提出使用扩散概率模型生成运动想象脑电信号的方法，以解决脑电数据采集的挑战。生成的合成数据在分类任务中达到95%以上的准确率，能有效补充数据集。


<details>
  <summary>Details</summary>
Motivation: 脑电数据采集面临传感器成本高、采集时间长和个体间变异性大等挑战，限制了脑机接口应用的发展。需要解决数据稀缺问题。

Method: 预处理真实脑电数据，训练扩散模型从噪声中重建脑电通道，使用KNN、CNN和U-Net等分类器评估生成信号的质量。

Result: 生成的合成脑电信号分类准确率超过95%，与真实信号具有低均方误差和高相关性。

Conclusion: 扩散模型生成的合成脑电信号能有效补充数据集，改善脑机接口中的分类性能，解决数据稀缺问题。

Abstract: Electroencephalography (EEG) is a widely used, non-invasive method for
capturing brain activity, and is particularly relevant for applications in
Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data
remains a major challenge due to sensor costs, acquisition time, and
inter-subject variability. To address these limitations, this study proposes a
methodology for generating synthetic EEG signals associated with motor imagery
brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves
preprocessing real EEG data, training a diffusion model to reconstruct EEG
channels from noise, and evaluating the quality of the generated signals
through both signal-level and task-level metrics. For validation, we employed
classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks
(CNN), and U-Net to compare the performance of synthetic data against real data
in classification tasks. The generated data achieved classification accuracies
above 95%, with low mean squared error and high correlation with real signals.
  Our results demonstrate that synthetic EEG signals produced by diffusion
models can effectively complement datasets, improving classification
performance in EEG-based BCIs and addressing data scarcity.

</details>


### [15] [Two Phases Leakage Detection Strategy Supported by DMAs](https://arxiv.org/abs/2510.17836)
*G. Messa,G. Acconciaioco,S. Ripani,L. Bozzelli,A. Simone,O. Giustolisi*

Main category: eess.SP

TL;DR: 提出了一种基于模型的两阶段泄漏检测策略，包括DMA识别和管道预定位两个阶段，通过检测异常来识别和预定位泄漏点，并优化压力计布置以减少误报。


<details>
  <summary>Details</summary>
Motivation: 为水务公司提供一种能够识别DMA级别异常并以最小检查成本定位泄漏点的策略，提高泄漏检测效率和准确性。

Method: 采用两阶段方法：首先识别DMA区域，然后在识别出的DMA内进行管道预定位。使用AMSI指标来限制误报，并提出了压力计位置设计策略。

Result: 该策略能够有效识别DMA级别的泄漏异常，并通过管道序列预定位减少检查成本。在意大利阿普利亚的两个真实供水网络中得到验证。

Conclusion: 提出的两阶段泄漏检测策略为水务公司提供了有效的泄漏管理工具，能够以最小成本实现泄漏的识别和定位，具有实际应用价值。

Abstract: The present work proposes a novel two phases model-based strategy for leakage
detection. The two phases are: the identification of the district metering area
(DMA) and the pipe pre-localization into the identified DMA. The strategy is
based on detecting and pre-localizing the punctual leakage as anomaly with
respect to the normal working conditions. A further novelty is the fact that
the pre-localization phase returns the sequence of pipes to inspect, which
makes the strategy attractive for water utilities, whose aim is to identify the
anomaly at DMA level and, successively, to localize it with the minimum
inspection cost. Furthermore, a random database is useful to test the
performance of the strategy with respect to the configuration of DMAs and the
pressure metering system. Consequently, a novel strategy to design the location
of pressure meters is also proposed. It is demonstrated that the entire
strategy limits false positives during the DMA identification phase by using
the recently proposed index named Asset Management Support Indicator (AMSI).
AMSI is invariant with respect to the deterioration, i.e., it is sensitive to
its increase causing punctual leakage. The strategy is studied and discussed
using two real Apulian WDNs managed by Acquedotto Pugliese.

</details>


### [16] [Majority Vote Compressed Sensing](https://arxiv.org/abs/2510.18008)
*Henrik Hellström,Jiwon Jeong,Ayfer Özgür,Viktoria Fodor,Carlo Fischione*

Main category: eess.SP

TL;DR: 提出了一种基于随机变换和多数投票的非相干空中计算方案MVCS，利用数据稀疏性在更少信道使用次数下实现高维数据向量聚合，相比现有方法显著降低了通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有非相干空中计算方案需要超过d次信道使用来计算函数，当数据向量稀疏时，这种冗余通信成本过高。需要利用稀疏性来降低通信开销。

Method: 使用随机变换将高维稀疏数据投影到低维空间，通过多数投票空中计算方案传输投影向量，在接收端利用1比特压缩感知技术恢复原始数据聚合。

Result: MVCS方案在T=O(kn log(d)/ε²)次信道使用下，能以ℓ₂范数误差ε估计聚合数据向量∑xᵢ，相比现有方法显著减少了通信需求。

Conclusion: MVCS方案成功利用数据稀疏性，通过随机投影和压缩感知技术，在非相干空中计算中实现了高效的通信效率，适用于直方图估计和分布式机器学习等应用。

Abstract: We consider the problem of non-coherent over-the-air computation (AirComp),
where $n$ devices carry high-dimensional data vectors
$\mathbf{x}_i\in\mathbb{R}^d$ of sparsity $\lVert\mathbf{x}_i\rVert_0\leq k$
whose sum has to be computed at a receiver. Previous results on non-coherent
AirComp require more than $d$ channel uses to compute functions of
$\mathbf{x}_i$, where the extra redundancy is used to combat non-coherent
signal aggregation. However, if the data vectors are sparse, sparsity can be
exploited to offer significantly cheaper communication. In this paper, we
propose to use random transforms to transmit lower-dimensional projections
$\mathbf{s}_i\in\mathbb{R}^T$ of the data vectors. These projected vectors are
communicated to the receiver using a majority vote (MV)-AirComp scheme, which
estimates the bit-vector corresponding to the signs of the aggregated
projections, i.e., $\mathbf{y} = \text{sign}(\sum_i\mathbf{s}_i)$. By
leveraging 1-bit compressed sensing (1bCS) at the receiver, the real-valued and
high-dimensional aggregate $\sum_i\mathbf{x}_i$ can be recovered from
$\mathbf{y}$. We prove analytically that the proposed MVCS scheme estimates the
aggregated data vector $\sum_i \mathbf{x}_i$ with $\ell_2$-norm error
$\epsilon$ in $T=\mathcal{O}(kn\log(d)/\epsilon^2)$ channel uses. Moreover, we
specify algorithms that leverage MVCS for histogram estimation and distributed
machine learning. Finally, we provide numerical evaluations that reveal the
advantage of MVCS compared to the state-of-the-art.

</details>


### [17] [MCANet: A Coherent Multimodal Collaborative Attention Network for Advanced Modulation Recognition in Adverse Noisy Environments](https://arxiv.org/abs/2510.18336)
*Wangye Jiang,Haoming Yang,Xinyu Lu,Mingyuan Wang,Huimei Sun,Jingya Zhang*

Main category: eess.SP

TL;DR: MCANet是一种多模态深度学习框架，通过精炼特征提取和全局建模来改进自动调制识别，在低信噪比条件下表现优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统发展，自动调制识别在认知无线电系统中对提高频谱效率至关重要。传统方法在复杂、嘈杂环境中，特别是低信噪比条件下面临挑战。

Method: 提出MCANet（多模态协作注意力网络），采用精炼特征提取和全局建模来支持其融合策略。

Result: 在多个基准数据集上的实验结果表明，MCANet优于主流AMR模型，在低信噪比条件下具有更好的鲁棒性。

Conclusion: MCANet通过多模态深度学习方法有效解决了自动调制识别在复杂噪声环境中的挑战，特别是在低信噪比条件下表现出色。

Abstract: As wireless communication systems evolve, automatic modulation recognition
(AMR) plays a key role in improving spectrum efficiency, especially in
cognitive radio systems. Traditional AMR methods face challenges in complex,
noisy environments, particularly in low signal-to-noise ratio (SNR) conditions.
This paper introduces MCANet (Multimodal Collaborative Attention Network), a
multimodal deep learning framework designed to address these challenges. MCANet
employs refined feature extraction and global modeling to support its fusion
strategy.Experimental results across multiple benchmark datasets show that
MCANet outperforms mainstream AMR models, offering better robustness in low-SNR
conditions.

</details>


### [18] [AWSPNet: Attention-based Dual-Tree Wavelet Scattering Prototypical Network for MIMO Radar Target Recognition and Jamming Suppression](https://arxiv.org/abs/2510.18422)
*Yizhen Jia,Siyao Xiao,Wenkai Jia,Hui Chen,Wen-Qin Wang*

Main category: eess.SP

TL;DR: 提出了一种基于注意力的双树小波散射原型网络(AWSPNet)，用于雷达目标识别和干扰抑制，在低信噪比(-6dB)下达到90.45%的准确率。


<details>
  <summary>Details</summary>
Motivation: 数字射频存储器电子对抗技术的进步对雷达系统生存能力构成严重威胁，需要开发能够在低信噪比环境下区分真实目标和复杂干扰信号的鲁棒方法。

Method: 使用双树复小波变换提取抗噪声和信号平移的特征，结合注意力机制和预训练骨干网络进行特征精炼，采用监督对比学习策略和原型网络进行分类。

Result: 在-6dB信噪比下达到90.45%的识别准确率，通过t-SNE可视化验证了特征可分性，结合时域滑动窗口方法实现了干扰识别和抑制。

Conclusion: AWSPNet在复杂电磁环境中具有实际应用潜力，能够有效识别和抑制各种类型的干扰信号。

Abstract: The increasing of digital radio frequency memory based electronic
countermeasures poses a significant threat to the survivability and
effectiveness of radar systems. These jammers can generate a multitude of
deceptive false targets, overwhelming the radar's processing capabilities and
masking targets. Consequently, the ability to robustly discriminate between
true targets and complex jamming signals, especially in low signal-to-noise
ratio (SNR) environments, is of importance. This paper introduces the
attention-based dual-tree wavelet scattering prototypical network (AWSPNet), a
deep learning framework designed for simultaneous radar target recognition and
jamming suppression. The core of AWSPNet is the encoder that leverages the
dual-tree complex wavelet transform to extract features that are inherently
robust to noise and signal translations. These features are further refined by
an attention mechanism and a pre-trained backbone network. To address the
challenge of limited labeled data and enhance generalization, we employ a
supervised contrastive learning strategy during the training phase. The
classification is performed by a prototypical network, which is particularly
effective in few-shot learning scenarios, enabling rapid adaptation to new
signal types. We demonstrate the efficacy of our approach through extensive
experiments. The results show that AWSPNet achieves 90.45\% accuracy at -6 dB
SNR. Furthermore, we provide a physical interpretation of the network's inner
workings through t-SNE visualizations, which analyze the feature separability
at different stages of the model. Finally, by integrating AWSPNet with a
time-domain sliding window approach, we present a complete algorithm capable of
not only identifying but also effectively suppressing various types of jamming,
thereby validating its potential for practical application in complex
electromagnetic environments.

</details>


### [19] [Microsecond Federated SVD on Grassmann Manifold for Real-time IoT Intrusion Detection](https://arxiv.org/abs/2510.18501)
*Tung-Anh Nguyen,Van-Phuc Bui,Shashi Raj Pandey,Kim Hue Ta,Nguyen H. Tran,Petar Popovski*

Main category: eess.SP

TL;DR: FedSVD是一种基于奇异值分解和Grassmann流形优化的无监督联邦学习框架，用于物联网网络中的实时异常检测，无需标记数据或集中式数据共享。


<details>
  <summary>Details</summary>
Motivation: 物联网网络需要实时异常检测，但传统方法依赖标记数据或集中式数据共享，不适合资源受限的设备和延迟敏感的应用。

Method: 利用奇异值分解和Grassmann流形优化，在低功耗设备上实现联邦学习，显著减少通信开销和计算成本。

Result: FedSVD在性能上与深度学习基线相当，同时将推理延迟降低10倍以上，适合延迟敏感的物联网应用。

Conclusion: FedSVD是一种高效的无监督联邦学习框架，适用于物联网网络中的实时异常检测，具有低延迟和低资源消耗的优势。

Abstract: This paper introduces FedSVD, a novel unsupervised federated learning
framework for real-time anomaly detection in IoT networks. By leveraging
Singular Value Decomposition (SVD) and optimization on the Grassmann manifolds,
FedSVD enables accurate detection of both known and unknown intrusions without
relying on labeled data or centralized data sharing. Tailored for deployment on
low-power devices like the NVIDIA Jetson AGX Orin, the proposed method
significantly reduces communication overhead and computational cost.
Experimental results show that FedSVD achieves performance comparable to deep
learning baselines while reducing inference latency by over 10x, making it
suitable for latency-sensitive IoT applications.

</details>


### [20] [Delay Management Using Packet Fragmentation in Wireless Industrial Automation Systems](https://arxiv.org/abs/2510.18646)
*Anwar Ahmed Khan,Shama Siddiqui,Indrakshi Dey*

Main category: eess.SP

TL;DR: 比较FROG-MAC和FPS-MAC两种MAC协议在工业自动化环境中的性能，发现FROG-MAC在能效和延迟方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 工业自动化应用对延迟管理有严格要求，需要高效的MAC协议来保证关键数据的及时传输，特别是在存在异构数据的工业环境中。

Method: 使用Contiki作为仿真平台，采用单跳星型拓扑模拟工业环境，比较FROG-MAC和FPS-MAC两种协议的性能。

Result: FROG-MAC在能效和延迟方面都优于FPS-MAC，这得益于其能够中断信道中正在进行的低优先级传输的固有特性。

Conclusion: FROG-MAC协议在工业异构无线网络中具有更好的性能表现，特别是在能效和延迟控制方面。

Abstract: Managing delay is one of the core requirements of industrial automation
applications due to the high risk associated for equipment and human lives.
Using efficient Media Access Control (MAC) schemes guarantees the timely
transmission of critical data, particularly in the industrial environments
where heterogeneous data is inherently expected. This paper compares the
performance of Fragmentation based MAC (FROG-MAC) against Fuzzy Priority
Scheduling based MAC (FPS-MAC), both of which have been designed to optimize
the performance of heterogenous wireless networks. Contiki has been used as a
simulation platform and a single hop star topology has been assumed to resemble
the industrial environment. It has been shown that FROG-MAC has the potential
to outperform FPS-MAC in terms of energy efficiency and delay both, due to its
inherent feature of interrupting ongoing lower priority transmission on the
channel.

</details>


### [21] [A Comparative Analysis of High-Level vs. Low-Level Simulations for Dynamic MAC Protocols in Wireless Sensor Networks](https://arxiv.org/abs/2510.18662)
*Shama Siddiqui,Anwar Ahmed Khan,Indrakshi Dey*

Main category: eess.SP

TL;DR: 比较了ADP-MAC协议在高级理论模拟和详细实现模拟中的性能差异，发现两种模拟方法在能耗和延迟趋势上存在显著不同。


<details>
  <summary>Details</summary>
Motivation: 在无线传感器网络中，评估MAC协议性能时，需要验证高级理论模拟结果与详细实现结果的一致性，以确保协议在真实场景中的可靠性。

Method: 使用MATLAB进行高级理论模拟，使用TinyOS在Mica2平台上开发详细实现，基于能耗和延迟对ADP-MAC协议进行性能评估。

Result: 高级模拟显示能耗随轮询间隔增加而减少，延迟增加；而详细实现显示能耗和延迟都随轮询间隔增加而增加。

Conclusion: 高级和低级模拟对ADP-MAC的趋势显著不同，这归因于高级研究中缺乏现实假设。

Abstract: Simulation studies are conducted at different levels of details for assessing
the performance of Media Access Control (MAC) protocols in Wireless Sensor
Networks (WSN). In the present-day scenario where hundreds of MAC protocols
have been proposed, it is important to assess the quality of performance
evaluation being conducted for each of the proposed protocols. It therefore
becomes crucial to compare the results of high-level theoretical simulations
with the detailed implementation results before any network protocol could be
deployed for a real-world scenario. In this work, we present a comparison of
high-level theoretical and detailed implementation results for Adaptive and
Dynamic Polling-MAC (ADP-MAC). MATLAB has been used for conducting initial
theoretical simulations and TinyOS has been used to develop the detailed
implementation of protocol for Mica2 platform. Performance evaluation of
ADP-MAC using the two levels of simulation has been conducted based on energy
and delay. In the high-level implementation, energy consumption was found to be
decreasing whereas delay was found to be increasing for increasing channel
polling intervals. On the other hand, when detailed implementation was
developed, it was observed that both energy consumption and delay revealed an
increasing trend with the increasing polling intervals. Therefore, it has been
shown that the trends for high- and low-level simulations for ADP-MAC are
significantly different, due to the lack of realistic assumptions in the
higher-level study.

</details>


### [22] [mSQUID: Model-Based Leanred Modulo Recovery at Low Sampling Rates](https://arxiv.org/abs/2510.18729)
*Yhonatan Kvich,Rotem Arie,Hana Hasan,Shaik Basheeruddin Shah,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 提出了一种基于模型深度展开网络的模数采样信号恢复方法mSQUID，结合压缩感知和深度学习，通过软量化模块处理折叠引入的非线性失真，在低采样率和高斯噪声下实现优越重建性能。


<details>
  <summary>Details</summary>
Motivation: 模数采样通过将输入信号折叠到有限区间来避免信号削波，但折叠引入的非线性失真在噪声和量化条件下给信号恢复带来挑战。需要开发能有效处理这种非线性失真的恢复方法。

Method: 提出模数软量化展开迭代解码器(mSQUID)，结合经典压缩感知求解器的可解释性和学习的灵活性。关键创新是软量化模块，以可微分和可学习的方式引导解趋向折叠范围的离散倍数。

Result: 在低采样率和高斯噪声下实现优越重建性能，能同时恢复幅度差异大且频带分离的信号。相比传统采样方法，显著减少运行时间，适合实时资源受限系统。

Conclusion: mSQUID方法有效解决了模数采样中的非线性失真恢复问题，在噪声和量化条件下表现出色，为实时资源受限系统提供了可行的解决方案。

Abstract: Modulo sampling enables acquisition of signals with unlimited dynamic range
by folding the input into a bounded interval prior to sampling, thus
eliminating the risk of signal clipping and preserving information without
requiring highresolution ADCs. While this enables low-cost hardware, the
nonlinear distortion introduced by folding presents recovery challenges,
particularly under noise and quantization. We propose a model-based deep
unfolding network tailored to this setting, combining the interpretability of
classical compress sensing (CS) solvers with the flexibility of learning. A key
innovation is a soft-quantization module that encodes the modulo prior by
guiding the solution toward discrete multiples of the folding range in a
differentiable and learnable way. Our method, modulo soft-quantized unfolded
iterative decoder (mSQUID), achieves superior reconstruction performance at low
sampling rates under additive Gaussian noise. We further demonstrate its
utility in a challenging case where signals with vastly different amplitudes
and disjoint frequency bands are acquired simultaneously and quantized. In this
scenario, classical sampling often struggles due to weak signal distortion or
strong signal clipping, while our approach is able to recover the input
signals. Our method also offers significantly reduced runtimes, making it
suitable for real-time, resource-limited systems.

</details>


### [23] [Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless Networks](https://arxiv.org/abs/2510.18743)
*Kasun R. Wijewardhana,Animesh Yadav,Ming Zeng,Mohamed Elsayed,Octavia A. Dobre,Zhiguo Ding*

Main category: eess.SP

TL;DR: 本文提出了无线馈电的夹持天线系统(Wi-PASS)，通过无线馈电为波导供电，解决了传统PASS系统依赖有线馈电的限制，实现了更灵活的部署和更远的覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 传统波导夹持天线系统(PASS)依赖有线馈电，限制了部署灵活性，只能部署在基站附近区域，无法有效服务远距离用户，成本效益低。

Method: 采用无线馈电技术为波导供电，开发了无线馈电夹持天线系统(Wi-PASS)，通过无线方式为波导提供能量。

Result: Wi-PASS在室内和室外场景中展现出优于传统PASS的优势，数值结果显示其数据速率高于传统固定天线系统。

Conclusion: Wi-PASS具有更高的可行性和性能，是解决毫米波和太赫兹频段传播损耗的有效方案，未来需要进一步研究以推进其部署。

Abstract: Waveguide-based pinching-antenna systems (PASS) have recently emerged as a
promising solution to mitigate severe propagation losses in millimeter-wave and
terahertz bands by intelligently and flexibly establishing line-of-sight links.
However, their reliance on wire-based feeding confines deployment to areas near
the base station (BS), limiting installation flexibility and making them
cost-ineffective for serving distant users or regions. To overcome this
challenge, this article proposes wireless-fed pinchingantenna systems
(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer
a practical and cost-efficient means to extend coverage beyond the BS vicinity.
Several indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.
Numerical results further show that Wi-PASS deliver higher data rates than
conventional fixed-antenna systems, confirming the superior feasibility and
performance of Wi-PASS. Key future research directions are also discussed to
advance Wi-PASS deployment.

</details>


### [24] [Analyse comparative d'algorithmes de restauration en architecture dépliée pour des signaux chromatographiques parcimonieux](https://arxiv.org/abs/2510.18760)
*Mouna Gharbi,Silvia Villa,Emilie Chouzenoux,Jean-Christophe Pesquet,Laurent Duval*

Main category: eess.SP

TL;DR: 比较三种展开式架构在参数化色谱信号数据库上的性能，特别关注适用于物理化学峰信号表征的指标


<details>
  <summary>Details</summary>
Motivation: 研究稀疏假设下退化观测数据恢复问题，传统迭代优化方法与深度学习技术相结合，展开式方法受益于这两类方法

Method: 在参数化色谱信号数据库上对三种展开式架构进行对比研究

Result: 这些方法表现出良好性能，特别是在使用适用于物理化学峰信号表征的指标时

Conclusion: 展开式方法在色谱信号恢复中具有优势，特别是当采用专门针对物理化学峰信号特性的评估指标时

Abstract: Data restoration from degraded observations, of sparsity hypotheses, is an
active field of study. Traditional iterative optimization methods are now
complemented by deep learning techniques. The development of unfolded methods
benefits from both families. We carry out a comparative study of three
architectures on parameterized chromatographic signal databases, highlighting
the performance of these approaches, especially when employing metrics adapted
to physico-chemical peak signal characterization.

</details>


### [25] [SO(3)-invariant PCA with application to molecular data](https://arxiv.org/abs/2510.18827)
*Michael Fraiman,Paulina Hoyos,Tamir Bendory,Joe Kileel,Oscar Mickelin,Nir Sharon,Amit Singer*

Main category: eess.SP

TL;DR: 提出了SO(3)不变PCA方法，用于处理未知方向的三维体积数据，无需显式数据增强，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统PCA在处理具有任意方向的三维数据（如结构生物学数据）时面临挑战，需要大量旋转副本导致计算成本过高。

Method: 通过利用底层代数结构，开发了SO(3)不变PCA框架，隐式考虑所有旋转，仅需计算协方差矩阵条目的平方根。

Result: 在真实分子数据集上验证了方法的有效性，计算复杂度大幅降低。

Conclusion: 该方法为大规模高维重建问题开辟了新可能性，是三维体积数据分析的有效工具。

Abstract: Principal component analysis (PCA) is a fundamental technique for
dimensionality reduction and denoising; however, its application to
three-dimensional data with arbitrary orientations -- common in structural
biology -- presents significant challenges. A naive approach requires
augmenting the dataset with many rotated copies of each sample, incurring
prohibitive computational costs. In this paper, we extend PCA to 3D volumetric
datasets with unknown orientations by developing an efficient and principled
framework for SO(3)-invariant PCA that implicitly accounts for all rotations
without explicit data augmentation. By exploiting underlying algebraic
structure, we demonstrate that the computation involves only the square root of
the total number of covariance entries, resulting in a substantial reduction in
complexity. We validate the method on real-world molecular datasets,
demonstrating its effectiveness and opening up new possibilities for
large-scale, high-dimensional reconstruction problems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [Conformal Lesion Segmentation for 3D Medical Images](https://arxiv.org/abs/2510.17897)
*Binyu Tan,Zhiyuan Wang,Jinhao Duan,Kaidi Xu,Heng Tao Shen,Xiaoshuang Shi,Fumin Shen*

Main category: eess.IV

TL;DR: 提出了一种风险约束的医学图像分割框架CLS，通过保形预测校准数据驱动阈值，确保测试时的假阴性率低于目标容忍度ε，为高风险临床应用提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型使用固定阈值（如0.5）区分病灶与背景，缺乏对假阴性率等关键指标的统计保证，限制了在高风险临床应用中的可靠部署。

Method: CLS框架通过保形预测校准数据驱动阈值：首先保留校准集分析每个样本在FNR容忍度下的阈值设置，定义FNR特定损失函数，识别满足目标容忍度的关键阈值，然后基于用户指定的风险水平α确定测试时置信阈值。

Result: 在六个3D病灶分割数据集和五个骨干模型上验证了CLS的统计合理性和预测性能，证明其能提供严格的FNR约束，同时产生更精确可靠的分割结果。

Conclusion: CLS通过保形化关键阈值将校准集中观察到的统计规律推广到新测试数据，为临床实践中部署风险感知分割提供了可操作的见解。

Abstract: Medical image segmentation serves as a critical component of precision
medicine, enabling accurate localization and delineation of pathological
regions, such as lesions. However, existing models empirically apply fixed
thresholds (e.g., 0.5) to differentiate lesions from the background, offering
no statistical guarantees on key metrics such as the false negative rate (FNR).
This lack of principled risk control undermines their reliable deployment in
high-stakes clinical applications, especially in challenging scenarios like 3D
lesion segmentation (3D-LS). To address this issue, we propose a
risk-constrained framework, termed Conformal Lesion Segmentation (CLS), that
calibrates data-driven thresholds via conformalization to ensure the test-time
FNR remains below a target tolerance $\varepsilon$ under desired risk levels.
CLS begins by holding out a calibration set to analyze the threshold setting
for each sample under the FNR tolerance, drawing on the idea of conformal
prediction. We define an FNR-specific loss function and identify the critical
threshold at which each calibration data point just satisfies the target
tolerance. Given a user-specified risk level $\alpha$, we then determine the
approximate $1-\alpha$ quantile of all the critical thresholds in the
calibration set as the test-time confidence threshold. By conformalizing such
critical thresholds, CLS generalizes the statistical regularities observed in
the calibration set to new test data, providing rigorous FNR constraint while
yielding more precise and reliable segmentations. We validate the statistical
soundness and predictive performance of CLS on six 3D-LS datasets across five
backbone models, and conclude with actionable insights for deploying risk-aware
segmentation in clinical practice.

</details>
