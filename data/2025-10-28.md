<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 13]
- [eess.SP](#eess.SP) [Total: 22]
- [eess.IV](#eess.IV) [Total: 16]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Fundamental Limits of Coded Caching with Fixed Subpacketization](https://arxiv.org/abs/2510.22145)
*Minquan Cheng,Yifei Huang,Youlong Wu,Jinyan Wang*

Main category: cs.IT

TL;DR: 本文研究了编码缓存系统中传输负载与子分组化水平之间的基本权衡关系，提出了针对任意固定子分组化水平下传输负载的通用下界，并证明了分区方案能够达到最优的速率-子分组化权衡。


<details>
  <summary>Details</summary>
Motivation: 编码缓存通过创建编码多播机会显著降低传输负载，但更高的子分组化水平虽然可能带来更低的传输负载，却会导致更高的编码复杂度。现有研究缺乏对给定子分组化水平下最小传输负载这一基本问题的解答。

Method: 通过重新表述集中式编码缓存方案对应的放置-传递数组的组合结构，提出了针对任意固定子分组化水平的传输负载通用下界，并利用组合结构和排序集合的并集大小计算来建立新的最优性结果。

Result: 提出的下界恢复了二分图方案（包括著名的Maddah-Ali和Niesen方案及其共轭方案）和分组二分图方案的现有最优性结果，并证明了分区方案能够达到最优的速率-子分组化权衡。

Conclusion: 本文建立了编码缓存系统中传输负载与子分组化水平之间的基本权衡关系，为设计低复杂度高效编码缓存方案提供了理论基础。

Abstract: Coded caching is a promising technique to create coded multicast
opportunities for cache-aided networks. By splitting each file into $F$ equal
packets (i.e., the subpacketization level $F$) and letting each user cache a
set of packets, the transmission load can be significantly reduced via coded
multicasting. It has been shown that a higher subpacketization level could
potentially lead to a lower transmission load, as more packets can be combined
for efficient transmission. On the other hand, a larger $F$ indicates a higher
coding complexity and is problematic from a practical perspective when $F$ is
extremely large. Despite many works attempting to design coded caching schemes
with low subpacketization levels, a fundamental problem remains open: What is
the minimum transmission load given any fixed subpacketization level? In this
paper, we consider the classical cache-aided networks with identically uncoded
placement and one-shot delivery strategy, and investigate the fundamental
trade-off between the transmission load and the subpacketization level. We
propose a \emph{general} lower bound on the transmission load for any fixed
subpacketization by reformulating the centralized coded caching schemes via the
combinatorial structure of the corresponding placement delivery array. The
lower bound also recovers existing optimality results for the bipartite graph
scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the
conjugate MN scheme) as well as the grouping bipartite graph scheme.
Furthermore, by carefully exploiting the combinatorial structure and computing
the union size of sorted sets, we establish a new optimality result, i.e., the
partition scheme can achieve the optimal rate-subpacketization trade-off.

</details>


### [2] [Robust MIMO Channel Estimation Using Energy-Based Generative Diffusion Models](https://arxiv.org/abs/2510.22230)
*Ziqi Diao,Xingyu Zhou,Le Liang,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种结合能量基生成扩散模型和Metropolis-Hastings原理的新型大规模MIMO信道估计框架，显著提高了估计精度，特别是在导频开销有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统的信道估计面临导频开销过大和估计延迟高的根本性限制，需要克服这些障碍。

Method: 通过将扩散过程重新参数化并融入能量函数，框架显式估计未归一化的对数先验，同时使用Metropolis-Hastings修正来优化采样轨迹、减轻偏差并增强鲁棒性。

Result: 数值结果表明，与传统的参数化扩散模型和其他基线方法相比，所提方法显著提高了估计精度。

Conclusion: 该框架能够实现准确的后验采样，从而实现高保真度的信道估计，特别是在导频开销有限的情况下表现优异。

Abstract: Channel estimation for massive multiple-input multiple-output (MIMO) systems
is fundamentally constrained by excessive pilot overhead and high estimation
latency. To overcome these obstacles, recent studies have leveraged deep
generative networks to capture the prior distribution of wireless channels. In
this paper, we propose a novel estimation framework that integrates an
energy-based generative diffusion model (DM) with the Metropolis-Hastings (MH)
principle. By reparameterizing the diffusion process with an incorporated
energy function, the framework explicitly estimates the unnormalized log-prior,
while MH corrections refine the sampling trajectory, mitigate deviations, and
enhance robustness, ultimately enabling accurate posterior sampling for
high-fidelity channel estimation. Numerical results reveal that the proposed
approach significantly improves estimation accuracy compared with conventional
parameterized DMs and other baseline methods, particularly in cases with
limited pilot overhead.

</details>


### [3] [Infinitely many families of distance-optimal binary linear codes with respect to the sphere packing bound](https://arxiv.org/abs/2510.22259)
*Hao Chen,Conghui Xie,Cunsheng Ding*

Main category: cs.IT

TL;DR: 本文解决了编码理论中一个75年未解的开放问题：是否存在关于球包界的无限族距离最优线性码。作者给出了肯定的答案，并附带构造了若干小最小距离的最优二进制码。


<details>
  <summary>Details</summary>
Motivation: 自1950年Hamming提出汉明码和球包界以来，编码理论界一直存在一个开放问题：是否存在关于球包界的无限族距离最优线性码，其最小距离可以任意大。这个75年未解的问题成为本文的主要研究动机。

Method: 作者通过构造性的方法，建立了满足球包界条件的无限族距离最优线性码，并附带构造了若干小最小距离的二进制最优码和二进制五重码。

Result: 成功解决了75年未解的开放问题，证明了存在关于球包界的无限族距离最优线性码，其最小距离可以任意大。同时构造了多个无限族的小最小距离最优二进制码和二进制五重码。

Conclusion: 本文彻底解决了编码理论中一个长期存在的开放问题，为距离最优码的研究提供了新的理论支撑，并提出了若干值得进一步研究的新问题。

Abstract: R. W. Hamming published the Hamming codes and the sphere packing bound in
1950. In the past 75 years, infinite families of distance-optimal linear codes
over finite fields with minimum distance at most 8 with respect to the sphere
packing bound have been reported in the literature. However, it is a
75-year-old open problem in coding theory whether there is an infinite family
of distance-optimal linear codes over finite fields with arbitrarily large
minimum distance with respect to the sphere packing bound. This main objective
of this paper is to settle this long-standing open problem in coding theory.
  As by-products, several infinite families of distance-optimal binary codes
with small minimum distances are presented. Two infinite families of binary
five-weight codes are reported. Some open problems are also proposed.

</details>


### [4] [Optimal Sampling and Scheduling for Remote Fusion Estimation of Correlated Wiener Processes](https://arxiv.org/abs/2510.22288)
*Aimin Li,Elif Uysal*

Main category: cs.IT

TL;DR: 本文研究了分布式传感器网络中异步相关信息的融合估计问题，提出了采样、调度和估计策略的联合设计方法，证明了AoI优化与MSE优化的等价性。


<details>
  <summary>Details</summary>
Motivation: 分布式传感器网络中，传感器观测重叠区域的动态过程，但由于随机延迟，这些相关观测异步到达融合中心，需要解决如何融合异步相关信息以实现准确远程融合估计的问题。

Method: 建立了分离原理，识别出联合最优策略：最优融合估计器是基于AoI的加权和融合估计器，最优调度器是MAF调度器，最优采样可以在给定最优估计器和MAF调度器的情况下设计。

Result: 证明了在无限时域平均成本准则下，AoI优化等价于MSE优化，即使在存在强传感器间相关性的情况下也是如此。识别出MSE最优采样器就是AoI最优采样器。

Conclusion: 信息新鲜度可以作为相关感知环境中最优估计的设计替代指标，这一结果强调了信息新鲜度在相关感知环境中的重要性。

Abstract: In distributed sensor networks, sensors often observe a dynamic process
within overlapping regions. Due to random delays, these correlated observations
arrive at the fusion center asynchronously, raising a central question: How can
one fuse asynchronous yet correlated information for accurate remote fusion
estimation? This paper addresses this challenge by studying the joint design of
sampling, scheduling, and estimation policies for monitoring a correlated
Wiener process. Though this problem is coupled, we establish a separation
principle and identify the joint optimal policy: the optimal fusion estimator
is a weighted-sum fusion estimator conditioned on Age of Information (AoI), the
optimal scheduler is a Maximum Age First (MAF) scheduler that prioritizes the
most stale source, and the optimal sampling can be designed given the optimal
estimator and the MAF scheduler. To design the optimal sampling, we show that,
under the infinite-horizon average-cost criterion, optimizing AoI is equivalent
to optimizing MSE under pull-based communications, despite the presence of
strong inter-sensor correlations. This structural equivalence allows us to
identify the MSE-optimal sampler as one that is AoI-optimal. This result
underscores an insight: information freshness can serve as a design surrogate
for optimal estimation in correlated sensing environments.

</details>


### [5] [Energy-Efficient UAV-Enabled MEC Systems: NOMA, FDMA, or TDMA Offloading?](https://arxiv.org/abs/2510.22306)
*Qingjie Wu,Miao Cui,Guangchi Zhang,Beixiong Zheng,Xiaoli Chu,Qingqing Wu*

Main category: cs.IT

TL;DR: 该论文比较了无人机移动边缘计算系统中NOMA、FDMA和TDMA三种多址接入方案的能量效率，发现在有限块长度场景下TDMA比FDMA更节能，而NOMA不一定比FDMA更节能。


<details>
  <summary>Details</summary>
Motivation: 研究无人机移动边缘计算系统中不同多址接入方案在有限块长度场景下的能量效率，确定哪种方案最节能。

Method: 通过理论分析三种方案的最小能耗，并提出了联合优化任务卸载比例、卸载时间和无人机位置的交替优化算法。

Result: TDMA在无限和有限块长度情况下都比FDMA能耗更低；NOMA在有限块长度下不一定比FDMA更节能，特别是在信道条件和任务数据大小相对对称时。

Conclusion: TDMA是最节能的方案，提出的优化算法能有效降低MEC相关能耗。

Abstract: Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) systems can
use different multiple access schemes to coordinate multi-user task offloading.
However, it is still unknown which scheme is the most energy-efficient,
especially when the offloading blocklength is finite. To answer this question,
this paper minimizes and compares the MEC-related energy consumption of
non-orthogonal multiple access (NOMA), frequency division multiple access
(FDMA), and time division multiple access (TDMA)-based offloading schemes
within UAV-enabled MEC systems, considering both infinite and finite
blocklength scenarios. Through theoretically analysis of the minimum energy
consumption required by these three schemes, two novel findings are presented.
First, TDMA consistently achieves lower energy consumption than FDMA in both
infinite and finite blocklength cases, due to the degrees of freedom afforded
by sequential task offloading. Second, NOMA does not necessarily achieve lower
energy consumption than FDMA when the offloading blocklength is finite,
especially when the channel conditions and the offloaded task data sizes of two
user equipments (UEs) are relatively symmetric. Furthermore, an alternating
optimization algorithm that jointly optimizes the portions of task offloaded,
the offloading times of all UEs, and the UAV location is proposed to solve the
formulated energy consumption minimization problems. Simulation results verify
the correctness of our analytical findings and demonstrate that the proposed
algorithm effectively reduces MEC-related energy consumption compared to
benchmark schemes that do not optimize task offloading portions and/or
offloading times.

</details>


### [6] [Resource Allocation for XR with Edge Offloading: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.22505)
*Alperen Duru,Mohammad Mozaffari,Ticao Zhang,Mehrnaz Afshang*

Main category: cs.IT

TL;DR: 提出基于强化学习的资源分配框架，动态分配上下行时隙并做出卸载决策，通过部分卸载可扩展覆盖范围55%，降低能耗34%。


<details>
  <summary>Details</summary>
Motivation: 未来XR应用需要高能效、高数据速率和低延迟的无线通信，需要智能自适应的资源分配和边缘卸载来支持这些需求。

Method: 使用强化学习框架动态分配上下行时隙，根据XR头显能力和网络条件做出卸载决策，进行数值分析权衡帧丢失率和能效。

Result: 部分卸载相比始终或不卸载可扩展覆盖范围55%，降低能耗达34%，头显本地计算能力在卸载决策中起关键作用。

Conclusion: 头显本地计算能力越强，越能高效本地处理，减少卸载需求，提升节能效果。

Abstract: Future immersive XR applications will require energy-efficient, high data
rate, and low-latency wireless communications in uplink and downlink. One of
the key considerations for supporting such XR applications is intelligent and
adaptive resource allocation with edge offloading. To address these demands,
this paper proposes a reinforcement learning-based resource allocation
framework that dynamically allocates uplink and downlink slots while making
offloading decisions based on the XR headset's capabilities and network
conditions. The paper presents a numerical analysis of the tradeoff between
frame loss rate (FLR) and energy efficiency, identifying decision regions for
partial offloading to optimize performance. Results show that for the used set
of system parameters, partial offloading can extend the coverage area by 55%
and reduce energy consumption by up to 34%, compared to always or never
offloading. The results demonstrate that the headset's local computing
capability plays a crucial role in offloading decisions. Higher computing
abilities enable more efficient local processing, reduce the need for
offloading, and enhance energy savings.

</details>


### [7] [End-to-end Learning of Probabilistic and Geometric Constellation Shaping with Iterative Receivers](https://arxiv.org/abs/2510.22608)
*Harindu Jayarathne,Dileepa Marasinghe,Nandana Rajatheva,Matti Latva-aho*

Main category: cs.IT

TL;DR: 提出了一种端到端的星座成形学习方法，通过成形编码器辅助的收发器架构，联合优化符号概率分布和星座几何形状，在两种迭代接收器架构下相比标准APSK和QAM获得了0.3dB和0.15dB的BER增益。


<details>
  <summary>Details</summary>
Motivation: 为了获得更高效的符号概率分布和优化的星座几何形状，提高通信系统的比特误码率性能。

Method: 使用端到端学习方法，通过成形编码器产生零概率更高的成形比特，联合优化概率分布和星座几何形状，并扩展应用深度展开技术实现完整的迭代检测和解码循环。

Result: 在两种接收器架构下，学习到的星座相比标准APSK或QAM分别获得了最大0.3dB和0.15dB的BER增益；在块衰落信道条件下，迭代方案相比标准APSK获得了0.1dB的BER增益。

Conclusion: 端到端学习方法能够有效优化星座成形，在多种接收器架构和信道条件下均能带来显著的BER性能提升。

Abstract: An end-to-end learning method for constellation shaping with a
shaping-encoder assisted transceiver architecture is presented. The shaping
encoder, which produces shaping bits with a higher probability of zeros, is
used to produce an efficient symbol probability distribution. Both the
probability distribution and the constellation geometry are jointly optimized,
using end-to-end learning. Optimized constellations are evaluated using two
iterative receiver architectures. Bit error rate (BER) performance gain is
quantified against standard amplitude phase-shift keying (APSK) and quadrature
amplitude modulation (QAM) constellations. A maximum BER gain of 0.3 dB and
0.15 dB are observed under two receivers for the learned constellations
compared to standard APSK or QAM. The basic approach is extended to incorporate
the full iterative detection and decoding loop, using the deep unfolding
technique. A bit error rate gain of 0.1 dB is observed for the iterative scheme
with learned constellations under block fading channel conditions, when
compared to standard APSK.

</details>


### [8] [Graph-Theoretic Characterization of Noise Capacity of Conditional Disclosure of Secrets](https://arxiv.org/abs/2510.22671)
*Zhou Li,Siyan Qin,Xiang Zhang,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了条件秘密披露(CDS)问题中的噪声容量，确定了噪声容量达到最大值1的充要条件，并为任意CDS实例推导了线性噪声容量的上界。


<details>
  <summary>Details</summary>
Motivation: 在CDS问题中，Alice和Bob希望通过引入噪声来安全地向Carol披露秘密，但仅当他们的输入满足特定函数关系时才成功。研究目标是确定噪声容量，即安全披露给Carol的最大秘密比特数与Alice和Bob共同持有的独立噪声比特总数之比。

Method: 通过图论方法分析CDS实例，其中函数关系f用图表示，定义覆盖参数ρ和未合格路径中的未合格边数d等关键参数。

Result: 建立了噪声容量达到最大值1的充要条件，并推导出线性噪声容量的上界为(ρ-1)(d-1)/(ρd-1)。

Conclusion: 该工作为CDS协议中的噪声效率提供了理论界限，有助于设计更优化的条件秘密披露方案。

Abstract: In the problem of conditional disclosure of secrets (CDS), two parties, Alice
and Bob, each has an input and shares a common secret. Their goal is to reveal
the secret to a third party, Carol, as efficiently as possible, only if the
inputs of Alice and Bob satisfy a certain functional relation $f $. To prevent
leakage of the secret to Carol when the input combination is unqualified, both
Alice and Bob introduce noise. This work aims to determine the noise capacity,
defined as the maximum number of secret bits that can be securely revealed to
Carol, normalized by the total number of independent noise bits held jointly by
Alice and Bob. Our contributions are twofold. First, we establish the necessary
and sufficient conditions under which the CDS noise capacity attains its
maximum value of $1$. Second, in addition to the above best-case scenarios, we
derive an upper bound on the linear noise capacity for any CDS instance. In
particular, this upper bound is equal to $(\rho-1)(d-1)/(\rho d-1)$, where
$\rho$ is the covering parameter of the graph representation of $f$, and $d$ is
the number of unqualified edges in residing unqualified path.

</details>


### [9] [Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication](https://arxiv.org/abs/2510.22718)
*Yujie Wan,Chenxuan Liu,Shuai Wang,Tong Zhang,James Jianqiao Yu,Kejiang Ye,Dusit Niyato,Chengzhong Xu*

Main category: cs.IT

TL;DR: 提出ECO-GS系统，通过本地小模型和远程大模型的协作切换，在低成本设备上优化高斯泼溅渲染质量，并开发了PMM和ILO算法来解决协作状态和资源分配的联合优化问题。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅(GS)在低成本设备上渲染质量下降，需要一种既能保证实时性又能保证保真度的解决方案。

Method: 提出边缘协作GS(ECO-GS)系统，结合IRAC框架联合优化协作状态和边缘功率分配，使用PMM算法求解非凸问题，并开发ILO算法加速计算。

Result: PMM算法能获得临界点解，ILO算法相比PMM计算时间减少超过100倍，实验证明了PMM的优越性和ILO的实时执行能力。

Conclusion: ECO-GS系统通过智能的模型切换策略和高效的优化算法，在低成本设备上实现了渲染质量和实时性的平衡。

Abstract: Gaussian splatting (GS) struggles with degraded rendering quality on low-cost
devices. To address this issue, we present edge collaborative GS (ECO-GS),
where each user can switch between a local small GS model to guarantee
timeliness and a remote large GS model to guarantee fidelity. However, deciding
how to engage the large GS model is nontrivial, due to the interdependency
between rendering requirements and resource conditions. To this end, we propose
integrated rendering and communication (IRAC), which jointly optimizes
collaboration status (i.e., deciding whether to engage large GS) and edge power
allocation (i.e., enabling remote rendering) under communication constraints
across different users by minimizing a newly-derived GS switching function.
Despite the nonconvexity of the problem, we propose an efficient penalty
majorization minimization (PMM) algorithm to obtain the critical point
solution. Furthermore, we develop an imitation learning optimization (ILO)
algorithm, which reduces the computational time by over 100x compared to PMM.
Experiments demonstrate the superiority of PMM and the real-time execution
capability of ILO.

</details>


### [10] [On the Arikan Transformations of Binary-Input Discrete Memoryless Channels](https://arxiv.org/abs/2510.22896)
*Yadong Jiao,Xiaoyan Cheng,Yuansheng Tang,Ming Xu*

Main category: cs.IT

TL;DR: 本文提出了一种新方法，将极化码构造中的合成信道表征为二进制对称信道的随机切换信道，并推导了合成信道输出字母表中具有相同似然比的元素数量的下界。


<details>
  <summary>Details</summary>
Motivation: 由于合成信道输出字母表规模庞大，目前缺乏高效实用的方法来评估其可靠性，这阻碍了极化码的构造。

Method: 通过将极化码构造中的合成信道生成转换为代数操作，开发了一种方法来表征对称信道下的合成信道。

Result: 成功将合成信道表征为二进制对称信道的随机切换信道，并推导了输出字母表中具有相同似然比元素数量的下界。

Conclusion: 该方法为评估极化码合成信道的可靠性提供了理论基础，有助于解决极化码构造中的关键问题。

Abstract: The polar codes introduced by Arikan in 2009 achieve the capacity of
binary-input discrete memoryless channels (BIDMCs) with low complexity encoding
and decoding. Identifying the unreliable synthetic channels, generated by
Arikan transformation during the construction of these polar codes, is crucial.
Currently, because of the large size of the output alphabets of synthetic
channels, there is no efficient and practical approach to evaluate their
reliability in general. To tackle this problem, by converting the generation of
synthetic channels in polar code construction into algebraic operations, in
this paper we develop a method to characterize the synthetic channels as random
switching channels of binary symmetric channels when the underlying channels
are symmetric. Moreover, a lower bound for the average number of elements that
possess the same likelihood ratio within the output alphabet of any synthetic
channel generated in polar codes is also derived.

</details>


### [11] [On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges](https://arxiv.org/abs/2510.23230)
*Raúl Gutiérrez,Víctor Rampérez,Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.IT

TL;DR: 本文对信息融合技术用于提升信息质量的研究文献进行了系统性综述，分析了不同融合方法对信息质量的影响、信息质量的表征与评估方法，并提出了未来研究挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 信息融合领域近年来受到广泛关注，但现有文献综述多局限于特定问题或系统类型，缺乏对信息融合如何影响信息质量的系统性认知，特别是在信息质量表征、评估方法以及适应动态系统环境方面的研究不足。

Method: 采用文献综述方法，系统分析信息融合技术在提升信息质量方面的研究现状，重点关注不同融合方法对信息质量的影响机制、信息质量的表征与评估体系。

Result: 识别了信息融合在提升信息质量方面的关键问题，包括不同融合方法的效果差异、信息质量评估的领域依赖性、以及融合过程适应动态环境的能力等。

Conclusion: 提出了信息融合技术用于提升信息质量的研究挑战和未来方向，强调需要建立更系统化的理论框架和评估标准，以适应不同应用场景的需求。

Abstract: The information fusion field has recently been attracting a lot of interest
within the scientific community, as it provides, through the combination of
different sources of heterogeneous information, a fuller and/or more precise
understanding of the real world than can be gained considering the above
sources separately. One of the fundamental aims of computer systems, and
especially decision support systems, is to assure that the quality of the
information they process is high. There are many different approaches for this
purpose, including information fusion. Information fusion is currently one of
the most promising methods. It is particularly useful under circumstances where
quality might be compromised, for example, either intrinsically due to
imperfect information (vagueness, uncertainty) or because of limited resources
(energy, time). In response to this goal, a wide range of research has been
undertaken over recent years. To date, the literature reviews in this field
have focused on problem-specific issues and have been circumscribed to certain
system types. Therefore, there is no holistic and systematic knowledge of the
state of the art to help establish the steps to be taken in the future. In
particular, aspects like what impact different information fusion methods have
on information quality, how information quality is characterised, measured and
evaluated in different application domains depending on the problem data type
or whether fusion is designed as a flexible process capable of adapting to
changing system circumstances and their intrinsically limited resources have
not been addressed. This paper aims precisely to review the literature on
research into the use of information fusion techniques specifically to improve
information quality, analysing the above issues in order to identify a series
of challenges and research directions, which are presented in this paper.

</details>


### [12] [Pinching-antenna-enabled Federated Learning: Tail Latency, Participation, and Convergence Analysis](https://arxiv.org/abs/2510.23315)
*Yushen Lin,Zihan Chen,Zhiguo Ding*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated learning (FL) in wireless networks is limited by straggler delays
from unpredictable channel conditions. In this paper, we investigate the
pinching-antenna system (PASS), which dynamically 'pinches' the radiator along
a dielectric waveguide to shorten the worst links. In synchronous FL (SFL), we
prove that PASS shortens the worst-link distance, and it increases the on-time
completion probability in asynchronous FL (AFL). Accordingly, SFL exhibits
stochastic dominance on round time, while AFL yields explicit latency and
participation gains. We then pair physical-layer (PHY)-aware sampling with
error-feedback compression and prove that pinching raises the minimum inclusion
probability, thus shrinking both the sampling variability and
compression-induced floors in a Lyapunov analysis. Simulations demonstrate
consistent wall clock speedups and markedly shorter latency tails. By
addressing stragglers at their PHY root, PASS complements higher-layer
scheduling and accelerates wireless FL in both SFL and AFL.

</details>


### [13] [Efficient Repair of (k+2, k) Degraded Read Friendly MDS Array Codes With Sub-packetization 2](https://arxiv.org/abs/2510.23316)
*Jie Li,Xiaohu Tang*

Main category: cs.IT

TL;DR: 提出了两种具有两个奇偶校验节点的退化读友好MDS阵列码构造，子分组级别为2，适用于任意码长。第一种构造在相同参数下实现最小修复带宽，第二种构造支持两种修复机制以优化修复带宽或重建访问。


<details>
  <summary>Details</summary>
Motivation: 设计在小型有限域上具有最小修复带宽和优化重建访问的MDS阵列码，以改善存储系统的节点修复性能。

Method: 提出了两种构造方法：第一种构造专注于最小化修复带宽，第二种构造提供两种修复机制（允许或不允许辅助节点计算）来分别优化修复带宽或重建访问。

Result: 第一种构造在所有现有相同参数构造中实现了最小修复带宽，并且相对于Zhang等人的下界是渐近最优的。第二种构造提供了修复带宽和重建访问之间的权衡选择。

Conclusion: 这些构造为存储系统提供了高效的节点修复解决方案，在修复带宽和重建访问方面都达到了优化，适用于任意码长和小型有限域。

Abstract: In this paper, we present two constructions of degraded read friendly (DRF)
MDS array codes with two parity nodes and a sub-packetization level of 2 over
small finite fields, applicable for any arbitrary code length. The first
construction achieves the smallest repair bandwidth among all existing
constructions with the same parameters, and is asymptotically optimal with
respect to the lower bound on the average repair bandwidth characterized by
Zhang et al. The second construction supports two repair mechanisms, depending
on whether computation within the helper nodes is permitted or not during the
node repair process, thereby optimizing either the repair bandwidth or the
rebuilding access.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [14] [Automated Tinnitus Detection Through Dual-Modality Neuroimaging: EEG Microstate Analysis and Resting-State fMRI Classification Using Deep Learning](https://arxiv.org/abs/2510.21748)
*Kiana Kiashemshaki,Sina Samieirad,Sarvenaz Erfani,Aryan Jalaeianbanayan,Nasibeh Asadi Isakan,Hossein Najafzadeh*

Main category: eess.SP

TL;DR: 该研究应用机器学习分析EEG和fMRI数据，成功识别耳鸣患者的神经生物标志物，树基分类器和混合模型在耳鸣分类中表现出色，准确率最高达99.0%。


<details>
  <summary>Details</summary>
Motivation: 耳鸣影响10-15%人口但缺乏客观诊断生物标志物，需要开发基于神经影像的客观诊断方法。

Method: 分析64通道EEG数据（80人）和静息态fMRI数据（38人）。EEG分析提取微状态特征和全局场功率小波图像；fMRI使用切片级CNN和混合模型（VGG16、ResNet50结合决策树、随机森林、SVM）。采用5折交叉验证评估模型性能。

Result: EEG微状态分析显示耳鸣患者gamma波段微状态B出现率降低（健康：56.56 vs 耳鸣：43.81，p<0.001），alpha覆盖减少。树基分类器准确率达98.8%，VGG16小波EEG在delta和alpha波段分别达95.4%和94.1%。fMRI切片17准确率达99.0%，混合VGG16-决策树模型准确率98.95%。

Conclusion: EEG和fMRI提供了有效的耳鸣神经生物标志物，树基和混合模型性能优越，表明耳鸣是多网络障碍需要多模态分析。

Abstract: Objective: Tinnitus affects 10-15% of the population yet lacks objective
diagnostic biomarkers. This study applied machine learning to EEG and fMRI data
to identify neural signatures distinguishing tinnitus patients from healthy
controls. Methods: Two datasets were analyzed: 64-channel EEG recordings from
80 participants (40 tinnitus, 40 controls) and resting-state fMRI data from 38
participants (19 tinnitus, 19 controls). EEG analysis extracted microstate
features across four to seven clustering states and five frequency bands,
producing 440 features per subject. Global Field Power signals were also
transformed into wavelet images for deep learning. fMRI data were analyzed
using slice-wise convolutional neural networks and hybrid models combining
pre-trained architectures (VGG16, ResNet50) with Decision Tree, Random Forest,
and SVM classifiers. Model performance was evaluated using 5-fold
cross-validation based on accuracy, precision, recall, F1-score, and ROC-AUC.
Results: EEG microstate analysis revealed altered network dynamics in tinnitus,
particularly reduced gamma-band microstate B occurrence (healthy: 56.56 vs
tinnitus: 43.81, p < 0.001) and diminished alpha coverage. Tree-based
classifiers achieved up to 98.8% accuracy, while VGG16 on wavelet-transformed
EEG yielded 95.4% and 94.1% accuracy for delta and alpha bands, respectively.
fMRI analysis identified 12 high-performing axial slices (>=90% accuracy), with
slice 17 reaching 99.0%. The hybrid VGG16-Decision Tree model achieved 98.95%
+/- 2.94% accuracy. Conclusion: EEG and fMRI provided effective neural
biomarkers for tinnitus classification. Tree-based and hybrid models
demonstrated superior performance, highlighting tinnitus as a multi-network
disorder requiring multimodal analysis.

</details>


### [15] [Monitoring Real-Time ECG Signals on Mobile Systems](https://arxiv.org/abs/2510.21789)
*Beyazit Bestami Yuksel*

Main category: eess.SP

TL;DR: 开发了一个基于移动系统的实时心电图监测系统，通过非侵入方法采集ECG信号，当信号异常时提供警报，具有便携性和扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 实现便携式实时心电图监测，为患者提供及时的健康警报和潜在的远程医疗支持。

Method: 使用Visual Studio .NET平台开发软件，通过Einthoven三角法放置电极，串口传输数据，在移动设备上图形化显示ECG信号。

Result: 成功开发出完全便携的系统，能够实时监测ECG信号并在异常时发出警报。

Conclusion: 该系统为未来多用途医疗监测系统（如在线患者监测、位置跟踪和除颤干预）奠定了基础。

Abstract: This study focuses on the connection of a development kit that enables
real-time monitoring of electrocardiogram (ECG) signals using a mobile system.
A software developed on the Visual Studio .NET platform reads real-time ECG
signals from the human body through non invasive methods and displays them
graphically on the mobile system. ECG electrodes placed on specific areas of
the body using the method known as Einthoven's triangle. Subsequently, the
software initiates data flow through the serial port, and these data displayed
as signal values on the mobile device's screen via a graphical interface. When
the monitored ECG signals fall below a certain threshold or reach a critical
value, the system provides feedback with an alert based on medical data. The
developed system is fully portable. Additionally, the implemented system has
the potential to form the basis for a multi-purpose system in the future, such
as online patient monitoring, patient location tracking, and even initial
intervention using the defibrillation method.

</details>


### [16] [Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification](https://arxiv.org/abs/2510.21969)
*Weiyu Chen,Arnaud Delorme*

Main category: eess.SP

TL;DR: 提出AS-MMD方法解决小样本P300检测中的跨数据集迁移问题，结合目标加权损失、分割批归一化和RBF-MMD正则化，在EEG Conformer上实现优于目标单独训练和池化训练的性能。


<details>
  <summary>Details</summary>
Motivation: 解决小样本P300检测中跨数据集迁移时的数据偏移问题，当目标数据集样本极少（10个试次/被试）而源数据集较大（80个试次/被试）时，传统迁移学习方法效果不佳。

Method: 提出AS-MMD方法，包含三个组件：目标加权损失与热身训练、分割批归一化（共享仿射参数但独立运行统计）、无参数RBF核最大均值差异正则化。基于EEG Conformer实现，方法对骨干网络不可知且不影响推理时模型。

Result: 在两个视觉oddball ERP数据集上的双向迁移实验中，AS-MMD在准确率和AUC指标上均优于目标单独训练和池化训练（Active Visual Oddball: 0.66/0.74; ERP CORE P3: 0.61/0.65），增益在配对t检验中显著。消融实验表明三个组件均有贡献。

Conclusion: AS-MMD方法有效解决了小样本P300检测中的跨数据集迁移问题，通过精心设计的组件组合提升了迁移性能，为脑电信号分析中的小样本学习提供了有效解决方案。

Abstract: Detecting single-trial P300 from EEG is difficult when only a few labeled
trials are available. When attempting to boost a small target set with a large
source dataset through transfer learning, cross-dataset shift arises. To
address this challenge, we study transfer between two public visual-oddball ERP
datasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict
small-sample regime (target: 10 trials/subject; source: 80 trials/subject). We
introduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which
combines (i) a target-weighted loss with warm-up tied to the square root of the
source/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared
affine parameters and per-domain running statistics, and (iii) a parameter-free
logit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)
term using the median-bandwidth heuristic. Implemented on an EEG Conformer,
AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.
Across both transfer directions, it outperforms target-only and pooled training
(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with
gains over pooling significant under corrected paired t-tests. Ablations
attribute improvements to all three components.

</details>


### [17] [Experimental Demonstration of Multi-Object Tracking in Integrated Sensing and Communication](https://arxiv.org/abs/2510.22180)
*Maximilian Bauhofer,Marcus Henninger,Meik Kottkamp,Lucas Giroto,Philip Grill,Alexander Felix,Thorsten Wild,Stephan ten Brink,Silvio Mandelli*

Main category: eess.SP

TL;DR: 该论文展示了在真实工厂环境中使用5G兼容的ISAC概念验证系统，基于概率假设密度(PHD)滤波器在距离和多普勒速度域进行多目标跟踪，取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 将跟踪技术集成到蜂窝通信系统中对于广泛的ISAC应用场景至关重要，但现有的多目标跟踪算法尚未应用于具有杂波和非理想硬件等挑战的真实世界ISAC环境。

Method: 使用5G兼容的ISAC概念验证系统在真实工厂环境中采集测量数据，通过雷达对象模拟器生成类行人目标，采用概率假设密度(PHD)滤波器在距离和多普勒速度域进行多目标跟踪，详细描述了从测量采集到评估的完整处理流程。

Result: 端到端评估显示良好的多目标跟踪性能，在现实但具有挑战性的场景中，平均绝对误差<1.5米，检测率>91%。

Conclusion: 该研究成功展示了在真实ISAC环境中应用PHD滤波器进行多目标跟踪的可行性，为集成感知与通信系统的实际部署提供了重要参考。

Abstract: For a wide range of envisioned integrated sensing and communication (ISAC)
use cases, it is necessary to incorporate tracking techniques into cellular
communication systems. While numerous multi-object tracking algorithms exist,
they have not yet been applied to real-world ISAC, with its challenges such as
clutter and non-optimal hardware. In this work, we showcase multi-object
tracking based on the probability hypothesis density (PHD) filter in the range
and Doppler speed domain. The measurements are taken with a 5G compliant ISAC
proof-of-concept in a real factory environment, where the pedestrian-like
objects are generated by a radar object emulator. We detail the complete
pipeline, from measurement acquisition to evaluation, with a focus on the
post-processing of the raw captured data and the tracking itself. Our
end-to-end evaluation and comparison to simulations show good multi-object
tracking performance with mean absolute error <1.5m and detection rates >91%
for realistic but challenging scenarios.

</details>


### [18] [Angular Estimation Comparison with ISAC PoC](https://arxiv.org/abs/2510.22297)
*Alexander Felix,Rudolf Hoffmann,Marcus Henninger,Stephan ten Brink,Silvio Mandelli*

Main category: eess.SP

TL;DR: 评估基于最小DFT角度样本集的角度估计方法，发现插值方法在泛化性上更优，而OMP在单目标估计中最准确，DFT插值整体性能最佳


<details>
  <summary>Details</summary>
Motivation: ISAC系统采用模拟或混合波束成形结构限制了角度能力，需要基于最少角度样本实现目标角度估计和分离

Method: 使用ISAC概念验证系统在ARENA2036工业场景中扫描多个波束，评估基于最小DFT角度样本集的不同角度估计方法

Result: 插值方法在不同角度场景中泛化性更好；OMP对单个强目标估计最准确；DFT插值方法整体性能最佳

Conclusion: DFT插值方法在角度估计中表现最优，为ISAC用例的成功实施提供了有效解决方案

Abstract: The introduction of Integrated Sensing and Communications (ISAC) in cellular
systems is not expected to result in a shift away from the popular choice of
cost- and energy-efficient analog or hybrid beamforming structures. However,
this comes at the cost of limiting the angular capabilities to a confined space
per acquisitions. Thus, as a prerequisite for the successful implementation of
numerous ISAC use cases, the need for an optimal angular estimation of targets
and their separation based on the minimal number of angular samples arises.
  In this work, different approaches for angular estimation based on a minimal,
DFT-based set of angular samples are evaluated. The samples are acquired
through sweeping multiple beams of an ISAC proof of concept (PoC) in the
industrial scenario of the ARENA2036. The study's findings indicate that
interpolation approaches are more effective for generalizing across different
types of angular scenarios. While the orthogonal matching pursuit (OMP)
approach exhibits the most accurate estimation for a single, strong and clearly
discriminable target, the DFT-based interpolation approach demonstrates the
best overall estimation performance.

</details>


### [19] [Data-driven, Wavelet-based Identification and Reduced-order Modeling of Linear Systems with Closely Spaced Modes](https://arxiv.org/abs/2510.22406)
*Anargyros Michaloliakos,Benjamin J. Chang,Lawrence A. Bergman,Alexander F. Vakakis*

Main category: eess.SP

TL;DR: 提出基于小波变换的模态识别和降阶建模框架，有效解决传统傅里叶方法在识别密集模态和非经典阻尼系统时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶方法难以可靠识别密集模态和准确捕捉模态相互作用，特别是在具有非经典阻尼分布的机械系统中。

Method: 利用连续小波变换(CWT)的时频分辨率优势，通过选择小波谱中的谐波区域来隔离模态，应用逆小波变换(ICWT)重构时域模态动态，再通过希尔伯特变换提取瞬时相位构建复数模态矩阵。

Result: 该方法在数值非经典阻尼和飞机结构实验模型上验证有效，能够准确解析复杂模态相互作用并重现结构系统的动态响应。

Conclusion: 所提出的基于小波的方法为具有密集模态和非经典阻尼的机械系统提供了可靠的模态识别和降阶建模解决方案。

Abstract: This work presents a purely data-driven, wavelet-based framework for modal
identification and reduced-order modeling of mechanical systems with assumed
linear dynamics characterized by closely spaced modes with classical or
non-classical damping distribution. Traditional Fourier-based methods often
fail to reliably identify closely spaced modes or accurately capture modal
interactions and complexities. To address these limitations, we propose a
methodology leveraging the enhanced time -frequency resolution capabilities of
the continuous wavelet transform (CWT). By selecting appropriate harmonic
regions within the wavelet spectra, we effectively isolate modes, and then
invert them back in the temporal domain by applying the inverse CWT (ICWT). In
this way we reconstruct the corresponding modal dynamics in the time domain.
Using the Hilbert transform, instantaneous phases are extracted for each
identified mode, enabling the introduction of a complexified modal matrix which
robustly characterizes the system's modal properties, even under challenging
perturbations such as noise and uncertainties due to modal interference and
unmodeled effects. The identified modal parameters are utilized to reconstruct
the frequency response functions (FRFs) of the system and to develop a
reduced-order model (ROM) that captures accurately the system's dominant
dynamical behavior valid in a specified frequency range.. Validation of the
methodology is conducted both with a numerical non-classical damping and an
experimental testbed representing a model of an airplane structure. Results
demonstrate the effectiveness of the proposed approach in resolving intricate
modal interactions and accurately reproducing the dynamic response of complex
structural systems.

</details>


### [20] [Genetic Optimization of a Software-Defined GNSS Receiver](https://arxiv.org/abs/2510.22417)
*Laura Train,Rodrigo Castellanos,Miguel Gómez-López*

Main category: eess.SP

TL;DR: 提出基于遗传算法的优化框架，用于自动优化GNSS接收机的跟踪环路参数，以解决高动态环境下传统接收机性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 商用GNSS接收机在高动态条件下性能显著下降，传统跟踪环路带宽无法应对同步参数的快速变化，而软件定义无线电接收机的手动调优过程复杂且难以保证最优性能。

Method: 使用遗传算法自主探索接收机配置空间，优化相位、频率和延迟跟踪的环路参数，并在GNSS-SDR开源架构中使用模拟GPS L1信号进行验证。

Result: 优化后的配置在三种动态场景下均能保持稳健准确的PVT解：静态情况下最大位置和速度误差分别为6米和0.08米/秒；火箭飞行情况下为12米和2.5米/秒；低地球轨道卫星情况下为5米和0.2米/秒。

Conclusion: 进化优化使SDR接收机能够在各种动态条件下保持稳健准确的PVT解，解决了高动态环境下的GNSS接收机性能问题。

Abstract: Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)
receivers face significant limitations under high-dynamic conditions,
particularly in high-acceleration environments such as those experienced by
launch vehicles. These performance degradations, often observed as
discontinuities in the navigation solution, arise from the inability of
traditional tracking loop bandwidths to cope with rapid variations in
synchronization parameters. Software-Defined Radio (SDR) receivers overcome
these constraints by enabling flexible reconfiguration of tracking loops;
however, manual tuning involves a complex, multidimensional search and seldom
ensures optimal performance. This work introduces a genetic algorithm-based
optimization framework that autonomously explores the receiver configuration
space to determine optimal loop parameters for phase, frequency, and delay
tracking. The approach is validated within an SDR environment using
realistically simulated GPS L1 signals for three representative dynamic regimes
-guided rocket flight, Low Earth Orbit (LEO) satellite, and static
receiver-processed with the open-source GNSS-SDR architecture. Results
demonstrate that evolutionary optimization enables SDR receivers to maintain
robust and accurate Position, Velocity, and Time (PVT) solutions across diverse
dynamic conditions. The optimized configurations yielded maximum position and
velocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and
2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case.

</details>


### [21] [Data-driven Exponential Framing for Pulsive Temporal Patterns without Repetition or Singularity](https://arxiv.org/abs/2510.22472)
*Yohei Kono,Yoshiyuki Tajima*

Main category: eess.SP

TL;DR: 提出了一种名为数据驱动指数框架（DEF）的方法，用于从小数据集中提取不重复或单一的脉冲时间模式，通过时间延迟嵌入和Hankel矩阵分析来量化模式持续时间。


<details>
  <summary>Details</summary>
Motivation: 在制造业应用中，从不重复或单一的小数据集中提取脉冲时间模式具有重要意义，但科学界对此关注不足。

Method: 基于时间延迟嵌入和数据驱动的Hankel矩阵分析，在时间延迟坐标上建立线性动态系统模型，推导具有不同指数衰减常数的离散时间基，并通过滑动窗口拟合子序列来量化模式持续时间。

Result: 玩具模型实验显示DEF能识别具有不同长度的多个模式；在冲压机电流测量中的应用表明该方法能从真实世界振荡数据中提取多个模式。

Conclusion: DEF方法能够有效量化时间模式的持续时间，为从小数据集中提取不重复的脉冲模式提供了可行方案。

Abstract: Extracting pulsive temporal patterns from a small dataset without their
repetition or singularity shows significant importance in manufacturing
applications but does not sufficiently attract scientific attention. We propose
to quantify how long temporal patterns appear without relying on their
repetition or singularity, enabling to extract such temporal patterns from a
small dataset. Inspired by the celebrated time delay embedding and data-driven
Hankel matrix analysis, we introduce a linear dynamical system model on the
time-delay coordinates behind the data to derive the discrete-time bases each
of which has a distinct exponential decay constant. The derived bases are
fitted onto subsequences that are extracted with a sliding window in order to
quantify how long patterns are dominant in the set of subsequences. We call the
quantification method Data-driven Exponential Framing (DEF). A toy model-based
experiment shows that DEF can identify multiple patterns with distinct lengths.
DEF is also applied to electric current measurement on a punching machine,
showing its possibility to extract multiple patterns from real-world
oscillatory data.

</details>


### [22] [Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO](https://arxiv.org/abs/2510.22557)
*Wang Liu,Cunhua Pan,Hong Ren,Wei Zhang,Cheng-Xiang Wang,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出基于CNN-GPT2的框架解决毫米波通信中大规模天线阵列的近场波束预测问题，通过联合采样角度和距离域来降低导频开销，并处理非线性动态变化。


<details>
  <summary>Details</summary>
Motivation: 大规模天线阵列在毫米波通信中面临近场波束预测挑战，传统远场假设不再适用，需要联合采样角度和距离域，导致导频开销急剧增加，且最优波束索引呈现非线性动态变化。

Method: 设计上行链路导频传输策略，采用宽波束模拟预编码和频率变化数字预编码进行高效信道探测；接收信号经预处理后通过CNN特征提取器，再由GPT-2模型捕获多帧间时序依赖，端到端预测近场波束索引。

Result: 提出的CNN-GPT2框架能够有效处理近场波束预测中的非线性动态特性，通过时序建模降低导频开销。

Conclusion: CNN-GPT2框架为毫米波通信中大规模天线阵列的近场波束预测提供了有效解决方案，解决了传统方法面临的导频开销大和时序建模困难的问题。

Abstract: The emergence of extremely large-scale antenna arrays (ELAA) in
millimeter-wave (mmWave) communications, particularly in high-mobility
scenarios, highlights the importance of near-field beam prediction. Unlike the
conventional far-field assumption, near-field beam prediction requires
codebooks that jointly sample the angular and distance domains, which leads to
a dramatic increase in pilot overhead. Moreover, unlike the far-field case
where the optimal beam evolution is temporally smooth, the optimal near-field
beam index exhibits abrupt and nonlinear dynamics due to its joint dependence
on user angle and distance, posing significant challenges for temporal
modeling. To address these challenges, we propose a novel Convolutional Neural
Network-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beam
prediction framework. Specifically, an uplink pilot transmission strategy is
designed to enable efficient channel probing through widebeam analog precoding
and frequency-varying digital precoding. The received pilot signals are
preprocessed and passed through a CNN-based feature extractor, followed by a
GPT-2 model that captures temporal dependencies across multiple frames and
directly predicts the near-field beam index in an end-to-end manner.

</details>


### [23] [Parametric Channel Estimation and Design for Active-RIS-Assisted Communications](https://arxiv.org/abs/2510.22621)
*Md. Shahriar Sadid,Ali A. Nasir,Saad Al-Ahmadi,Samir Al-Ghadhban*

Main category: eess.SP

TL;DR: 提出了一种针对主动RIS的参量信道估计方法，通过自适应MLE和正交角度对码本，用极少的导频实现近最优性能


<details>
  <summary>Details</summary>
Motivation: 解决RIS技术中用户到RIS信道状态信息获取困难的问题，特别是级联信道结构和高导频开销的限制，主动RIS通过信号放大改善了实际部署的可行性

Method: 集成主动RIS模型与自适应最大似然估计器，采用自适应RIS配置策略和正交角度对码书替代传统DFT码书

Result: 在相同总功率预算下，与被动RIS相比，主动RIS通过消除乘性衰落和分配更多资源给数据传输，实现了更高的频谱效率

Conclusion: 所提出的方法用极少的导频就能达到近最优性能，显著优于非参量方法，且主动RIS在频谱效率方面优于传统被动RIS

Abstract: Reconfigurable Intelligent Surface (RIS) technology has emerged as a key
enabler for future wireless communications. However, its potential is
constrained by the difficulty of acquiring accurate user-to-RIS channel state
information (CSI), due to the cascaded channel structure and the high pilot
overhead of non-parametric methods. Unlike a passive RIS, where the reflected
signal suffers from multiplicative path loss, an active RIS amplifies the
signal, improving its practicality in real deployments. In this letter, we
propose a parametric channel estimation method tailored for active RISs. The
proposed approach integrates an active RIS model with an adaptive Maximum
Likelihood Estimator (MLE) to recover the main channel parameters using a
minimal number of pilots. To further enhance performance, an adaptive active
RIS configuration strategy is employed, which refines the beam direction based
on an initial user location estimate. Moreover, an orthogonal angle-pair
codebook is used instead of the conventional Discrete Fourier Transform (DFT)
codebook, significantly reducing the codebook size and ensuring reliable
operation for both far-field and near-field users. Extensive simulations
demonstrate that the proposed method achieves near-optimal performance with
very few pilots compared to non-parametric approaches. Its performance is also
benchmarked against that of a traditional passive RIS under the same total
power budget to ensure fairness. Results show that active RIS yields higher
spectral efficiency (SE) by eliminating the multiplicative fading inherent in
passive RISs and allocating more resources to data transmission

</details>


### [24] [Enhancing WiFi CSI Fingerprinting: A Deep Auxiliary Learning Approach](https://arxiv.org/abs/2510.22731)
*Yong Huang,Wenjing Wang,Dalong Zhang,Junjie Wang,Chen Chen,Yan Cao,Wei Wang*

Main category: eess.SP

TL;DR: CSI2Q是一个新颖的CSI指纹识别系统，通过将频域CSI测量转换为时域信号，并采用深度辅助学习策略从IQ指纹模型转移知识，在开放世界设置下实现了与IQ方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RF指纹识别技术依赖专用设备捕获IQ样本，限制了广泛应用。虽然商用WiFi设备可获取CSI用于轻量级RF指纹识别，但在开放世界设置中面临CSI测量粒度粗的挑战。

Method: 首先将频域CSI测量转换为与IQ样本共享特征空间的时域信号；然后采用深度辅助学习策略从IQ指纹模型向CSI模型转移有用知识；最后结合OpenMax函数估计未知设备的可能性。

Result: 在包含85个设备的合成CSI数据集和两个分别包含10和25个WiFi路由器的真实CSI数据集上评估，系统在合成CSI数据集上准确率提升至少16%，在实验室CSI数据集上提升20%，在野外CSI数据集上提升17%。

Conclusion: CSI2Q系统成功解决了CSI测量在开放世界设置中的粗粒度问题，实现了与基于IQ的方法相当的性能，为轻量级RF指纹识别提供了可行方案。

Abstract: Radio frequency (RF) fingerprinting techniques provide a promising supplement
to cryptography-based approaches but rely on dedicated equipment to capture
in-phase and quadrature (IQ) samples, hindering their wide adoption. Recent
advances advocate easily obtainable channel state information (CSI) by
commercial WiFi devices for lightweight RF fingerprinting, while falling short
in addressing the challenges of coarse granularity of CSI measurements in an
open-world setting. In this paper, we propose CSI2Q, a novel CSI fingerprinting
system that achieves comparable performance to IQ-based approaches. Instead of
extracting fingerprints directly from raw CSI measurements, CSI2Q first
transforms frequency-domain CSI measurements into time-domain signals that
share the same feature space with IQ samples. Then, we employ a deep auxiliary
learning strategy to transfer useful knowledge from an IQ fingerprinting model
to the CSI counterpart. Finally, the trained CSI model is combined with an
OpenMax function to estimate the likelihood of unknown ones. We evaluate CSI2Q
on one synthetic CSI dataset involving 85 devices and two real CSI datasets,
including 10 and 25 WiFi routers, respectively. Our system achieves accuracy
increases of at least 16% on the synthetic CSI dataset, 20% on the in-lab CSI
dataset, and 17% on the in-the-wild CSI dataset.

</details>


### [25] [Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition](https://arxiv.org/abs/2510.22772)
*Yizhuo Wu,Francesco Fioranelli,Chang Gao*

Main category: eess.SP

TL;DR: 提出了Neural-HAR，一种专为资源受限平台上的实时雷达人体活动识别设计的维度门控CNN加速器，核心是参数高效的GateCNN网络，在FPGA上实现了107.5μs延迟和15mW动态功耗。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/RNN解决方案在边缘部署上过于笨重，即使是轻量级ViT/SSM变体也经常超出实际计算和内存预算，需要开发更高效的雷达人体活动识别方案。

Method: 使用GateCNN网络，通过嵌入多普勒向量强调频率随时间演变，应用双路径门控卷积调制多普勒感知内容特征，并辅以残差路径稳定训练。

Result: 在UoG2020连续雷达数据集上达到86.4%准确率，仅需2.7k参数和0.28M FLOPs，FPGA原型实现107.5μs延迟和15mW动态功耗。

Conclusion: Neural-HAR展示了在资源受限平台上实现实时、节能边缘推理的可行性，为雷达人体活动识别提供了高效的解决方案。

Abstract: Radar-based human activity recognition (HAR) is attractive for unobtrusive
and privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy
for edge deployment, and even lightweight ViT/SSM variants often exceed
practical compute and memory budgets. We introduce Neural-HAR, a
dimension-gated CNN accelerator tailored for real-time radar HAR on
resource-constrained platforms. At its core is GateCNN, a parameter-efficient
Doppler-temporal network that (i) embeds Doppler vectors to emphasize frequency
evolution over time and (ii) applies dual-path gated convolutions that modulate
Doppler-aware content features with temporal gates, complemented by a residual
path for stable training. On the University of Glasgow UoG2020 continuous radar
dataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M
FLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.
Our FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\mu$s latency and
15 mW dynamic power using LUT-based ROM and distributed RAM only (zero
DSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and
HLS conversion scripts are available at https://github.com/lab-emi/AIRHAR.

</details>


### [26] [Rmd: Robust Modal Decomposition with Constrained Bandwidth](https://arxiv.org/abs/2510.22895)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yang Yifan,Tan Chenxing,Fu Weifeng*

Main category: eess.SP

TL;DR: 提出了一种带带宽约束的鲁棒模态分解方法，结合了数值优化和频谱分解两种方法的优点，通过相空间映射和带宽约束增强抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 现有的模态分解方法分为两类：数值优化方法会产生虚假模态，频谱分解方法对噪声敏感且难以处理非线性信号。需要一种能结合两者优点的模态分解方法。

Method: 提出RMD方法，通过将时间序列映射到相空间的轨迹-GRAM矩阵来保持信号内在结构，并在分解过程中加入带宽约束以增强抗噪性。

Result: 在合成数据和真实数据集上的广泛实验表明该方法有效且通用，适用于毫米波雷达回波、心电图、心音图和轴承故障检测等应用。

Conclusion: RMD方法成功结合了两种模态分解方法的优势，在保持信号结构的同时增强了抗噪能力，具有很好的实用价值。

Abstract: Modal decomposition techniques, such as Empirical Mode Decomposition (EMD),
Variational Mode Decomposition (VMD), and Singular Spectrum Analysis (SSA),
have advanced time-frequency signal analysis since the early 21st century.
These methods are generally classified into two categories: numerical
optimization-based methods (EMD, VMD) and spectral decomposition methods (SSA)
that consider the physical meaning of signals. The former can produce spurious
modes due to the lack of physical constraints, while the latter is more
sensitive to noise and struggles with nonlinear signals. Despite continuous
improvements in these methods, a modal decomposition approach that effectively
combines the strengths of both categories remains elusive. This paper thus
proposes a Robust Modal Decomposition (RMD) method with constrained bandwidth,
which preserves the intrinsic structure of the signal by mapping the time
series into its trajectory-GRAM matrix in phase space. Moreover, the method
incorporates bandwidth constraints during the decomposition process, enhancing
noise resistance. Extensive experiments on synthetic and real-world datasets,
including millimeter-wave radar echoes, electrocardiogram (ECG),
phonocardiogram (PCG), and bearing fault detection data, demonstrate the
method's effectiveness and versatility. All code and dataset samples are
available on GitHub: https://github.com/Einstein-sworder/RMD.

</details>


### [27] [Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function](https://arxiv.org/abs/2510.22913)
*Thanyanee Srichaisak,Arissa Ieochai,Aueaphum Aueawattthanaphisut*

Main category: eess.SP

TL;DR: 开发了一种集成多模态传感器的可穿戴设备，用于上肢康复，通过低延迟AI推理和安全约束的辅助策略，在健康成人中验证了减少震颤、改善运动范围和任务完成率的技术可行性。


<details>
  <summary>Details</summary>
Motivation: 上肢无力和震颤限制了日常生活活动并降低了家庭康复的依从性，需要开发能够提供实时辅助的可穿戴技术。

Method: 使用轻量级节点集成表面肌电、惯性测量单元和弯曲/力传感器，采用设备端INT8推理（Tiny 1D-CNN/Transformer）和安全约束的辅助策略（角度/扭矩/加速度限制；停滞/超时保护）。12名健康成人执行三项日常生活活动任务。

Result: 辅助显著降低震颤指数（-0.092，95% CI [-0.102, -0.079]），增加运动范围（+12.65%，95% CI [+8.43, +13.89]）和任务完成率（+2.99次/分钟，95% CI [+2.61, +3.35]）。设备端延迟中位数为8.7ms，所有会话均完成且无设备相关不良事件。

Conclusion: 多模态传感与低延迟、安全约束的辅助在技术可行性试验中改善了运动质量和任务完成率，支持推进至IRB批准的临床研究。

Abstract: Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of
daily living (ADL) and reduce adherence to home rehabilitation. Objective: To
assess technical feasibility and clinician-relevant signals of a sensor-fused
wearable targeting the triceps brachii and extensor pollicis brevis. Methods: A
lightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and
flex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and
a safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).
Healthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:
Tremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).
Secondary: EMG median-frequency slope (fatigue trend), closed-loop latency,
session completion, and device-related adverse events. Analyses used
subject-level paired medians with BCa 95\% CIs; exact Wilcoxon $p$-values are
reported in the Results. Results: Assistance was associated with lower tremor
prominence and improved task throughput: TI decreased by $-0.092$ (95\% CI
[$-0.102$, $-0.079$]), ROM increased by $+12.65\%$ (95\% CI [$+8.43$,
$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\% CI [$+2.61$, $+3.35$]).
Median on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were
completed with no device-related adverse events. Conclusions: Multimodal
sensing with low-latency, safety-bounded assistance produced improved movement
quality (TI $\downarrow$) and throughput (ROM, Reps $\uparrow$) in a pilot
technical-feasibility setting, supporting progression to IRB-approved patient
studies. Trial registration: Not applicable (pilot non-clinical).

</details>


### [28] [Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy](https://arxiv.org/abs/2510.22947)
*Yi Tao,Zhen Gao,Fangquan Ye,Jingbo Xu,Tao Song,Weidong Li,Yu Su,Lu Peng,Xiaomei Wu,Tong Qin,Zhongxiang Li,Dezhi Zheng*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的无人机综合管控系统，通过多模态传感器融合感知、精确定位和协同反制，实现了无人机的识别、跟踪和处置闭环管理。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展，无人机安全管理问题日益突出，准确的识别、实时定位和有效反制成为空域安全保障的核心挑战。

Method: 系统采用深度学习技术，在检测层面融合射频频谱特征分析、雷达探测、光电识别等方法；在定位层面基于多传感器数据融合和空天地一体化通信网络；在反制层面采用软杀伤与硬杀伤相结合的综合措施。

Result: 该系统显著提升了低空无人机管控的响应效率和处置精度，形成了从预警到最终处置的闭环管控流程。

Conclusion: 基于深度学习的无人机综合管控系统能够有效解决无人机安全管理问题，为空域安全提供可靠保障。

Abstract: The development of the low-altitude economy has led to a growing prominence
of uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate
identification, real-time localization, and effective countermeasures have
become core challenges in airspace security assurance. This paper introduces an
integrated UAV management and control system based on deep learning, which
integrates multimodal multi-sensor fusion perception, precise positioning, and
collaborative countermeasures. By incorporating deep learning methods, the
system combines radio frequency (RF) spectral feature analysis, radar
detection, electro-optical identification, and other methods at the detection
level to achieve the identification and classification of UAVs. At the
localization level, the system relies on multi-sensor data fusion and the
air-space-ground integrated communication network to conduct real-time tracking
and prediction of UAV flight status, providing support for early warning and
decision-making. At the countermeasure level, it adopts comprehensive measures
that integrate ``soft kill'' and ``hard kill'', including technologies such as
electromagnetic signal jamming, navigation spoofing, and physical interception,
to form a closed-loop management and control process from early warning to
final disposal, which significantly enhances the response efficiency and
disposal accuracy of low-altitude UAV management.

</details>


### [29] [PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming](https://arxiv.org/abs/2510.22948)
*Zhaoming Hu,Ruikang Zhong,Xidong Mu,Dengao Li,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种夹持天线系统增强的移动边缘计算架构，通过深度强化学习方法联合优化上行链路波束成形和任务卸载，以最小化网络延迟。


<details>
  <summary>Details</summary>
Motivation: 在动态无线环境中提高任务卸载效率和延迟性能，解决高频MEC系统中的显著路径损耗和潜在信号阻塞问题。

Method: 使用夹持天线系统建立短距离视距链路，将网络延迟最小化问题建模为马尔可夫决策过程，并提出了负载平衡感知的近端策略优化算法。

Result: 仿真结果表明，所提出的自适应上行链路波束成形方法比固定波束成形基线和传统MIMO辅助MEC具有更强的收敛能力，特别是在用户设备数量多或发射功率高的场景下。

Conclusion: 夹持天线系统增强的移动边缘计算架构结合自适应波束成形策略，能够有效改善动态无线环境中的任务卸载性能和延迟表现。

Abstract: A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)
architecture is investigated to improve the task offloading efficiency and
latency performance in dynamic wireless environments. By leveraging dielectric
waveguides and flexibly adjustable pinching antennas, PASS establishes
short-distance line-of-sight (LoS) links while effectively mitigating the
significant path loss and potential signal blockage, making it a promising
solution for high-frequency MEC systems. We formulate a network latency
minimization problem to joint optimize uplink PASS beamforming and task
offloading. The resulting problem is modeled as a Markov decision process (MDP)
and solved via the deep reinforcement learning (DRL) method. To address the
instability introduced by the $\max$ operator in the objective function, we
propose a load balancing-aware proximal policy optimization (LBPPO) algorithm.
LBPPO incorporates both node-level and waveguide-level load balancing
information into the policy design, maintaining computational and transmission
delay equilibrium, respectively. Simulation results demonstrate that the
proposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit
stronger convergence capability than fixed-PA baselines and conventional
MIMO-assisted MEC, especially in scenarios with a large number of UEs or high
transmit power.

</details>


### [30] [Planning Oriented Integrated Sensing and Communication](https://arxiv.org/abs/2510.23021)
*Xibin Jin,Guoliang Li,Shuai Wang,Fan Liu,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: eess.SP

TL;DR: 提出了一个规划导向的集成感知与通信框架，通过降低规划瓶颈障碍物的感知不确定性来提升自动驾驶车辆的安全性和效率


<details>
  <summary>Details</summary>
Motivation: 现有ISAC设计过于关注感知精度和通信吞吐量，忽视了关键障碍物对运动效率的影响，需要建立物理层优化与运动规划之间的桥梁

Method: 推导了连接ISAC发射功率与感知不确定性的闭合形式安全边界，基于Cramér-Rao边界和占用膨胀原理，构建了双层功率分配和运动规划问题

Result: 在高保真城市驾驶环境中的综合仿真显示，PISAC相比现有方法实现了高达40%的成功率提升和超过5%的通行时间缩短

Conclusion: PISAC框架有效增强了自动驾驶的安全性和效率，验证了其在连接物理层优化与运动级规划方面的有效性

Abstract: Integrated sensing and communication (ISAC) enables simultaneous
localization, environment perception, and data exchange for connected
autonomous vehicles. However, most existing ISAC designs prioritize sensing
accuracy and communication throughput, treating all targets uniformly and
overlooking the impact of critical obstacles on motion efficiency. To overcome
this limitation, we propose a planning-oriented ISAC (PISAC) framework that
reduces the sensing uncertainty of planning-bottleneck obstacles and expands
the safe navigable path for the ego-vehicle, thereby bridging the gap between
physical-layer optimization and motion-level planning. The core of PISAC lies
in deriving a closed-form safety bound that explicitly links ISAC transmit
power to sensing uncertainty, based on the Cram\'er-Rao Bound and occupancy
inflation principles. Using this model, we formulate a bilevel power allocation
and motion planning (PAMP) problem, where the inner layer optimizes the ISAC
beam power distribution and the outer layer computes a collision-free
trajectory under uncertainty-aware safety constraints. Comprehensive
simulations in high-fidelity urban driving environments demonstrate that PISAC
achieves up to 40% higher success rates and over 5% shorter traversal times
than existing ISAC-based and communication-oriented benchmarks, validating its
effectiveness in enhancing both safety and efficiency.

</details>


### [31] [HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap](https://arxiv.org/abs/2510.23147)
*Parisa Kanani,Mohammad Javad Omidi,Mahmoud Modarres-Hashemi,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出了一种基于高空平台站(HAPS)的集成感知与通信(ISAC)架构，结合无人机群构建可扩展的智能3D网络，显著提升6G网络性能。


<details>
  <summary>Details</summary>
Motivation: 为实现6G网络的超高数据速率和无处不在覆盖的宏伟目标，需要创新的网络架构解决方案。

Method: 在平流层部署HAPS作为通信枢纽和环境传感器，结合协作无人机群形成双用途系统。

Result: 仿真结果表明该方法显著提升网络性能、改善感知精度，并确保更公平的用户服务分配，优于传统仅无人机基线。

Conclusion: 概述了该技术在智慧城市等大规模环境中的潜在应用和部署路线图。

Abstract: To meet the ambitious goals of next-generation 6G networks, including
ultra-high data rates and ubiquitous coverage, we propose a novel high-altitude
platform station (HAPS)-based integrated sensing and communication (ISAC)
architecture. Operating in the stratosphere, the HAPS functions as both a
powerful communication hub and an advanced environmental sensor. Combined with
a fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose
system forms a scalable and intelligent 3D network. Simulation results indicate
that this approach significantly boosts network performance, improves sensing
accuracy, and ensures a fairer service distribution across users, outperforming
conventional UAV-only baselines. We conclude by outlining the prospective
applications and a deployment roadmap for this technology for smart cities and
other large-scale environments.

</details>


### [32] [Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals](https://arxiv.org/abs/2510.23186)
*Lukas Henneke,Frank Kurth*

Main category: eess.SP

TL;DR: 提出一种通过合成无线协议信号训练来学习判别性嵌入的方法，无需依赖真实RF信号记录，在真实RF信号数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的RF信号识别方法通常依赖大量训练数据且难以泛化到未见信号，需要开发不依赖真实信号记录的鲁棒方法。

Method: 通过合成无线协议信号训练学习判别性嵌入，避免使用真实RF信号记录。

Result: 在真实RF信号数据集上验证，学习到的嵌入能够准确区分先前未见过的真实世界信号。

Conclusion: 该方法在RF信号分类和异常检测方面具有潜力，展示了合成数据训练的泛化能力。

Abstract: Radio frequency (RF) signal recognition plays a critical role in modern
wireless communication and security applications. Deep learning-based
approaches have achieved strong performance but typically rely heavily on
extensive training data and often fail to generalize to unseen signals. In this
paper, we propose a method to learn discriminative embeddings without relying
on real-world RF signal recordings by training on signals of synthetic wireless
protocols. We validate the approach on a dataset of real RF signals and show
that the learned embeddings capture features enabling accurate discrimination
of previously unseen real-world signals, highlighting its potential for robust
RF signal classification and anomaly detection.

</details>


### [33] [Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC](https://arxiv.org/abs/2510.23355)
*Pengyu Gao,Qu Luo,Jing Zhu,Gaojie Chen,Pei Xiao,Chuan Heng Foh*

Main category: eess.SP

TL;DR: 提出了一种结合S-ALOHA和SCMA的新型非协调随机接入协议，通过干扰消除解码策略和用户阻挡机制提高了大规模机器通信的系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决未来大规模机器通信场景中对大规模连接和低接入延迟的迫切需求，克服传统正交多址接入在非协调环境下的局限性。

Method: 将经典时隙ALOHA协议与稀疏码多址接入技术结合，用户在任意时隙随机选择SCMA码本接入网络，采用干扰消除优先的解码策略，并引入用户阻挡机制来管理流量负载。

Result: 仿真结果表明，相比传统基于OMA的URA方案，所提出的SCMA赋能URA方案具有更高的最大吞吐量，理论分析的准确性和用户阻挡机制的有效性得到验证。

Conclusion: SCMA赋能的非协调随机接入协议能够有效提升大规模机器通信的系统性能，通过创新的解码策略和流量管理机制解决了码本碰撞问题。

Abstract: In this paper, a novel uncoordinated random access (URA) protocol is
presented to address the pressing demand for massive connectivity with low
access latency in future massive machine type communication (mMTC) scenarios.
The proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA)
protocol with sparse code multiple access (SCMA) technique, referred to as
SCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook
to access the communication network in an arbitrary time slot whenever they
want without scheduling. However, due to the lack of central coordination in
the proposed URA scheme, SCMA codebook collisions become inevitable, making
decoding challenging and leading to increased access failures. To cope with the
decoding issue, an interference-canceling (IC) first decoding strategy is
proposed at the access point (AP), which can partially tackles collision
problems, contributing to a higher system throughput. Taking the proposed
IC-first decoding strategy into account, a closed-form theoretical expression
of the throughput is derived. Moreover, to alleviate the throughput degradation
under the congested user traffic, a user barring mechanism is introduced to
manage the traffic load. Firstly, a closed-form expression of idle codebook
probability is developed to help indicate the system state, i.e., congested or
not. Then, in addition to the estimated real-time load, the AP adaptively
adjusts the access probability and redistributes the actual access load.
Finally, simulation results demonstrate that the proposed SCMA-empowered URA
scheme enjoys higher maximum throughput, compared to the conventional
orthogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of
the presented theoretical analysis and the effectiveness of the user barring
mechanism are verified.

</details>


### [34] [Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity](https://arxiv.org/abs/2510.23440)
*Donatella Darsena,Ivan Iudice,Vincenzo Galdi,Francesco Verde*

Main category: eess.SP

TL;DR: 提出了一种基于随机空时编码的堆叠智能超表面架构，通过引入人工时间变化实现多用户分集，在减少信道状态信息获取开销的同时获得满意的和速率性能。


<details>
  <summary>Details</summary>
Motivation: 传统空间超表面架构的重配置速率受限于信道相干时间，难以在慢变信道中充分利用多用户分集。需要一种新方案来降低CSIT获取开销并实现可扩展的大规模下行连接。

Method: 在堆叠智能超表面输入级集成空时超表面层，在每个信道相干时间内引入随机时间变化，结合随机导向矢量和基于信号质量测量的有限用户反馈进行波束成形。

Result: 数值结果表明，所提出的ST-SIM架构在显著减少CSIT获取和反馈开销的同时，能够获得令人满意的和速率性能。

Conclusion: 随机空时编码SIM架构为密集网络中的可扩展下行连接提供了有效解决方案，通过人工时间变化和部分CSIT实现了多用户分集增益与开销之间的良好平衡。

Abstract: Stacked intelligent metasurfaces (SIMs) represent a key enabler for
next-generation wireless networks, offering beamforming gains while
significantly reducing radio-frequency chain requirements. In conventional
space-only SIM architectures, the rate of reconfigurability of the SIM is equal
to the inverse of the channel coherence time. This paper investigates a novel
beamforming strategy for massive downlink connectivity using a randomized
space-time (ST) coded SIM. In addition to conventional space-only metasurface
layers, the proposed design integrates a ST metasurface layer at the input
stage of the SIM that introduces random time variations over each channel
coherence time interval. These artificial time variations enable opportunistic
user scheduling and exploitation of multiuser diversity under slow channel
dynamics. To mitigate the prohibitive overhead associated with full channel
state information at the transmitter (CSIT), we propose a partial-CSIT-based
beamforming scheme that leverages randomized steering vectors and limited
user-side feedback based on signal quality measurements. Numerical results
demonstrate that the proposed ST-SIM architecture achieves satisfactory
sum-rate performance while significantly reducing CSIT acquisition and feedback
overhead, thereby enabling scalable downlink connectivity in dense networks.

</details>


### [35] [Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems](https://arxiv.org/abs/2510.23467)
*Shreya Khisa,Ali Amhaz,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: 提出了一种基于夹持天线系统(PASS)的联合上下行框架，通过优化天线激活因子和功率分配，实现在相同频谱资源上同时服务上下行用户，相比TDD方案可获得60-90%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 传统TDD方案将上下行传输分配到不同时隙，频谱利用率低。本文旨在通过PASS系统实现上下行同时传输，提高频谱效率和系统容量。

Method: 将非凸的求和速率优化问题分解为天线激活和功率分配两个子问题：使用基于距离和空间相关性的算法解决天线激活问题；采用逐次凸逼近(SCA)算法解决资源分配问题。

Result: 数值结果表明，所提框架相比TDD方案可获得约60-90%的性能增益，显著提升了频谱利用效率。

Conclusion: PASS框架通过联合优化天线激活和功率分配，成功实现了上下行同时传输，大幅提升了系统性能，为未来无线通信系统提供了有效的解决方案。

Abstract: In this paper, we explore a novel joint uplink and downlink framework
utilizing a pinching antenna system (PASS). We consider two waveguides, one
dedicated to transmission and one to reception, and both of them are connected
to a base station (BS). Each type of waveguide consists of several pinching
antennas (PAs) in some preconfigured positions. In this framework, we assume
the BS can serve downlink and uplink user equipments (UEs) at the same time
using the same spectrum resources through the presented PASS. In this aspect,
we formulate a sum rate optimization problem that jointly optimizes the antenna
activation factor, the BS transmit power, and the UE's transmit power, subject
to power budget constraints for the BS and the UEs, as well as minimum rate
requirements for the UEs. The formulated problem is highly non-convex and
difficult to solve directly. Hence, we divide the main problem into two
sub-problems: the antenna activation sub-problem and the power allocation
sub-problem. Then, we solve the antenna activation problem utilizing a distance
and spatial correlation-based algorithm. Meanwhile, the resource allocation
problem is solved using a successive convex approximation (SCA)-based
algorithm. Numerical results show that our proposed framework can achieve
around 60-90\% performance gains over its time division duplex (TDD) where the
uplink and downlink transmissions are served in different orthogonal time
slots.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [36] [HDR Image Reconstruction using an Unsupervised Fusion Model](https://arxiv.org/abs/2510.21815)
*Kumbha Nagaswetha*

Main category: eess.IV

TL;DR: 提出一种基于深度学习的多曝光融合方法，使用CNN融合欠曝光和过曝光的LDR图像，在无监督训练下生成高质量的HDR图像。


<details>
  <summary>Details</summary>
Motivation: 传统数码相机动态范围有限，无法捕捉自然场景中的宽亮度范围，而人眼视觉系统能够感知这些亮度变化。

Method: 使用卷积神经网络融合不同曝光的LDR图像（欠曝光和过曝光图像），欠曝光图像保留亮部细节，过曝光图像保留暗部信息，网络学习融合这些互补信息。采用无监督训练，无需真实HDR图像作为标签，并引入定制化损失函数。

Result: 使用MEF-SSIM指标评估，该方法在视觉质量上优于现有的融合方法。

Conclusion: 该方法能够有效生成高质量的HDR图像，在无真实HDR标签的情况下实现实用化的HDR成像，适用于真实世界应用场景。

Abstract: High Dynamic Range (HDR) imaging aims to reproduce the wide range of
brightness levels present in natural scenes, which the human visual system can
perceive but conventional digital cameras often fail to capture due to their
limited dynamic range. To address this limitation, we propose a deep
learning-based multi-exposure fusion approach for HDR image generation. The
method takes a set of differently exposed Low Dynamic Range (LDR) images,
typically an underexposed and an overexposed image, and learns to fuse their
complementary information using a convolutional neural network (CNN). The
underexposed image preserves details in bright regions, while the overexposed
image retains information in dark regions; the network effectively combines
these to reconstruct a high-quality HDR output. The model is trained in an
unsupervised manner, without relying on ground-truth HDR images, making it
practical for real-world applications where such data is unavailable. We
evaluate our results using the Multi-Exposure Fusion Structural Similarity
Index Measure (MEF-SSIM) and demonstrate that our approach achieves superior
visual quality compared to existing fusion methods. A customized loss function
is further introduced to improve reconstruction fidelity and optimize model
performance.

</details>


### [37] [Inverse Design of Metasurface for Spectral Imaging](https://arxiv.org/abs/2510.21924)
*Rongzhou Chen,Haitao Nie,Shuo Zhu,Yaping Zhao,Chutian Wang,Edmund Y. Lam*

Main category: eess.IV

TL;DR: 提出了一种物理数据协同驱动的可重构超表面设计框架，用于短波红外区域的紧凑压缩光谱成像，通过端到端联合优化实现了7.6 dB的峰值信噪比提升。


<details>
  <summary>Details</summary>
Motivation: 在计算光学中，联合优化光学调制和算法解码的超表面逆向设计面临重大挑战，特别是在高光谱成像等应用中。

Method: 使用相变材料Ge2Sb2Se4Te1制造可重构超表面，开发了基于32万模拟几何结构的可微分神经模拟器，提出软形状正则化技术保持可制造性，实现超表面几何、光谱编码函数和深度重建网络的端到端联合优化。

Result: 优化系统将重建保真度峰值信噪比提高了7.6 dB，增强了噪声鲁棒性并改善了测量矩阵条件数。

Conclusion: 该方法展示了在高性能高光谱成像应用中的巨大潜力。

Abstract: Inverse design of metasurfaces for the joint optimization of optical
modulation and algorithmic decoding in computational optics presents
significant challenges, especially in applications such as hyperspectral
imaging. We introduce a physics-data co-driven framework for designing
reconfigurable metasurfaces fabricated from the phase-change material
Ge2Sb2Se4Te1 to achieve compact, compressive spectral imaging in the shortwave
infrared region. Central to our approach is a differentiable neural simulator,
trained on over 320,000 simulated geometries, that accurately predicts spectral
responses across 11 crystallization states. This differentiability enables
end-to-end joint optimization of the metasurface geometry, its spectral
encoding function, and a deep reconstruction network. We also propose a soft
shape regularization technique that preserves manufacturability during
gradient-based updates. Experiments show that our optimized system improves
reconstruction fidelity by up to 7.6 dB in the peak-signal-to-noise ratio, with
enhanced noise resilience and improved measurement matrix conditioning,
underscoring the potential of our approach for high-performance hyperspectral
imaging.

</details>


### [38] [Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement](https://arxiv.org/abs/2510.22154)
*Yunhong Tao,Wenbing Tao,Xiang Xiang*

Main category: eess.IV

TL;DR: 提出了一种基于两阶段架构的频率-空间交互驱动网络FSIDNet，用于低光图像增强，通过分别恢复振幅和相位信息，并结合跨阶段信息交换来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法要么忽略频域信息的重要性，要么无法有效促进信息传播和流动，限制了性能提升。

Method: 采用两阶段架构：第一阶段恢复低光图像的振幅以改善亮度，第二阶段恢复相位信息以细化精细结构；开发频率-空间交互块融合互补信息；构建信息交换模块关联两个阶段。

Result: 在多个基准数据集上的实验表明，该方法在视觉结果和定量指标方面均取得优异性能，同时保持良好的模型效率。

Conclusion: FSIDNet通过有效利用频域和空间域的互补信息，以及促进两阶段间的信息流动，显著提升了低光图像增强的性能。

Abstract: Low-light image enhancement (LLIE) aims at improving the perception or
interpretability of an image captured in an environment with poor illumination.
With the advent of deep learning, the LLIE technique has achieved significant
breakthroughs. However, existing LLIE methods either ignore the important role
of frequency domain information or fail to effectively promote the propagation
and flow of information, limiting the LLIE performance. In this paper, we
develop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE
based on two-stage architecture. To be specific, the first stage is designed to
restore the amplitude of low-light images to improve the lightness, and the
second stage devotes to restore phase information to refine fine-grained
structures. Considering that Frequency domain and spatial domain information
are complementary and both favorable for LLIE, we further develop two
frequency-spatial interaction blocks which mutually amalgamate the
complementary spatial and frequency information to enhance the capability of
the model. In addition, we construct the Information Exchange Module (IEM) to
associate two stages by adequately incorporating cross-stage and cross-scale
features to effectively promote the propagation and flow of information in the
two-stage network structure. Finally, we conduct experiments on several widely
used benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate
that our method achieves the excellent performance in terms of visual results
and quantitative metrics while preserving good model efficiency.

</details>


### [39] [Expert Validation of Synthetic Cervical Spine Radiographs Generated with a Denoising Diffusion Probabilistic Model](https://arxiv.org/abs/2510.22166)
*Austin A. Barr,Brij S. Karmur,Anthony J. Winder,Eddie Guo,John T. Lysack,James N. Scott,William F. Morrish,Muneer Eesa,Morgan Willson,David W. Cadotte,Michael M. H. Yang,Ian Y. M. Chan,Sanju Lama,Garnette R. Sutherland*

Main category: eess.IV

TL;DR: 使用去噪扩散概率模型(DDPM)生成逼真的颈椎侧位X光片，在临床图灵测试中专家无法区分真实与合成图像，为医学影像机器学习提供了大规模合成数据集解决方案。


<details>
  <summary>Details</summary>
Motivation: 神经外科机器学习面临大规模高质量影像数据集稀缺的挑战，合成数据提供了可扩展且保护隐私的解决方案。

Method: 基于4,963张颈椎X光片训练DDPM模型，通过训练/验证损失和FID监控性能，进行盲法临床图灵测试评估合成图像质量。

Result: 专家正确识别真实图像的概率仅为29%，真实与合成图像的真实性评分无显著差异(p>0.05)，最近邻分析显示无记忆化现象。

Conclusion: DDPM生成的颈椎X光片在真实性和质量上与临床真实图像统计上无法区分，为ML应用提供了大规模神经影像数据集的新方法。

Abstract: Machine learning in neurosurgery is limited by challenges in assembling
large, high-quality imaging datasets. Synthetic data offers a scalable,
privacy-preserving solution. We evaluated the feasibility of generating
realistic lateral cervical spine radiographs using a denoising diffusion
probabilistic model (DDPM) trained on 4,963 images from the Cervical Spine
X-ray Atlas. Model performance was monitored via training/validation loss and
Frechet inception distance, and synthetic image quality was assessed in a
blinded "clinical Turing test" with six neuroradiologists and two
spine-fellowship trained neurosurgeons. Experts reviewed 50 quartets containing
one real and three synthetic images, identifying the real image and rating
realism on a 4-point Likert scale. Experts correctly identified the real image
in 29% of trials (Fleiss' kappa=0.061). Mean realism scores were comparable
between real (3.323) and synthetic images (3.228, 3.258, and 3.320; p=0.383,
0.471, 1.000). Nearest-neighbor analysis found no evidence of memorization. We
also provide a dataset of 20,063 synthetic radiographs. These results
demonstrate that DDPM-generated cervical spine X-rays are statistically
indistinguishable in realism and quality from real clinical images, offering a
novel approach to creating large-scale neuroimaging datasets for ML
applications in landmarking, segmentation, and classification.

</details>


### [40] [Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy](https://arxiv.org/abs/2510.22239)
*Jahidul Arafat,Sanjaya Poudel*

Main category: eess.IV

TL;DR: 提出了CFU Net，一种用于无标记检测染色质包装改变的层次分割架构，通过合成数据训练实现自动核分割，在癌症早期检测中达到94%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 手动核分割限制了在早期癌症检测中生物标志物发现所需的大规模分析，且缺乏标注的csPWS成像数据阻碍了标准深度学习方法的应用。

Method: 使用基于物理的渲染生成合成多模态数据，结合五层架构元素（ConvNeXt骨干、特征金字塔网络、UNet++密集连接、双重注意力和深度监督），采用三阶段课程学习从对抗性RGB预训练到光谱微调和组织学验证。

Result: 在合成测试数据上达到近乎完美的性能（Dice 0.9879，IoU 0.9895），INT8量化实现74.9%压缩和0.15秒推理，比手动分析提高240倍吞吐量，从超过一万个自动分割的细胞核中提取的染色质生物标志物区分正常与癌前组织达到94%分类准确率。

Conclusion: 为专业显微镜中的合成到真实迁移学习提供了通用框架，并为临床样本的社区验证提供了开放资源。

Abstract: Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enables
label free detection of nanoscale chromatin packing alterations that occur
before visible cellular transformation. However, manual nuclear segmentation
limits population scale analysis needed for biomarker discovery in early cancer
detection. The lack of annotated csPWS imaging data prevents direct use of
standard deep learning methods. We present CFU Net, a hierarchical segmentation
architecture trained with a three stage curriculum on synthetic multimodal
data. CFU Net achieves near perfect performance on held out synthetic test data
that represent diverse spectroscopic imaging conditions without manual
annotations (Dice 0.9879, IoU 0.9895). Our approach uses physics based
rendering that incorporates empirically supported chromatin packing statistics,
Mie scattering models, and modality specific noise, combined with a curriculum
that progresses from adversarial RGB pretraining to spectroscopic fine tuning
and histology validation. CFU Net integrates five architectural elements
(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,
dual attention, and deep supervision) that together improve Dice over a
baseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantization
with 74.9 percent compression and 0.15 second inference, giving a 240 times
throughput gain over manual analysis. Applied to more than ten thousand
automatically segmented nuclei from synthetic test data, the pipeline extracts
chromatin biomarkers that distinguish normal from pre cancerous tissue with
large effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percent
classification accuracy. This work provides a general framework for synthetic
to real transfer learning in specialized microscopy and open resources for
community validation on clinical specimens.

</details>


### [41] [TraceTrans: Translation and Spatial Tracing for Surgical Prediction](https://arxiv.org/abs/2510.22379)
*Xiyu Luo,Haodong LI,Xinxing Cheng,He Zhao,Yang Hu,Xuan Song,Tianyang Zhang*

Main category: eess.IV

TL;DR: TraceTrans是一个新颖的可变形图像翻译模型，专门用于术后预测，能在生成与目标分布对齐的图像的同时，明确显示与术前输入的空间对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有图像翻译方法主要关注匹配目标分布，但忽略了源图像与翻译图像之间的空间对应关系，导致结构不一致和幻觉问题，这在临床应用中会影响预测的可靠性和可解释性。

Method: 采用编码器进行特征提取，双解码器分别预测空间变形和合成翻译图像，通过预测的变形场对生成输出施加空间约束，确保与源图像的解剖一致性。

Result: 在医学美容和脑部MRI数据集上的广泛实验表明，TraceTrans能够提供准确且可解释的术后预测结果。

Conclusion: TraceTrans在可靠临床部署方面具有潜力，能够生成解剖一致且可解释的术后预测图像。

Abstract: Image-to-image translation models have achieved notable success in converting
images across visual domains and are increasingly used for medical tasks such
as predicting post-operative outcomes and modeling disease progression.
However, most existing methods primarily aim to match the target distribution
and often neglect spatial correspondences between the source and translated
images. This limitation can lead to structural inconsistencies and
hallucinations, undermining the reliability and interpretability of the
predictions. These challenges are accentuated in clinical applications by the
stringent requirement for anatomical accuracy. In this work, we present
TraceTrans, a novel deformable image translation model designed for
post-operative prediction that generates images aligned with the target
distribution while explicitly revealing spatial correspondences with the
pre-operative input. The framework employs an encoder for feature extraction
and dual decoders for predicting spatial deformations and synthesizing the
translated image. The predicted deformation field imposes spatial constraints
on the generated output, ensuring anatomical consistency with the source.
Extensive experiments on medical cosmetology and brain MRI datasets demonstrate
that TraceTrans delivers accurate and interpretable post-operative predictions,
highlighting its potential for reliable clinical deployment.

</details>


### [42] [Low-Light Image Enhancement Using Gamma Learning And Attention-Enabled Encoder-Decoder Networks](https://arxiv.org/abs/2510.22547)
*Bibhabasu Debnath,Sahana Ray,Sanjay Ghosh*

Main category: eess.IV

TL;DR: 提出GAtED方法，结合自适应伽马校正和注意力增强的编码器-解码器网络，有效解决低光照图像增强问题，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 低光照图像存在噪声放大、光照不足、对比度降低、颜色失真和细节丢失等问题，需要开发简单高效的框架来同时处理全局光照调整和局部细节恢复。

Method: 采用双阶段深度学习架构：第一阶段使用自适应伽马校正模块学习每个像素的伽马值；第二阶段使用带卷积块注意力模块的编码器-解码器网络恢复细节。使用复合损失函数进行训练。

Result: 在LOL-v1、LOL-v2真实和合成数据集上达到PSNR 29.96 dB和SSIM 0.9458，优于现有方法。在其他数据集上获得最佳NIQE分数，感知质量更好，伪影更少。

Conclusion: GAtED方法能有效处理全局光照调整和局部细节增强，为低光照图像增强提供了实用解决方案。

Abstract: Images acquired in low-light environments present significant obstacles for
computer vision systems and human perception, especially for applications
requiring accurate object recognition and scene analysis. Such images typically
manifest multiple quality issues: amplified noise, inadequate scene
illumination, contrast reduction, color distortion, and loss of details. While
recent deep learning methods have shown promise, developing simple and
efficient frameworks that naturally integrate global illumination adjustment
with local detail refinement continues to be an important objective. To this
end, we introduce a dual-stage deep learning architecture that combines
adaptive gamma correction with attention-enhanced refinement to address these
fundamental limitations. The first stage uses an Adaptive Gamma Correction
Module (AGCM) to learn suitable gamma values for each pixel based on both local
and global cues, producing a brightened intermediate output. The second stage
applies an encoder-decoder deep network with Convolutional Block Attention
Modules (CBAM) to this brightened image, in order to restore finer details. We
train the network using a composite loss that includes L1 reconstruction, SSIM,
total variation, color constancy, and gamma regularization terms to balance
pixel accuracy with visual quality. Experiments on LOL-v1, LOL-v2 real, and
LOL-v2 synthetic datasets show our method reaches PSNR of upto 29.96 dB and
upto 0.9458 SSIM, outperforming existing approaches. Additional tests on DICM,
LIME, MEF, and NPE datasets using NIQE, BRISQUE, and UNIQUE metrics confirm
better perceptual quality with fewer artifacts, achieving the best NIQE scores
across all datasets. Our GAtED (Gamma learned and Attention-enabled
Encoder-Decoder) method effectively handles both global illumination adjustment
and local detail enhancement, offering a practical solution for low-light
enhancement.

</details>


### [43] [Structure Aware Image Downscaling](https://arxiv.org/abs/2510.22551)
*G B Kevin Arjun,Suvrojit Mitra,Sanjay Ghosh*

Main category: eess.IV

TL;DR: 提出一种基于图像滤波和边缘检测的结构感知图像下采样方法，通过边缘引导插值和纹理增强来保持图像细节，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 图像下采样在显示技术和可视化工具中至关重要，但传统方法容易导致边缘模糊和纹理丢失，需要开发能保持结构完整性和视觉保真度的新方法。

Method: 三步骤方法：1) 使用高效边缘检测算子计算边缘图；2) 边缘引导插值保持细节；3) 融合原始图像的局部纹理增强组件来恢复高频信息。

Result: 在4倍下采样时，DIV2K数据集PSNR达39.07 dB，RealSR数据集达38.71 dB，优于现有方法，有效避免边缘模糊和纹理损失。

Conclusion: 提出的结构感知下采样方法通过整合边缘信息和自适应滤波，在视觉质量和性能指标上均表现优异，能有效保留关键图像特征。

Abstract: Image downscaling is one of the key operations in recent display technology
and visualization tools. By this process, the dimension of an image is reduced,
aiming to preserve structural integrity and visual fidelity. In this paper, we
propose a new image downscaling method which is built on the core ideas of
image filtering and edge detection. In particular, we present a
structure-informed downscaling algorithm that maintains fine details through
edge-aware processing. The proposed method comprises three steps: (i) edge map
computation, (ii) edge-guided interpolation, and (iii) texture enhancement. To
faithfully retain the strong structures in an image, we first compute the edge
maps by applying an efficient edge detection operator. This is followed by an
edge-guided interpolation to preserve fine details after resizing. Finally, we
fuse local texture enriched component of the original image to the interpolated
one to restore high-frequency information. By integrating edge information with
adaptive filtering, our approach effectively minimizes artifacts while
retaining crucial image features. To demonstrate the effective downscaling
capability of our proposed method, we validate on four datasets: DIV2K, BSD100,
Urban100, and RealSR. For downscaling by 4x, our method could achieve as high
as 39.07 dB PSNR on the DIV2K dataset and 38.71 dB on the RealSR dataset.
Extensive experimental results confirm that the proposed image downscaling
method is capable of achieving superior performance in terms of both visual
quality and performance metrics with reference to recent methods. Most
importantly, the downscaled images by our method do not suffer from edge
blurring and texture loss, unlike many existing ones.

</details>


### [44] [Learning Event-guided Exposure-agnostic Video Frame Interpolation via Adaptive Feature Blending](https://arxiv.org/abs/2510.22565)
*Junsik Jung,Yoonki Cho,Woo Jae Kim,Lin Wang,Sune-eui Yoon*

Main category: eess.IV

TL;DR: 提出了一种新颖的事件引导框架，通过目标自适应事件采样和目标自适应重要性映射来解决曝光无关视频帧插值问题，在严重低帧率模糊视频上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于事件的方法在严重低帧率模糊视频上效果不佳，主要原因是缺乏时间约束。事件相机具有高时间分辨率，但需要更好的方法来利用这些信息。

Method: 采用目标自适应事件采样(TES)在目标时间戳和未知曝光时间周围采样事件，以及目标自适应重要性映射(TIM)生成考虑时间邻近性和空间相关性的重要性图，自适应融合连续特征。

Result: 在合成和真实数据集上的广泛实验表明，该方法在曝光无关视频帧插值场景中具有显著效果。

Conclusion: 所提出的TES和TIM组件能够有效解决曝光无关视频帧插值问题，在严重低帧率模糊视频上取得良好效果。

Abstract: Exposure-agnostic video frame interpolation (VFI) is a challenging task that
aims to recover sharp, high-frame-rate videos from blurry, low-frame-rate
inputs captured under unknown and dynamic exposure conditions. Event cameras
are sensors with high temporal resolution, making them especially advantageous
for this task. However, existing event-guided methods struggle to produce
satisfactory results on severely low-frame-rate blurry videos due to the lack
of temporal constraints. In this paper, we introduce a novel event-guided
framework for exposure-agnostic VFI, addressing this limitation through two key
components: a Target-adaptive Event Sampling (TES) and a Target-adaptive
Importance Mapping (TIM). Specifically, TES samples events around the target
timestamp and the unknown exposure time to better align them with the
corresponding blurry frames. TIM then generates an importance map that
considers the temporal proximity and spatial relevance of consecutive features
to the target. Guided by this map, our framework adaptively blends consecutive
features, allowing temporally aligned features to serve as the primary cues
while spatially relevant ones offer complementary support. Extensive
experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our approach in exposure-agnostic VFI scenarios.

</details>


### [45] [TVMC: Time-Varying Mesh Compression via Multi-Stage Anchor Mesh Generation](https://arxiv.org/abs/2510.22646)
*He Huang,Qi Yang,Yiling Xu,Zhu Li,Jenq-Neng Hwang*

Main category: eess.IV

TL;DR: 提出TVMC框架，通过多阶段粗到精的锚网格生成进行帧间预测，有效压缩时变网格数据，在MPEG动态网格序列上实现最先进的压缩性能。


<details>
  <summary>Details</summary>
Motivation: 时变网格具有动态连接性和变化的顶点数量，在增强现实等应用中前景广阔，但由于高保真表示需要大量数据，实际应用仍面临挑战。现有压缩方法难以处理拓扑不一致性和运动引起的伪影。

Method: 三阶段锚网格生成：初始阶段通过快速拓扑对齐利用时间相干性；卡尔曼滤波运动估计模块生成粗锚网格；基于二次误差度量的细化步骤优化顶点位置形成精细锚网格。基于精细锚网格编码帧间运动，自适应量化和压缩残差位移。

Result: 在标准MPEG动态网格序列上的实验表明，TVMC实现了最先进的压缩性能。相比最新的V-DMC标准，实现了10.2%~16.9%的BD-rate增益，同时保持高质量重建。

Conclusion: TVMC通过分层策略保持一致的连接性和高质量表面逼近，实现了动态几何的高效紧凑表示，为时变网格压缩提供了有效解决方案。

Abstract: Time-varying meshes, characterized by dynamic connectivity and varying vertex
counts, hold significant promise for applications such as augmented reality.
However, their practical utilization remains challenging due to the substantial
data volume required for high-fidelity representation. While various
compression methods attempt to leverage temporal redundancy between consecutive
mesh frames, most struggle with topological inconsistency and motion-induced
artifacts. To address these issues, we propose Time-Varying Mesh Compression
(TVMC), a novel framework built on multi-stage coarse-to-fine anchor mesh
generation for inter-frame prediction. Specifically, the anchor mesh is
progressively constructed in three stages: initial, coarse, and fine. The
initial anchor mesh is obtained through fast topology alignment to exploit
temporal coherence. A Kalman filter-based motion estimation module then
generates a coarse anchor mesh by accurately compensating inter-frame motions.
Subsequently, a Quadric Error Metric-based refinement step optimizes vertex
positions to form a fine anchor mesh with improved geometric fidelity. Based on
the refined anchor mesh, the inter-frame motions relative to the reference base
mesh are encoded, while the residual displacements between the subdivided fine
anchor mesh and the input mesh are adaptively quantized and compressed. This
hierarchical strategy preserves consistent connectivity and high-quality
surface approximation, while achieving an efficient and compact representation
of dynamic geometry. Extensive experiments on standard MPEG dynamic mesh
sequences demonstrate that TVMC achieves state-of-the-art compression
performance. Compared to the latest V-DMC standard, it delivers a significant
BD-rate gain of 10.2% ~ 16.9%, while preserving high reconstruction quality.
The code is available at https://github.com/H-Huang774/TVMC.

</details>


### [46] [Understanding What Is Not Said:Referring Remote Sensing Image Segmentation with Scarce Expressions](https://arxiv.org/abs/2510.22760)
*Kai Ye,Bowen Liu,Jianghang Lin,Jiayi Ji,Pingyang Dai,Liujuan Cao*

Main category: eess.IV

TL;DR: 提出弱参考表达学习(WREL)范式，利用类别名称作为弱参考表达与少量精确表达结合，在有限标注条件下实现高效的遥感图像分割训练。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中获取高质量参考表达特别困难，因为存在大量小尺寸、密集分布的目标和复杂背景。

Method: 提出LRB-WREL方法，集成可学习参考库(LRB)通过样本特定的提示嵌入来优化弱参考表达，结合师生优化框架和动态调度的EMA更新。

Result: 在不同弱参考数据比例的新基准上验证了WREL和LRB-WREL的有效性，表明它们可以接近甚至超过使用完全标注参考表达训练的模型。

Conclusion: 混合参考训练在理论上具有可证明的性能上界，WREL和LRB-WREL为遥感图像分割提供了一种有效的弱监督学习解决方案。

Abstract: Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances
in remote sensing images according to referring expressions. Unlike Referring
Image Segmentation on general images, acquiring high-quality referring
expressions in the remote sensing domain is particularly challenging due to the
prevalence of small, densely distributed objects and complex backgrounds. This
paper introduces a new learning paradigm, Weakly Referring Expression Learning
(WREL) for RRSIS, which leverages abundant class names as weakly referring
expressions together with a small set of accurate ones to enable efficient
training under limited annotation conditions. Furthermore, we provide a
theoretical analysis showing that mixed-referring training yields a provable
upper bound on the performance gap relative to training with fully annotated
referring expressions, thereby establishing the validity of this new setting.
We also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to
refine weakly referring expressions through sample-specific prompt embeddings
that enrich coarse class-name inputs. Combined with a teacher-student
optimization framework using dynamically scheduled EMA updates, LRB-WREL
stabilizes training and enhances cross-modal generalization under noisy weakly
referring supervision. Extensive experiments on our newly constructed benchmark
with varying weakly referring data ratios validate both the theoretical
insights and the practical effectiveness of WREL and LRB-WREL, demonstrating
that they can approach or even surpass models trained with fully annotated
referring expressions.

</details>


### [47] [Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting Data](https://arxiv.org/abs/2510.22812)
*Shashank N. Sridhara,Birendra Kathariya,Fangjun Pu,Peng Yin,Eduardo Pavez,Antonio Ortega*

Main category: eess.IV

TL;DR: RALHE是一种针对3D高斯泼溅数据的区域自适应学习分层编码方法，通过分层潜在表示和轻量级解码器网络，在带宽受限应用中有效压缩3DGS模型。


<details>
  <summary>Details</summary>
Motivation: 3DGS模型大小限制了其在带宽受限应用（如体媒体流）中的部署，需要高效的压缩方法。

Method: 利用3DGS几何的八叉树结构获得分层多分辨率表示，通过过拟合潜在表示和轻量级解码器网络，结合自回归概率模型进行比特率估计。

Result: 在低比特率（小于1MB）下，相比基线3DGS压缩方法，渲染PSNR增益高达2dB。

Conclusion: RALHE框架能够有效压缩3DGS数据，在保持高质量渲染的同时显著减少模型大小。

Abstract: We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D
Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for
novel view synthesis, the size of trained models limits its deployment in
bandwidth-constrained applications such as volumetric media streaming. To
address this, we propose a learned hierarchical latent representation that
builds upon the principles of "overfitted" learned image compression (e.g.,
Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS
data have irregular spatial distributions of Gaussians (geometry) and consist
of multiple attributes (signals) defined on the irregular geometry. Our codec
is designed to account for these differences between images and 3DGS.
Specifically, we leverage the octree structure of the voxelized 3DGS geometry
to obtain a hierarchical multi-resolution representation. Our approach overfits
latents to each Gaussian attribute under a global rate constraint. These
latents are decoded independently through a lightweight decoder network. To
estimate the bitrate during training, we employ an autoregressive probability
model that leverages octree-derived contexts from the 3D point structure. The
multi-resolution latents, decoder, and autoregressive entropy coding networks
are jointly optimized for each Gaussian attribute. Experiments demonstrate that
the proposed RALHE compression framework achieves a rendering PSNR gain of up
to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS
compression methods.

</details>


### [48] [USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding](https://arxiv.org/abs/2510.22990)
*Youssef Megahed,Robin Ducharme,Mark Walker,Steven Hawken,Adrian D. C. Chan*

Main category: eess.IV

TL;DR: 提出了USF-MAE，首个基于超声数据的自监督掩码自编码基础模型，在370,000张超声图像上预训练，在三个下游分类任务中优于传统CNN和ViT基线。


<details>
  <summary>Details</summary>
Motivation: 解决超声图像解释的挑战，包括高噪声、操作者依赖性和有限视野，以及深度学习模型因标注数据稀缺和领域差距导致的迁移性受限问题。

Method: 使用视觉Transformer编码器-解码器架构，通过重建掩码图像块从无标签数据中学习模态特定表示，在46个开源数据集的370,000张2D和3D超声图像上预训练。

Result: 在三个下游分类基准测试中，USF-MAE的F1分数分别为81.6%（乳腺癌）、79.6%（卵巢肿瘤）和82.4%（胃肠道间质瘤），均优于传统基线模型。

Conclusion: USF-MAE展示了强大的跨解剖结构泛化能力，即使不使用预训练标签，也能接近或超过监督基础模型的性能。

Abstract: Ultrasound imaging is one of the most widely used diagnostic modalities,
offering real-time, radiation-free assessment across diverse clinical domains.
However, interpretation of ultrasound images remains challenging due to high
noise levels, operator dependence, and limited field of view, resulting in
substantial inter-observer variability. Current Deep Learning approaches are
hindered by the scarcity of large labeled datasets and the domain gap between
general and sonographic images, which limits the transferability of models
pretrained on non-medical data. To address these challenges, we introduce the
Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),
the first large-scale self-supervised MAE framework pretrained exclusively on
ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound
images curated from 46 open-source datasets, collectively termed OpenUS-46,
spanning over twenty anatomical regions. This curated dataset has been made
publicly available to facilitate further research and reproducibility. Using a
Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked
image patches, enabling it to learn rich, modality-specific representations
directly from unlabeled data. The pretrained encoder was fine-tuned on three
public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D
(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all
tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,
achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using
labels during pretraining, USF-MAE approached the performance of the supervised
foundation model UltraSam on breast cancer classification and surpassed it on
the other tasks, demonstrating strong cross-anatomical generalization.

</details>


### [49] [Equivariance2Inverse: A Practical Self-Supervised CT Reconstruction Method Benchmarked on Real, Limited-Angle, and Blurred Data](https://arxiv.org/abs/2510.23317)
*Dirk Elias Schut,Adriaan Graas,Robert van Liere,Tristan van Leeuwen*

Main category: eess.IV

TL;DR: 本文分析了六种自监督CT重建方法的模型假设，并在真实世界和合成数据上进行了基准测试，发现像素独立噪声假设在闪烁体模糊数据上表现不佳，而旋转不变性假设能改善有限角度重建效果。基于这些发现，作者结合了Robust Equivariant Imaging和Sparse2Inverse方法的成功概念，提出了新的自监督CT重建方法Equivariance2Inverse。


<details>
  <summary>Details</summary>
Motivation: 自监督CT重建方法在真实应用中很有吸引力，因为它们不需要真实标签训练数据。但这些方法在训练时使用了简化的X射线物理模型，可能对闪烁体模糊、扫描几何或噪声分布做出不准确假设，导致在真实成像环境中鲁棒性较差。

Method: 回顾了六种近期自监督CT重建方法的模型假设，在真实世界2DeteCT数据集和合成数据上进行基准测试（包含有无闪烁体模糊和有限角度扫描几何的情况），并基于成功概念开发了新的Equivariance2Inverse方法。

Result: 基准测试结果表明：假设噪声像素独立的方法在闪烁体模糊数据上表现不佳；假设旋转不变性的方法在有限角度重建中效果更好。

Conclusion: 通过结合Robust Equivariant Imaging和Sparse2Inverse的成功概念，提出了新的自监督CT重建方法Equivariance2Inverse，能够更好地处理真实世界成像环境中的挑战。

Abstract: Deep learning has shown impressive results in reducing noise and artifacts in
X-ray computed tomography (CT) reconstruction. Self-supervised CT
reconstruction methods are especially appealing for real-world applications
because they require no ground truth training examples. However, these methods
involve a simplified X-ray physics model during training, which may make
inaccurate assumptions, for example, about scintillator blurring, the scanning
geometry, or the distribution of the noise. As a result, they can be less
robust to real-world imaging circumstances. In this paper, we review the model
assumptions of six recent self-supervised CT reconstruction methods. Moreover,
we benchmark these methods on the real-world 2DeteCT dataset and on synthetic
data with and without scintillator blurring and a limited-angle scanning
geometry. The results of our benchmark show that methods that assume that the
noise is pixel-wise independent do not perform well on data with scintillator
blurring, and that assuming rotation invariance improves results on
limited-angle reconstructions. Based on these findings, we combined successful
concepts of the Robust Equivariant Imaging and Sparse2Inverse methods in a new
self-supervised CT reconstruction method called Equivariance2Inverse.

</details>


### [50] [KongNet: A Multi-headed Deep Learning Model for Detection and Classification of Nuclei in Histopathology Images](https://arxiv.org/abs/2510.23559)
*Jiaqi Lv,Esha Sadia Nasir,Kesi Xu,Mostafa Jahanifar,Brinder Singh Chohan,Behnaz Elhaminia,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: KongNet是一种多头部深度学习架构，具有共享编码器和并行细胞类型专用解码器，用于组织病理学图像中的细胞核检测和分类，在多个挑战赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 准确检测和分类组织病理学图像中的细胞核对于诊断和研究应用至关重要，需要一种能够处理不同组织和染色类型的有效方法。

Method: 采用多任务学习架构，包含共享编码器和并行细胞类型专用解码器，每个解码器联合预测细胞核中心点、分割掩码和轮廓，使用SCSE注意力模块和复合损失函数。

Result: 在MONKEY挑战赛中Track 1获得第一名、Track 2获得第二名；在2025 MIDOG挑战赛中轻量级变体KongNet-Det获得第一名；在PUMA挑战赛中排名前三；在PanNuke和CoNIC数据集上达到最先进性能。

Conclusion: 专用多解码器设计对于跨不同组织和染色类型的细胞核检测和分类非常有效，预训练模型权重和推理代码已公开发布以支持未来研究。

Abstract: Accurate detection and classification of nuclei in histopathology images are
critical for diagnostic and research applications. We present KongNet, a
multi-headed deep learning architecture featuring a shared encoder and
parallel, cell-type-specialised decoders. Through multi-task learning, each
decoder jointly predicts nuclei centroids, segmentation masks, and contours,
aided by Spatial and Channel Squeeze-and-Excitation (SCSE) attention modules
and a composite loss function. We validate KongNet in three Grand Challenges.
The proposed model achieved first place on track 1 and second place on track 2
during the MONKEY Challenge. Its lightweight variant (KongNet-Det) secured
first place in the 2025 MIDOG Challenge. KongNet pre-trained on the MONKEY
dataset and fine-tuned on the PUMA dataset ranked among the top three in the
PUMA Challenge without further optimisation. Furthermore, KongNet established
state-of-the-art performance on the publicly available PanNuke and CoNIC
datasets. Our results demonstrate that the specialised multi-decoder design is
highly effective for nuclei detection and classification across diverse tissue
and stain types. The pre-trained model weights along with the inference code
have been publicly released to support future research.

</details>


### [51] [Revising Second Order Terms in Deep Animation Video Coding](https://arxiv.org/abs/2510.23561)
*Konstantin Schmidt,Thomas Richter*

Main category: eess.IV

TL;DR: 本文改进了第一阶运动模型(FOMM)，通过用全局旋转替换雅可比变换来处理头部旋转，同时显著降低P帧比特率40-80%，并应用先进的归一化技术来稳定对抗训练。


<details>
  <summary>Details</summary>
Motivation: FOMM模型虽然比特率低且计算复杂度适中，但由于基于源图像扭曲生成面部动画，在处理强烈头部运动（特别是头部旋转）时存在严重限制。

Method: 用全局旋转替换FOMM中的雅可比变换来处理头部旋转，并应用最先进的归一化技术来稳定对抗训练。

Result: 改进后的系统在处理头部旋转时表现更好，同时P帧比特率降低了40%到80%，通过LPIPS和DISTS指标验证了优化的成功。

Conclusion: 提出的优化方法有效解决了FOMM在处理头部旋转时的局限性，显著降低了比特率并提高了视觉质量。

Abstract: First Order Motion Model is a generative model that animates human heads
based on very little motion information derived from keypoints. It is a
promising solution for video communication because first it operates at very
low bitrate and second its computational complexity is moderate compared to
other learning based video codecs. However, it has strong limitations by
design. Since it generates facial animations by warping source-images, it fails
to recreate videos with strong head movements. This works concentrates on one
specific kind of head movements, namely head rotations. We show that replacing
the Jacobian transformations in FOMM by a global rotation helps the system to
perform better on items with head-rotations while saving 40% to 80% of bitrate
on P-frames. Moreover, we apply state-of-the-art normalization techniques to
the discriminator to stabilize the adversarial training which is essential for
generating visually appealing videos. We evaluate the performance by the
learned metics LPIPS and DISTS to show the success our optimizations.

</details>
