<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.IT](#cs.IT) [Total: 15]
- [eess.IV](#eess.IV) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Modal Parameter Extraction via Propeller-Driven Vibration Testing](https://arxiv.org/abs/2601.08123)
*Gabriele Dessena,Alessandro Pontillo*

Main category: eess.SP

TL;DR: 评估螺旋桨驱动振动测试作为地面振动测试的替代方案，通过实验验证其在提取低频模态信息方面的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统地面振动测试（GVT）通常耗时且成本高昂，需要寻找更高效的替代方法。螺旋桨驱动振动测试（PVT）作为一种输出式测试方法，可以补充或替代传统GVT。

Method: 使用悬臂铝7075-T6翼梁，配备7个加速度计，通过外置电动机和螺旋桨激励。进行7次测试：无电机基线、5个恒定油门工况和手动上下油门扫频。采用自然激励技术与Loewner框架（NExT-LF）进行模态参数识别。

Result: 螺旋桨激励下主要共振峰仍可观测，低油门工况会引入窄带谐波可能掩盖结构峰值，扫频可减少持续重叠。前两阶模态匹配良好（MAC > 0.99），第三阶模态重复性降低（MAC = 0.827）且频率偏移较大，这与螺旋桨引起的弯扭耦合和非理想扫频控制一致。

Conclusion: PVT可作为GVT的可行补充，用于提取低频模态信息，未来应研究自动油门调度和考虑耦合效应的测试规划。

Abstract: Ground Vibration Testing (GVT) supports aircraft certification but often requires lengthy and costly campaigns. Propeller-driven Vibration Testing (PVT) is assessed here as an output-only alternative, in line with Operational Modal Analysis approaches such as Taxi Vibration Testing and Flight Vibration Testing. A cantilever Aluminium 7075-T6 wing spar is instrumented with seven accelerometers and excited by an outboard electric motor and propeller. Seven runs are carried out: a motor-off baseline, five constant-throttle cases, and a manual up-down throttle sweep. The acquired spectra indicate that the dominant resonances remain observable under propeller excitation, while low-throttle conditions introduce narrowband harmonics that may mask structural peaks; the sweep reduces persistent overlap. Modal parameters are identified for the baseline and sweep cases using the Natural Excitation Technique with the Loewner Framework (NExT-LF). The first two modes remain closely matched (Modal Assurance Criterion (MAC) > 0.99), whereas the third mode shows reduced repeatability (MAC = 0.827) and a larger frequency shift, consistent with propeller-induced bending--torsion coupling and non-ideal sweep control. Overall, PVT provides a viable complement to GVT for extracting low-frequency modal information and motivates pursuing future work on automated throttle scheduling and coupling-aware test planning.

</details>


### [2] [Variable-Length Wideband CSI Feedback via Loewner Interpolation and Deep Learning](https://arxiv.org/abs/2601.08300)
*Meilin Li,Wei Xu,Zhixiang Hu,An Liu*

Main category: eess.SP

TL;DR: 提出用于U6G频段FDD大规模MIMO系统的可变长度宽带CSI反馈方案，采用Loewner插值框架和神经网络的混合方法，支持可变长度反馈并提高恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于压缩感知和深度学习的CSI反馈方案在宽带信道（如U6G）中面临能量泄漏问题，导致恢复精度瓶颈，需要更有效的压缩和反馈方案。

Method: 1. 引入Loewner插值框架生成动态基函数，在频域高效压缩；2. 通过神经网络在空域进一步压缩；3. 设计无速率自编码器，支持可变长度反馈；4. 采用重要性排序和自适应量化策略增强鲁棒性。

Result: 仿真结果表明，该方案在相同或更少反馈开销下获得更高的CSI反馈精度，并提高了频谱效率，优于基线方案。

Conclusion: 提出的混合Loewner插值和神经网络方法有效解决了宽带信道CSI反馈中的能量泄漏问题，实现了可变长度反馈和精度与开销的灵活权衡。

Abstract: In this paper, we propose a variable-length wideband channel state information (CSI) feedback scheme for Frequency Division Duplex (FDD) massive multiple-input multipleoutput (MIMO) systems in U6G band (6425MHz-7125MHz). Existing compressive sensing (CS)-based and deep learning (DL)- based schemes preprocess the channel by truncating it in the angular-delay domain. However, the energy leakage effect caused by the Discrete Fourier Transform (DFT) basis will be more serious and leads to a bottleneck in recovery accuracy when applied to wideband channels such as those in U6G. To solve this problem, we introduce the Loewner Interpolation (LI) framework which generates a set of dynamic bases based on the current CSI matrix, enabling highly efficient compression in the frequency domain. Then, the LI basis is further compressed in the spatial domain through a neural network. To achieve a flexible trade-off between feedback overhead and recovery accuracy, we design a rateless auto-encoder trained with tail dropout and a multi-objective learning schedule, supporting variable-length feedback with a singular model. Meanwhile, the codewords are ranked by importance, ensuring that the base station (BS) can still maintain acceptable reconstruction performance under limited feedback with tail erasures. Furthermore, an adaptive quantization strategy is developed for the feedback framework to enhance robustness. Simulation results demonstrate that the proposed scheme could achieve higher CSI feedback accuracy with less or equal feedback overhead, and improve spectral efficiency compared with baseline schemes.

</details>


### [3] [Meta-Backscatter: Long-Distance Battery-Free Metamaterial-Backscatter Sensing and Communication](https://arxiv.org/abs/2601.08307)
*Taorui Liu,Xu Liu,Zhiquan Xu,Houfeng Chen,Hongliang Zhang,Lingyang Song*

Main category: eess.SP

TL;DR: 本文提出了基于超材料标签的元反向散射系统，突破了传统反向散射通信的有限距离限制，为无电池物联网提供了统一设计框架和研究路线图。


<details>
  <summary>Details</summary>
Motivation: 传统反向散射标签通信距离有限（通常仅几米），严重制约了无电池物联网的实际部署。超材料标签通过亚波长单元集中反射信号功率，有望突破这一关键通信距离障碍。

Method: 建立统一设计框架：1）回顾反向散射通信发展历史、工作原理和标签分类；2）提出超材料标签及其兼容收发器的设计方法学；3）实现元反向散射系统原型并进行实验验证。

Result: 开发了元反向散射系统原型，实验结果表明超材料标签相比采用全向天线的传统标签能显著扩展通信距离，同时保持无电池物联网的低成本、超低功耗和鲁棒性优势。

Conclusion: 元反向散射系统为解决无电池物联网通信距离限制提供了有前景的解决方案，论文总结了关键挑战并指出了未来研究方向，为领域发展提供了前瞻性路线图。

Abstract: Battery-free Internet of Things (BF-IoT) enabled by backscatter communication is a rapidly evolving technology offering advantages of low cost, ultra-low power consumption, and robustness. However, the practical deployment of BF-IoT is significantly constrained by the limited communication range of common backscatter tags, which typically operate with a range of merely a few meters due to inherent round-trip path loss. Meta-backscatter systems that utilize metamaterial tags present a promising solution, retaining the inherent advantages of BF-IoT while breaking the critical communication range barrier. By leveraging densely paved sub-wavelength units to concentrate the reflected signal power, metamaterial tags enable a significant communication range extension over existing BF-IoT tags that employ omni-directional antennas. In this paper, we synthesize the principles and paradigms of metamaterial sensing to establish a unified design framework and a forward-looking research roadmap. Specifically, we first provide an overview of backscatter communication, encompassing its development history, working principles, and tag classification. We then introduce the design methodology for both metamaterial tags and their compatible transceivers. Moreover, we present the implementation of a meta-backscatter system prototype and report the experimental results based on it. Finally, we conclude by highlighting key challenges and outlining potential avenues for future research.

</details>


### [4] [Bio-RV: Low-Power Resource-Efficient RISC-V Processor for Biomedical Applications](https://arxiv.org/abs/2601.08428)
*Vijay Pratap Sharma,Annu Kumar,Mohd Faisal Khan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: eess.SP

TL;DR: Bio-RV是一款紧凑型RISC-V处理器，专为生物医学控制应用设计，具有低功耗、确定性执行和最小硬件复杂度的特点。


<details>
  <summary>Details</summary>
Motivation: 为生物医学控制应用（如加速器生物医学SoC和植入式起搏器系统）开发一款资源高效、低功耗的处理器，满足超低功耗、安全关键系统的需求。

Method: 设计了一个多周期RV32I核心，提供显式执行控制和外部指令加载功能，支持受控固件部署、ASIC启动和后硅测试，作为轻量级主机控制器协调加速器配置和数据传输。

Result: 在FPGA原型上仅需708个LUT和235个触发器，采用180nm CMOS技术，工作频率50MHz，硬件占用小，后布局结果显示符合最小能耗设计目标。

Conclusion: Bio-RV通过优先考虑确定性执行、最小硬件复杂度和集成灵活性而非峰值计算速度，成功满足了超低功耗、安全关键生物医学系统的需求。

Abstract: This work presents Bio-RV, a compact and resource-efficient RISC-V processor intended for biomedical control applications, such as accelerator-based biomedical SoCs and implantable pacemaker systems. The proposed Bio-RV is a multi-cycle RV32I core that provides explicit execution control and external instruction loading with capabilities that enable controlled firmware deployment, ASIC bring-up, and post-silicon testing. In addition to coordinating accelerator configuration and data transmission in heterogeneous systems, Bio-RV is designed to function as a lightweight host controller, handling interfaces with pacing, sensing, electrogram (EGM), telemetry, and battery management modules. With 708 LUTs and 235 flip-flops on FPGA prototypes, Bio-RV, implemented in a 180 nm CMOS technology, operate at 50 MHz and feature a compact hardware footprint. According to post-layout results, the proposed architectural decisions align with minimal energy use. Ultimately, Bio-RV prioritises deterministic execution, minimal hardware complexity, and integration flexibility over peak computing speed to meet the demands of ultra-low-power, safety-critical biomedical systems.

</details>


### [5] [Effective outdoor pathloss prediction: A multi-layer segmentation approach with weighting map](https://arxiv.org/abs/2601.08436)
*Yuan Gao,Tao Wen,Wenjing Xie,Jianbo Du,Yong Zeng,Dusit Niyato,Shugong Xu*

Main category: eess.SP

TL;DR: 提出基于ResNet的路径损耗预测模型，通过生成Tx/Rx深度图、距离图和加权图来捕捉环境特征，在计算复杂度降低60%的同时，预测精度比现有方法提升1.2-3.0 dB。


<details>
  <summary>Details</summary>
Motivation: 传统射线追踪和基于模型的方法存在计算复杂度高、模型与现实环境不匹配的问题。深度学习为路径损耗预测提供了更准确且计算效率更高的替代方案。

Method: 使用ResNet架构，从地理数据生成Tx深度图、Rx深度图、距离图，并创新性地引入加权图来强调Tx-Rx直接路径相邻区域，以更好地捕捉信号反射和衍射的影响。

Result: 在ITU挑战赛2024和ICASSP 2023数据集上的实验表明，该模型比PPNet、RPNet和Vision Transformer（ViT）的性能提升1.2-3.0 dB，同时FLOPs减少60%。消融研究证实加权图显著提升了预测性能。

Conclusion: 提出的基于ResNet的路径损耗预测模型在保持低计算复杂度的同时，显著提升了预测精度，加权图的引入有效捕捉了信号传播的关键物理特性。

Abstract: Predicting pathloss by considering the physical environment is crucial for effective wireless network planning. Traditional methods, such as ray tracing and model-based approaches, often face challenges due to high computational complexity and discrepancies between models and real-world environments. In contrast, deep learning has emerged as a promising alternative, offering accurate path loss predictions with reduced computational complexity. In our research, we introduce a ResNet-based model designed to enhance path loss prediction. We employ innovative techniques to capture key features of the environment by generating transmission (Tx) and reception (Rx) depth maps, as well as a distance map from the geographic data. Recognizing the significant attenuation caused by signal reflection and diffraction, particularly at high frequencies, we have developed a weighting map that emphasizes the areas adjacent to the direct path between Tx and Rx for path loss prediction. {Extensive simulations demonstrate that our model outperforms PPNet, RPNet, and Vision Transformer (ViT) by 1.2-3.0 dB using dataset of ITU challenge 2024 and ICASSP 2023. In addition, the floating point operations (FLOPs) of the proposed model is 60\% less than those of benchmarks.} Additionally, ablation studies confirm that the inclusion of the weighting map significantly enhances prediction performance.

</details>


### [6] [SDP: A Unified Protocol and Benchmarking Framework for Reproducible Wireless Sensing](https://arxiv.org/abs/2601.08463)
*Di Zhang,Jiawei Huang,Yuanhao Cui,Xiaowen Cao,Tony Xiao Han,Xiaojun Jing,Christos Masouros*

Main category: eess.SP

TL;DR: SDP（传感数据协议）是一个协议级抽象和统一基准，用于标准化无线传感，将学习任务与硬件异构性解耦，提高可重复性和公平比较。


<details>
  <summary>Details</summary>
Motivation: 无线传感领域缺乏统一且可重复的实验基础。与计算机视觉不同，无线传感依赖于硬件相关的信道测量，其表示、预处理流程和评估协议在不同设备和数据集间差异很大，阻碍了公平比较和可重复性。

Method: 提出SDP协议，作为标准化层，强制执行确定性物理层净化、规范张量构建以及标准化的训练和评估流程，将学习性能与硬件特定伪影解耦。

Result: SDP在保持竞争力的准确率的同时，显著提高了稳定性，在复杂活动识别任务中将种子间性能方差降低了数量级。真实世界实验展示了协议在异构硬件上的互操作性。

Conclusion: SDP通过提供统一协议和基准，实现了可重复和可比较的无线传感研究，支持从临时实验向可靠工程实践的转变。

Abstract: Learning-based wireless sensing has made rapid progress, yet the field still lacks a unified and reproducible experimental foundation. Unlike computer vision, wireless sensing relies on hardware-dependent channel measurements whose representations, preprocessing pipelines, and evaluation protocols vary significantly across devices and datasets, hindering fair comparison and reproducibility.
  This paper proposes the Sensing Data Protocol (SDP), a protocol-level abstraction and unified benchmark for scalable wireless sensing. SDP acts as a standardization layer that decouples learning tasks from hardware heterogeneity. To this end, SDP enforces deterministic physical-layer sanitization, canonical tensor construction, and standardized training and evaluation procedures, decoupling learning performance from hardware-specific artifacts. Rather than introducing task-specific models, SDP establishes a principled protocol foundation for fair evaluation across diverse sensing tasks and platforms. Extensive experiments demonstrate that SDP achieves competitive accuracy while substantially improving stability, reducing inter-seed performance variance by orders of magnitude on complex activity recognition tasks. A real-world experiment using commercial off-the-shelf Wi-Fi hardware further illustrating the protocol's interoperability across heterogeneous hardware. By providing a unified protocol and benchmark, SDP enables reproducible and comparable wireless sensing research and supports the transition from ad hoc experimentation toward reliable engineering practice.

</details>


### [7] [Drone Surveillance via Coordinated Beam Sweeping in MIMO-ISAC Networks](https://arxiv.org/abs/2601.08483)
*Palatip Jopanya,Diana P. M. Osorio,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出一种与5G SSB同步的无人机协同监视方案，通过波束扫描在多AP配置下同时检测低空无人机，并设计预编码器保证感知与通信信号正交性。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络部署和无人机应用增加，需要在不干扰正常通信的前提下实现低空无人机监视。现有方案难以同时满足通信和感知需求，且易受干扰影响。

Method: 采用多AP协作配置，AP协同发送感知波束覆盖体素网格，与5G SSB突发同步。设计预编码器保证感知波束与SSB正交，最大化感知SINR同时确保用户SINR，并最小化直连链路影响。

Result: 提出的预编码器性能优于非协调预编码器，对无人机高度变化影响最小，能有效实现通信与感知的协同工作。

Conclusion: 该方案成功实现了5G通信与无人机监视的协同，通过创新的预编码设计解决了信号干扰问题，为未来集成通信感知系统提供了可行方案。

Abstract: This paper introduces a scheme for drone surveillance coordinated with the fifth generation (5G) synchronization signal block (SSB) cell-search procedure to simultaneously detect low-altitude drones within a volumetric surveillance grid. Herein, we consider a multistatic configuration where multiple access points (APs) collaboratively illuminate the volume while independently transmitting SSB broadcast signals. Both tasks are performed through a beam sweeping. In the proposed scheme, coordinated APs send sensing beams toward a grid of voxels within the volumetric surveillance region simultaneously with the 5G SSB burst. To prevent interference between communication and sensing signals, we propose a precoder design that guarantees orthogonality of the sensing beam and the SSB in order to maximize the sensing signal-to-interference-plus-noise ratio (SINR) while ensuring a specified SINR for users, as well as minimizing the impact of the direct link. The results demonstrate that the proposed precoder outperforms the non-coordinated precoder and is minimally affected by variations in drone altitude.

</details>


### [8] [Airborne Particle Communication Through Time-varying Diffusion-Advection Channels](https://arxiv.org/abs/2601.08534)
*Fatih Merdan,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 该论文研究了时变对流条件下的空气粒子通信，将其建模为线性时变信道，推导了信道脉冲响应，定义了信道色散时间作为信道记忆度量，并通过仿真验证了波形设计对性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有粒子通信研究大多假设恒定流动条件，而实际宏观环境（如大气风）具有时变特性。需要为复杂流动环境中的粒子通信提供更现实的建模基础。

Method: 将时变对流下的空气粒子通信建模为线性时变信道，使用移动坐标系方法推导出闭式、时间相关的信道脉冲响应。通过功率延迟分布表征信道，定义信道色散时间作为信道记忆的物理度量。在定向时变风条件下进行系统级仿真。

Result: 推导出了时变扩散对流信道的闭式信道脉冲响应。定义了信道色散时间作为符号持续时间选择的指导准则。仿真表明波形设计对性能至关重要，当色散得到充分控制时，可以使用单一粒子类型实现多符号调制。

Conclusion: 时变扩散对流信道可以使用通信理论工具进行系统建模和设计，为复杂流动环境中的粒子通信提供了现实基础。信道色散时间是一个有物理意义的信道记忆度量，波形设计是实现高效通信的关键。

Abstract: Particle based communication using diffusion and advection has emerged as an alternative signaling paradigm recently. While most existing studies assume constant flow conditions, real macro scale environments such as atmospheric winds exhibit time varying behavior. In this work, airborne particle communication under time varying advection is modeled as a linear time varying (LTV) channel, and a closed form, time dependent channel impulse response is derived using the method of moving frames. Based on this formulation, the channel is characterized through its power delay profile, leading to the definition of channel dispersion time as a physically meaningful measure of channel memory and a guideline for symbol duration selection. System level simulations under directed, time varying wind conditions show that waveform design is critical for performance, enabling multi symbol modulation using a single particle type when dispersion is sufficiently controlled. The results demonstrate that time varying diffusion advection channels can be systematically modeled and engineered using communication theoretic tools, providing a realistic foundation for particle based communication in complex flow environments.

</details>


### [9] [Stable Filtering for Efficient Dimensionality Reduction of Streaming Manifold Data](https://arxiv.org/abs/2601.08685)
*Nicholas P. Bertrand,Eva Yezerets,Han Lun Yap,Adam S. Charles,Christopher J. Rozell*

Main category: eess.SP

TL;DR: 提出随机滤波(RF)方法，利用随机降维技术保持数据非线性流形结构，无需训练且计算高效，适用于大规模流数据降维。


<details>
  <summary>Details</summary>
Motivation: 现代科学工程领域产生海量数据，传统存储、传输和处理面临巨大挑战。需要无需昂贵训练、能保持数据底层几何结构（低维吸引子流形）的降维工具，但流形结构通常未知。

Method: 提出随机滤波(RF)方法，基于随机降维理论框架，通过特定实例化实现非线性流形结构在嵌入空间中的可证明保持。方法具有数据独立性和计算高效性。

Result: 开发了RF的实际应用方法，包括新颖方法、分析和实验验证。在多个模拟和真实数据示例中展示了RF的实际效益，证明了其在各种科学应用中的实用性。

Conclusion: 随机滤波(RF)是一种实用、高效的降维方法，能保持数据非线性流形结构，无需训练，适用于处理大规模流数据，在科学工程领域具有广泛应用前景。

Abstract: Many areas in science and engineering now have access to technologies that enable the rapid collection of overwhelming data volumes. While these datasets are vital for understanding phenomena from physical to biological and social systems, the sheer magnitude of the data makes even simple storage, transmission, and basic processing highly challenging. To enable efficient and accurate execution of these data processing tasks, we require new dimensionality reduction tools that 1) do not need expensive, time-consuming training, and 2) preserve the underlying geometry of the data that has the information required to understand the measured system. Specifically, the geometry to be preserved is that induced by the fact that in many applications, streaming high-dimensional data evolves on a low-dimensional attractor manifold. Importantly, we may not know the exact structure of this manifold a priori. To solve these challenges, we present randomized filtering (RF), which leverages a specific instantiation of randomized dimensionality reduction to provably preserve non-linear manifold structure in the embedded space while remaining data-independent and computationally efficient. In this work we build on the rich theoretical promise of randomized dimensionality reduction to develop RF as a real, practical approach. We introduce novel methods, analysis, and experimental verification to illuminate the practicality of RF in diverse scientific applications, including several simulated and real-data examples that showcase the tangible benefits of RF.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [10] [Efficient Synthesis for Two-Dimensional Strand Arrays with Row Constraints](https://arxiv.org/abs/2601.07968)
*Boaz Moav,Ryan Gabrys,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究DNA合成中的空间约束问题，分析在每行每周期最多合成一条链的限制下，两条链在单行中的期望完成时间，提出多种策略并给出理论界限。


<details>
  <summary>Details</summary>
Motivation: 受大规模DNA合成技术的驱动，研究在空间约束下的DNA链合成问题，其中链在阵列中排列，每行每合成周期最多只能合成一条链。

Method: 将过程分解为马尔可夫链，推导期望合成时间的分析上下界；提出简单的滞后优先策略；研究单符号前瞻策略；设计动态规划算法计算最优离线调度。

Result: 滞后优先策略实现渐近期望完成时间(q+3)L/2；无前瞻的在线策略无法超越此界限；二进制情况下单符号前瞻将时间改进为7L/3；动态规划算法可计算任意固定序列对的最优离线调度。

Conclusion: 首次为空间约束下的合成提供分析界限，为未来研究此类设置中的最优合成策略奠定基础。

Abstract: We study the theoretical problem of synthesizing multiple DNA strands under spatial constraints, motivated by large-scale DNA synthesis technologies. In this setting, strands are arranged in an array and synthesized according to a fixed global synthesis sequence, with the restriction that at most one strand per row may be synthesized in any synthesis cycle. We focus on the basic case of two strands in a single row and analyze the expected completion time under this row-constrained model. By decomposing the process into a Markov chain, we derive analytical upper and lower bounds on the expected synthesis time. We show that a simple laggard-first policy achieves an asymptotic expected completion time of (q+3)L/2 for any alphabet of size q, and that no online policy without look-ahead can asymptotically outperform this bound. For the binary case, we show that allowing a single-symbol look-ahead strictly improves performance, yielding an asymptotic expected completion time of 7L/3. Finally, we present a dynamic programming algorithm that computes the optimal offline schedule for any fixed pair of sequences. Together, these results provide the first analytical bounds for synthesis under spatial constraints and lay the groundwork for future studies of optimal synthesis policies in such settings.

</details>


### [11] [Distributed Detection under Stringent Resource Constraints](https://arxiv.org/abs/2601.07989)
*Abdelaziz Bounhar,Mireille Sarkiss,Michèle Wigger*

Main category: cs.IT

TL;DR: 该论文研究了分布式检测中的Stein指数，分析了三种严格通信约束下离散无记忆信道（DMC）的性能，发现部分连接DMC与全连接DMC存在性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究在严格通信约束下分布式检测的性能极限，特别是当传感器通过DMC向决策中心传输信息时，不同信道结构对检测性能的影响。

Method: 分析三种通信约束场景：1) 信道使用次数随观测数n次线性增长；2) 信道使用次数为n但施加几乎必然的块输入成本约束；3) 仅在期望上施加块输入约束。通过调整Han的零速率编码策略适应部分连接DMC，并针对全连接DMC提出新的编码策略和逆证明。

Result: 发现部分连接DMC的Stein指数与Han和Kobayashi以及Shalaby和Papamarcou在无噪声零速率通信场景下的指数相同。全连接DMC在前两种场景下Stein指数退化到本地测试水平，传感器和通信变得无用。第三种场景中传感器仍有帮助但性能下降。

Conclusion: DMC的连接性结构对分布式检测的Stein指数有决定性影响：部分连接DMC能保持零速率通信的性能优势，而全连接DMC在严格约束下性能严重退化，仅在期望约束下保留部分优势。

Abstract: This paper identifies the Stein-exponent of distributed detection when the sensor communicates to the decision center over a discrete memoryless channel (DMC) subject to one of three stringent communication constraints: 1) The number of channel uses of the DMC grows sublinearly in the number of source observations n; 2) The number of channel uses is n but a block-input cost constraint is imposed almost surely, which grows sublinearly in n; 3) The block-input constraint is imposed only on expectation. We identify a dichotomy in the Stein-exponent of all these setups depending on the structure of the DMC's transition law. Under any of these three constraints, when the DMC is partially-connected (i.e., some outputs cannot be induced by certain inputs) the Stein-exponent matches the exponent identified by Han and Kobayashi and by Shalaby and Papamarcou for the scenario where communication is of zero-rate but over a noiseless link. We prove our result by adapting Han's zero-rate coding strategy to partially-connected DMCs.
  In contrast, for fully-connected DMCs, in our scenarios 1) and 2) the Stein-exponent collapses to that of a local test at the decision center, rendering the remote sensor and communication useless. %To prove this result, we propose new converse proofs relying on change of measure arguments.
  In scenario 3), the sensor remains beneficial even for fully-connected DMCs, however also collapses compared to the case of a partially-connected DMC. Moreover, the Stein-exponent is larger when the expectation constraint is imposed only under the null hypothesis compared to when it is imposed under both hypotheses. To prove these results, we propose both new coding strategies and new converse proofs.

</details>


### [12] [The many faces of multivariate information](https://arxiv.org/abs/2601.08030)
*Thomas F. Varley*

Main category: cs.IT

TL;DR: 提出一个统一的框架Δ^k，将多种高阶信息度量（双总相关、S信息、O信息）统一为参数化函数，揭示高阶冗余和协同交互的层次结构。


<details>
  <summary>Details</summary>
Motivation: 从多元数据中提取高阶结构对理解复杂系统至关重要，但现有信息论度量众多且分散，缺乏统一框架来系统描述高阶冗余和协同交互。

Method: 提出参数化函数Δ^k，通过调整参数k可恢复不同现有度量（Δ^0=S信息，Δ^1=双总相关，Δ^2=负O信息），并利用熵共轭框架推导其共轭函数Γ^k。

Result: Δ^k形成高阶协同的层次结构：Δ^k>0表示系统以高于k阶的交互为主，Δ^k<0表示以低于k阶的交互为主，Δ^k=0表示系统完全由k阶协同组成。Γ^k则形成类似的高阶冗余层次结构。

Conclusion: 该统一框架为高阶冗余和协同交互提供了新见解，将现有分散的度量整合为更连贯的结构，有助于系统研究复杂系统中的高阶信息共享。

Abstract: Extracting higher-order structures from multivariate data has become an area of intensive study in complex systems science, as these multipartite interactions can reveal insights into fundamental features of complex systems like emergent phenomena. Information theory provides a natural language for exploring these interactions, as it elegantly formalizes the problem of comparing ``wholes" and ``parts" using joint, conditional, and marginal entropies. A large number of distinct statistics have been developed over the years, all aiming to capture different aspects of ``higher-order" information sharing. Here, we show that three of them (the dual total correlation, S-information, and O-information) are special cases of a more general function, $Δ^{k}$ which is parameterized by a free parameter $k$. For different values of $k$, we recover different measures: $Δ^{0}$ is equal to the S-information, $Δ^{1}$ is equal to the dual total correlation, and $Δ^{2}$ is equal to the negative O-information. Generally, the $Δ^{k}$ function is arranged into a hierarchy of increasingly high-order synergies; for a given value of $k$, if $Δ^{k}>0$, then the system is dominated by interactions with order greater than $k$, while if $Δ^{k}<0$, then the system is dominated by interactions with order lower than $k$. $Δ^{k}=0$ if the system is composed entirely of synergies of order-k. Using the entropic conjugation framework, we also find that the conjugate of $Δ^{k}$, which we term $Γ^{k}$ is arranged into a similar hierarchy of increasingly high-order redundancies. These results provide new insights into the nature of both higher-order redundant and synergistic interactions, and helps unify the existing zoo of measures into a more coherent structure.

</details>


### [13] [Cardinality-consistent flag codes with longer type vectors](https://arxiv.org/abs/2601.08144)
*Junfeng Jia,Yanxun Chang*

Main category: cs.IT

TL;DR: 提出两种旗码构造：最优距离旗码和长类型向量旗码，均基于循环轨道旗码的统一框架，在有限域上实现相同基数


<details>
  <summary>Details</summary>
Motivation: 旗码作为常数维码的推广，能表示嵌套子空间序列。需要构建具有良好距离特性和较大基数的旗码，以扩展编码理论的应用范围

Method: 基于循环轨道旗码的统一构造框架，针对n=sk+h的有限域，设计两种旗码：1) 最长可能类型向量(1,2,...,k,n-k,...,n-1)的最优距离旗码；2) 更长类型向量(1,2,...,k+h,2k+h,...,(s-2)k+h,n-k,...,n-1)的旗码

Result: 成功构造出两种旗码家族，均达到相同基数∑_{i=1}^{s-1}q^{ik+h}+1，其中最优距离旗码具有最佳距离特性，长类型向量旗码则扩展了类型向量的范围

Conclusion: 该统一构造框架有效生成了具有相同基数的两类旗码，为旗码设计提供了系统方法，扩展了编码理论在嵌套子空间序列表示中的应用

Abstract: Flag codes generalize constant dimension codes by considering sequences of nested subspaces with prescribed dimensions as codewords. A comprehensive construction, which unites cyclic orbit flag codes, yields two families of flag codes on $\mathbb{F}^n_q$ (where $n=sk+h$ with $s\geq 2$ and $0\leq h < k$): optimum distance flag codes of the longest possible type vector $(1, 2, \ldots, k, n-k, \ldots, n-1)$ and flag codes with longer type vectors $(1, 2, \ldots, k+h, 2k+h, \ldots, (s-2)k+h, n-k, \ldots, n-1)$. These flag codes achieve the same cardinality $\sum^{s-1}_{i=1}q^{ik+h}+1$.

</details>


### [14] [From Antenna Abundance to Antenna Intelligence in 6G Gigantic MIMO Systems](https://arxiv.org/abs/2601.08326)
*Emil Björnson,Amna Irshad,Özlem Tugfe Demir,Giuseppe Thadeu Freitas de Abreu,Alva Kosasih,Vitaly Petrov*

Main category: cs.IT

TL;DR: 论文提出通过智能非均匀稀疏阵列设计，用更少天线实现优于传统大规模MIMO的性能，减少硬件复杂度、成本和功耗。


<details>
  <summary>Details</summary>
Motivation: 当前大规模MIMO系统依赖大量天线实现高谱效，但未来网络需要数百天线，带来硬件复杂度、成本和功耗问题。需要更智能的阵列设计来减少天线数量需求。

Method: 重新审视经典均匀阵列设计，利用非均匀稀疏阵列和站点特定的天线布局：1）基于预优化的不规则阵列；2）实时可移动天线。这些设计灵感来自无线定位研究，通过非均匀空间采样减少冗余。

Result: 数值模拟显示，这些概念可以显著提高平均和速率等通信性能指标，用更少天线实现优于传统大规模MIMO的多用户MIMO性能。

Conclusion: 未来天线阵列设计需要范式转变：用天线智能替代单纯的天线数量。这为高效、自适应、可持续的"巨型MIMO"系统开辟了新机遇。

Abstract: Current cellular systems achieve high spectral efficiency through Massive MIMO, which leverages an abundance of antennas to create favorable propagation conditions for multiuser spatial multiplexing. Looking towards future networks, the extrapolation of this paradigm leads to systems with many hundreds of antennas per base station, raising concerns regarding hardware complexity, cost, and power consumption. This article suggests more intelligent array designs that reduce the need for excessive antenna numbers. We revisit classical uniform array design principles and explain how their uniform spatial sampling leads to unnecessary redundancies in practical deployment scenarios. By exploiting non-uniform sparse arrays with site-specific antenna placements -- based on either pre-optimized irregular arrays or real-time movable antennas -- we demonstrate how superior multiuser MIMO performance can be achieved with far fewer antennas. These principles are inspired by previous works on wireless localization. We explain and demonstrate numerically how these concepts can be adapted for communications to improve the average sum rate and similar metrics. The results suggest a paradigm shift for future antenna array design, where antenna intelligence replaces sheer antenna count. This opens new opportunities for efficient, adaptable, and sustainable Gigantic MIMO systems.

</details>


### [15] [Movable Antenna for Integrating Near-field Channel Estimation and Localization](https://arxiv.org/abs/2601.08357)
*Chongjia Sun,Ziwei Wan,Lipeng Zhu,Zhenyu Xiao,Zhen Gao,Rui Zhang*

Main category: cs.IT

TL;DR: 提出了一种用于MA辅助宽带近场ISAC的多阶段设计框架，通过子区域划分、NOMP角度估计、LSRC散射体定位和信道估计优化，显著提升MA感知精度和通信性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线(MA)通过自适应调整天线位置为未来无线通信系统带来新的自由度，其大范围移动使无线信道传输进入近场区域，这为集成感知与通信(ISAC)带来了新的性能提升机会。

Method: 1) 将MA移动区域划分为多个子区域，在每个子区域使用牛顿化正交匹配追踪算法(NOMP)进行高精度角度估计；2) 提出近场子区域射线聚类定位方法(LSRC)，通过联合处理所有子区域的角度估计来识别散射体位置；3) 基于估计的散射体位置，细化近场信道估计以提升通信性能。

Result: 仿真结果表明，所提方案能显著增强MA感知精度和信道估计性能，为MA辅助的近场ISAC提供了高效解决方案。

Conclusion: 该多阶段设计框架有效解决了MA辅助宽带近场ISAC中的感知和通信优化问题，通过子区域划分和联合处理策略实现了高精度散射体定位和信道估计，为未来无线通信系统提供了新的技术途径。

Abstract: Movable antenna (MA) introduces a new degree of freedom for future wireless communication systems by enabling the adaptive adjustment of antenna positions. Its large-range movement renders wireless channels transmission into the near-field region, which brings new performance enhancement for integrated sensing and communication (ISAC). This paper proposes a novel multi-stage design framework for broadband near-field ISAC assisted by MA. The framework first divides the MA movement area into multiple subregions, and employs the Newtonized orthogonal matching pursuit algorithm (NOMP) to achieve high-precision angle estimation in each subregion. Subsequently, a method called near-field localization via subregion ray clustering (LSRC) is proposed for identifying the positions of scatterers. This method finds the coordinates of each scatterer by jointly processing the angle estimates across all subregions. Finally, according to the estimated locations of the scatterers, the near-field channel estimation (CE) is refined for improving communication performance. Simulation results demonstrate that the proposed scheme can significantly enhance MA sensing accuracy and CE, providing an efficient solution for MA-aided near-field ISAC.

</details>


### [16] [On the Generalization Error of Differentially Private Algorithms Via Typicality](https://arxiv.org/abs/2601.08386)
*Yanxiao Liu,Chun Hei Michael Shiu,Lele Wang,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文从信息论角度研究随机学习算法的泛化误差，特别关注差分隐私算法的更紧致边界，改进了现有互信息和最大泄漏的边界。


<details>
  <summary>Details</summary>
Motivation: 随机学习算法的泛化误差通常可以通过互信息和最大泄漏来界定，但现有边界可能不够紧致。本文旨在为差分隐私算法推导更精确的泛化误差边界，通过改进互信息和最大泄漏的估计方法。

Method: 采用信息论视角，使用典型性论证和差分隐私算法的稳定性特性。第一部分严格改进了Rodríguez-Gálvez等人的互信息边界；第二部分推导了学习算法最大泄漏的新上界。

Result: 获得了更紧致的互信息边界和新的最大泄漏上界，这些信息度量边界可直接转化为泛化误差保证，特别适用于差分隐私算法。

Conclusion: 通过信息论方法和差分隐私算法的稳定性特性，本文为随机学习算法（特别是差分隐私算法）提供了更精确的泛化误差边界，改进了现有理论结果。

Abstract: We study the generalization error of stochastic learning algorithms from an information-theoretic perspective, with a particular emphasis on deriving sharper bounds for differentially private algorithms. It is well known that the generalization error of stochastic learning algorithms can be bounded in terms of mutual information and maximal leakage, yielding in-expectation and high-probability guarantees, respectively. In this work, we further upper bound mutual information and maximal leakage by explicit, easily computable formulas, using typicality-based arguments and exploiting the stability properties of private algorithms. In the first part of the paper, we strictly improve the mutual-information bounds by Rodríguez-Gálvez et al. (IEEE Trans. Inf. Theory, 2021). In the second part, we derive new upper bounds on the maximal leakage of learning algorithms. In both cases, the resulting bounds on information measures translate directly into generalization error guarantees.

</details>


### [17] [An Efficient Algorithm to Sample Quantum Low-Density Parity-Check Codes](https://arxiv.org/abs/2601.08387)
*Paolo Santini*

Main category: cs.IT

TL;DR: 提出一种高效的随机稀疏矩阵采样算法，用于生成量子LDPC码的校验矩阵，该算法基于信息集解码(ISD)技术，具有纯组合特性而非代数约束。


<details>
  <summary>Details</summary>
Motivation: 现有量子LDPC码构造方法多为代数方法，需要特定性质（如准循环性），限制了随机性和灵活性。需要一种更通用、更随机的稀疏自正交矩阵采样方法。

Method: 采用纯组合方法，利用信息集解码(ISD)技术逐行采样稀疏矩阵H，确保H满足自正交条件HH^T=0。算法可推广到非二进制有限域和更一般的量子稳定子LDPC码。

Result: 算法理论上确定了可采样的参数范围及预期计算复杂度，数值模拟和基准测试证实了方法的可行性和高效性。

Conclusion: 提出了一种简单而有效的随机稀疏自正交矩阵采样算法，为量子LDPC码构造提供了更灵活、更通用的工具，突破了传统代数方法的限制。

Abstract: In this paper, we present an efficient algorithm to sample random sparse matrices to be used as check matrices for quantum Low-Density Parity-Check (LDPC) codes. To ease the treatment, we mainly describe our algorithm as a technique to sample a dual-containing binary LDPC code, hence, a sparse matrix $\mathbf H\in\mathbb F_2^{r\times n}$ such that $\mathbf H\mathbf H^\top = \mathbf 0$. However, as we show, the algorithm can be easily generalized to sample dual-containing LDPC codes over non binary finite fields as well as more general quantum stabilizer LDPC codes.
  While several constructions already exist, all of them are somewhat algebraic as they impose some specific property (e.g., the matrix being quasi-cyclic). Instead, our algorithm is purely combinatorial as we do not require anything apart from the rows of $\mathbf H$ being sparse enough. In this sense, we can think of our algorithm as a way to sample sparse, self-orthogonal matrices that are as random as possible.
  Our algorithm is conceptually very simple and, as a key ingredient, uses Information Set Decoding (ISD) to sample the rows of $\mathbf H$, one at a time. The use of ISD is fundamental as, without it, efficient sampling would not be feasible. We give a theoretical characterization of our algorithm, determining which ranges of parameters can be sampled as well as the expected computational complexity. Numerical simulations and benchmarks confirm the feasibility and efficiency of our approach.

</details>


### [18] [Distribution Estimation with Side Information](https://arxiv.org/abs/2601.08535)
*Haricharan Balasundaram,Andrew Thangaraj*

Main category: cs.IT

TL;DR: 该论文研究离散分布估计问题，在存在额外侧信息（如词向量语义相似性）的情况下，探讨如何利用这些信息改进估计精度，分析了局部模型和偏序模型两种场景。


<details>
  <summary>Details</summary>
Motivation: 在文本语料库等大字母表数据集中，词向量嵌入等语义相似性信息自然存在，这些侧信息可能有助于改进离散分布估计。论文旨在探索如何利用这类额外信息来提升估计性能。

Method: 提出两种侧信息模型：1）局部模型：未知分布在已知分布的邻域内；2）偏序模型：字母表被划分为已知的高概率和低概率集合。理论分析了两种模型下平方误差风险的改进。

Result: 理论刻画了侧信息带来的平方误差风险改进，并在自然语言和合成数据上的仿真实验验证了这些改进效果。

Conclusion: 利用自然语言处理中可用的语义侧信息（如词向量相似性）可以显著改进离散分布估计，为处理大字母表数据集提供了新的理论框架和实用方法。

Abstract: We consider the classical problem of discrete distribution estimation using i.i.d. samples in a novel scenario where additional side information is available on the distribution. In large alphabet datasets such as text corpora, such side information arises naturally through word semantics/similarities that can be inferred by closeness of vector word embeddings, for instance. We consider two specific models for side information--a local model where the unknown distribution is in the neighborhood of a known distribution, and a partial ordering model where the alphabet is partitioned into known higher and lower probability sets. In both models, we theoretically characterize the improvement in a suitable squared-error risk because of the available side information. Simulations over natural language and synthetic data illustrate these gains.

</details>


### [19] [On the Optimality of Decode and Forward for Some Cooperative Broadcast Channels](https://arxiv.org/abs/2601.08592)
*Nicolas Le Gouic,Yossef Steinberg,Michèle Wigger*

Main category: cs.IT

TL;DR: 本文为具有单向协作的更强广播信道提出了新的容量区域边界点，通过简单的叠加编码和译码转发方案实现。


<details>
  <summary>Details</summary>
Motivation: 研究具有从强接收器到弱接收器单向协作的广播信道的容量区域，探索现有边界之外的性能提升可能性。

Method: 采用发射机叠加编码结合强接收器译码转发的简单编码方案，在强接收器协助弱接收器的协作框架下工作。

Result: 为高斯广播信道和BEC-BSC混合信道确定了新的容量区域边界点，扩展了现有容量区域的覆盖范围。

Conclusion: 简单的叠加编码加译码转发方案能够为具有单向协作的广播信道实现新的容量边界点，为这类协作通信系统提供了理论指导。

Abstract: This article characterizes new boundary points on the capacity region of certain classes of more capable broadcast channels (BC) with uni-directional cooperation from the stronger to the weaker receiver. The new boundary points are achieved by a simple coding scheme that employs superposition coding at the transmitter with decode and forward at the stronger receiver. We evaluate our general result for Gaussian BCs and for a BC consisting of a binary erasure channel (BEC) to the stronger receiver and a binary symmetric channel (BSC) to the weaker receiver.

</details>


### [20] [LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms](https://arxiv.org/abs/2601.08780)
*Namhyun Kim,Sadjad Alikhani,Ahmed Alkhateeb*

Main category: cs.IT

TL;DR: LWM-Spectro是一个基于Transformer的基础模型，通过在时间-频率谱图上进行大规模I/Q数据预训练，利用自监督掩码建模、对比学习和混合专家架构学习通用无线表示，在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 无线通信中的I/Q基带信号天然编码了物理层和信道特性，但由于通信系统异构、传播环境多样以及标记数据有限，直接从原始信号学习鲁棒且可迁移的表示仍然具有挑战性。

Method: 提出LWM-Spectro模型：1) 将I/Q数据表示为时间-频率谱图；2) 使用Transformer架构；3) 结合自监督掩码建模和对比学习；4) 采用混合专家(MoE)架构；5) 在大规模I/Q数据上进行预训练。

Result: 模型在调制分类和联合SNR/移动性识别等下游任务中表现出色，在少样本和数据丰富场景下均优于最先进的深度学习基线，提供了统一的无线学习基础。

Conclusion: LWM-Spectro通过学习通用无线表示，有效解决了无线信号表示学习的挑战，为各种无线学习任务提供了强大的基础模型，在少样本和数据丰富场景下均表现出优越性能。

Abstract: The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.

</details>


### [21] [Quantum CSS LDPC Codes based on Dyadic Matrices for Belief Propagation-based Decoding](https://arxiv.org/abs/2601.08636)
*Alessio Baldelli,Massimo Battaglioni,Jonathan Mandelbaum,Sisi Miao,Laurent Schmalen*

Main category: cs.IT

TL;DR: 提出基于二元矩阵的代数构造方法，设计具有围长6的经典和量子LDPC码，并满足CAMEL-ensemble四进制BP解码器的兼容性条件


<details>
  <summary>Details</summary>
Motivation: 量子LDPC码在量子纠错中需要在纠错能力和实现复杂度之间取得平衡，需要设计既能满足性能要求又具有实际可行性的量子纠错码

Method: 首先基于二元矩阵构造经典二进制准二元LDPC码（Tanner图围长为6），然后扩展到CSS框架，构建满足CAMEL-ensemble四进制BP解码器兼容性条件的两个分量奇偶校验矩阵

Result: 该方法能够设计出满足兼容性条件的量子LDPC码，确保所有不可避免的长度为4的循环都聚集在单个变量节点中，从而通过消去该变量节点来减轻其负面影响

Conclusion: 提出的代数构造方法为设计实用的量子LDPC码提供了有效途径，特别是在与CAMEL-ensemble解码器兼容的量子纠错方案中具有应用价值

Abstract: Quantum low-density parity-check (QLDPC) codes provide a practical balance between error-correction capability and implementation complexity in quantum error correction (QEC). In this paper, we propose an algebraic construction based on dyadic matrices for designing both classical and quantum LDPC codes. The method first generates classical binary quasi-dyadic LDPC codes whose Tanner graphs have girth 6. It is then extended to the Calderbank-Shor-Steane (CSS) framework, where the two component parity-check matrices are built to satisfy the compatibility condition required by the recently introduced CAMEL-ensemble quaternary belief propagation decoder. This compatibility condition ensures that all unavoidable cycles of length 4 are assembled in a single variable node, allowing the mitigation of their detrimental effects by decimating that variable node.

</details>


### [22] [Multivariate Polynomial Codes for Efficient Matrix Chain Multiplication in Distributed Systems](https://arxiv.org/abs/2601.08708)
*Jesús Gómez-Vilardebò*

Main category: cs.IT

TL;DR: 提出两种新的多元多项式编码方案，专门用于分布式环境中的矩阵链乘法，在计算和存储开销之间实现更好的权衡


<details>
  <summary>Details</summary>
Motivation: 分布式计算集群中矩阵链乘法的性能受限于"拖后腿者"问题，现有编码计算策略主要针对两个矩阵相乘的简单情况，扩展到矩阵链乘法时计算和存储开销显著增加，限制了可扩展性

Method: 提出两种专门为矩阵链乘法设计的多元多项式编码方案，与单变量多项式编码扩展相比，这些方案通过多元编码方法优化存储效率

Result: 多元编码方案虽然增加了计算成本，但能显著降低存储开销，揭示了计算效率和存储效率之间的基本权衡关系

Conclusion: 多元编码方案作为大规模分布式线性代数任务的实用解决方案具有潜力，特别是在存储效率至关重要的场景中

Abstract: We study the problem of computing matrix chain multiplications in a distributed computing cluster. In such systems, performance is often limited by the straggler problem, where the slowest worker dominates the overall computation latency. To resolve this issue, several coded computing strategies have been proposed, primarily focusing on the simplest case: the multiplication of two matrices. These approaches successfully alleviate the straggler effect, but they do so at the expense of higher computational complexity and increased storage needs at the workers. However, in many real-world applications, computations naturally involve long chains of matrix multiplications rather than just a single two-matrix product. Extending univariate polynomial coding to this setting has been shown to amplify the costs -- both computation and storage overheads grow significantly, limiting scalability. In this work, we propose two novel multivariate polynomial coding schemes specifically designed for matrix chain multiplication in distributed environments. Our results show that while multivariate codes introduce additional computational cost at the workers, they can dramatically reduce storage overhead compared to univariate extensions. This reveals a fundamental trade-off between computation and storage efficiency, and highlights the potential of multivariate codes as a practical solution for large-scale distributed linear algebra tasks.

</details>


### [23] [On the Algebraic Structure Underlying the Support Enumerators of Linear Codes](https://arxiv.org/abs/2601.08744)
*Nitin Kenjale,Anuradha S. Garge*

Main category: cs.IT

TL;DR: 论文引入了支撑分布和支撑枚举器的概念，作为经典重量分布和重量枚举器的细化，用于捕捉线性分组码中坐标级别的活动信息。


<details>
  <summary>Details</summary>
Motivation: 经典重量分布和重量枚举器只关注码字的整体重量，无法提供坐标级别的详细信息。为了更深入地理解码结构，需要开发能够捕捉坐标级别活动的新工具。

Method: 引入支撑分布和支撑枚举器概念，建立计算第i个坐标非零的码字数量的公式，推导支撑枚举器的MacWilliams型恒等式，将线性码与其对偶码的支撑分布联系起来。

Result: 得到了支撑分布的计数公式，建立了支撑枚举器的对偶关系恒等式，并基于支撑分布相等性推导了自对偶码的条件。

Conclusion: 支撑分布和支撑枚举器提供了比经典重量分布更详细的码结构信息，补充了基于重量的经典对偶理论，为理解坐标级别信息及其在对偶变换中的行为提供了新视角。

Abstract: In this paper, we have introduced the concepts of support distribution and the support enumerator as refinements of the classical weight distribution and weight enumerator respectively, capturing coordinate level activity in linear block codes. More precisely, we have established formula for counting codewords in the linear code C whose i-th coordinate is nonzero. Moreover, we derived a MacWilliam's type identity, relating the normalized support enumerators of a linear code and its dual, explaining how coordinate information transforms under duality. Using this identity we deduce a condition for self duality based on the equality of support distributions. These results provide a more detailed understanding of code structure and complement classical weight based duality theory.

</details>


### [24] [Majority-Logic Decoding of Binary Locally Recoverable Codes: A Probabilistic Analysis](https://arxiv.org/abs/2601.08765)
*Hoang Ly,Emina Soljanin,Philip Whiting*

Main category: cs.IT

TL;DR: 本文研究二进制线性局部可修复码在多数逻辑解码下的纠错性能，推导了在BEC和BSC信道上的译码失败概率上界，揭示了最坏情况保证与随机信道下典型性能之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 局部可修复码的结构特性已被广泛研究，但其在随机擦除和错误下的性能尚未得到充分探索。本文旨在分析LRCs在多数逻辑解码下的纠错和擦除校正性能。

Method: 针对具有固定局部性和可变可用性的二进制线性LRCs，使用多数逻辑解码，推导在BEC和BSC信道上的译码失败概率上界，分析比特错误率和块错误率与局部性和可用性参数的关系。

Result: 在可用性满足温和增长条件下，块译码失败概率渐近消失，多数逻辑解码能够成功校正几乎所有线性权重的错误和擦除模式。结果显示最坏情况保证与随机信道下典型性能存在显著差距。

Conclusion: 局部可修复码在多数逻辑解码下具有良好的随机信道性能，能够有效校正线性权重的错误模式，这为实际分布式存储系统的设计提供了理论支持。

Abstract: Locally repairable codes (LRCs) were originally introduced to enable efficient recovery from erasures in distributed storage systems by accessing only a small number of other symbols. While their structural properties-such as bounds and constructions-have been extensively studied, the performance of LRCs under random erasures and errors has remained largely unexplored. In this work, we study the error- and erasure-correction performance of binary linear LRCs under majority-logic decoding (MLD). Focusing on LRCs with fixed locality and varying availability, we derive explicit upper bounds on the probability of decoding failure over the memoryless Binary Erasure Channel (BEC) and Binary Symmetric Channel (BSC). Our analysis characterizes the behavior of the bit-error rate (BER) and block-error rate (BLER) as functions of the locality and availability parameters. We show that, under mild growth conditions on the availability, the block decoding failure probability vanishes asymptotically, and that majority-logic decoding can successfully correct virtually all of error and erasure patterns of weight linear in the blocklength. The results reveal a substantial gap between worst-case guarantees and typical performance under stochastic channel models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [25] [Application of Ideal Observer for Thresholded Data in Search Task](https://arxiv.org/abs/2601.07976)
*Hongwei Lin,Howard C. Gifford*

Main category: eess.IV

TL;DR: 开发了一种基于阈值视觉搜索的拟人化模型观察器，通过选择性处理高显著性特征来提升图像质量评估性能，在噪声环境下尤其有效。


<details>
  <summary>Details</summary>
Motivation: 现有任务型图像质量评估方法需要改进，特别是在噪声环境下如何有效模拟人类视觉系统，同时减少计算资源需求。

Method: 采用两阶段框架：候选区域选择和决策制定。在候选选择阶段使用阈值化数据筛选感兴趣区域，阶段特异性特征处理优化性能。通过阈值化过滤低显著性特征。

Result: 阈值化能提升观察器性能，特别是在噪声环境中。中间阈值通常优于无阈值处理，表明保留相关特征比保留所有特征更有效。模型能用较少图像有效训练，并与人类性能保持一致。

Conclusion: 该框架能预测临床现实任务中的人类视觉搜索性能，为有限资源下的模型训练提供解决方案，在计算机视觉、机器学习、国防安全等领域有广泛应用前景。

Abstract: This study advances task-based image quality assessment by developing an anthropomorphic thresholded visual-search model observer. The model is an ideal observer for thresholded data inspired by the human visual system, allowing selective processing of high-salience features to improve discrimination performance. By filtering out irrelevant variability, the model enhances diagnostic accuracy and computational efficiency.
  The observer employs a two-stage framework: candidate selection and decision-making. Using thresholded data during candidate selection refines regions of interest, while stage-specific feature processing optimizes performance. Simulations were conducted to evaluate the effects of thresholding on feature maps, candidate localization, and multi-feature scenarios. Results demonstrate that thresholding improves observer performance by excluding low-salience features, particularly in noisy environments. Intermediate thresholds often outperform no thresholding, indicating that retaining only relevant features is more effective than keeping all features.
  Additionally, the model demonstrates effective training with fewer images while maintaining alignment with human performance. These findings suggest that the proposed novel framework can predict human visual search performance in clinically realistic tasks and provide solutions for model observer training with limited resources. Our novel approach has applications in other areas where human visual search and detection tasks are modeled such as in computer vision, machine learning, defense and security image analysis.

</details>


### [26] [Temporal-Enhanced Interpretable Multi-Modal Prognosis and Risk Stratification Framework for Diabetic Retinopathy (TIMM-ProRS)](https://arxiv.org/abs/2601.08240)
*Susmita Kar,A S M Ahsanul Sarkar Akib,Abdul Hasib,Samin Yaser,Anas Bin Azim*

Main category: eess.IV

TL;DR: TIMM-ProRS是一个集成Vision Transformer、CNN和GNN的多模态深度学习框架，利用视网膜图像和时序生物标志物进行糖尿病视网膜病变诊断，在多个数据集上达到97.8%准确率和0.96 F1分数。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变影响全球数百万人且预计将显著增加，存在严重失明风险并给医疗系统带来压力。诊断复杂性源于视觉症状与年龄相关性黄斑变性、高血压性视网膜病变等疾病的重叠，以及在欠发达地区的高误诊率。

Method: 提出TIMM-ProRS深度学习框架，集成Vision Transformer、卷积神经网络和图神经网络，采用多模态融合技术。独特之处在于同时利用视网膜图像和时序生物标志物（HbA1c、视网膜厚度）来捕捉多模态和时序动态特征。

Result: 在APTOS 2019（训练）、Messidor-2、RFMiD、EyePACS和Messidor-1（验证）等多个数据集上全面评估，模型达到97.8%准确率和0.96 F1分数，性能优于RSG-Net和DeepDR等现有方法。

Conclusion: 该方法实现了早期、精确且可解释的诊断，支持可扩展的远程医疗管理，有助于提升全球眼健康的可持续性。

Abstract: Diabetic retinopathy (DR), affecting millions globally with projections indicating a significant rise, poses a severe blindness risk and strains healthcare systems. Diagnostic complexity arises from visual symptom overlap with conditions like age-related macular degeneration and hypertensive retinopathy, exacerbated by high misdiagnosis rates in underserved regions. This study introduces TIMM-ProRS, a novel deep learning framework integrating Vision Transformer (ViT), Convolutional Neural Network (CNN), and Graph Neural Network (GNN) with multi-modal fusion. TIMM-ProRS uniquely leverages both retinal images and temporal biomarkers (HbA1c, retinal thickness) to capture multi-modal and temporal dynamics. Evaluated comprehensively across diverse datasets including APTOS 2019 (trained), Messidor-2, RFMiD, EyePACS, and Messidor-1 (validated), the model achieves 97.8\% accuracy and an F1-score of 0.96, demonstrating state-of-the-art performance and outperforming existing methods like RSG-Net and DeepDR. This approach enables early, precise, and interpretable diagnosis, supporting scalable telemedical management and enhancing global eye health sustainability.

</details>


### [27] [Region of interest detection for efficient aortic segmentation](https://arxiv.org/abs/2601.08683)
*Loris Giordano,Ine Dirks,Tom Lenaerts,Jef Vandemeulebroucke*

Main category: eess.IV

TL;DR: 提出一种基于目标ROI检测的高效主动脉分割方法，通过检测-分割级联模型，在减少计算资源的同时达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 胸主动脉夹层和动脉瘤是最致命的主动脉疾病，治疗的主要障碍在于医学图像的准确分析。主动脉3D图像分割通常繁琐困难，基于深度学习的分割模型虽然理想，但在困难病例中无法提供可用输出且计算成本高，导致临床采用受限。

Method: 提出创新的高效主动脉分割方法，使用目标ROI检测。与经典检测模型不同，提出简单高效的检测模型，可广泛应用于检测单个ROI。检测模型作为多任务模型训练，使用编码器-解码器架构进行分割，并在瓶颈处附加全连接网络进行检测。比较了应用于完整图像的一步分割模型、nnU-Net以及由检测和分割步骤组成的级联模型的性能。

Result: 实现了平均Dice相似系数0.944，所有病例均超过0.9，同时仅使用三分之一的计算能力。这种简单解决方案实现了最先进的性能，同时紧凑且鲁棒。

Conclusion: 该方法是临床应用的理想解决方案，在减少计算资源的同时达到最先进的分割性能，解决了传统深度学习分割模型在临床应用中遇到的困难。

Abstract: Thoracic aortic dissection and aneurysms are the most lethal diseases of the aorta. The major hindrance to treatment lies in the accurate analysis of the medical images. More particularly, aortic segmentation of the 3D image is often tedious and difficult. Deep-learning-based segmentation models are an ideal solution, but their inability to deliver usable outputs in difficult cases and their computational cost cause their clinical adoption to stay limited. This study presents an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. In contrast to classical detection models, we propose a simple and efficient detection model that can be widely applied to detect a single ROI. Our detection model is trained as a multi-task model, using an encoder-decoder architecture for segmentation and a fully connected network attached to the bottleneck for detection. We compare the performance of a one-step segmentation model applied to a complete image, nnU-Net and our cascade model composed of a detection and a segmentation step. We achieve a mean Dice similarity coefficient of 0.944 with over 0.9 for all cases using a third of the computing power. This simple solution achieves state-of-the-art performance while being compact and robust, making it an ideal solution for clinical applications.

</details>


### [28] [A Single-Parameter Factor-Graph Image Prior](https://arxiv.org/abs/2601.08749)
*Tianyang Wang,Ender Konukoglu,Hans-Andrea Loeliger*

Main category: eess.IV

TL;DR: 提出一种具有分段常数局部参数的自适应分段平滑图像模型，使用因子图和NUP先验，通过共轭梯度步和高斯消息传递进行迭代计算，应用于去噪和对比度增强。


<details>
  <summary>Details</summary>
Motivation: 传统图像模型往往使用固定参数，无法适应不同图像区域的特征变化。需要一种能够自动调整局部参数以适应图像内容的自适应模型。

Method: 使用因子图框架，引入NUP（参数未知的正态分布）先验，将图像建模为分段平滑模型。通过迭代的共轭梯度步骤和高斯消息传递算法进行参数估计和图像恢复。

Result: 提出的模型和算法在图像去噪和对比度增强任务中表现出良好性能，能够自动适应图像局部特征，有效恢复图像细节。

Conclusion: 基于因子图和NUP先验的自适应分段平滑图像模型为图像处理提供了一种灵活有效的框架，能够自动调整局部参数以适应图像内容，在去噪和对比度增强等应用中取得良好效果。

Abstract: We propose a novel piecewise smooth image model with piecewise constant local parameters that are automatically adapted to each image. Technically, the model is formulated in terms of factor graphs with NUP (normal with unknown parameters) priors, and the pertinent computations amount to iterations of conjugate-gradient steps and Gaussian message passing. The proposed model and algorithms are demonstrated with applications to denoising and contrast enhancement.

</details>


### [29] [M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding](https://arxiv.org/abs/2601.08758)
*Juntao Jiang,Jiangning Zhang,Yali Bi,Jinsheng Bai,Weixuan Liu,Weiwei Jin,Zhucun Xue,Yong Liu,Xiaobin Hu,Shuicheng Yan*

Main category: eess.IV

TL;DR: 提出了M3CoTBench基准测试，专门评估医疗图像理解中思维链推理的正确性、效率、影响和一致性，填补了当前医疗基准只关注最终答案而忽略推理过程的空白。


<details>
  <summary>Details</summary>
Motivation: 当前医疗图像理解基准主要关注最终答案而忽略推理路径，这种不透明的过程缺乏可靠的判断依据，难以辅助医生诊断。思维链推理与临床思维过程自然契合，但缺乏专门的评估标准。

Method: 构建了M3CoTBench基准，包含：1）涵盖24种检查类型的多样化多难度数据集；2）13个不同难度的任务；3）专门针对临床推理设计的思维链评估指标（正确性、效率、影响、一致性）；4）对多个多模态大语言模型的性能分析。

Result: 系统评估了多种医疗成像任务中的思维链推理，揭示了当前多模态大语言模型在生成可靠且临床可解释的推理方面的局限性。

Conclusion: M3CoTBench基准旨在促进开发透明、可信且诊断准确的医疗AI系统，填补了医疗图像理解中思维链推理评估的空白，为未来研究提供了重要基准。

Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. To address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features 1) a diverse, multi-level difficulty dataset covering 24 examination types, 2) 13 varying-difficulty tasks, 3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning, and 4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare. Project page at https://juntaojianggavin.github.io/projects/M3CoTBench/.

</details>
