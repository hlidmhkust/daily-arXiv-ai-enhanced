<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.IT](#cs.IT) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Phase-Only Positioning in Distributed MIMO Under Phase Impairments: AP Selection Using Deep Learning](https://arxiv.org/abs/2602.05034)
*Fatih Ayten,Musa Furkan Keskin,Akshay Jain,Mehmet C. Ilter,Ossi Kaltiokallio,Jukka Talvitie,Elena Simona Lohan,Mikko Valkama*

Main category: eess.SP

TL;DR: 提出深度学习天线点选择框架，在相位同步误差下实现高精度分布式MIMO定位


<details>
  <summary>Details</summary>
Motivation: 载波相位定位可实现厘米级精度，但相位同步误差对分布式MIMO系统的影响研究不足，需要解决相位同步误差下的高精度定位问题

Method: 1. 提出双曲线交点方法，在相位同步误差下仍能实现高精度定位；2. 引入基于深度学习的分布式MIMO天线点选择框架，优化相位同步误差下的定位性能

Result: 仿真结果表明，相比现有方法，所提框架提高了定位精度，同时将推理复杂度降低了约19.7%

Conclusion: 深度学习天线点选择框架能有效应对相位同步误差，在分布式MIMO系统中实现高精度定位，同时降低计算复杂度

Abstract: Carrier phase positioning (CPP) can enable cm-level accuracy in next-generation wireless systems, while recent literature shows that accuracy remains high using phase-only measurements in distributed MIMO (D-MIMO). However, the impact of phase synchronization errors on such systems remains insufficiently explored. To address this gap, we first show that the proposed hyperbola intersection method achieves highly accurate positioning even in the presence of phase synchronization errors, when trained on appropriate data reflecting such impairments. We then introduce a deep learning (DL)-based D-MIMO antenna point (AP) selection framework that ensures high-precision localization under phase synchronization errors. Simulation results show that the proposed framework improves positioning accuracy compared to prior-art methods, while reducing inference complexity by approximately 19.7%.

</details>


### [2] [Integrated Sensing, Communication, and Control for UAV-Assisted Mobile Target Tracking](https://arxiv.org/abs/2602.05209)
*Zhiyu Chen,Ming-Min Zhao,Songfu Cai,Ming Lei,Min-Jian Zhao*

Main category: eess.SP

TL;DR: 提出无人机辅助目标跟踪的集成感知、通信与控制框架，通过随机模型预测控制和波束成形联合设计，实现精确跟踪、可靠通信和稳定控制。


<details>
  <summary>Details</summary>
Motivation: 无人机在关键任务应用中需要同时感知动态环境、确保可靠通信并实现精确控制，但现有方法难以在统一框架内同时保证跟踪精度、通信可靠性和控制稳定性。

Method: 1) 建立离散时间线性控制模型；2) 使用扩展卡尔曼滤波器估计目标状态；3) 推导给定控制输入下的最优波束成形闭式解；4) 通过松弛凸近似将非凸约束转化为可处理的凸优化问题。

Result: 数值结果表明，所提ISCC框架在保持稳定通信的同时，跟踪精度接近非因果基准，并显著优于传统控制和跟踪方法。

Conclusion: 提出的集成感知、通信与控制框架能够有效解决无人机目标跟踪中的多目标优化问题，在保证通信可靠性的同时实现高精度跟踪，为关键任务应用提供了可行的解决方案。

Abstract: Unmanned aerial vehicles (UAVs) are increasingly deployed in mission-critical applications such as target tracking, where they must simultaneously sense dynamic environments, ensure reliable communication, and achieve precise control. A key challenge here is to jointly guarantee tracking accuracy, communication reliability, and control stability within a unified framework. To address this issue, we propose an integrated sensing, communication, and control (ISCC) framework for UAV-assisted target tracking, where the considered tracking system is modeled as a discrete-time linear control process, with the objective of driving the deviation between the UAV and target states toward zero. We formulate a stochastic model predictive control (MPC) optimization problem for joint control and beamforming design, which is highly non-convex and intractable in its original form. To overcome this difficulty, the target state is first estimated using an extended Kalman filter (EKF). Then, by deriving the closed-form optimal beamforming solution under a given control input, the original problem is equivalently reformulated into a tractable control-oriented form. Finally, we convexify the remaining non-convex constraints via a relaxation-based convex approximation, yielding a computationally tractable convex optimization problem that admits efficient global solution. Numerical results show that the proposed ISCC framework achieves tracking accuracy comparable to a non-causal benchmark while maintaining stable communication, and it significantly outperforms the conventional control and tracking method.

</details>


### [3] [A Migration-Assisted Deep Learning Scheme for Imaging Defects Inside Cylindrical Structures via GPR: A Case Study for Tree Trunks](https://arxiv.org/abs/2602.05308)
*Jiwei Qian,Yee Hui Lee,Kaixuan Cheng,Qiqi Dai,Arda Yalcinkaya,Mohamed Lokman Mohd Yusof,James Wang,Abdulkadir C. Yucel*

Main category: eess.SP

TL;DR: 提出迁移辅助深度学习方案，用于重建圆柱结构内部缺陷的形状和介电常数，结合双介电常数估计网络、改进的Kirchhoff迁移和形状重建网络


<details>
  <summary>Details</summary>
Motivation: 探地雷达在圆柱结构内部缺陷成像中应用广泛，但由于复杂的波散射现象和现有信号处理与深度学习技术的精度限制，准确重建缺陷形状和介电常数仍然具有挑战性

Method: 三阶段处理方案：1) 双介电常数估计网络提取缺陷和圆柱结构的介电常数；2) 改进的Kirchhoff迁移将缺陷反射信号映射到成像域；3) 形状重建网络处理迁移图像恢复缺陷精确形状

Result: 在合成数据和实验数据（实验室树干模型和真实树干样本）上验证，相比现有深度学习方法表现出优越性能，在活树上的泛化测试证实了现场部署的可行性

Conclusion: 提出的迁移辅助深度学习方案能有效重建圆柱结构内部缺陷的形状和介电常数，其原理可进一步应用于其他圆周探地雷达成像场景

Abstract: Ground-penetrating radar (GPR) has emerged as a prominent tool for imaging internal defects in cylindrical structures, such as columns, utility poles, and tree trunks. However, accurately reconstructing both the shape and permittivity of the defects inside cylindrical structures remains challenging due to complex wave scattering phenomena and the limited accuracy of the existing signal processing and deep learning techniques. To address these issues, this study proposes a migration-assisted deep learning scheme for reconstructing the shape and permittivity of defects within cylindrical structures. The proposed scheme involves three stages of GPR data processing. First, a dual-permittivity estimation network extracts the permittivity values of the defect and the cylindrical structure, the latter of which is estimated with the help of a novel structural similarity index measure-based autofocusing technique. Second, a modified Kirchhoff migration incorporating the extracted permittivity of the cylindrical structure maps the signals reflected from the defect to the imaging domain. Third, a shape reconstruction network processes the migrated image to recover the precise shape of the defect. The image of the interior defect is finally obtained by combining the reconstructed shape and extracted permittivity of the defect. The proposed scheme is validated using both synthetic and experimental data from a laboratory trunk model and real tree trunk samples. Comparative results show superior performance over existing deep learning methods, while generalization tests on live trees confirm its feasibility for in-field deployment. The underlying principle can further be applied to other circumferential GPR imaging scenarios. The code and database are available at: https://github.com/jwqian54/Migration-Assisted-DL.

</details>


### [4] [Joint Optimization of Latency and Accuracy for Split Federated Learning in User-Centric Cell-Free MIMO Networks](https://arxiv.org/abs/2602.05342)
*Zitong Wang,Cheng Zhang,Wen Wang,Shuigen Yang,Haiming Wang,Yongming Huang*

Main category: eess.SP

TL;DR: 提出用户中心化分割联邦学习框架，用于CF-MIMO网络，通过联合优化功率控制、波束成形、模型分割和AP聚类来最小化延迟-准确率比


<details>
  <summary>Details</summary>
Motivation: 在用户中心化无小区MIMO网络中支持分割联邦学习，需要解决通信和计算资源约束下的模型训练效率问题，特别是AP集群规模对训练精度的影响

Method: 提出UCSFL框架，用户在本地部署分割子模型，AP侧DPU维护完整模型，采用两级聚合；理论分析发现AP集群规模是关键因素，提出延迟-准确率比指标，并设计联合优化问题的分解算法

Result: 仿真验证算法收敛性，UCSFL相比基线方案有效降低VGG16模型的延迟-准确率比，能自适应调整分割和聚类策略，MNIST手写数字分类实验显示显著加速VGG16收敛

Conclusion: UCSFL框架通过联合优化通信和计算资源，在用户中心化CF-MIMO网络中实现了高效的分割联邦学习，平衡了训练延迟和模型精度

Abstract: This paper proposes a user-centric split federated learning (UCSFL) framework for user-centric cell-free multiple-input multiple-output (CF-MIMO) networks to support split federated learning (SFL). In the proposed UCSFL framework, users deploy split sub-models locally, while complete models are maintained and updated at access point (AP)-side distributed processing units (DPUs), followed by a two-level aggregation procedure across DPUs and the central processing unit (CPU). Under standard machine learning (ML) assumptions, we provide a theoretical convergence analysis for UCSFL, which reveals that the AP-cluster size is a key factor influencing model training accuracy. Motivated by this result, we introduce a new performance metric, termed the latency-to-accuracy ratio, defined as the ratio of a user's per-iteration training latency to the weighted size of its AP cluster. Based on this metric, we formulate a joint optimization problem to minimize the maximum latency-to-accuracy ratio by jointly optimizing uplink power control, downlink beamforming, model splitting, and AP clustering. The resulting problem is decomposed into two sub-problems operating on different time scales, for which dedicated algorithms are developed to handle the short-term and long-term optimizations, respectively. Simulation results verify the convergence of the proposed algorithms and demonstrate that UCSFL effectively reduces the latency-to-accuracy ratio of the VGG16 model compared with baseline schemes. Moreover, the proposed framework adaptively adjusts splitting and clustering strategies in response to varying communication and computation resources. An MNIST-based handwritten digit classification example further shows that UCSFL significantly accelerates the convergence of the VGG16 model.

</details>


### [5] [Beamformed Fingerprint-Based Transformer Network for Trajectory Estimation and Path Determination in Outdoor mmWave MIMO Systems](https://arxiv.org/abs/2602.05554)
*Mohammad Shamsesalehi,Mahmoud Ahmadian Attari,Mohammad Amin Maleki Sadr,Benoit Champagne*

Main category: eess.SP

TL;DR: 该论文提出了一种基于毫米波MIMO系统的两阶段轨迹估计与路径规划方法：第一阶段使用Transformer网络从波束赋形指纹序列预测设备轨迹；第二阶段使用改进的iRRT*算法基于轨迹估计寻找最优路径。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信在精确定位和轨迹估计方面具有重要应用价值。现有方法在室外毫米波MIMO系统中对设备轨迹估计和路径规划的效率和准确性有待提升，需要开发能够处理波束赋形指纹序列并实现高效导航的新方法。

Method: 提出两阶段方法：1) 使用基于注意力机制的Transformer网络处理毫米波MIMO室外系统中捕获的波束赋形指纹序列，预测无线设备轨迹；2) 开发基于改进的Informed RRT* (iRRT*)算法，利用第一阶段得到的轨迹估计确定到目标位置的最优路径。

Result: 数值实验验证了方法的有效性：Transformer轨迹估计器优于现有方法，并能成功泛化到训练集外的新轨迹；提出的iRRT*算法能够持续提供到目标的最短路径。实验使用28GHz室外传播的射线追踪仿真生成的无线电测量数据集。

Conclusion: 该研究成功开发了适用于毫米波MIMO室外系统的轨迹估计和路径规划框架，结合深度学习和路径规划算法，为高效导航应用提供了有效解决方案，在轨迹预测准确性和路径优化方面均表现出优越性能。

Abstract: Radio transmissions in millimeter wave (mmWave) bands have gained significant interest for applications demanding precise device localization and trajectory estimation. This paper explores novel neural network (NN) architectures suitable for trajectory estimation and path determination in a mmWave multiple-input multiple-output (MIMO) outdoor system based on localization data from beamformed fingerprint (BFF). The NN architecture captures sequences of BFF signals from different users, and through the application of learning mechanisms, subsequently estimate their trajectories. In turn, this information is employed to find the shortest path to the target, thereby enabling more efficient navigation. Specifically, we propose a two-stage procedure for trajectory estimation and optimal path finding. In the first stage, a transformer network (TN) based on attention mechanisms is developed to predict trajectories of wireless devices using BFF sequences captured in a mmWave MIMO outdoor system. In the second stage, a novel algorithm based on Informed Rapidly-exploring Random Trees (iRRT*) is employed to determine the optimal path to target locations using trajectory estimates derived in the first stage. The effectiveness of the proposed schemes is validated through numerical experiments, using a comprehensive dataset of radio measurements, generated using ray tracing simulations to model outdoor propagation at 28 GHz. We show that our proposed TN-based trajectory estimator outperforms other methods from the recent literature and can successfully generalize to new trajectories outside the training set. Furthermore, our proposed iRRT* algorithm is able to consistently provide the shortest path to the target.

</details>


### [6] [Depth estimation of a monoharmonic source using a vertical linear array at fixed distance](https://arxiv.org/abs/2602.05560)
*Yangjin Xu,Wei Gao,Xiaolei Li,Qinghang Zeng*

Main category: eess.SP

TL;DR: 提出OCMS-D方法，利用垂直线性阵列在未知海底参数条件下估计固定距离单频声源的深度


<details>
  <summary>Details</summary>
Motivation: 在未知海底环境参数的情况下，使用垂直线性阵列估计固定距离单频声源的深度具有挑战性，相关研究较少

Method: 基于正交约束模态搜索的深度估计方法（OCMS-D），利用传播简正模的稀疏性和模态深度函数的正交性，先估计固定源-阵距离下的简正模参数，再利用这些参数估计源深度，同时利用振幅分布和符号模式信息确保精度

Result: 数值模拟评估了不同条件下的性能，黄海实验中4秒时间窗口的深度估计绝对误差小于2.4米，SWellEx-96实验中10秒时间窗口的浅源误差小于5.4米，深源误差小于10.8米

Conclusion: OCMS-D方法在未知海底参数条件下能有效估计单频声源深度，实验验证了其有效性

Abstract: Estimating the depth of a monoharmonic sound source at a fixed range using a vertical linear array (VLA) is challenging in the absence of seabed environmental parameters, and relevant research remains scarce. The orthogonality constrained modal search based depth estimation (OCMS-D) method is proposed in this paper, which enables the estimation of the depth of a monoharmonic source at a fixed range using a VLA under unknown seabed parameters. Using the sparsity of propagating normal modes and the orthogonality of mode depth functions, OCMS-D estimates the normal mode parameters under a fixed source-array distance at first. The estimated normal mode parameters are then used to estimate the source depth. To ensure the precision of the source depth estimation, the method utilizes information on both the amplitude distribution and the sign (positive/negative) patterns of the estimated mode depth functions at the inferred source depth. Numerical simulations evaluate the performance of OCMS-D under different conditions. The effectiveness of OCMS-D is also verified by the Yellow Sea experiment and the SWellEx-96 experiment. In the Yellow Sea experiment, the depth estimation absolute errors by OCMS-D with a 4-second time window are less than 2.4 m. And the depth estimation absolute errors in the SWellEx-96 experiment with a 10-second time window are less than 5.4 m for the shallow source and less than 10.8 m for the deep source.

</details>


### [7] [Physics-Aware Tensor Reconstruction for Radio Maps in Pixel-Based Fluid Antenna Systems](https://arxiv.org/abs/2602.05579)
*Mu Jia,Hao Sun,Junting Chen,Pooi-Yuen Kam*

Main category: eess.SP

TL;DR: 提出PR-LRTC框架，通过物理正则化低秩张量补全方法，在稀疏测量下重建高保真无线地图，解决像素天线和流体天线系统的CSI获取开销问题。


<details>
  <summary>Details</summary>
Motivation: 像素天线和流体天线系统面临过高的信道状态信息获取开销。现有方法在极端稀疏条件下难以恢复精细的阴影细节，需要结合物理先验知识来提升重建质量。

Method: 提出物理正则化低秩张量补全框架，将信号场建模为三阶张量，结合环境低秩性和确定性天线物理特性。利用有效空中自由度理论推导差分增益拓扑图作为物理先验进行正则化，采用ADMM算法求解优化问题。

Result: 在10%采样率下，PR-LRTC相比基线方法获得4dB增益，有效保留锐利的阴影边缘，为低开销波束管理提供鲁棒的物理合规解决方案。

Conclusion: PR-LRTC框架通过整合物理先验与低秩张量补全，在稀疏测量条件下实现了高质量的无线地图重建，为下一代天线系统的低开销波束管理提供了有效解决方案。

Abstract: The deployment of pixel-based antennas and fluid antenna systems (FAS) is hindered by prohibitive channel state information (CSI) acquisition overhead. While radio maps enable proactive mode selection, reconstructing high-fidelity maps from sparse measurements is challenging. Existing physics-agnostic or data-driven methods often fail to recover fine-grained shadowing details under extreme sparsity. We propose a Physics-Regularized Low-Rank Tensor Completion (PR-LRTC) framework for radio map reconstruction. By modeling the signal field as a three-way tensor, we integrate environmental low-rankness with deterministic antenna physics. Specifically, we leverage Effective Aerial Degrees-of-Freedom (EADoF) theory to derive a differential gain topology map as a physical prior for regularization. The resulting optimization problem is solved via an efficient Alternating Direction Method of Multipliers (ADMM)-based algorithm. Simulations show that PR-LRTC achieves a 4 dB gain over baselines at a 10% sampling ratio. It effectively preserves sharp shadowing edges, providing a robust, physics-compliant solution for low-overhead beam management.

</details>


### [8] [Physics-Inspired Target Shape Detection and Reconstruction in mmWave Communication Systems](https://arxiv.org/abs/2602.05581)
*Ziqing Xing,Zhaoyang Zhang,Xin Tong,Zhaohui Yang,Chongwen Huang*

Main category: eess.SP

TL;DR: 该论文提出了一种毫米波ISAC系统中多基站协作感知方法，通过融合散射和反射信号来重建远程目标形状


<details>
  <summary>Details</summary>
Motivation: 毫米波ISAC系统具有大带宽优势，可实现高精度感知。在多基站MIMO-OFDM毫米波通信系统中，如何利用包含反射和散射信号的收集信号来感知远程目标形状是一个重要问题

Method: 1. 基于Lambertian散射模型表征毫米波的散射和反射效应；2. 使用周期图技术进行粗略散射点检测；3. 结合子空间方法实现更精确的散射和反射点检测；4. 设计基于Hough变换和PCA的重建算法处理单个凸多边形目标；5. 提出融合散射和反射点的方法提高重建精度和完整性

Result: 广泛的仿真结果验证了所提算法的有效性，表明该方法能够准确重建远程目标形状

Conclusion: 该研究为毫米波ISAC系统中的多基站协作感知提供了一种有效的目标形状重建方法，通过融合散射和反射信号提高了感知精度

Abstract: The integration of sensing and communication (ISAC) is an essential function of future wireless systems. Due to its large available bandwidth, millimeter-wave (mmWave) ISAC systems are able to achieve high sensing accuracy. In this paper, we consider the multiple base-station (BS) collaborative sensing problem in a multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) mmWave communication system. Our aim is to sense a remote target shape with the collected signals which consist of both the reflection and scattering signals. We first characterize the mmWave's scattering and reflection effects based on the Lambertian scattering model. Then we apply the periodogram technique to obtain rough scattering point detection, and further incorporate the subspace method to achieve more precise scattering and reflection point detection. Based on these, a reconstruction algorithm based on Hough Transform and principal component analysis (PCA) is designed for a single convex polygon target scenario. To improve the accuracy and completeness of the reconstruction results, we propose a method to further fuse the scattering and reflection points. Extensive simulation results validate the effectiveness of the proposed algorithms.

</details>


### [9] [Sound Field Estimation Using Optimal Transport Barycenters in the Presence of Phase Errors](https://arxiv.org/abs/2602.05715)
*Yuyang Liu,Johan Karlsson,Filip Elvander*

Main category: eess.SP

TL;DR: 提出基于最优传输(OT)框架的声场重建方法，解决传感器校准误差引起的相位扰动问题，相比传统方法能获得更准确的平面波系数估计。


<details>
  <summary>Details</summary>
Motivation: 声场重建中，传感器位置和响应特性的校准误差会导致平面波系数的相位扰动，传统方法会产生有偏估计或非凸解，需要新的鲁棒估计方法。

Method: 提出最优传输(OT)框架，将观测相关的偏移系数表示为非负测度集合，通过OT将这些测度的支撑集在相空间中传输到最优平均值（重心），该重心通过相同的提升规则与估计的平面波系数关联。通过添加常数到地面成本确保传输矩阵的稀疏性，保持解的凸一致性。

Result: 仿真结果表明，在同时存在加性噪声和相位扰动的情况下，所提方法比基线方法能提供更准确的系数估计。

Conclusion: 基于最优传输的框架能有效处理声场重建中的相位扰动问题，提供鲁棒且准确的平面波系数估计，解决了传统方法的局限性。

Abstract: This study introduces a novel approach for estimating plane-wave coefficients in sound field reconstruction, specifically addressing challenges posed by error-in-variable phase perturbations. Such systematic errors typically arise from sensor mis-calibration, including uncertainties in sensor positions and response characteristics, leading to measurement-induced phase shifts in plane wave coefficients. Traditional methods often result in biased estimates or non-convex solutions. To overcome these issues, we propose an optimal transport (OT) framework. This framework operates on a set of lifted non-negative measures that correspond to observation-dependent shifted coefficients relative to the unperturbed ones. By applying OT, the supports of the measures are transported toward an optimal average in the phase space, effectively morphing them into an indistinguishable state. This optimal average, known as barycenter, is linked to the estimated plane-wave coefficients using the same lifting rule. The framework addresses the ill-posed nature of the problem, due to the large number of plane waves, by adding a constant to the ground cost, ensuring the sparsity of the transport matrix. Convex consistency of the solution is maintained. Simulation results confirm that our proposed method provides more accurate coefficient estimations compared to baseline approaches in scenarios with both additive noise and phase perturbations.

</details>


### [10] [Reciprocity Calibration of Dual-Antenna Repeaters via MMSE Estimation](https://arxiv.org/abs/2602.05724)
*Shoma Hara,Takumi Takahashi,Hiroki Iimori,Hideki Ochiai,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出一种贝叶斯互易性校准方法，确保中继辅助MIMO系统中上下行信道互易性，基于MMSE准则，结合统计先验知识，开发冯·米塞斯降噪器提高相位对齐精度。


<details>
  <summary>Details</summary>
Motivation: 在中继辅助MIMO系统中，确保上下行信道互易性对系统性能至关重要。传统确定性非线性最小二乘方法在估计精度和计算复杂度方面存在局限，需要更有效的校准方法。

Method: 提出基于最小均方误差准则的贝叶斯互易性校准算法，结合信号模型、噪声和先验分布的完整统计知识。开发冯·米塞斯降噪器，利用目标参数位于复平面圆上的特性，提高相位对齐精度。

Result: 仿真结果表明，所提出的MMSE算法相比传统确定性非线性最小二乘方法显著提高了估计精度，同时保持相当的计算复杂度。该方法还表现出极快的收敛速度，适合实际应用。

Conclusion: 提出的贝叶斯互易性校准方法在中继辅助MIMO系统中能有效确保信道互易性，具有高精度、低复杂度和快速收敛的特点，适合实际系统部署。

Abstract: This paper proposes a novel Bayesian reciprocity calibration method that consistently ensures uplink and downlink channel reciprocity in repeater-assisted multiple-input multiple-output (MIMO) systems. The proposed algorithm is formulated under the minimum mean-square error (MMSE) criterion. Its Bayesian framework incorporates complete statistical knowledge of the signal model, noise, and prior distributions, enabling a coherent design that achieves both low computational complexity and high calibration accuracy. To further enhance phase alignment accuracy, which is critical for calibration tasks, we develop a von Mises denoiser that exploits the fact that the target parameters lie on the circle in the complex plane. Simulation results demonstrate that the proposed MMSE algorithm achieves substantially improved estimation accuracy compared with conventional deterministic non-linear least-squares (NLS) methods, while maintaining comparable computational complexity. Furthermore, the proposed method exhibits remarkably fast convergence, making it well suited for practical implementation.

</details>


### [11] [Discrete Aware Tensor Completion via Convexized $\ell_0$-Norm Approximation](https://arxiv.org/abs/2602.05802)
*Niclas Führling,Getuar Rexhepi,Giuseppe Abreu*

Main category: eess.SP

TL;DR: 提出一种用于离散有限字母集低秩张量补全的新算法，通过添加离散感知正则化器增强传统核范数最小化方法


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量补全方法通常处理连续值，但在许多实际应用中（如图像处理中的RGB值），张量条目来自离散有限字母集。现有方法未能有效利用这种离散特性。

Method: 在传统核范数最小化框架基础上，添加ℓ₀范数正则化器来强制离散性，通过分数规划将ℓ₀范数近似为连续可微函数，并在近端梯度框架下求解优化问题。

Result: 仿真结果表明，新方法在归一化均方误差和收敛性方面均优于现有技术，包括核范数最小化方法及其与矩阵分解的混合方法。

Conclusion: 提出的离散感知低秩张量补全方法能有效处理离散字母集数据，在性能和收敛速度上优于传统方法，为图像处理等应用提供了更好的解决方案。

Abstract: We consider a novel algorithm, for the completion of partially observed low-rank tensors, where each entry of the tensor can be chosen from a discrete finite alphabet set, such as in common image processing problems, where the entries represent the RGB values. The proposed low-rank tensor completion (TC) method builds on the conventional nuclear norm (NN) minimization-based low-rank TC paradigm, through the addition of a discrete-aware regularizer, which enforces discreteness in the objective of the problem, by an $\ell_0$-norm regularizer that is approximated by a continuous and differentiable function normalized via fractional programming (FP) under a proximal gradient (PG) framework, in order to solve the proposed problem. Simulation results demonstrate the superior performance of the new method both in terms of normalized mean square error (NMSE) and convergence, compared to the conventional state of-the-art (SotA) techniques, including NN minimization approaches, as well as a mixture of the latter with a matrix factorization approach.

</details>


### [12] [IDSOR: Intensity- and Distance-Aware Statistical Outlier Removal for Weather-Robust LiDAR Point Clouds](https://arxiv.org/abs/2602.05876)
*Chenyang Yan,Mats Bengtsson*

Main category: eess.SP

TL;DR: 提出IDSOR方法，利用强度和距离信息自适应过滤雨雪天气下的LiDAR噪声点，保持结构细节


<details>
  <summary>Details</summary>
Motivation: 雨雪天气下LiDAR点云常受天气噪声污染，影响感知和场景理解的安全性

Method: 提出强度和距离感知的统计离群点去除方法，结合强度线索和邻域稀疏性，使用经验性的距离依赖天气噪声分布设计阈值

Result: 在模拟增强的平交道口测量和冬季恶劣驾驶数据集上，IDSOR实现了良好的精确率-召回率权衡，在WADS上保持精确率和召回率均超过90%

Conclusion: IDSOR能有效抑制天气噪声点同时保持精细结构细节，无需繁琐的手动参数调优

Abstract: LiDAR point clouds captured in rain or snow are often corrupted by weather-induced returns, which can degrade perception and safety-critical scene understanding. This paper proposes Intensity- and Distance-Aware Statistical Outlier Removal (IDSOR), a range-adaptive filtering method that jointly exploits intensity cues and neighborhood sparsity. By incorporating an empirical, range-dependent distribution of weather returns into the threshold design, IDSOR suppresses weather-induced points while preserving fine structural details without cumbersome manual parameter tuning. We also propose a variant that uses a previously proposed method to estimate the weather return distribution from data, and integrates it into IDSOR. Experiments on simulation-augmented level-crossing measurements and on the Winter Adverse Driving dataset (WADS) demonstrate that IDSOR achieves a favorable precision-recall trade-off, maintaining both precision and recall above 90% on WADS.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [13] [Smart Diagnosis and Early Intervention in PCOS: A Deep Learning Approach to Women's Reproductive Health](https://arxiv.org/abs/2602.04944)
*Shayan Abrar,Samura Rahman,Ishrat Jahan Momo,Mahjabin Tasnim Samiha,B. M. Shahria Alam,Mohammad Tahmid Noor,Nishat Tasnim Niloy*

Main category: eess.IV

TL;DR: 该研究开发了一个基于DenseNet201和ResNet50的迁移学习框架，用于分类卵巢超声图像，检测多囊卵巢综合征，达到了99.80%的验证准确率，并使用可解释AI技术提高模型透明度。


<details>
  <summary>Details</summary>
Motivation: 多囊卵巢综合征是育龄妇女常见疾病，会导致不孕、代谢综合征和心血管风险等长期并发症，早期检测至关重要。需要开发自动化系统来辅助临床诊断。

Method: 使用DenseNet201和ResNet50构建迁移学习框架，在3856张卵巢超声图像数据集上训练。采用MixUp和CutMix数据增强策略，图像预处理为224x224像素。使用SHAP、Grad-CAM和LIME等可解释AI技术分析模型决策。

Result: DenseNet201达到99.80%的峰值验证准确率，验证损失为0.617（alpha值分别为0.25和0.4）。模型表现出色，可解释AI技术成功提供了模型决策的视觉解释。

Conclusion: 该研究提出了一个有效的自动化医疗图像诊断系统，结合高性能分类模型和可解释AI技术，提高了诊断的准确性和透明度，有望在临床实践中得到应用。

Abstract: Polycystic Ovary Syndrome (PCOS) is a widespread disorder in women of reproductive age, characterized by a hormonal imbalance, irregular periods, and multiple ovarian cysts. Infertility, metabolic syndrome, and cardiovascular risks are long-term complications that make early detection essential. In this paper, we design a powerful framework based on transfer learning utilizing DenseNet201 and ResNet50 for classifying ovarian ultrasound images. The model was trained on an online dataset containing 3856 ultrasound images of cyst-infected and non-infected patients. Each ultrasound frame was resized to 224x224 pixels and encoded with precise pathological indicators. The MixUp and CutMix augmentation strategies were used to improve generalization, yielding a peak validation accuracy of 99.80% by Densenet201 and a validation loss of 0.617 with alpha values of 0.25 and 0.4, respectively. We evaluated the model's interpretability using leading Explainable AI (XAI) approaches such as SHAP, Grad-CAM, and LIME, reasoning with and presenting explicit visual reasons for the model's behaviors, therefore increasing the model's transparency. This study proposes an automated system for medical picture diagnosis that may be used effectively and confidently in clinical practice.

</details>


### [14] [AI-Based Detection of In-Treatment Changes from Prostate MR-Linac Images](https://arxiv.org/abs/2602.04983)
*Seungbin Park,Peilin Wang,Ryan Pennell,Emily S. Weg,Himanshu Nagar,Timothy McClure,Mert R. Sabuncu,Daniel Margolis,Heejong Kim*

Main category: eess.IV

TL;DR: 利用AI模型分析MR-Linac图像序列，成功检测放疗期间（平均2天间隔）的细微治疗诱导变化，模型性能优于放射科医生。


<details>
  <summary>Details</summary>
Motivation: 探索常规获取的纵向MR-Linac图像是否可用于表征放疗期间的治疗诱导变化，特别是短时间间隔（平均2天）内的细微变化。

Method: 回顾性研究761名患者的0.35T MR-Linac图像，使用深度学习模型通过预测配对图像的时间顺序来表征治疗诱导变化。模型首先用首末分次图像训练，然后用所有配对图像训练。通过定量指标、与放射科医生对比、显著性图分析和输入消融实验评估性能。

Result: F1-FL模型达到接近完美的性能（AUC 0.99），显著优于放射科医生；All-pairs模型AUC为0.97。主要预测驱动区域为前列腺、膀胱和耻骨联合。模型成功检测到放疗诱导的变化。

Conclusion: 模型能准确预测MR-Linac分次的时间顺序，并在1-几天内检测到放疗诱导的变化，包括前列腺和邻近器官的改变。这突显了MR-Linac在超越图像引导的高级图像分析方面的潜力。

Abstract: Purpose: To investigate whether routinely acquired longitudinal MR-Linac images can be leveraged to characterize treatment-induced changes during radiotherapy, particularly subtle inter-fraction changes over short intervals (average of 2 days). Materials and Methods: This retrospective study included a series of 0.35T MR-Linac images from 761 patients. An artificial intelligence (deep learning) model was used to characterize treatment-induced changes by predicting the temporal order of paired images. The model was first trained with the images from the first and the last fractions (F1-FL), then with all pairs (All-pairs). Model performance was assessed using quantitative metrics (accuracy and AUC), compared to a radiologist's performance, and qualitative analyses - the saliency map evaluation to investigate affected anatomical regions. Input ablation experiments were performed to identify the anatomical regions altered by radiotherapy. The radiologist conducted an additional task on partial images reconstructed by saliency map regions, reporting observations as well. Quantitative image analysis was conducted to investigate the results from the model and the radiologist. Results: The F1-FL model yielded near-perfect performance (AUC of 0.99), significantly outperforming the radiologist. The All-pairs model yielded an AUC of 0.97. This performance reflects therapy-induced changes, supported by the performance correlation to fraction intervals, ablation tests and expert's interpretation. Primary regions driving the predictions were prostate, bladder, and pubic symphysis. Conclusion: The model accurately predicts temporal order of MR-Linac fractions and detects radiation-induced changes over one or a few days, including prostate and adjacent organ alterations confirmed by experts. This underscores MR-Linac's potential for advanced image analysis beyond image guidance.

</details>


### [15] [Personalized White Matter Bundle Segmentation for Early Childhood](https://arxiv.org/abs/2602.05104)
*Elyssa M. McMaster,Michael E. Kim,Nancy R. Newlin,Gaurav Rudravaram,Adam M. Saunders,Aravind R. Krishnan,Jongyeon Yoon,Ji S. Kim,Bryce L. Geeraert,Meaghan V. Perdue,Catherine Lebel,Daniel Moyer,Kurt G. Schilling,Laurie E. Cutting,Bennett A. Landman*

Main category: eess.IV

TL;DR: 提出一种针对儿科白质束分割的深度学习方法，通过改进TractSeg架构，在儿科数据上取得显著提升，并创建了60标签的个体化图谱


<details>
  <summary>Details</summary>
Motivation: 现有白质分割方法缺乏儿科特异性，需要开发专门针对儿科数据的深度学习模型来提升分割准确性，以更好地理解神经发育和儿科白质疾病

Method: 基于TractSeg的2D UNet架构进行改进：调整输入以匹配儿科专家定义的白质束、采用k折交叉验证、使用掩码Dice损失函数，并在56个手动标注的白质束数据集上进行训练

Result: 在所有评估指标（Dice分数、体积重叠、体积过分割）上均显示出统计学显著改进（除一个体积重叠指标外），结合TractSeg和本模型输出创建了60标签的个体化图谱，在TractSeg无法生成解剖学合理输出的情况下仍能产生平滑连续的分割掩码

Conclusion: 提出的儿科特异性深度学习方法显著提升了白质束分割的准确性，能够生成可靠的个体化解剖图谱，有助于在群体水平上理解神经发育，并为儿科白质疾病和障碍提供更精确的解剖估计

Abstract: White matter segmentation methods from diffusion magnetic resonance imaging range from streamline clustering-based approaches to bundle mask delineation, but none have proposed a pediatric-specific approach. We hypothesize that a deep learning model with a similar approach to TractSeg will improve similarity between an algorithm-generated mask and an expert-labeled ground truth. Given a cohort of 56 manually labelled white matter bundles, we take inspiration from TractSeg's 2D UNet architecture, and we modify inputs to match bundle definitions as determined by pediatric experts, evaluation to use k fold cross validation, the loss function to masked Dice loss. We evaluate Dice score, volume overlap, and volume overreach of 16 major regions of interest compared to the expert labeled dataset. To test whether our approach offers statistically significant improvements over TractSeg, we compare Dice voxels, volume overlap, and adjacency voxels with a Wilcoxon signed rank test followed by false discovery rate correction. We find statistical significance across all bundles for all metrics with one exception in volume overlap. After we run TractSeg and our model, we combine their output masks into a 60 label atlas to evaluate if TractSeg and our model combined can generate a robust, individualized atlas, and observe smoothed, continuous masks in cases that TractSeg did not produce an anatomically plausible output. With the improvement of white matter pathway segmentation masks, we can further understand neurodevelopment on a population level scale, and we can produce reliable estimates of individualized anatomy in pediatric white matter diseases and disorders.

</details>


### [16] [Diffusion-aided Extreme Video Compression with Lightweight Semantics Guidance](https://arxiv.org/abs/2602.05201)
*Maojun Zhang,Haotian Wu,Richeng Jin,Deniz Gunduz,Krystian Mikolajczyk*

Main category: eess.IV

TL;DR: 提出基于生成先验的视频压缩框架，通过语义表征和条件扩散模型在极低码率下重建视频


<details>
  <summary>Details</summary>
Motivation: 传统视频编码器和学习方法依赖低层次时空冗余，在极低码率下难以实现语义重建。生成模型（特别是扩散模型）通过高层次语义理解和强大视觉合成能力，为视频压缩提供了新范式。

Method: 1. 压缩视频的高层次语义表征；2. 使用条件扩散模型从语义重建帧；3. 用全局相机轨迹和前景分割表征运动信息：背景运动用相机姿态参数紧凑表示，前景动态用稀疏分割掩码表示。

Result: 显著提升了压缩效率，能够在极低码率下实现良好的视频重建质量

Conclusion: 集成生成先验的视频压缩框架能够大幅降低码率同时保持重建保真度，为极低码率视频压缩提供了有效解决方案

Abstract: Modern video codecs and learning-based approaches struggle for semantic reconstruction at extremely low bit-rates due to reliance on low-level spatiotemporal redundancies. Generative models, especially diffusion models, offer a new paradigm for video compression by leveraging high-level semantic understanding and powerful visual synthesis. This paper propose a video compression framework that integrates generative priors to drastically reduce bit-rate while maintaining reconstruction fidelity. Specifically, our method compresses high-level semantic representations of the video, then uses a conditional diffusion model to reconstruct frames from these semantics. To further improve compression, we characterize motion information with global camera trajectories and foreground segmentation: background motion is compactly represented by camera pose parameters while foreground dynamics by sparse segmentation masks. This allows for significantly boosts compression efficiency, enabling descent video reconstruction at extremely low bit-rates.

</details>


### [17] [Context-Aware Asymmetric Ensembling for Interpretable Retinopathy of Prematurity Screening via Active Query and Vascular Attention](https://arxiv.org/abs/2602.05208)
*Md. Mehedi Hassan,Taufiq Hasan*

Main category: eess.IV

TL;DR: 提出CAA Ensemble模型，通过模拟临床推理的双流架构解决ROP自动筛查难题，在高度不平衡数据集上取得SOTA性能，并提供可解释性可视化。


<details>
  <summary>Details</summary>
Motivation: ROP是儿童可预防性失明的主要原因，但自动筛查面临数据有限和病情复杂的挑战。现有深度学习模型依赖大型私有数据集和被动多模态融合，在小型不平衡公共队列上泛化能力差。

Method: 提出上下文感知非对称集成模型(CAA Ensemble)，包含两个专门化流：1) MS-AQNet作为结构专家，使用临床上下文作为动态查询向量控制视觉特征提取；2) VascuMIL编码血管拓扑图，在门控多实例学习网络中识别血管迂曲。通过协同元学习器集成这些正交信号。

Result: 在188名婴儿(6,004张图像)的高度不平衡队列中，在两项临床任务上取得SOTA性能：广泛ROP分期的宏F1分数0.93，Plus疾病检测的AUC 0.996。模型提供"玻璃盒"透明度，通过反事实注意力热图和血管威胁图展示临床元数据如何指导视觉搜索。

Conclusion: 该研究证明架构归纳偏置可以作为医学AI数据鸿沟的有效桥梁，同时提供可解释性工具支持临床决策。

Abstract: Retinopathy of Prematurity (ROP) is among the major causes of preventable childhood blindness. Automated screening remains challenging, primarily due to limited data availability and the complex condition involving both structural staging and microvascular abnormalities. Current deep learning models depend heavily on large private datasets and passive multimodal fusion, which commonly fail to generalize on small, imbalanced public cohorts. We thus propose the Context-Aware Asymmetric Ensemble Model (CAA Ensemble) that simulates clinical reasoning through two specialized streams. First, the Multi-Scale Active Query Network (MS-AQNet) serves as a structure specialist, utilizing clinical contexts as dynamic query vectors to spatially control visual feature extraction for localization of the fibrovascular ridge. Secondly, VascuMIL encodes Vascular Topology Maps (VMAP) within a gated Multiple Instance Learning (MIL) network to precisely identify vascular tortuosity. A synergistic meta-learner ensembles these orthogonal signals to resolve diagnostic discordance across multiple objectives. Tested on a highly imbalanced cohort of 188 infants (6,004 images), the framework attained State-of-the-Art performance on two distinct clinical tasks: achieving a Macro F1-Score of 0.93 for Broad ROP staging and an AUC of 0.996 for Plus Disease detection. Crucially, the system features `Glass Box' transparency through counterfactual attention heatmaps and vascular threat maps, proving that clinical metadata dictates the model's visual search. Additionally, this study demonstrates that architectural inductive bias can serve as an effective bridge for the medical AI data gap.

</details>


### [18] [Towards Segmenting the Invisible: An End-to-End Registration and Segmentation Framework for Weakly Supervised Tumour Analysis](https://arxiv.org/abs/2602.05453)
*Budhaditya Mukhopadhyay,Chirag Mandal,Pavan Tummala,Naghmeh Mahmoodian,Andreas Nürnberger,Soumick Chatterjee*

Main category: eess.IV

TL;DR: 提出混合配准-分割框架，利用MRI可见的肿瘤信息通过配准生成CT伪标签，但实验显示对于CT中不可见的肿瘤，配准方法效果有限


<details>
  <summary>Details</summary>
Motivation: 解决肝脏肿瘤消融中的临床挑战：肿瘤在术前MRI中清晰可见，但在术中CT中由于病理组织和健康组织对比度低而几乎不可见，需要探索跨模态弱监督方法

Method: 提出混合配准-分割框架，结合MSCGUNet进行跨模态图像配准和UNet分割模块，通过配准辅助生成CT图像的伪标签

Result: 在CHAOS数据集上对健康肝脏解剖结构获得0.72的Dice分数，但在包含肿瘤的临床数据上性能大幅下降至0.16，显示当前配准方法在目标模态缺乏对应视觉特征时的根本局限性

Conclusion: 虽然通过配准进行空间标签传播对于可见结构是可行的，但分割真正不可见的病理仍然是一个开放挑战，配准标签转移无法弥补目标模态中判别特征的缺失

Abstract: Liver tumour ablation presents a significant clinical challenge: whilst tumours are clearly visible on pre-operative MRI, they are often effectively invisible on intra-operative CT due to minimal contrast between pathological and healthy tissue. This work investigates the feasibility of cross-modality weak supervision for scenarios where pathology is visible in one modality (MRI) but absent in another (CT). We present a hybrid registration-segmentation framework that combines MSCGUNet for inter-modal image registration with a UNet-based segmentation module, enabling registration-assisted pseudo-label generation for CT images. Our evaluation on the CHAOS dataset demonstrates that the pipeline can successfully register and segment healthy liver anatomy, achieving a Dice score of 0.72. However, when applied to clinical data containing tumours, performance degrades substantially (Dice score of 0.16), revealing the fundamental limitations of current registration methods when the target pathology lacks corresponding visual features in the target modality. We analyse the "domain gap" and "feature absence" problems, demonstrating that whilst spatial propagation of labels via registration is feasible for visible structures, segmenting truly invisible pathology remains an open challenge. Our findings highlight that registration-based label transfer cannot compensate for the absence of discriminative features in the target modality, providing important insights for future research in cross-modality medical image analysis. Code an weights are available at: https://github.com/BudhaTronix/Weakly-Supervised-Tumour-Detection

</details>


### [19] [Disc-Centric Contrastive Learning for Lumbar Spine Severity Grading](https://arxiv.org/abs/2602.05738)
*Sajjan Acharya,Pralisha Kansakar*

Main category: eess.IV

TL;DR: 提出基于椎间盘中心的腰椎管狭窄症自动分级方法，结合对比预训练和椎间盘级微调，在矢状位T2加权MRI上实现78.1%的平衡准确率


<details>
  <summary>Details</summary>
Motivation: 传统腰椎管狭窄症严重程度评估依赖人工判读，存在主观性和效率问题。需要开发自动化的分级方法，特别是针对椎间盘层面的特征分析，以提高诊断准确性和临床实用性。

Method: 采用椎间盘中心的方法，结合对比预训练和椎间盘级微调。使用每个椎间盘的单一解剖定位感兴趣区域，通过对比学习使模型关注有意义的椎间盘特征，减少对无关图像外观差异的敏感性。框架包含椎间盘定位的辅助回归任务，并应用加权焦点损失处理类别不平衡。

Result: 实验显示达到78.1%的平衡准确率，严重到正常的误分类率降至2.13%，相比从头开始的监督训练有明显改善。虽然检测中度严重程度的椎间盘仍有挑战，但椎间盘级特征聚焦为腰椎管狭窄评估提供了实用方法。

Conclusion: 基于椎间盘中心的对比预训练和微调方法能有效自动评估腰椎管狭窄严重程度，显著降低严重误分类率，为临床提供实用的椎间盘级评估工具，尽管中度严重程度的检测仍需改进。

Abstract: This work examines a disc-centric approach for automated severity grading of lumbar spinal stenosis from sagittal T2-weighted MRI. The method combines contrastive pretraining with disc-level fine-tuning, using a single anatomically localized region of interest per intervertebral disc. Contrastive learning is employed to help the model focus on meaningful disc features and reduce sensitivity to irrelevant differences in image appearance. The framework includes an auxiliary regression task for disc localization and applies weighted focal loss to address class imbalance. Experiments demonstrate a 78.1% balanced accuracy and a reduced severe-to-normal misclassification rate of 2.13% compared with supervised training from scratch. Detecting discs with moderate severity can still be challenging, but focusing on disc-level features provides a practical way to assess the lumbar spinal stenosis.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Correcting Contextual Deletions in DNA Nanopore Readouts](https://arxiv.org/abs/2602.05072)
*Yuan-Pon Chen,Olgica Milenkovic,João Ribeiro,Jin Sima*

Main category: cs.IT

TL;DR: 研究DNA存储中纳米孔测序器产生的上下文相关删除错误，针对两种运行长度阈值k的模型：对数增长和常数阈值，分别设计纠删码并分析最优冗余度。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储系统中，纳米孔测序器产生的删除错误并非独立随机，而是与序列上下文相关。现有模型假设删除独立且与上下文无关，不符合实际纳米孔错误模式，需要研究上下文相关的删除错误模型。

Method: 研究确定性单删除跟随长度至少k的完整游程的模型。分两种k值情况：1) k=C log n（常数C∈(0,1)），研究能纠正t个上下文删除的纠删码，分析最小冗余度；2) k为常数，考虑极端上下文删除信道，每个长度≥k的游程后都发生删除，分析最大可达速率。

Result: 对于k=C log n情况，最小冗余度（忽略低阶项）在(1-C)t log n和2(1-C)t log n之间，是任意t-删除纠正码冗余度的(1-C)倍。构造了高效可编解码的码，当t=1且C>1/2时，冗余度基本匹配非构造性上界。对于常数k的极端信道，获得了任意常数k下最大可达速率的尖锐界限。

Conclusion: 上下文相关的删除错误模型更符合纳米孔测序实际，相比独立删除模型，上下文感知的纠删码可以显著降低冗余度。研究为DNA数据存储系统提供了更高效的编码方案，针对不同错误模式给出了理论界限和构造性编码方法。

Abstract: The problem of designing codes for deletion-correction and synchronization has received renewed interest due to applications in DNA-based data storage systems that use nanopore sequencers as readout platforms. In almost all instances, deletions are assumed to be imposed independently of each other and of the sequence context. These assumptions are not valid in practice, since nanopore errors tend to occur within specific contexts. We study contextual nanopore deletion-errors through the example setting of deterministic single deletions following (complete) runlengths of length at least $k$. The model critically depends on the runlength threshold $k$, and we examine two regimes for $k$: a) $k=C\log n$ for a constant $C\in(0,1)$; in this case, we study error-correcting codes that can protect from a constant number $t$ of contextual deletions, and show that the minimum redundancy (ignoring lower-order terms) is between $(1-C)t\log n$ and $2(1-C)t\log n$, meaning that it is a ($1-C$)-fraction of that of arbitrary $t$-deletion-correcting codes. To complement our non-constructive redundancy upper bound, we design efficiently and encodable and decodable codes for any constant $t$. In particular, for $t=1$ and $C>1/2$ we construct efficient codes with redundancy that essentially matches our non-constructive upper bound; b) $k$ equal a constant; in this case we consider the extremal problem where the number of deletions is not bounded and a deletion is imposed after every run of length at least $k$, which we call the extremal contextual deletion channel. This combinatorial setting arises naturally by considering a probabilistic channel that introduces contextual deletions after each run of length at least $k$ with probability $p$ and taking the limit $p\to 1$. We obtain sharp bounds on the maximum achievable rate under the extremal contextual deletion channel for arbitrary constant $k$.

</details>


### [21] [On QC and GQC algebraic geometry codes](https://arxiv.org/abs/2602.05097)
*Matteo Bonini,Arianna Dionigi,Francesco Ghiandoni*

Main category: cs.IT

TL;DR: 提出从代数曲线构造准循环码和广义准循环码的新方法，适用于Kummer扩张曲线，包括超椭圆曲线、范迹曲线和Hermitian曲线，可获得灵活共指数的准循环码。


<details>
  <summary>Details</summary>
Motivation: 传统基于椭圆曲线的准循环码构造方法有限，需要更通用的构造框架来获得具有灵活参数的准循环码和广义准循环码。

Method: 利用代数曲线理论，特别是有理函数域的Kummer扩张曲线（包括超椭圆曲线、范迹曲线和Hermitian曲线），通过已知的自同构群分类推导准循环码的构造。

Result: 获得了新的准循环码和广义准循环码构造方法，能够产生具有灵活共指数的准循环码，并推导了明确的参数公式。

Conclusion: 该方法扩展了准循环码的构造范围，为从更广泛的代数曲线类构造具有灵活参数的准循环码提供了通用框架。

Abstract: We present new constructions of quasi-cyclic (QC) and generalized quasi-cyclic (GQC) codes from algebraic curves. Unlike previous approaches based on elliptic curves, our method applies to curves that are Kummer extensions of the rational function field, including hyperelliptic, norm-trace, and Hermitian curves. This allows QC codes with flexible co-index. Explicit parameter formulas are derived using known automorphism-group classifications.

</details>


### [22] [Enabling Large-Scale Channel Sounding for 6G: A Framework for Sparse Sampling and Multipath Component Extraction](https://arxiv.org/abs/2602.05405)
*Yi Chen,Li Ming,Chong Han*

Main category: cs.IT

TL;DR: 提出稀疏非均匀采样框架与LR-SAGE算法，实现50倍测量加速、98%数据量减少和99.96%计算复杂度降低，为6G AI-native系统构建大规模ISAC数据集


<details>
  <summary>Details</summary>
Motivation: 实现6G人工智能与集成感知通信需要大规模真实信道数据集，但传统频域信道探测方法因避免时延模糊需要大量频点导致效率低下

Method: 提出抛物线频率采样(PFS)策略进行非均匀频点分布，结合似然修正空间交替广义期望最大化(LR-SAGE)算法提取多径分量，修正非均匀采样和分子吸收效应导致的似然失真

Result: 在280-300GHz频段验证，实现50倍测量加速、98%数据量减少、99.96%后处理计算复杂度降低，同时准确捕获与传统详尽测量一致的多径分量和信道特性

Conclusion: 该框架为构建AI-native 6G系统所需的大规模ISAC数据集提供了基础使能技术，能够充分挖掘AI缩放定律的潜力

Abstract: Realizing the 6G vision of artificial intelligence (AI) and integrated sensing and communication (ISAC) critically requires large-scale real-world channel datasets for channel modeling and data-driven AI models. However, traditional frequency-domain channel sounding methods suffer from low efficiency due to a prohibitive number of frequency points to avoid delay ambiguity. This paper proposes a novel channel sounding framework involving sparse nonuniform sampling along with a likelihood-rectified space-alternating generalized expectation-maximization (LR-SAGE) algorithm for multipath component extraction. This framework enables the acquisition of channel datasets that are tens or even hundreds of times larger within the same channel measurement duration, thereby providing the massive data required to harness the full potential of AI scaling laws. Specifically, we propose a Parabolic Frequency Sampling (PFS) strategy that non-uniformly distributes frequency points, effectively eliminating delay ambiguity while reducing sampling overhead by orders of magnitude. To efficiently extract multipath components (MPCs) from the channel data measured by PFS, we develop a LR-SAGE algorithm, rectifying the likelihood distortion caused by nonuniform sampling and molecular absorption effect. Simulation results and experimental validation at 280--300~GHz confirm that the proposed PFS and LR-SAGE algorithm not only achieve 50$\times$ faster measurement, a 98\% reduction in data volume and a 99.96\% reduction in post-processing computational complexity, but also successfully captures MPCs and channel characteristics consistent with traditional exhaustive measurements, demonstrating its potential as a fundamental enabler for constructing the massive ISAC datasets required by AI-native 6G systems.

</details>


### [23] [Explicit List-Decodable Linearized Reed-Solomon Subspace Codes via Subspace Designs](https://arxiv.org/abs/2602.05462)
*Kuo Shang,Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文构造了在任意域上的线性化和秩度量码，实现了高效的列表解码，解码半径可达错误率ρ，码率为1-ρ-ε，并扩展到折叠线性化Reed-Solomon码。


<details>
  <summary>Details</summary>
Motivation: 和秩度量在多种编码场景中有重要应用，但现有构造缺乏高效的列表解码方案。本文旨在填补这一空白，构建可高效列表解码的和秩度量码。

Method: 构造线性化Reed-Solomon码的子码，通过子空间设计限制消息多项式在特定子空间上。解码框架基于线性代数，将解码问题转化为低维结构化仿射子空间的求解。

Result: 成功构造了可高效列表解码的和秩度量码，解码列表大小受h^poly(1/ε)限制。首次实现了折叠线性化Reed-Solomon码在唯一解码半径之外的列表解码。

Conclusion: 本文提供了构建可高效解码的和秩度量码的新框架，首次实现了正码率码在唯一解码半径之外的列表解码，为和秩度量编码开辟了新方向。

Abstract: The sum-rank metric is the mixture of the Hamming and rank metrics. The sum-rank metric found its application in network coding, locally repairable codes, space-time coding, and quantum-resistant cryptography. Linearized Reed-Solomon (LRS) codes are the sum-rank analogue of Reed-Solomon codes and strictly generalize both Reed-Solomon and Gabidulin codes.
  In this work, we construct an explicit family of $\mathbb{F}_h$-linear sum-rank metric codes over arbitrary fields $\mathbb{F}_h$. Our construction enables efficient list decoding up to a fraction $ρ$ of errors in the sum-rank metric with rate $1-ρ-\varepsilon$, for any desired $ρ\in (0,1)$ and $\varepsilon>0$. Our codes are subcodes of LRS codes, obtained by restricting message polynomials to an $\mathbb{F}_h$-subspace derived from subspace designs, and the decoding list size is bounded by $h^{\mathrm{poly}(1/\varepsilon)}$.
  Beyond the standard LRS setting, we further extend our linear-algebraic decoding framework to folded Linearized Reed-Solomon (FLRS) codes. We show that folded evaluations satisfy appropriate interpolation conditions and that the corresponding solution space forms a low-dimensional, structured affine subspace. This structure enables effective control of the list size and yields the first explicit positive-rate FLRS subcodes that are efficiently list decodable beyond the unique-decoding radius. To the best of our knowledge, this also constitutes the first explicit construction of positive-rate sum-rank metric codes that admit efficient list decoding beyond the unique decoding radius, thereby providing a new general framework for constructing efficiently decodable codes under the sum-rank metric.

</details>


### [24] [Low-complexity Design for Beam Coverage in Near-field and Far-field: A Fourier Transform Approach](https://arxiv.org/abs/2602.05666)
*Chao Zhou,Changsheng You,Cong Zhou,Li Chen,Yi Gong,Chengwen Xing*

Main category: cs.IT

TL;DR: 提出基于傅里叶变换的低复杂度波束覆盖设计方法，适用于远场和近场多天线系统，在保证性能的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的波束覆盖优化方法计算复杂度高，需要设计一种低复杂度且高效的波束覆盖设计方法。

Method: 1) 将远场波束覆盖视为空间频率滤波问题，通过天线域权重整形实现角度覆盖；2) 提出考虑有限天线数滚降效应的保护性缩放设计；3) 扩展到近场情况，利用一阶泰勒近似将2D波束覆盖设计转化为2D逆傅里叶变换。

Result: 提出的FT-based方法在实现与传统采样优化方法相当的波束成形性能的同时，显著降低了计算复杂度。在近场情况下观察到固有的范围散焦效应。

Conclusion: 基于傅里叶变换的波束覆盖设计为多天线系统提供了一种高效低复杂度的解决方案，适用于远场和近场场景，具有实际应用价值。

Abstract: In this paper, we study efficient beam coverage design for multi-antenna systems in both far-field and near-field cases. To reduce the computational complexity of existing sampling-based optimization methods, we propose a new low-complexity yet efficient beam coverage design. To this end, we first formulate a general beam coverage optimization problem to maximize the worst-case beamforming gain over a target region. For the far-field case, we show that the beam coverage design can be viewed as a spatial-frequency filtering problem, where angular coverage can be achieved by weight-shaping in the antenna domain via an inverse FT, yielding an infinite-length weighting sequence. Under the constraint of a finite number of antennas, a surrogate scheme is proposed by directly truncating this sequence, which inevitably introduces a roll-off effect at the angular boundaries, yielding degraded worst-case beamforming gain. To address this issue, we characterize the finite-antenna-induced roll-off effect, based on which a roll-off-aware design with a protective zoom is developed to ensure a flat beamforming-gain profile within the target angular region. Next, we extend the proposed method to the near-field case. Specifically, by applying a first-order Taylor approximation to the near-field channel steering vector (CSV), the two-dimensional (2D) beam coverage design (in both angle and inverse-range) can be transformed into a 2D inverse FT, leading to a low-complexity beamforming design. Furthermore, an inherent near-field range defocusing effect is observed, indicating that sufficiently wide angular coverage results in range-insensitive beam steering. Finally, numerical results demonstrate that the proposed FT-based approach achieves a comparable worst-case beamforming performance with that of conventional sampling-based optimization methods while significantly reducing the computational complexity.

</details>


### [25] [Generalized Pinsker Inequality for Bregman Divergences of Negative Tsallis Entropies](https://arxiv.org/abs/2602.05744)
*Guglielmo Beretta,Tommaso Cesari,Roberto Colomboni*

Main category: cs.IT

TL;DR: 该论文针对Tsallis熵生成的Bregman散度（即β-散度），建立了广义Pinsker不等式，确定了最优常数C_{α,K}。


<details>
  <summary>Details</summary>
Motivation: 传统Pinsker不等式将KL散度下界与总变差关联，但在使用Tsallis损失的概率预测和在线学习等应用中，需要针对更一般的Bregman散度建立类似的不等式。

Method: 针对由负α-Tsallis熵生成的Bregman散度D_α（即β-散度），在概率单纯形Δ^K的相对内部对任意概率分布p,q，建立形如D_α(p∥q) ≥ (C_{α,K}/2)·∥p-q∥₁²的不等式。

Result: 证明了该广义Pinsker不等式的尖锐性，并显式确定了所有(α,K)组合下的最优常数C_{α,K}。

Conclusion: 该工作将经典Pinsker不等式推广到更一般的Bregman散度，为使用Tsallis损失的机器学习应用提供了理论工具。

Abstract: The Pinsker inequality lower bounds the Kullback--Leibler divergence $D_{\textrm{KL}}$ in terms of total variation and provides a canonical way to convert $D_{\textrm{KL}}$ control into $\lVert \cdot \rVert_1$-control. Motivated by applications to probabilistic prediction with Tsallis losses and online learning, we establish a generalized Pinsker inequality for the Bregman divergences $D_α$ generated by the negative $α$-Tsallis entropies -- also known as $β$-divergences. Specifically, for any $p$, $q$ in the relative interior of the probability simplex $Δ^K$, we prove the sharp bound \[
  D_α(p\Vert q) \ge \frac{C_{α,K}}{2}\cdot \|p-q\|_1^2, \] and we determine the optimal constant $C_{α,K}$ explicitly for every choice of $(α,K)$.

</details>


### [26] [MU-MIMO Uplink Timely Throughput Maximization for Extended Reality Applications](https://arxiv.org/abs/2602.05751)
*Ravi Sharan Bhagavathula,Pavan Koteshwar Srinath,Alvaro Valcarce Rial,Baltasar-Beferull Lozano*

Main category: cs.IT

TL;DR: 提出一种基于加权比例公平的启发式算法，通过上行MU-MIMO调度最大化XR应用的跨层及时吞吐量


<details>
  <summary>Details</summary>
Motivation: 研究扩展现实(XR)应用的跨层及时吞吐量最大化问题，XR应用对时延敏感，需要保证信息的新鲜度

Method: 使用PAoI度量表征及时调度机会，将其作为用户满意度约束纳入网络侧优化问题；针对NP难问题，提出无信令的加权比例公平迭代启发式算法，权重根据PAoI度量推导

Result: 广泛的数值仿真结果表明，所提算法在不牺牲整体系统吞吐量的情况下，在XR容量方面持续优于现有基线方法

Conclusion: 通过PAoI约束和加权比例公平调度，能够有效提升XR应用的及时吞吐量性能，为XR服务提供更好的QoS保障

Abstract: In this work, we study the cross-layer timely throughput maximization for extended reality (XR) applications through uplink multi-user MIMO (MU-MIMO) scheduling. Timely scheduling opportunities are characterized by the peak age of information (PAoI)-metric and are incorporated into a network-side optimization problem as constraints modeling user satisfaction. The problem being NP-hard, we resort to a signaling-free, weighted proportional fair-based iterative heuristic algorithm, where the weights are derived with respect to the PAoI metric. Extensive numerical simulation results demonstrate that the proposed algorithm consistently outperforms existing baselines in terms of XR capacity without sacrificing the overall system throughput.

</details>


### [27] [Price of universality in vector quantization is at most 0.11 bit](https://arxiv.org/abs/2602.05790)
*Alina Harbuzova,Or Ordentlich,Yury Polyanskiy*

Main category: cs.IT

TL;DR: 论文证明存在一个通用码本，对于所有可能的X统计特性都近似最优，性能仅比针对特定X统计特性优化的水填充码本低0.11比特/维度。


<details>
  <summary>Details</summary>
Motivation: 在LLM部署中，使用低精度近似Ŵ代替真实W（仅权重量化）可提高效率。理论上最优的量化算法依赖于X的统计特性，需要将矢量量化码本与X的PCA方向对齐（水填充分配），但这在实际中难以实现。

Method: 通过理论证明存在一个通用码本，该码本对所有可能的X统计特性都近似最优。等价地，证明了在ℝⁿ中存在一个网络，能同时以所有希尔伯特范数近似最优地覆盖球面。

Result: 存在一个通用码本，其性能至少与针对特定X统计特性优化的水填充码本相当，但码率仅降低0.11比特/维度。该码本可作为低精度存储格式的理想候选。

Conclusion: 虽然存在性证明是非构造性的，但该结果表明存在一个对所有统计特性都近似最优的通用量化码本，为低精度存储格式设计提供了理论基础。

Abstract: Fast computation of a matrix product $W^\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\widehat W$ in place of true $W$ ("weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as "waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.
  Equivalently, our result shows existence of a net in $\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.

</details>
