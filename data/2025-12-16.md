<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 15]
- [eess.IV](#eess.IV) [Total: 4]
- [eess.SP](#eess.SP) [Total: 22]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Differentially Private Community Detection in $h$-uniform Hypergraphs](https://arxiv.org/abs/2512.12031)
*Javad Zahedi Moghaddam,Aria Nosratinia*

Main category: cs.IT

TL;DR: 本文研究了在保护h-均匀超图连接隐私条件下的精确恢复阈值，分析了三种差分隐私机制在密集h-均匀随机块模型中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究在保护超图连接隐私的前提下，社区检测的精确恢复能力。随着超图数据在社交网络、生物信息学等领域的广泛应用，如何在差分隐私约束下进行有效的社区结构分析成为一个重要问题。

Method: 采用h-均匀随机块模型(h-HSBM)作为观测模型，研究三种差分隐私机制：基于稳定性的机制、基于采样的机制和基于扰动的机制。通过理论分析计算每种机制的精确恢复阈值，并研究隐私预算(ε, δ)对恢复区域的影响。

Result: 基于采样的机制和随机响应机制能保证纯ε-超边差分隐私(δ=0)，而基于稳定性的机制无法达到这一隐私水平。隐私预算的最小值对于基于稳定性和贝叶斯采样的机制与超图参数的对数相关，而对于随机响应机制仅依赖于超图大小。

Conclusion: 在超图社区检测中，不同的差分隐私机制对精确恢复能力有不同的影响。隐私预算的需求与超图结构参数密切相关，为实际应用中在隐私保护和恢复精度之间的权衡提供了理论指导。

Abstract: This paper studies the exact recovery threshold subject to preserving the privacy of connections in $h$-uniform hypergraphs. Privacy is characterized by the $(ε, δ)$-hyperedge differential privacy (DP), an extension of the notion of $(ε, δ)$-edge DP in the literature. The hypergraph observations are modeled through a $h$-uniform stochastic block model ($h$-HSBM) in the dense regime. We investigate three differentially private mechanisms: stability-based, sampling-based, and perturbation-based mechanisms. We calculate the exact recovery threshold for each mechanism and study the contraction of the exact recovery region due to the privacy budget, $(ε, δ)$. Sampling-based mechanisms and randomized response mechanisms guarantee pure $ε$-hyperedge DP where $δ=0$, while the stability-based mechanisms cannot achieve this level of privacy. The dependence of the limits of the privacy budget on the parameters of the $h$-uniform hypergraph is studied. More precisely, it is proven rigorously that the minimum privacy budget scales logarithmically with the ratio between the density of in-cluster hyperedges and the cross-cluster hyperedges for stability-based and Bayesian sampling-based mechanisms, while this budget depends only on the size of the hypergraph for the randomized response mechanism.

</details>


### [2] [A Framework for Scalable Digital Twin Deployment in Smart Campus Building Facility Management](https://arxiv.org/abs/2512.12149)
*Thyda Siv*

Main category: cs.IT

TL;DR: 本研究提出一个可扩展的数字孪生框架，整合3D激光扫描、BIM建模和物联网数据可视化，用于校园建筑设施管理，并在Georgia Tech的Price Gilbert大楼进行案例验证。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生研究往往局限于孤立领域（如点云几何或能源分析），缺乏将建筑几何、设备元数据和运营数据整合到统一设施管理平台的可扩展、可互操作工作流程。

Method: 方法包括：(1) 地面激光扫描和结构化点云处理；(2) 开发包含建筑、机械、电气、管道、输送和传感器系统的丰富BIM模型；(3) 在数字孪生管理平台中创建连接设备元数据、维护策略和模拟物联网数据的数字孪生环境。

Result: 在Georgia Tech的Price Gilbert大楼案例中，成功建模了509个设备项目并嵌入OmniClass分类，开发了10个交互式仪表板可视化系统性能。框架实现了集中资产文档管理、改进系统可见性，并增强了预防性和反应性维护工作流程。

Conclusion: 尽管大多数物联网数据因现有传感器基础设施有限而采用模拟数据，但原型验证了可扩展数字孪生用于设施管理的可行性，并为实时监控、分析集成和未来自主建筑运营建立了参考模型。

Abstract: Digital twin (DT) offers significant opportunities for enhancing facility management (FM) in campus environments. However, existing research often focuses narrowly on isolated domains, such as point-cloud geometry or energy analytics, without providing a scalable and interoperable workflow that integrates building geometry, equipment metadata, and operational data into a unified FM platform. This study proposes a comprehensive framework for scalable digital-twin deployment in smart campus buildings by integrating 3D laser scanning, BIM modeling, and IoT-enabled data visualization to support facility operations and maintenance. The methodology includes: (1) reality capture using terrestrial laser scanning and structured point-cloud processing; (2) development of an enriched BIM model incorporating architectural, mechanical, electrical, plumbing, conveying, and sensor systems; and (3) creation of a digital-twin environment that links equipment metadata, maintenance policies, and simulated IoT data within a digital-twin management platform. A case study of the Price Gilbert Building at Georgia Tech demonstrates the implementation of this workflow. A total of 509 equipment items were modeled and embedded with OmniClass classifications into the digital twin. Ten interactive dashboards were developed to visualize system performance. Results show that the proposed framework enables centralized asset documentation, improved system visibility, and enhanced preventive and reactive maintenance workflows. Although most IoT data were simulated due to limited existing sensor infrastructure, the prototype validates the feasibility of a scalable digital twin for facility management and establishes a reference model for real-time monitoring, analytics integration, and future autonomous building operations.

</details>


### [3] [Large and Small Model Collaboration for Air Interface](https://arxiv.org/abs/2512.12170)
*Yiming Cui,Jiajia Guo,Xiao Li,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: 提出LASCO和E-LASCO框架，通过大模型作为通用知识库、小模型作为轻量插件，实现无线通信中CSI反馈任务的环境特定自适应，降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要依赖大模型的跨环境通用知识，忽视了环境特定自适应的潜在增益。直接微调大模型存在训练成本高、多用户场景推理效率低、灾难性遗忘风险以及模型参数访问受限等问题。

Method: 建立协作框架：大模型作为通用信道知识库，小模型作为轻量插件捕获环境特定知识。具体实例化为LASCO框架：大模型生成初始CSI重建，通过参考SAM和代理SAM学习环境引起的重建偏移，并将偏移传回大模型。进一步提出E-LASCO，引入可学习的协作系数来控制不同环境中大模型和小模型的贡献。

Result: 数值结果表明，LASCO和E-LASCO使大模型能够以显著降低的训练成本、更少的数据收集需求和更快的自适应速度，实现环境特定的性能增益。

Conclusion: 提出的协作框架有效解决了大模型环境自适应中的实际问题，通过大小模型协作实现了高效的环境特定适应，为无线通信中的AI模型应用提供了新思路。

Abstract: Large artificial intelligence models (LAMs) have shown strong capability in wireless communications, yet existing works mainly rely on their generalized knowledge across environments while overlooking the potential gains of environment-specific adaptation. Directly fine-tuning LAMs for adaptation is often impractical due to prohibitive training costs, low inference efficiency in multi-user scenarios, and the risk of catastrophic forgetting, in addition to the limited accessibility of model parameters. To address these limitations, we establish a collaborative framework for air interface. In this framework, unlike prior approaches that either depend solely on LAMs or require direct fine-tuning, LAMs are exploited as a universal channel knowledge base while small artificial intelligence models (SAMs) are employed as lightweight plugins to capture environment-specific knowledge, facilitating efficient environment-specific adaptation of LAMs. Subsequently, we instantiate this framework for CSI feedback tasks, and develop a large and small collaboration framework for CSI feedback, referred to as LASCO. LASCO operates by letting the base LAM produce an initial CSI reconstruction, learning the environment-induced reconstruction shift through a reference SAM and a proxy SAM, and transferring this shift back to the LAM. To further enhance adaptability, we introduce elastic-LASCO (E-LASCO), which augments LASCO with learnable collaboration coefficients that control the contribution of LAMs and SAMs across different environments. Numerical results demonstrate that LASCO and E-LASCO enables LAMs to achieve environment-specific performance gains with significantly reduced training costs, lower data collection requirements, and faster adaptation speed.

</details>


### [4] [Hulls of Free Linear Codes over a Non-Unital Ring](https://arxiv.org/abs/2512.12335)
*Anup Kushwaha,Om Prakash*

Main category: cs.IT

TL;DR: 该论文研究了非幺环E上自由线性码的壳码，包括壳码的生成矩阵、构造方法、置换等价性以及最优码分类。


<details>
  <summary>Details</summary>
Motivation: 研究非幺环E上自由线性码的壳码性质，为构造具有特定壳秩的码提供理论支持，并应用于最优码分类。

Method: 1. 分析E-线性码各种壳的剩余码和挠码；2. 推导自由E-线性码壳的生成矩阵显式形式；3. 提出四种构造方法从较小长度和壳秩的码构造更大码；4. 研究自由E-线性码的置换等价性和壳变化问题。

Result: 1. 获得了自由E-线性码壳的生成矩阵显式形式；2. 提出了有效的构造方法；3. 给出了长度不超过8的最优自由E-线性码的完整分类。

Conclusion: 该研究为理解非幺环上码的壳结构提供了系统框架，提出的构造方法可用于生成具有特定壳性质的码，最优码分类结果具有理论和应用价值。

Abstract: This paper investigates the hull codes of free linear codes over a non-unital ring $ E= \langle κ,τ\mid 2 κ=2 τ=0,~ κ^2=κ,~ τ^2=τ,~ κτ=κ,~ τκ=τ\rangle$. Initially, we examine the residue and torsion codes of various hulls of $E$-linear codes and obtain an explicit form of the generator matrix of the hull of a free $E$-linear code. Then, we propose four build-up construction methods to construct codes with a larger length and hull-rank from codes with a smaller length and hull-rank. Some illustrative examples are also given to support our build-up construction methods. Subsequently, we study the permutation equivalence of two free $E$-linear codes and discuss the hull-variation problem. As an application, we classify optimal free $E$-linear codes for lengths up to $8$.

</details>


### [5] [ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems](https://arxiv.org/abs/2512.12366)
*Babak Badnava,Jacob Chakareski,Morteza Hashemi*

Main category: cs.IT

TL;DR: ElasticVR框架通过可扩展的360度视频分块和边缘-客户端无线多连接架构，实现VR计算任务的弹性卸载，采用多智能体深度强化学习优化QoE和能耗


<details>
  <summary>Details</summary>
Motivation: 高保真360度视频流媒体VR应用需要大量计算和数据速率，现有系统缺乏根据用户和系统资源动态调整计算和数据速率的能力

Method: 集成可扩展360度视频分块到边缘-客户端无线多连接架构，提出两种多智能体深度强化学习方案：CPPG（集中训练集中执行）和IPPG（集中训练分散执行）

Result: ElasticVR框架相比无弹性VR计算，PSNR提升43.21%，响应时间降低42.35%，能耗降低56.83%

Conclusion: ElasticVR框架通过弹性VR计算任务卸载和多智能体深度强化学习，有效平衡通信、计算、能耗和QoE之间的权衡，显著提升VR性能

Abstract: Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.

</details>


### [6] [Linear Codes with Certain Dimension of Hermitian Hulls](https://arxiv.org/abs/2512.12519)
*Jiabin Wang,Jinquan Luo*

Main category: cs.IT

TL;DR: 研究有限域F_q²上酉空间中Hermitian ℓ-互补码的计数公式和渐近性质，发现Hermitian自正交码与无限制码在渐近重量分布上相似，并证明当字母表大小趋于无穷时，Hermitian自正交码类中的MDS码是渐近稠密的。


<details>
  <summary>Details</summary>
Motivation: 研究Hermitian ℓ-互补码的计数和渐近性质，探索Hermitian自正交码与无限制码在渐近行为上的关系，特别关注最小距离约束下的渐近特性。

Method: 提供Hermitian ℓ-互补码计数公式的闭式表达式，分析Hermitian自正交码与无限制码的渐近重量分布相似性，研究最小距离至少为d的Hermitian自正交码的渐近行为。

Result: 得到了Hermitian ℓ-互补码的闭式计数公式，发现了Hermitian自正交码与无限制码在渐近重量分布上的相似性，证明了当字母表大小趋于无穷时，Hermitian自正交码类中的MDS码是渐近稠密的。

Conclusion: Hermitian ℓ-互补码具有可计算的计数公式，Hermitian自正交码在渐近意义上与无限制码相似，且在字母表足够大时，Hermitian自正交码类中存在大量MDS码。

Abstract: In this paper, we study the enumerative and asymptotic properties related to Hermitian $\ell$-complementary codes on the unitary space over $\F_{q^2}$. We provide some closed form expressions for the counting formulas of Hermitian $\ell$-complementary codes. There is a similarity in the asymptotic weight distribution between Hermitian self-orthogonal codes and unrestricted codes. Furthermore, we study the asymptotic behavior of Hermitian self-orthogonal codes whose minimum distance is at least $d$. In particular, we conclude that MDS codes within the class of Hermitian self-orthogonal codes are asymptotically dense when the alphabet size approaches to infinity.

</details>


### [7] [Vertical Heterogeneous Networks Beyond 5G: CoMP Coverage Enhancement and Optimization](https://arxiv.org/abs/2512.12563)
*Tian Shi,Wenkun Wen,Peiran Wu,Minghua Xia*

Main category: cs.IT

TL;DR: 论文提出了一种在垂直异构网络中利用无人机作为空中基站，通过协调多点传输框架提升稀疏空中用户下行覆盖性能的方法，包括随机部署和优化部署两种策略。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络对低空经济至关重要，但在动态三维空间中为稀疏分布的空中用户提供可靠连接仍面临重大挑战。现有网络难以应对非均匀用户分布和高移动性环境。

Method: 提出协调多点传输框架，使空中基站和地面基站能够联合传输。考虑两种无人机部署策略：1）随机部署，使用随机几何分析推导闭式覆盖表达式；2）优化部署，采用覆盖感知加权K-means聚类算法最大化服务不足区域的协作覆盖。

Result: 理论分析和蒙特卡洛模拟表明，提出的CoMP-enabled VHetNet显著提高了下行覆盖概率，特别是在稀疏空中用户场景中。优化部署策略相比随机部署能更好地提升覆盖性能。

Conclusion: 智能无人机协调和几何感知部署能够为低空无线网络提供稳健、自适应的连接，展示了在5G+垂直异构网络中利用无人机增强覆盖的潜力。

Abstract: Low-altitude wireless networks are increasingly vital for the low-altitude economy, enabling wireless coverage in high-mobility and hard-to-reach environments. However, providing reliable connectivity to sparsely distributed aerial users in dynamic three-dimensional (3D) spaces remains a significant challenge. This paper investigates downlink coverage enhancement in vertical heterogeneous networks (VHetNets) beyond 5G, where uncrewed aerial vehicles (UAVs) operate as emerging aerial base stations (ABSs) alongside legacy terrestrial base stations (TBSs). To improve coverage performance, we propose a coordinated multi-point (CoMP) transmission framework that enables joint transmission from ABSs and TBSs. This approach mitigates the limitations of non-uniform user distributions and enhances reliability for sparse aerial users. Two UAV deployment strategies are considered: \textit{i)} random UAV placement, analyzed using stochastic geometry to derive closed-form coverage expressions, and \textit{ii)} optimized UAV placement using a coverage-aware weighted $K$-means clustering algorithm to maximize cooperative coverage in underserved areas. Theoretical analyses and Monte Carlo simulations demonstrate that the proposed CoMP-enabled VHetNet significantly improves downlink coverage probability, particularly in scenarios with sparse aerial users. These findings highlight the potential of intelligent UAV coordination and geometry-aware deployment to enable robust, adaptive connectivity in low-altitude wireless networks.

</details>


### [8] [Linear Binary Codes Correcting One or More Errors](https://arxiv.org/abs/2512.12591)
*Timofei Izhitskii*

Main category: cs.IT

TL;DR: 本文研究线性二进制码的纠错能力，针对单纠错情况给出了达到汉明界的构造方法及最小码长精确表达式，对一般情况通过陪集结构分析得到参数下界。


<details>
  <summary>Details</summary>
Motivation: 研究线性二进制码的纠错能力，特别是如何构造最优的纠错码并确定其参数界限，这对于可靠数据传输和存储具有重要意义。

Method: 对于单纠错情况，采用构造性方法达到汉明界并推导最小码长精确表达式；对于一般情况，通过分析线性码的陪集结构来推导参数下界。

Result: 证明了单纠错线性二进制码可以达到汉明界，并给出了最小码长的精确表达式；对于一般情况，通过陪集分析得到了线性码参数的简单下界。

Conclusion: 本文为线性二进制纠错码提供了理论分析框架，对单纠错情况给出了最优构造，对一般情况提供了参数下界，有助于设计更高效的纠错码系统。

Abstract: This paper examines linear binary codes capable of correcting one or more errors. For the single-error-correcting case, it is shown that the Hamming bound is achieved by a constructive method, and an exact expression for the minimal codeword length is derived. For the general case, a simple lower bound for the parameters of linear codes is derived from an analysis of the coset structure.

</details>


### [9] [C-PASS: Center-Fed Pinching Antenna System](https://arxiv.org/abs/2512.12619)
*Xu Gan,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出了一种新型的中心馈电夹持天线系统(C-PASS)架构，通过从中心端口馈电实现空间复用增益，相比传统端馈PASS可获得两倍自由度并提升复用增益。


<details>
  <summary>Details</summary>
Motivation: 传统端馈夹持天线系统(PASS)存在性能限制，需要新的天线架构来提升通信系统的容量和自由度。

Method: 提出中心馈电夹持天线系统(C-PASS)，信号从中心输入端口馈入并向波导两侧传播，在单个波导中实现空间复用增益。

Result: 推导出C-PASS的自由度和功率缩放定律的闭式表达式，理论表明C-PASS相比传统PASS可获得两倍自由度，并额外获得O(P_T ln^4 N/N^2)的复用增益。

Conclusion: C-PASS通过增强的自由度和复用增益可实现显著的容量提升，为无线通信系统提供了性能改进的新方案。

Abstract: A novel architecture of the center-fed pinching antenna system (C-PASS) is proposed. In contrast to the conventional end-fed PASS, signals are fed from the center input ports and propagate towards both sides of the waveguide. By doing so, spatial-multiplexing gain can be achieved in a single waveguide. Based on the proposed C-PASS, closed-form expressions for the degree of freedom (DoF) and power scaling laws are derived. These theoretical results reveal that C-PASS can achieve \emph{twice} the DoF and an additional multiplexing gain of $\mathcal{O}(P_T \ln^4 N/N^2)$ compared to the conventional PASS, where $P_T$ and $N$ represent the transmit power and pinching antenna number, respectively. Numerical results are provided to demonstrate that substantial capacity improvements can be achieved through the enhanced DoF and multiplexing gain of the C-PASS.

</details>


### [10] [From Information Freshness to Semantics of Information and Goal-oriented Communications](https://arxiv.org/abs/2512.12758)
*Jiping Luo,Erfan Delfani,Mehrdad Salimnejad,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 该论文系统梳理了从传统失真度量到信息新鲜度指标（如AoI），再到面向任务的语义感知通信的发展脉络，提出了统一的语义感知度量框架，并分析了基于MDP和Lyapunov优化的调度策略设计方法。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要支持实时数据驱动的信息物理系统，传统以准确性、吞吐量和延迟为中心的通信范式已无法满足需求。这些系统中信息的价值取决于其与特定任务的语义相关性，因此需要新的通信范式。

Method: 论文系统化组织了现有的语义感知度量指标，包括内容和版本感知度量、上下文相关失真公式、历史相关错误持续性度量等。同时回顾了基于马尔可夫决策过程（MDP）和Lyapunov优化的分析工具，用于在语义性能标准和通信约束下表征最优或近似最优的时序和调度策略。

Result: 建立了一个统一的语义感知通信框架，阐明了面向任务的语义感知通信系统的设计原则。该框架能够显著提高效率、可靠性和任务性能，并为6G及未来网络的语义通信架构设计提供指导。

Conclusion: 论文通过综合信息论、控制论和网络视角，为面向任务的语义感知通信系统提供了一个连贯的框架，旨在指导6G及未来网络的语义通信架构设计，实现仅传输任务相关信息的选择性生成和传输。

Abstract: Future wireless networks must support real-time, data-driven cyber-physical systems in which communication is tightly coupled with sensing, inference, control, and decision-making. Traditional communication paradigms centered on accuracy, throughput, and latency are increasingly inadequate for these systems, where the value of information depends on its semantic relevance to a specific task. This paper provides a unified exposition of the progression from classical distortion-based frameworks, through information freshness metrics such as the Age of Information (AoI) and its variants, to the emerging paradigm of goal-oriented semantics-aware communication. We organize and systematize existing semantics-aware metrics, including content- and version-aware measures, context-dependent distortion formulations, and history-dependent error persistence metrics that capture lasting impact and urgency. Within this framework, we highlight how these metrics address the limitations of purely accuracy- or freshness-centric designs, and how they collectively enable the selective generation and transmission of only task-relevant information. We further review analytical tools based on Markov decision process (MDP) and Lyapunov optimization methods that have been employed to characterize optimal or near-optimal timing and scheduling policies under semantic performance criteria and communication constraints. By synthesizing these developments into a coherent framework, the paper clarifies the design principles underlying goal-oriented, semantics-aware communication systems. It illustrates how they can significantly improve efficiency, reliability, and task performance. The presented perspective aims to serve as a bridge between information-theoretic, control-theoretic, and networking viewpoints, and to guide the design of semantic communication architectures for 6G and beyond.

</details>


### [11] [Information-Theoretic Limits of Integrated Sensing and Communication with Finite Learning Capacity](https://arxiv.org/abs/2512.13292)
*Farshad Rostami Ghadi,F. Javier Lopez-Martinez,Kai-Kit Wong,Christos Masouros*

Main category: cs.IT

TL;DR: 该论文提出了一个统一的信息论框架，用于分析AI辅助的集成感知与通信系统，引入AI容量预算概念来量化学习模型有限表示能力对联合性能的限制，并推导了通信速率与感知失真的理论界限。


<details>
  <summary>Details</summary>
Motivation: 随着AI在ISAC系统中的广泛应用，学习模型有限的表示能力成为系统性能的关键瓶颈。需要建立理论框架来量化这种限制，为系统联合设计提供指导。

Method: 提出AI容量预算概念，推导通信速率与感知失真的上下界；针对高斯信道证明有限学习容量等价于加性噪声；扩展到衰落信道和MIMO系统；提出变分训练方法来实施容量约束。

Result: 建立了通信速率与感知失真的可达成区域界限；在高斯信道中得到闭式解；推导了学习-信息权衡定律；提出了可实施的训练方法；为ISAC系统联合设计提供了量化指导。

Conclusion: 该框架为AI辅助ISAC系统提供了理论基础，量化了学习模型容量限制对性能的影响，为下一代ISAC系统的模型规模、波形和硬件协同设计提供了定量指导原则。

Abstract: This paper develops a unified information-theoretic framework for artificial-intelligence (AI)-aided integrated sensing and communication (ISAC), where a learning component with limited representational capacity is embedded within the transceiver loop. The study introduces the concept of an AI capacity budget to quantify how the finite ability of a learning model constrains joint communication and sensing performance. Under this framework, the paper derives both converse (upper) and achievability (lower) bounds that define the achievable rate-sensing region. For Gaussian channels, the effect of limited learning capacity is shown to behave as an equivalent additive noise, allowing simple analytical expressions for the resulting communication rate and sensing distortion. The theory is then extended to Rayleigh and Rician fading as well as to multiple-input multiple-output (MIMO) systems through new matrix inequalities and a constructive mapping between AI capacity and effective noise covariance. Resource allocation between sensing and communication is optimized under this learning constraint, yielding closed-form conditions in the Gaussian case. A general learning-information trade-off law is also established, linking the representational power of the learning module to the achievable performance frontier. Finally, a practical variational training procedure is proposed to enforce the capacity constraint and to guide empirical evaluation. The derived scaling laws provide quantitative insight for co-designing model size, waveform, and hardware in next-generation ISAC systems.

</details>


### [12] [Machine learning discovers new champion codes](https://arxiv.org/abs/2512.13370)
*Yang-Hui He,Alexander Kasprzyk,Q Le,Dmitrii Riabchenko*

Main category: cs.IT

TL;DR: 使用Transformer预测线性码的最小汉明距离，结合遗传算法搜索，开发出发现最优线性码的新方法


<details>
  <summary>Details</summary>
Motivation: 线性纠错码是现代数字通信和存储系统的数学基础，但识别最优线性码（达到或超过已知最佳最小汉明距离的码）仍然具有挑战性

Method: 训练Transformer模型预测一类线性码的最小汉明距离，并将其与搜索空间上的遗传算法配对，开发出发现最优码的新方法

Result: 该方法有效减少了实现最优线性码所需的搜索空间，可用于研究构造纠错码

Conclusion: 该方法适用于广义环面码、Reed-Muller码、BCH码、代数几何码以及潜在的量子码的研究和构造

Abstract: Linear error-correcting codes form the mathematical backbone of modern digital communication and storage systems, but identifying champion linear codes (linear codes achieving or exceeding the best known minimum Hamming distance) remains challenging. By training a transformer to predict the minimum Hamming distance of a class of linear codes and pairing it with a genetic algorithm over the search space, we develop a novel method for discovering champion codes. This model effectively reduces the search space of linear codes needed to achieve champion codes. Our results present the use of this method in the study and construction of error-correcting codes, applicable to codes such as generalised toric, Reed-Muller, Bose-Chaudhuri-Hocquenghem, algebrogeometric, and potentially quantum codes.

</details>


### [13] [Two Families of Linear Codes Containing Non-GRS MDS Codes](https://arxiv.org/abs/2512.13429)
*Kanat Abdukhalikov,Gyanendra K. Verma*

Main category: cs.IT

TL;DR: 通过修改广义Reed-Solomon码的生成矩阵构造了两类新的线性码，研究了它们的MDS性质、非GRS特性、自正交和自对偶性质


<details>
  <summary>Details</summary>
Motivation: 在广义Reed-Solomon码的基础上构造新的线性码，探索具有MDS性质但非GRS结构的码，并研究其代数特性

Method: 通过修改GRS码的生成矩阵构造两类新码，推导其校验矩阵，建立MDS性质的充要条件，分析自正交和自对偶特性

Result: 成功构造了两类新线性码，确定了它们的MDS条件，发现了非GRS的MDS子族，并给出了自正交和自对偶码的显式构造和示例

Conclusion: 该方法能够产生具有良好代数性质的新线性码，特别是非GRS的MDS码，为编码理论提供了新的构造工具

Abstract: We construct two new families of linear codes by modifying the generator matrices of generalized Reed-Solomon (GRS) codes. For these codes, we explicitly derive parity-check matrices and establish necessary and sufficient conditions ensuring the MDS property. Additionally, we explore subfamilies within these constructions that are non-GRS MDS codes. We also characterize their self-orthogonal and self-dual properties and present some explicit constructions and examples.

</details>


### [14] [From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis](https://arxiv.org/abs/2512.13491)
*Łukasz Dębowski*

Main category: cs.IT

TL;DR: 论文证明了在特定假设下，神经缩放定律可以从Zipf定律推导出来，通过Heap定律和Hilberg假设作为中间步骤


<details>
  <summary>Details</summary>
Motivation: 探索机器学习中的神经缩放定律和定量语言学中的Zipf定律之间的演绎关系，揭示这两种看似不同的统计规律之间的内在联系

Method: 通过系统推导建立四个统计定律之间的逻辑链条：从Zipf定律推导Heap定律，从Heap定律推导Hilberg假设，再从Hilberg假设推导神经缩放定律，并用满足所有四个定律的Santa Fe过程作为示例验证

Result: 证明了在特定广泛假设下，神经缩放定律是Zipf定律的必然结果，建立了两种定律之间的数学联系

Conclusion: 神经缩放定律和Zipf定律之间存在深刻的数学联系，这种关系通过Heap定律和Hilberg假设作为桥梁，揭示了语言统计规律与机器学习性能缩放之间的内在一致性

Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.

</details>


### [15] [Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding](https://arxiv.org/abs/2512.13615)
*Ali Khalesi,Petros Elia*

Main category: cs.IT

TL;DR: 提出超图模型推广经典索引编码到多发送者场景，建立精确的线性码可达性-逆等价关系，超最小秩作为统一设计目标


<details>
  <summary>Details</summary>
Motivation: 将经典索引编码推广到多发送者场景，为分布式通信、缓存辅助通信、编码计算等应用提供统一理论框架

Method: 构建4-正则边信息超图模型，定义新的邻接表示和子超图拟合准则，引入超最小秩概念，建立线性码与超图拟合的等价关系

Result: 证明了线性码可达性-逆等价，最优标量线性广播长度等于超最小秩，提出Haemers型上下界，开发精确计算超最小秩的算法

Conclusion: 超图框架为多发送者索引编码提供精确理论表征，超最小秩可作为分布式通信系统的统一设计目标，算法复杂度优于现有近似方法

Abstract: This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [16] [Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT](https://arxiv.org/abs/2512.12236)
*Aujasvit Datta,Jiayun Wang,Asad Aali,Armeet Singh Jatyani,Anima Anandkumar*

Main category: eess.IV

TL;DR: CTO是一个统一的CT重建框架，通过扩展到连续函数空间实现跨采样率和图像分辨率的泛化，无需重新训练，比CNN和扩散方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建是一个不适定逆问题，现有深度学习方法容易过拟合到固定的采集设置，无法泛化到不同的采样率和图像分辨率，限制了实际部署的实用性。

Method: 提出CTO框架，在连续函数空间中操作，通过旋转等变的离散-连续卷积在正弦图和图像域联合处理，使其具有固有的分辨率和采样无关性。

Result: CTO实现了跨采样率和跨分辨率的一致性能，平均比CNN提升>4dB PSNR；相比最先进的扩散方法，推理速度快500倍，平均提升3dB PSNR。

Conclusion: CTO在多个采样率和分辨率上优于现有方法，提供了一个可扩展和可泛化的解决方案，使自动化CT重建更实用且易于部署。

Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.

</details>


### [17] [V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval](https://arxiv.org/abs/2512.12284)
*Donghyuk Kim,Sejeong Yang,Wonjin Shin,Joo-Young Kim*

Main category: eess.IV

TL;DR: V-Rex是首个软硬件协同设计的加速器，通过ReSV算法和动态KV缓存检索引擎，解决了流式视频LLM中KV缓存增长带来的计算和内存问题，在边缘设备上实现了实时高效推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频大语言模型在实时多模态任务中面临关键挑战：KV缓存随连续视频输入大幅增长，导致迭代预填充阶段存在计算量大、数据传输多、精度下降等问题，尤其在边缘部署场景中更为严重。

Method: 提出V-Rex软硬件协同设计加速器：1）算法层面引入ReSV训练免费动态KV缓存检索算法，利用时空相似性进行token聚类减少KV缓存；2）硬件层面设计紧凑低延迟加速器，包含基于位级和早期退出的动态KV缓存检索引擎。

Result: 在边缘部署中实现3.9-8.3 FPS的实时流式视频LLM推理，精度损失可忽略；动态检索引擎仅占2.2%功耗和2.0%面积，相比AGX Orin GPU实现1.9-19.7倍加速和3.1-18.5倍能效提升。

Conclusion: 这是首个全面解决KV缓存检索算法和硬件问题的方案，使资源受限的边缘设备能够实现实时流式视频LLM推理，为边缘AI应用开辟了新可能。

Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

</details>


### [18] [Leveraging Compression to Construct Transferable Bitrate Ladders](https://arxiv.org/abs/2512.12952)
*Krishna Srikar Durbha,Hassene Tmar,Ping-Hao Wu,Ioannis Katsavounidis,Alan C. Bovik*

Main category: eess.IV

TL;DR: 提出新的基于机器学习的比特率阶梯构建技术，通过分析压缩过程和感知相关测量来准确预测VMAF分数，相比传统方法显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统每标题/每镜头视频编码技术虽然能提供显著的比特率增益和更好的观看体验，但为每个视频构建凸包需要巨大的计算开销。需要更高效的机器学习方法来替代凸包构建。

Method: 提出新的基于机器学习的比特率阶梯构建框架，通过分析压缩过程并在压缩前对源视频进行感知相关测量，来准确预测压缩视频的VMAF分数。

Result: 在大规模视频语料库上评估，性能优于现有领先方法。同时研究了每镜头比特率阶梯在不同编码设置下的表现，并与固定比特率阶梯和通过穷举编码构建的最佳凸包进行比较。

Conclusion: 提出的机器学习方法能够有效替代计算密集的凸包构建，在保持视频质量的同时显著降低计算开销，为自适应比特率阶梯构建提供了高效解决方案。

Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.

</details>


### [19] [Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging](https://arxiv.org/abs/2512.13434)
*Youssef Megahed,Inok Lee,Robin Ducharme,Kevin Dick,Adrian D. C. Chan,Steven Hawken,Mark C. Walker*

Main category: eess.IV

TL;DR: 提出一种基于自监督超声基础模型（USF-MAE）的胎儿肾脏异常自动分类方法，相比传统卷积网络在多个指标上均有提升，尤其在多分类任务中表现显著。


<details>
  <summary>Details</summary>
Motivation: 产前超声是检测先天性肾脏和尿路异常的主要方法，但存在操作者依赖性和成像条件不理想等限制。研究旨在评估自监督超声基础模型在胎儿肾脏异常自动分类中的性能。

Method: 使用969张二维超声图像数据集，对预训练的自监督超声基础模型（USF-MAE）进行微调，用于正常肾脏、尿路扩张和多囊性肾发育不良的二元和多分类任务。与DenseNet-169卷积基线模型进行比较，采用交叉验证和独立测试集评估。

Result: USF-MAE在所有评估指标上均优于基线模型：验证集AUC提升1.87%、F1分数提升7.8%；独立测试集AUC提升2.32%、F1分数提升4.33%。多分类任务中提升最显著：AUC提升16.28%、F1分数提升46.15%。Score-CAM可视化显示模型关注临床相关肾脏结构。

Conclusion: 超声特异性自监督学习能够生成有用的表征作为下游诊断任务的基础。该框架为产前肾脏异常检测提供了稳健、可解释的方法，展示了基础模型在产科影像中的潜力。

Abstract: Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [20] [Modeling and Analysis of VOC-based Interplant Molecular Communication Channel](https://arxiv.org/abs/2512.12035)
*Bitop Maitra,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 论文提出了一个基于挥发性有机化合物(VOC)的植物间分子通信端到端框架，将通信过程分为发射、信道传播和接收三个阶段，分析了各阶段的衰减和延迟特性。


<details>
  <summary>Details</summary>
Motivation: VOC是生物系统中最丰富多样的信号分子之一，基于VOC的分子通信有望开发长距离、生物兼容的通信系统，能够连接纳米和微米级设备。

Method: 提出了一个全面的端到端框架，将VOC通信分为三个阶段：发射（叶片VOC生物合成和释放）、信道传播（湍流风中的平流扩散，应激诱导释放用高斯烟团模型，组成型释放用高斯烟羽模型）、接收（接收植物VOC吸收和生理响应）。

Result: 数值结果表明VOC信道呈现低通特性，带宽和容量受距离、风速和噪声的显著影响。虽然物理信道支持中等频率，但发射端的生物限制使得端到端信道只能传输缓慢变化的信号。

Conclusion: 基于VOC的分子通信系统具有开发长距离生物兼容通信的潜力，但受生物限制只能支持慢速信号传输，为植物间通信系统设计提供了理论基础。

Abstract: Molecular communication (MC) enables information transfer using particles inspired by biological systems. Volatile Organic Compounds (VOCs) are one of the most abundant and diverse classes of signaling molecules used by living or non-living objects. VOC-based MC holds great promise in developing long-range, bio-compatible communication systems capable of interfacing nano- and micro-scale devices. In this paper, we present a comprehensive end-to-end framework for VOC-based interplant MC from an ICT perspective. The communication process is divided into three stages: transmission (VOC biosynthesis and emission from leaves), channel propagation (advection-diffusion in turbulent wind via Gaussian puff for stress-induced VOC release and Gaussian plume for constitutive VOC release), and reception (VOC uptake and physiological response in the receiver plant). Each stage is analyzed by its attenuation and delay. Numerical results demonstrate that VOC-based channels exhibit low-pass behavior, with bandwidth and capacity heavily influenced by distance, wind velocity, and noise. Though the physical channel supports moderate frequencies, biological constraints at the transmitter restrict the end-to-end channel to slow-varying signals.

</details>


### [21] [Hierarchical Deep Learning for Joint Turbulence and PE Estimation in Multi-Aperture FSO Systems](https://arxiv.org/abs/2512.12178)
*Mohammad Taghi Dabiri,Meysam Ghanbari,Rula Ammuri,Mazen Hasna,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 提出一种基于透镜阵列的多孔径FSO接收机架构和分层深度学习框架，首次实现发射机指向误差、接收机到达角波动和湍流衰落的联合估计。


<details>
  <summary>Details</summary>
Motivation: 现有文献孤立处理FSO信道中的多种损伤（发射机指向误差、接收机到达角波动、湍流衰落），由于它们在接收信号中的乘性耦合，传统估计器无法同时恢复这些参数。

Method: 1）设计多孔径FSO接收机架构，利用透镜阵列的空间分集解耦这些相互交织的效应；2）提出分层深度学习框架，顺序估计到达角、发射机指向误差和湍流系数。

Result: 仿真结果表明，该方法达到接近MAP的精度，计算成本降低数个数量级，在估计精度和泛化能力方面显著优于端到端学习基线。

Conclusion: 这是首个实现这三个关键参数实用化联合估计的工作，为可靠、抗湍流的多孔径FSO系统铺平了道路。

Abstract: Accurate characterization of free-space optical (FSO) channels requires joint estimation of transmitter pointing errors, receiver angle-of-arrival (AoA) fluctuations, and turbulence-induced fading. However, existing literature addresses these impairments in isolation, since their multiplicative coupling in the received signal severely limits conventional estimators and prevents simultaneous recovery. In this paper, we introduce a novel multi-aperture FSO receiver architecture that leverages spatial diversity across a lens array to decouple these intertwined effects. Building on this hardware design, we propose a hierarchical deep learning framework that sequentially estimates AoA, transmitter pointing error, and turbulence coefficients. This decomposition significantly reduces learning complexity and enables robust inference even under strong atmospheric fading. Simulation results demonstrate that the proposed method achieves near-MAP accuracy with orders-of-magnitude lower computational cost, and substantially outperforms end-to-end learning baselines in terms of estimation accuracy and generalization. To the best of our knowledge, this is the first work to demonstrate practical joint estimation of these three key parameters, paving the way for reliable, turbulence-resilient multi-aperture FSO systems.

</details>


### [22] [A Sensing Dataset Protocol for Benchmarking and Multi-Task Wireless Sensing](https://arxiv.org/abs/2512.12180)
*Jiawei Huang,Di Zhang,Yuanhao Cui,Xiaowen Cao,Tony Xiao Han,Xiaojun Jing,Christos Masouros*

Main category: eess.SP

TL;DR: SDP是一个无线感知数据集协议和基准框架，通过统一的数据块架构、信号对齐和CP-ALS池化，为多模态多任务感知研究提供可复现的基础。


<details>
  <summary>Details</summary>
Motivation: 现有无线感知数据集和流程在感知模态上碎片化，阻碍了公平比较、迁移和可复现性，需要统一的协议和基准框架。

Method: 提出SDP协议，通过轻量级同步、频时对齐和重采样将异构无线信号映射到统一感知数据块架构，使用CP-ALS池化提供任务无关表示，建立统一的检测、识别和生命体征估计基准。

Result: 跨用户分割实验显示，SDP显著减少了不同种子间的方差（约88%），同时保持了竞争性的准确性和延迟。

Conclusion: SDP为多模态和多任务感知研究提供了可复现的基础，解决了现有无线感知研究的碎片化问题。

Abstract: Wireless sensing has become a fundamental enabler for intelligent environments, supporting applications such as human detection, activity recognition, localization, and vital sign monitoring. Despite rapid advances, existing datasets and pipelines remain fragmented across sensing modalities, hindering fair comparison, transfer, and reproducibility. We propose the Sensing Dataset Protocol (SDP), a protocol-level specification and benchmark framework for large-scale wireless sensing. SDP defines how heterogeneous wireless signals are mapped into a unified perception data-block schema through lightweight synchronization, frequency-time alignment, and resampling, while a Canonical Polyadic-Alternating Least Squares (CP-ALS) pooling stage provides a task-agnostic representation that preserves multipath, spectral, and temporal structures. Built upon this protocol, a unified benchmark is established for detection, recognition, and vital-sign estimation with consistent preprocessing, training, and evaluation. Experiments under the cross-user split demonstrate that SDP significantly reduces variance (approximately 88%) across seeds while maintaining competitive accuracy and latency, confirming its value as a reproducible foundation for multi-modal and multitask sensing research.

</details>


### [23] [Learning-Driven Dual-Line Laser Scanning for Fast and Accurate LEO Satellite Positioning](https://arxiv.org/abs/2512.12181)
*Mohammad Taghi Dabiri,Rula Ammuri,Mazen Hasna,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 提出一种基于学习驱动的双线激光扫描框架，用于快速精确的LEO卫星定位，使用正交线形激光束进行结构化光学扫描，无需移动部件，实现毫秒级定位


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星光学链路需要毫秒级波束对准以实现可靠的高速通信，传统高斯波束采集系统依赖多个顺序波束或机械转向，存在延迟和复杂性挑战

Method: 采用两个正交线形激光束在模糊区域进行结构化光学扫描，开发包含大气衰减、湍流和MRR反射的物理模型，训练数据驱动的神经估计器将接收的光学能量模式映射到卫星二维位置

Result: 学习驱动方法实现接近MAP的精度，典型误差7-10米，确定性扫描时间1-2毫秒；传统两阶段高斯波束方案误差相当但随机传感持续时间可达5毫秒

Conclusion: 该框架在定位精度、计算复杂度和传感延迟之间提供了有利的权衡，是下一代光学LEO跟踪系统的实用候选方案

Abstract: Accurate and low-latency positioning is a key enabler for optical links with Low Earth Orbit (LEO) satellites, where millisecond-level beam alignment is required to maintain reliable high-data-rate communication. This paper presents a learning-driven dual-line laser scanning framework for fast and precise satellite positioning. Unlike conventional Gaussian-beam acquisition systems that rely on multiple sequential beams or mechanical steering, the proposed approach employs two orthogonal line-shaped laser beams to perform structured optical scanning over the ambiguity region without any moving parts. A physics-based model incorporating atmospheric attenuation, turbulence, and MRR-based reflection is developed, and a data-driven neural estimator is trained to map received optical energy patterns to the satellite's two-dimensional position. Simulation results demonstrate that the learning-driven method achieves near-MAP accuracy with typical errors of 7-10 m and deterministic scanning time of 1-2 ms, while conventional two-stage Gaussian-beam schemes exhibit comparable errors but random sensing durations of up to 5 ms. The proposed framework therefore offers a favorable trade-off between positioning accuracy, computational complexity, and sensing latency, making it a practical candidate for next-generation optical LEO tracking systems.

</details>


### [24] [MRR-Based Line-Laser Scanning for Reliable Vehicular Positioning and Optical Communication](https://arxiv.org/abs/2512.12186)
*Mohammad Taghi Dabiri,Hossein Safi,Rula Ammuri,Mazen Hasna,Khalid Qaraqe,Harald Haas,Iman Tavakkolnia*

Main category: eess.SP

TL;DR: 提出一种无机械跟踪的光学联合感知、定位与通信系统，使用结构化线激光和调制后向反射器阵列，实现高速公路环境下的连续广域覆盖


<details>
  <summary>Details</summary>
Motivation: 高速车辆环境需要能够同时进行感知、定位和通信的光学系统，而现有方法依赖点光源或相机接收器，在高速公路动态条件下空间覆盖和更新率有限

Method: 结合结构化线激光照明与车辆上的调制后向反射器阵列，使用两个正交线激光进行同步纵向和横向扫描，提供连续广域覆盖；开发覆盖驱动的分析框架建模光束发散、扫描几何和驻留时间分配的耦合关系；设计优化方案调整扫描和发散参数以实现均匀覆盖和功率效率

Result: 仿真结果显示在固定扫描周期内，空间覆盖均匀性、链路稳定性和可靠性显著改善，为下一代车辆JSPC网络的可扩展、抗湍流光架构提供了实用路径

Conclusion: 该系统为下一代车辆光学联合感知、定位与通信网络建立了可扩展、抗湍流的实用架构路径

Abstract: High-speed vehicular environments require optical systems capable of joint sensing, positioning, and communication (JSPC) without mechanical tracking. Existing optical and integrated sensing-communication approaches often rely on point-source emitters or camera-based receivers, limiting spatial coverage and update rate under highway dynamics. This work introduces a new class of tracking-free optical JSPC systems that combine structured line-laser illumination with modulating retroreflector (MRR) arrays on vehicles. Two orthogonal line lasers perform synchronized longitudinal and transverse scanning to provide continuous, wide-area coverage across the roadway. A coverage-driven analytical framework models the coupling between beam divergence, scan geometry, and dwell-time allocation, enabling joint evaluation of sensing reliability and communication quality. An optimization scheme is developed to adapt scanning and divergence parameters for uniform coverage and power efficiency. Simulation results demonstrate significant improvements in spatial coverage uniformity, link stability, and reliability within a fixed scan period. These results establish a practical pathway toward scalable, turbulence-resilient optical architectures for next-generation vehicular JSPC networks.

</details>


### [25] [Rotatable Antenna Array-Enhanced Null Steering: Performance Analysis and Optimization](https://arxiv.org/abs/2512.12204)
*Yingqi Wen,Weidong Mei,Yike Xie,Beixiong Zheng,Zhi Chen,Boyu Ning*

Main category: eess.SP

TL;DR: 提出可旋转天线阵列架构，通过三维旋转控制增强空间灵活性，实现更好的零陷波束赋形性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定方向天线阵列在零陷波束赋形方面的自由度有限，需要新的架构来增强空间灵活性。

Method: 提出可旋转天线阵列架构，联合优化三维旋转角度，采用顺序更新算法和吉布斯采样来避免局部最优。

Result: 相比固定方向天线阵列，可旋转天线阵列显著放宽了实现有效零陷波束赋形的角度分离要求，并展现出优越的零陷性能。

Conclusion: 可旋转天线阵列架构为解决传统天线阵列在零陷波束赋形方面的局限性提供了有效解决方案。

Abstract: Conventional fixed-orientation antenna (FOA) arrays offer limited degrees of freedom (DoF) for flexible beamforming such as null steering. To address this limitation, we propose a new rotatable antenna array (RAA) architecture in this paper, which enables three-dimensional (3D) rotational control of an antenna array to provide enhanced spatial flexibility for null steering. To characterize its performance, we aim to jointly optimize the 3D rotational angles of the RAA, to maximize the beam gain over a given desired direction, while nulling those over multiple interference directions under zero-forcing (ZF) beamforming. However, this problem is non-convex and challenging to tackle due to the highly nonlinear expression of the beam gain in terms of the rotational angles. To gain insights, we first examine several special cases including both isotropic and directional antenna radiation patterns, deriving the conditions under which full beam gain can be achieved over the desired direction while meeting the nulling constraints for interference directions. These conditions clearly indicate that compared with FOA arrays, RAAs can significantly relax the angular separation requirement for achieving effective null steering. For other general cases, we propose a sequential update algorithm, that iteratively refines the 3D rotational angles by discretizing the 3D angular search space. To avoid undesired local optimum, a Gibbs sampling (GS) procedure is also employed between two consecutive rounds of sequential update for solution exploration. Simulation results verify our analytical results and show superior null-steering performance of RAAs to FOA arrays.

</details>


### [26] [Movable Access Points in Visible Light Communications: Opportunities, Challenges and Future Directions](https://arxiv.org/abs/2512.12214)
*Sylvester Aboagye,Telex M. N. Ngatched*

Main category: eess.SP

TL;DR: 提出移动接入点辅助可见光通信系统，通过动态重定位AP确保视距连接和对齐，在动态环境中优于RIS辅助、固定AP和纯RIS系统


<details>
  <summary>Details</summary>
Motivation: 可见光通信性能受设备方向、视距链路遮挡、发射半角和接收视场角影响，RIS虽然能缓解遮挡但对齐问题但数据率远低于直接视距链路

Method: 引入移动接入点辅助VLC系统概念，通过动态重定位AP提供新的自由度，确保视距连接和发射器-接收器对齐，同时为移动用户提供超高数据率

Result: 仿真结果显示MAPs在动态环境中优于RIS辅助、固定AP和纯RIS VLC系统

Conclusion: 移动接入点辅助VLC系统能有效解决VLC的视距和对齐问题，提供更高数据率，但需进一步研究与其他新兴无线技术的集成等挑战

Abstract: Visible light communication (VLC) is expected to be a key component of future wireless networks due to its abundant license-free spectrum, inherent high-level security, and the already deployed lighting infrastructure. VLC performance, however, depends on device orientation and the availability of an unobstructed line-of-sight (LoS) link, with transmitter semi-angle and receiver field-of-view (FoV) further affecting alignment, coverage, and reliability. Reconfigurable intelligent surfaces (RISs) can mitigate blockages, orientation issues, and mobility challenges, but their data rates remain far below those of direct LoS links. This article introduces the novel concept of movable access points (MAPs)-aided VLC systems, where dynamically repositioned APs provide new degrees of freedom to ensure LoS connectivity, and transmitter-receiver alignment while providing ultra-high data rates for mobile users. Simulation results show MAPs outperform RIS-aided, fixed-AP, and RIS-only VLC systems in dynamic environments. The article also outlines key challenges and future research directions, including integration with emerging wireless technologies.

</details>


### [27] [Robust Energy-Efficient Sleep-Mode Strategy for Multi-RIS-Aided Cell-Free Massive MIMO](https://arxiv.org/abs/2512.12223)
*Hongyi Luo,Wenyu Song,Daniel K. C. So,Zahra Mobini,Zhiguo Ding*

Main category: eess.SP

TL;DR: 本文提出了一种用于无小区大规模MIMO系统的节能传输方案，通过联合协调活动AP和多个被动RIS，在低负载期间优化能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络能耗增加，现有研究通常假设持续高流量负载，忽略了用户需求的动态性，导致低需求期间AP利用率不足和不必要的能源消耗。需要解决无小区大规模MIMO系统在低负载期间的能源效率问题。

Method: 提出动态AP睡眠模式策略，选择性停用某些AP，同时利用附近RIS维持覆盖。将EE最大化目标建模为分数规划问题，采用Dinkelbach方法结合交替优化，迭代求解三个耦合子问题：1) 通过混合分支定界和贪心算法进行AP选择；2) 使用序列凸逼近方法优化发射功率，以启发式迫零策略初始化；3) 使用梯度投影优化RIS相移。

Result: 仿真结果表明，所提方案在低和中度用户场景下，相比现有方法实现了显著更高的能源效率。

Conclusion: 通过联合协调活动AP和被动RIS的动态传输方案，能够有效提升无小区大规模MIMO系统在动态负载条件下的能源效率，为6G网络节能提供了有前景的解决方案。

Abstract: With the explosive growth of data traffic and the ubiquitous connectivity of wireless devices, the energy demands of wireless networks have inevitably escalated. Reconfigurable intelligent surface (RIS) has emerged as a promising solution for 6G networks due to its energy efficiency (EE) and low cost, while cell-free massive multiple-input multiple-output (CF-mMIMO) was proposed as an innovative network architecture without fixed cell boundaries to enhance these measures even further. However, existing studies often assume consistently high traffic loads, neglecting the dynamic nature of user demand. This can result in underutilized access points (APs) and unnecessary energy expenditure during low-demand periods. To tackle the challenge of EE in CF-mMIMO systems during low load periods, this paper proposes a novel energy-efficient transmission scheme that jointly coordinates active APs and multiple passive RISs. Specifically, a dynamic AP sleep-mode strategy is designed, where certain APs are selectively deactivated while nearby RISs assist in maintaining coverage. We formulate the EE maximization objective as a fractional programming problem and adopt the Dinkelbach method in conjunction with alternating optimization (AO) to iteratively solve the three coupled subproblems: (i) AP selection via a hybrid branch-and-bound (BnB) and greedy algorithm, (ii) transmit power optimization using a sequential convex approximation (SCA) method, initialized by a heuristic zero-forcing strategy, and (iii) RIS phase shift optimization using gradient projection. Simulation results show that the proposed scheme achieves significantly higher EE than existing methods in both low and moderate user scenarios.

</details>


### [28] [WATOS: Efficient LLM Training Strategies and Architecture Co-exploration for Wafer-scale Chip](https://arxiv.org/abs/2512.12279)
*Huizheng Wang,Zichuan Wang,Hongbin Wang,Jingxiang Hou,Taiquan Wei,Chao Li,Yang Hu,Shouyi Yin*

Main category: eess.SP

TL;DR: WATOS是一个针对大语言模型训练的晶圆级架构与训练策略协同探索框架，通过优化硬件参数和并行策略，相比现有方法显著提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型对计算、内存和互连带宽要求极高，晶圆级集成虽然提供了高密度集成方案，但有限的晶圆面积需要在计算、内存和通信资源之间进行权衡。现有方法无法有效解决这些挑战。

Method: 提出WATOS协同探索框架：1）定义高度可配置的硬件模板来探索晶圆级芯片的最佳架构参数；2）利用晶圆级芯片的高D2D带宽和细粒度操作优势，探索最优并行和资源分配策略，解决LLM训练中的内存利用率不足问题。

Result: 相比最先进的Megatron训练框架和Cerebras的权重流晶圆训练策略，WATOS在各种LLM模型上分别实现了平均2.74倍和1.53倍的整体吞吐量提升。同时揭示了晶圆级架构设计的重要见解。

Conclusion: WATOS成功解决了晶圆级LLM训练中的架构与策略协同优化挑战，显著提升了训练效率，并为晶圆级架构设计提供了有价值的指导。

Abstract: Training large language models (LLMs) imposes extreme demands on computation, memory capacity, and interconnect bandwidth, driven by their ever-increasing parameter scales and intensive data movement. Wafer-scale integration offers a promising solution by densely integrating multiple single-die chips with high-speed die-to-die (D2D) interconnects. However, the limited wafer area necessitates trade-offs among compute, memory, and communication resources. Fully harnessing the potential of wafer-scale integration while mitigating its architectural constraints is essential for maximizing LLM training performance. This imposes significant challenges for the co-optimization of architecture and training strategies. Unfortunately, existing approaches all fall short in addressing these challenges.
  To bridge the gap, we propose WATOS, a co-exploration framework for LLM training strategy and wafer-scale architecture. We first define a highly configurable hardware template designed to explore optimal architectural parameters for wafer-scale chips. Based on it, we capitalize on the high D2D bandwidth and fine-grained operation advantages inherent to wafer-scale chips to explore optimal parallelism and resource allocation strategies, effectively addressing the memory underutilization issues during LLM training. Compared to the state-of-the-art (SOTA) LLM training framework Megatron and Cerebras' weight streaming wafer training strategy, WATOS can achieve an average overall throughput improvement of 2.74x and 1.53x across various LLM models, respectively. In addition, we leverage WATOS to reveal intriguing insights about wafer-scale architecture design with the training of LLM workloads.

</details>


### [29] [XR Capacity Enhancement through Multi-Connected XR Tethering Groups](https://arxiv.org/abs/2512.12368)
*Muhammad Ahsen,Boyan Yanakiev,Claudio Rosa,Ramoni Adeogun*

Main category: eess.SP

TL;DR: 本文研究通过多连接XR系留组（TGr）提升5G-A网络中XR容量，采用选择合并和软合并两种协作方式，结合增强的联合OLLA算法，显著提高XR容量和eMBB吞吐量。


<details>
  <summary>Details</summary>
Motivation: 5G-A网络中XR应用面临高吞吐量、低延迟和高可靠性的严格约束，导致容量受限。需要创新方案来提升XR容量，同时不影响其他业务性能。

Method: 提出多连接XR系留组（TGr），包含XR设备和协作的5G-A设备。研究两种协作方式：选择合并（SC）和软合并。开发联合HARQ反馈处理算法和增强的联合外环链路自适应（OLLA）算法，利用多连接性提升频谱效率。

Result: 动态系统级仿真显示：软合并XR TGr在纯XR用户场景下提升容量23-42%，在XR与eMBB共存场景下提升38-173%。增强的联合OLLA算法在仅单设备提供CSI报告时也能实现类似性能增益。软合并XR TGr还能提升共存场景中的eMBB吞吐量。

Conclusion: 多连接XR系留组特别是软合并方案能显著提升5G-A网络中的XR容量，增强的联合OLLA算法能有效利用多连接性优势，即使在有限CSI反馈下也能保持性能，为未来XR应用部署提供重要技术支撑。

Abstract: Extended Reality (XR) applications have limited capacity in 5th generation-advanced (5G-A) cellular networks due to high throughput requirements coupled with strict latency and high reliability constraints. To enhance XR capacity in the downlink (DL), this paper investigates multi-connected XR tethering groups (TGrs), comprising an XR device and a cooperating 5G-A device. This paper presents investigations for two types of cooperation within XR TGr, i.e., selection combining (SC) and soft combining and their impact on the XR capacity of the network. These investigations consider joint hybrid automatic repeat request (HARQ) feedback processing algorithm and also propose enhanced joint Outer Loop Link Adaptation (OLLA) algorithm to leverage the benefits of multi-connectivity. These enhancements aim to improve the spectral efficiency of the network by limiting HARQ retransmissions and enabling the use of higher modulation and coding scheme (MCS) indices for given signal-to-interference-plus-noise ratio (SINR), all while maintaining or operating below than the target block error rate (BLER). Dynamic system-level simulation demonstrate that XR TGrs with soft combining achieve performance improvements of 23 - 42% in XR capacity with only XR users and 38-173% in the coexistence scenarios consisting of XR users and enhanced mobile broadband (eMBB) user. Furthermore, the enhanced joint OLLA algorithm enables similar performance gains even when only one device per XR TGr provides channel state information (CSI) reports, compared to scenarios where both devices report CSI. Notably, XR TGrs with soft combining also enhance eMBB throughput in coexistence scenarios.

</details>


### [30] [Comparing Stochastic and Ray-tracing Datasets in Machine Learning for Wireless Applications](https://arxiv.org/abs/2512.12449)
*João Morais,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: eess.SP

TL;DR: 研究无线通信中机器学习时，标准随机信道模型与射线追踪数据的适用性对比，发现随机模型评估可能高估或低估性能，提出任务感知的使用方案


<details>
  <summary>Details</summary>
Motivation: 无线系统中机器学习研究通常使用标准随机信道模型（如TDL/CDL/UMa），但这些模型的结构假设可能与实际传播情况存在差异。需要了解何时这些模型足够，何时需要更接近真实世界的射线追踪数据。

Method: 通过实证研究，在两个代表性任务（CSI压缩和时序信道预测）上，使用域内、跨域和小数据微调协议训练和评估模型，对比随机信道模型和射线追踪数据的表现。

Result: 在不同设置下，仅使用随机模型评估可能相对于射线追踪数据高估或低估性能。随机模型可用于可扩展的预训练和不需要强时空耦合的任务，但当这种耦合重要时，预训练和评估应基于空间一致或几何相似的射线追踪场景。

Conclusion: 提出任务感知的方案：随机模型可用于可扩展预训练和非强时空耦合任务；当需要强时空耦合时，应使用射线追踪数据进行预训练和评估。为未来基准测试和标准化提供初步指导。

Abstract: Machine learning for wireless systems is commonly studied using standardized stochastic channel models (e.g., TDL/CDL/UMa) because of their legacy in wireless communication standardization and their ability to generate data at scale. However, some of their structural assumptions may diverge from real-world propagation. This paper asks when these models are sufficient and when ray-traced (RT) data - a proxy for the real world - provides tangible benefits. To answer these questions, we conduct an empirical study on two representative tasks: CSI compression and temporal channel prediction. Models are trained and evaluated using in-domain, cross-domain, and small-data fine-tuning protocols. Across settings, we observe that stochastic-only evaluation may over- or under-estimate performance relative to RT. These findings support a task-aware recipe where stochastic models can be leveraged for scalable pre-training and for tasks that do not rely on strong spatiotemporal coupling. When that coupling matters, pre-training and evaluation should be grounded in spatially consistent or geometrically similar RT scenarios. This study provides initial guidance to inform future discussions on benchmarking and standardization.

</details>


### [31] [Wavelet-Packet-based Noise Signatures With Higher-Order Statistics for Anomaly Prediction](https://arxiv.org/abs/2512.12528)
*Indrakshi Dey,Ilias Cherkaoui,Mohamed Khalafalla Hassan*

Main category: eess.SP

TL;DR: 提出首个针对融合离散时间信号的噪声中心异常预测方法，使用小波包变换分离结构与残差，高阶统计量量化非高斯性和非线性耦合，构建紧凑噪声特征并采用马氏距离检测器


<details>
  <summary>Details</summary>
Motivation: 开发首个针对融合离散时间信号的噪声中心异常预测方法，旨在通过分析信号中的噪声特征来检测异常，传统方法可能忽略了噪声中的有用信息

Method: 使用小波包变换进行时频展开，通过正交投影分离信号结构与残差；采用高阶统计量（特别是三阶累积量及其双谱解释）量化残差的非高斯性和非线性耦合；构建紧凑噪声特征，并使用分析校准的马氏距离检测器得到闭式决策规则

Result: 建立了正交性、能量保持、累积量的高斯零行为以及由此产生的检验统计量的命题和证明；检测器在均值偏移替代假设下具有非中心卡方性能

Conclusion: 成功开发了首个噪声中心的异常预测方法，为融合离散时间信号提供了一种基于噪声特征分析的异常检测框架，具有理论保证和闭式决策规则

Abstract: This note develops the first-ever noise-centric anomaly prediction method for a fused discrete-time signal. A Wavelet Packet Transform (WPT) provides a time--frequency expansion in which structure and residual can be separated via orthogonal projection. Higher-Order Statistics (HOS), particularly the third-order cumulant (and its bispectral interpretation), quantify non-Gaussianity and nonlinear coupling in the extracted residual. Compact noise signatures are constructed and an analytically calibrated Mahalanobis detector yields a closed-form decision rule with non-central chi-square performance under mean-shift alternatives. Propositions and proofs establish orthonormality, energy preservation, Gaussian-null behavior of cumulants, and the resulting test statistics.

</details>


### [32] [Power Consumption and Energy Efficiency of Mid-Band XL-MIMO: Modeling, Scaling Laws, and Performance Insights](https://arxiv.org/abs/2512.12725)
*Jiachen Tian,Yu Han,Xiao Li,Shi Jin,Chao-Kai Wen*

Main category: eess.SP

TL;DR: 该论文研究了中频段超大规模MIMO系统的能效问题，提出了综合功耗模型和吞吐量分析框架，推导了能效与系统配置的缩放规律，验证了中频段XL-MIMO在能效方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 中频段超大规模MIMO系统虽然能提供更高吞吐量，但由于系统维度扩大导致功耗问题严重，需要深入研究高效系统设计和部署方案。

Method: 提出了综合功耗模型，涵盖主要硬件组件和信号处理过程的功耗；考虑近场传播特性，推导了吞吐量的闭式近似；建立了能效分析框架，推导了能效与关键系统配置的缩放规律；对代表性多天线技术进行了扩展和比较。

Result: 建立了中频段XL-MIMO系统的能效分析框架，推导了能效缩放规律，数值结果验证了吞吐量分析的紧密度和能效评估的有效性，揭示了中频段XL-MIMO系统在能效方面的潜力。

Conclusion: 中频段XL-MIMO系统在能效方面具有显著优势，提出的分析框架和缩放规律为系统设计提供了有价值的指导，验证了能量高效中频段XL-MIMO系统的潜力。

Abstract: Mid-band extra-large-scale multiple-input multiple-output (XL-MIMO), emerging as a critical enabler for future communication systems, is expected to deliver significantly higher throughput by leveraging the extended bandwidth and enlarged antenna aperture. However, power consumption remains a significant concern due to the enlarged system dimension, underscoring the need for thorough investigations into efficient system design and deployment. To this end, an in-depth study is conducted on mid-band XL-MIMO systems. Specifically, a comprehensive power consumption model is proposed, encompassing the power consumption of major hardware components and signal processing procedures, while capturing the influence of key system parameters. Considering typical near-field propagation characteristics, closed-form approximations of throughput are derived, providing an analytical framework for assessing energy efficiency (EE). Based on the proposed framework, the scaling law of EE with respect to key system configurations is derived, offering valuable insights for system design. Subsequently, extensions and comparisons are conducted among representative multi-antenna technologies, demonstrating the superiority of mid-band XL-MIMO in EE. Extensive numerical results not only verify the tightness of the throughput analysis but also validate the EE evaluations, unveiling the potential of energy-efficient mid-band XL-MIMO systems.

</details>


### [33] [Channel Estimation for Full-duplex Multi-tag Ambient Backscatter Communication Systems with I/Q Imbalance](https://arxiv.org/abs/2512.12811)
*Saeed Abdallah,Mahmoud A. Albreem,Bassel Al Homssi,Mohamed Saad,Abdulmalik Alwarafy*

Main category: eess.SP

TL;DR: 提出了一种用于全双工多标签环境反向散射通信系统的三阶段训练协议和信道估计方案，包括导频估计器和两种半盲估计器，在性能和计算复杂度之间提供不同权衡。


<details>
  <summary>Details</summary>
Motivation: 全双工多标签环境反向散射通信系统对下一代物联网网络至关重要，但多标签、自干扰和硬件损伤（如I/Q不平衡）使得准确信道估计对有效干扰管理不可或缺。大量信道参数和信号分量的镜像图像需要精心设计信道估计阶段以防止性能下降。

Method: 提出三阶段训练协议和导频估计方案确保信号正交性并避免误差平台；提出两种半盲估计器：基于决策导向准则和基于期望条件最大化框架，利用导频和数据符号实现更高估计精度；推导了两类估计的克拉美罗界。

Result: 导频估计器和ECM估计器接近各自的CRB，DD估计器性能介于两者之间。三种方案通过不同的性能与计算复杂度权衡支持不同应用场景。

Conclusion: 提出的三阶段训练协议和三种估计器为全双工多标签环境反向散射通信系统提供了有效的信道估计解决方案，在性能和复杂度之间提供灵活选择，适用于不同物联网应用需求。

Abstract: Ambient backscatter communication (AmBC) has emerged as a highly attractive paradigm for energy-efficient communication. Full-duplex multi-tag AmBC systems provide the scalability and efficient spectrum utilization essential for next generation Internet-of-Things (IoT) networks. However, the presence of multiple tags, self-interference and hardware impairments such as inphase/quadrature (I/Q) imbalance, makes accurate channel estimation indispensable for efficient interference management. The large number of channel parameters and the presence of mirror images of each signal component necessitate careful design of the channel estimation phase to prevent performance degradation. In this work, we propose a novel three-stage training protocol and pilot-based estimation scheme that ensure signal orthogonality and successfully avoid error floors. We also propose two semi-blind estimators, one based on decision-directed (DD) criterion and the other on the expectation conditional maximization (ECM) framework. By exploiting both pilots and data symbols, these two estimators achieve higher estimation accuracy than pilot-based estimation, at the cost of additional complexity. Cramer-Rao bounds (CRBs) for both types of estimation are also derived. The pilot-based estimator and the ECM estimator approach their respective CRBs, while the DD estimator performs mid-way between them. The three proposed solutions support different use cases by offering distinct tradeoffs between performance and computational complexity.

</details>


### [34] [A Comprehensive Survey of Channel Estimation Techniques for OTFS in 6G and Beyond Wireless Networks](https://arxiv.org/abs/2512.13032)
*Emir Aslandogan,Haci Ilhan,Burak Ahmet Ozden,Erdogan Aydin,Ertugrul Basar,Miaowen Wen,Marco Di Renzo,Vincent Poor*

Main category: eess.SP

TL;DR: 本文系统综述了OTFS调制系统中的信道估计技术，涵盖从基础方法到前沿方法的完整研究图景，包括DD/TF域估计、多种算法框架、联合估计检测策略，以及与下一代无线系统的集成挑战。


<details>
  <summary>Details</summary>
Motivation: OTFS调制作为应对高移动性场景和多普勒效应的关键技术，其信道估计性能对系统性能至关重要。然而，现有研究缺乏对OTFS信道估计技术的系统性梳理和分析，需要全面总结从基础到前沿的方法，并为未来研究方向提供指导。

Method: 文章采用系统性综述方法，首先分析OTFS在延迟-多普勒域的优势，然后分类讨论DD域和TF域的信道估计技术，包括分离导频、嵌入式导频和叠加导频方法。同时涵盖贝叶斯学习、匹配追踪、消息传递、深度学习等多种算法框架，以及联合信道估计与信号检测策略。

Result: 文章提供了OTFS信道估计技术的全面分析框架，识别了关键实现挑战，包括泄漏抑制、多普勒间干扰缓解、脉冲噪声处理、信令开销减少、保护空间需求、PAPR管理、波束斜视效应和硬件损伤等，为未来研究提供了明确方向。

Conclusion: OTFS调制在高移动性场景中具有显著优势，其信道估计技术已发展出多种有效方法。未来研究应关注与大规模MIMO、毫米波、RIS、ISAC等下一代无线系统的集成，同时解决实际部署中的关键挑战，推动OTFS技术的实际应用。

Abstract: Orthogonal time-frequency space (OTFS) modulation has emerged as a powerful wireless communication technology that is specifically designed to address the challenges of high-mobility scenarios and significant Doppler effects. Unlike conventional modulation schemes that operate in the time-frequency (TF) domain, OTFS projects signals to the delay-Doppler (DD) domain, where wireless channels exhibit sparse and quasi-static characteristics. This fundamental transformation enables superior channel estimation (CE) performance in challenging propagation environments characterized by high-mobility, severe multipath effects, and rapidly time-varying channel conditions. This article provides a systematic examination of CE techniques for OTFS systems, covering the extensive research landscape from foundational methods to cutting-edge approaches. We present a detailed analysis of DD and TF domain CE techniques presented in the literature, including separate pilot, embedded pilot, and superimposed pilot approaches. The article encompasses various algorithmic frameworks including Bayesian learning, matching pursuit-based techniques, message passing algorithms, deep learning (DL)-based methods, and recent CE approaches. Additionally, we explore joint CE and signal detection (SD) strategies, the integration of OTFS with next-generation wireless systems including massive multiple-input multiple-output (MIMO), millimeter wave (mmWave) communications, reconfigurable intelligent surfaces (RISs), and integrated sensing and communication (ISAC) systems. Critical implementation challenges are presented, including leakage suppression, inter-Doppler interference mitigation, impulsive noise handling, signaling overhead reduction, guard space requirements, peak-to-average power ratio (PAPR) management, beam squint effects, and hardware impairments.

</details>


### [35] [MR Fingerprinting for Imaging Brain Hemodynamics and Oxygenation](https://arxiv.org/abs/2512.13224)
*T. Coudert,A. Delphin,A. Barrier,E L Barbier,B. Lemasson,J M Warnking,T. Christen*

Main category: eess.SP

TL;DR: 本文综述了磁共振指纹技术（MRF）在脑血流动力学、氧合和灌注定量方面的研究进展，重点介绍了血管模拟几何模型、新型序列以及结合机器学习和深度学习的最新重建技术，并讨论了临床转化面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 过去十年中，多项研究探索了磁共振指纹技术（MRF）在脑血流动力学、氧合和灌注定量方面的潜力。随着模拟模型和重建框架的进步，血管参数估计的准确性显著提高，但需要系统综述这些进展以指导未来研究和临床转化。

Method: 本文是一篇综述性文章，系统回顾了血管MRF研究的关键进展，包括：1）血管模拟的几何模型改进；2）新型序列开发；3）结合机器学习和深度学习算法的最先进重建技术；4）临床前和临床应用案例。

Result: 综述显示，MRF技术在脑血管参数定量方面取得了显著进展，特别是在模拟精度和重建算法方面。机器学习和深度学习的应用进一步提升了参数估计的准确性和效率，为临床转化奠定了基础。

Conclusion: MRF技术在脑血流动力学定量方面展现出巨大潜力，但仍需解决临床转化中的关键问题。未来发展方向包括优化算法、验证临床有效性、提高计算效率等，以推动该技术从研究工具向临床应用转化。

Abstract: Over the past decade, several studies have explored the potential of magnetic resonance fingerprinting (MRF) for the quantification of brain hemodynamics, oxygenation, and perfusion. Recent advances in simulation models and reconstruction frameworks have also significantly enhanced the accuracy of vascular parameter estimation. This review provides an overview of key vascular MRF studies, emphasizing advancements in geometrical models for vascular simulations, novel sequences, and state-of-the-art reconstruction techniques incorporating machine learning and deep learning algorithms. Both pre-clinical and clinical applications are discussed. Based on these findings, we outline future directions and development areas that need to be addressed to facilitate their clinical translation. Evidence Level N/A. Technical Efficacy Stage 1.

</details>


### [36] [Interference-Free RIS-Aided Cell-Free Massive MIMO with Physical Layer Security](https://arxiv.org/abs/2512.13243)
*Sumeyra Hassan,Bin Li,Yalcin Sadi,Erdal Panayirci,H. Vincent Poor*

Main category: eess.SP

TL;DR: 本文提出了一种基于可重构智能表面(RIS)的无蜂窝大规模MIMO框架，通过联合优化主动波束成形、用户功率分配和RIS相位偏移矩阵来增强物理层安全并抑制多用户干扰。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络面临物理层安全和多用户干扰的挑战，特别是在密集无线环境中。传统方法难以同时解决这两个问题，需要一种能够增强安全性能并消除干扰的新型框架。

Method: 1. 设计基于CSI的预编码器来抑制多用户干扰；2. 采用交替优化框架，联合优化AP的主动波束成形、用户功率分配和RIS相位偏移矩阵；3. 使用黎曼流形优化框架和黎曼共轭梯度算法解决高度非凸的RIS相位偏移设计问题。

Result: 仿真结果表明，所提出的框架有效提高了保密和速率，并成功消除了干扰，证明了其在密集无线环境中构建安全、可扩展的无蜂窝大规模MIMO网络的潜力。

Conclusion: RIS辅助的无蜂窝大规模MIMO框架能够显著增强物理层安全性能并有效抑制多用户干扰，为下一代无线网络提供了一种安全、可扩展的解决方案。

Abstract: In this paper, a reconfigurable intelligent surface (RIS) assisted cell free massive MIMO (CFmMIMO) framework is designed to enhance physical layer security (PLS) and mitigate multi user (MU) interference in next generation wireless networks. A channel state information (CSI) based precoder is designed at the access point (AP) to suppress MU interference, enabling interference free reception for the legitimate users. To further enhance secrecy performance, we formulate a joint optimization problem that maximizes the secrecy sum rate using an alternating optimization (AO) framework, which iteratively updates the active beamforming at the AP, user power allocation, and the RIS phase shift matrix. The highly nonconvex problem is addressed under the Riemannian manifold optimization (RMO) framework and solved using a Riemannian Conjugate Gradient (RCG) algorithm for RIS phase shift design. Simulation results verify that the proposed framework effectively enhances the secrecy sum rate and eliminates interference, demonstrating its potential for secure and scalable CFmMIMO networks in dense wireless environments.

</details>


### [37] [From Nodes to Edges: Edge-Based Laplacians for Brain Signal Processing](https://arxiv.org/abs/2512.13420)
*Andrea Santoro,Marco Nurisso,Giovanni Petri*

Main category: eess.SP

TL;DR: 该研究采用边缘中心图信号处理分析脑网络，相比传统节点方法能更好捕捉脑区间的共波动信息，在任务解码中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统图信号处理方法主要关注节点信号，无法有效捕捉脑网络中边缘的动态信息。脑区间的相互作用（边缘信号）可能包含重要的功能组织信息，需要新的分析方法。

Method: 采用边缘中心的图信号处理框架，使用一维Hodge拉普拉斯算子描述结构连接性，处理定义在边缘上的信号来捕捉脑区间的共波动信息。基于Human Connectome Project的100名无关受试者数据构建边缘信号。

Result: 边缘方法在静态和动态场景下都取得了比传统节点方法更优的任务解码准确率，揭示了脑功能组织的独特方面。

Conclusion: 边缘聚焦的图信号处理策略在深化对脑连接性和功能动态理解方面具有重要潜力，为脑网络分析提供了新视角。

Abstract: Traditional graph signal processing (GSP) methods applied to brain networks focus on signals defined on the nodes. Thus, they are unable to capture potentially important dynamics occurring on the edges. In this work, we adopt an edge-centric GSP approach to analyze edge signals constructed from 100 unrelated subjects of the Human Connectome Project. Specifically, we describe structural connectivity through the lens of the 1-dimensional Hodge Laplacian, processing signals defined on edges to capture co-fluctuation information between brain regions. We demonstrate that edge-based approaches achieve superior task decoding accuracy in static and dynamic scenarios compared to conventional node-based techniques, thereby unveiling unique aspects of brain functional organization. These findings underscore the promise of edge-focused GSP strategies for deepening our understanding of brain connectivity and functional dynamics.

</details>


### [38] [Interference Mitigation Recommender System using U-Net Autoencoders](https://arxiv.org/abs/2512.13533)
*Hiten Prakash Kothari,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 提出模块化推荐系统，根据干扰特征自动选择最佳干扰抑制策略，通过分类、预测和专用U-Net自编码器提升通信鲁棒性


<details>
  <summary>Details</summary>
Motivation: 在动态通信环境中，单一干扰抑制方法无法适应所有干扰条件，需要自适应系统根据具体干扰特征选择最有效的抑制策略

Method: 设计三阶段模块化系统：SPS分类器识别干扰参数，SIR预测器估计信干比，专用U-Net自编码器库针对不同干扰条件，推荐器基于特征选择最佳抑制模型

Result: 在不同SIR水平和调制环境下，推荐策略相比单一方法显著降低误码率，提升系统鲁棒性

Conclusion: 自适应模型选择架构能有效增强动态通信环境中的干扰抑制能力，展示了模块化推荐系统的潜力

Abstract: Building on the previous work on interference mitigation, this paper introduces a modular recommender system that automatically selects the most effective interference mitigation strategy based on the interference characteristics present in the received signal. The system integrates three key stages: an SPS classifier module, a SIR predictor, and a bank of specialized U-Net autoencoders designed for different interference conditions. The classification block identifies the parameters required for cancellation. The recommender then directs the signal to the appropriate mitigation model, optionally incorporating SIR-based decisions for scenarios where successive interference cancellation may be advantageous. Experiments conducted across diverse SIR levels and modulation environments show that the recommender strategy improves robustness and reduces BER compared to using any single mitigation method alone. The results demonstrate the potential of adaptive, model-selective architectures to enhance interference resilience in dynamic communication environments.

</details>


### [39] [On the Ability of Deep Learning to Detect Signals with Unknown Parameters](https://arxiv.org/abs/2512.13542)
*Tom Anders,Hiten Prakash Kothari,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 该论文研究在加性高斯白噪声中检测未知参数信号的深度学习应用，比较DNN方法与统计方法及匹配滤波器的性能。


<details>
  <summary>Details</summary>
Motivation: 在通信、声纳、雷达等信号处理应用中，信号检测是核心问题。当信号参数未知时，不存在最优检测器（奈曼-皮尔逊意义下），而匹配滤波器需要完美的信号先验知识。近年来DNN在假设检验问题中表现出色，因此研究DNN在原始I/Q层面的信号检测应用。

Method: 训练和评估两种机器学习算法在三种感兴趣信号模型上的检测性能。同时训练一个统一数据集模型，在所有信号类型上进行评估。目标是最大化检测概率Pd，同时保持恒定的虚警概率PFA。

Result: 论文比较了DNN方法、统计方法以及匹配滤波器在信号检测问题上的性能，评估了不同方法在三种信号模型上的表现。

Conclusion: DNN方法在原始I/Q层面的信号检测问题上具有潜力，能够与统计方法和匹配滤波器进行竞争，特别是在信号参数未知的情况下。

Abstract: In many signal processing applications, including communications, sonar, radar, and localization, a fundamental problem is the detection of a signal of interest in background noise, known as signal detection [1] [2]. A simple version of this problem is the detection of a signal of interest with unknown parameters in Additive White Gaussian Noise (AWGN). When the parameters defining the signal are not known, an optimal detector (in the Neyman-Pearson sense) does not exist. An upper bound on the performance of any detector is the matched filter, which implies perfect sample by sample knowledge of the signal of interest. In recent years Deep Neural Networks (DNNs) have proven to be very effective at hypothesis testing problems such as object detection and image classification. This paper examines the application of DNN-based approaches to the signal detection problem at the raw I/Q level and compares them to statistically based approaches as well as the Matched Filter. These methods aim to maximize the Probability of Detection Pd while maintaining a constant Probability of False Alarm PF A. Two Machine Learning (ML) algorithms are trained and assessed on this signal detection problem, across three signal of interest models. A model was also trained on a unified dataset and assessed across all signals of interest.

</details>


### [40] [A new data weighted averaging algorithm to reduce tones in the signal band](https://arxiv.org/abs/2512.13605)
*Marta Laguna,Juana M. Martínez-Heredia,Manuel G. Satué*

Main category: eess.SP

TL;DR: 该论文分析了DWA方法产生杂散音调的机制，并提出了一种改进的DWA方法来消除这些杂散音调


<details>
  <summary>Details</summary>
Motivation: 多比特Σ-Δ调制器通过降低时钟频率和增加量化器级数来平衡复杂度和速度，但多位数模转换器(DAC)会降低系统性能。现有的数据加权平均(DWA)方法虽然能减少DAC误差的敏感性，但会在信号带内产生杂散音调

Method: 分析DWA方法产生杂散音调的机制，并提出一种改进的DWA算法来消除这些杂散音调

Result: 论文分析了DWA方法产生杂散音调的具体机制，并提出了一种能够消除这些杂散音调的改进DWA方法

Conclusion: 通过改进DWA算法，可以在保持减少DAC误差敏感性的同时，消除信号带内的杂散音调，从而提高多比特Σ-Δ调制器的整体性能

Abstract: Digital/Analog converters based on sigma-delta modulation are simple and unexpensive circuits featuring a signal bandwidth limited by speed constraints. Multi-bit modulators allow balancing complexity and speed by reducing the clock frequency and increasing the number of levels in the quantizer. In this case, the multi-bit digital to analog block (DAC) can reduce the performance of the entire system. Data Weighted Averaging (DWA) methods have been proposed to reduce the vulnerability to DAC errors at the cost of spurious tones in the signal band. This work analyzes the tone producing mechanism and proposes a modification of the DWA to remove spurious tones.

</details>


### [41] [Performance Limits of Hardware-Constrained THz Inter-Satellite MIMO-ISAC Systems](https://arxiv.org/abs/2512.13652)
*Haofan Dong,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 该论文建立了太赫兹星间链路MIMO ISAC系统在恒包络约束下的理论性能极限，揭示了硬件失真导致的容量上限，并推导了感知误差随导频开销的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 太赫兹星间链路虽提供巨大带宽，但受限于星载功率和热预算。硬件失真（功放非线性、ADC量化、振荡器相位噪声）会限制系统性能，需要建立理论框架来分析这些约束下的ISAC系统极限。

Method: 采用统一的链路预算框架，整合宽带波束斜视、孔径指向误差和有色噪声源，通过谱一致性原则确保相位噪声在通信和感知分析中只计算一次。感知界限通过Whittle-Fisher信息矩阵在恒定加速度运动模型下推导。

Result: 硬件失真导致容量上限，无法通过增加发射功率克服。感知误差方差随导频开销α按α^{-5}缩放，相位噪声方差按α^{-1}缩放。存在操作不可行区域α<α*≈0.16，感知精度随阵列尺寸改善，但临界SNR与阵列尺寸无关。

Conclusion: 研究为硬件高效的太赫兹星间链路星座设计提供了指导，揭示了硬件失真导致的根本限制，并识别了操作不可行区域，对实际系统设计具有重要意义。

Abstract: Terahertz inter-satellite links (THz-ISL) offer unprecedented bandwidth for future space networks but face fundamental constraints from onboard power and thermal budgets. This paper establishes theoretical performance limits for MIMO Integrated Sensing and Communication (ISAC) systems under per-element constant-envelope (CE) transmission constraints. We demonstrate that hardware distortions -- specifically power amplifier nonlinearity, ADC quantization, and oscillator phase noise -- impose a capacity ceiling that cannot be overcome by increasing transmit power. A unified link budget framework integrates wideband beam squint, aperture pointing errors, and colored noise sources through a spectral consistency principle that ensures residual phase noise is counted exactly once across communication and sensing analyses. The sensing bounds are derived via the Whittle-Fisher Information Matrix under a Constant Acceleration kinematic model with jerk noise, yielding closed-form scaling laws: residual phase noise variance scales as $α^{-1}$ while dynamic state-estimation error (DSE) variance scales as $α^{-5}$ with pilot overhead $α$. Numerical results show divergent MIMO scaling: sensing precision improves with array size ($\mathrm{RMSE} \propto 1/\sqrt{N_t N_r}$), while the critical SNR exhibits scale invariance regarding array size, implying that the distortion-limited transition point stabilizes regardless of the array scale. The steep $α^{-5}$ DSE scaling creates an operationally infeasible region at $α< α^* \approx 0.16$, where $α^* = (C_{\mathrm{DSE}}/C_{\mathrm{PN}})^{1/4}$ -- a constraint-driven threshold under the adopted baseline for LEO operation. These findings provide design guidelines for hardware-efficient THz-ISL constellations.

</details>
