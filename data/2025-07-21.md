<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.IT](#cs.IT) [Total: 8]
- [eess.IV](#eess.IV) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections](https://arxiv.org/abs/2507.13463)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: 论文提出了一种低复杂度算法，利用2D-DFT和MUSIC方法联合估计近场UM-MIMO系统中的目标位置和速度参数，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在近场UM-MIMO系统中，目标位置和速度参数耦合，传统联合估计方法计算复杂度高，需要一种高效且准确的解决方案。

Method: 通过2D-DFT将接收信号投影到角度-多普勒域，利用谱特性提取粗略估计，再通过一维MUSIC方法细化参数。

Result: 仿真结果显示，该方法在位置和速度估计上的NMSE达到-40 dB，同时显著降低了计算复杂度。

Conclusion: 提出的方法在近场UM-MIMO系统中实现了高效且准确的目标运动参数估计。

Abstract: Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large
antenna arrays at high frequencies, transitioning communication paradigm into
the radiative near-field (NF), where spherical wavefronts enable full-vector
estimation of both target location and velocity. However, location and motion
parameters become inherently coupled in this regime, making their joint
estimation computationally demanding. To overcome this, we propose a novel
approach that projects the received two-dimensional space-time signal onto the
angle-Doppler domain using a two-dimensional discrete Fourier transform
(2D-DFT). Our analysis reveals that the resulting angular spread is centered at
the target's true angle, with its width determined by the target's range.
Similarly, transverse motion induces a Doppler spread centered at the true
radial velocity, with the width of Doppler spread proportional to the
transverse velocity. Exploiting these spectral characteristics, we develop a
low-complexity algorithm that provides coarse estimates of angle, range, and
velocity, which are subsequently refined using one-dimensional multiple signal
classification (MUSIC) applied independently to each parameter. The proposed
method enables accurate and efficient estimation of NF target motion
parameters. Simulation results demonstrate a normalized mean squared error
(NMSE) of -40 dB for location and velocity estimates compared to maximum
likelihood estimation, while significantly reducing computational complexity.

</details>


### [2] [Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing](https://arxiv.org/abs/2507.13520)
*Sizhen Bian,Mengxi Liu,Paul Lukowicz*

Main category: eess.SP

TL;DR: 本文综述了被动人体电容（HBC）传感的原理、历史、硬件架构及应用，并探讨了环境变化带来的挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 利用人体固有静电特性实现高效、非侵入式行为感知，推动下一代可穿戴和智能环境系统的发展。

Method: 综述被动HBC传感的原理、硬件架构及跨领域应用，分析环境变化的挑战并提出缓解技术。

Result: 提供了开源资源，支持传感器融合和硬件增强的未来研究，促进创新。

Conclusion: 被动HBC传感在可穿戴和智能环境领域具有潜力，需进一步研究以克服环境敏感性并优化性能。

Abstract: Passive body-area electrostatic field sensing, also referred to as human body
capacitance (HBC), is an energy-efficient and non-intrusive sensing modality
that exploits the human body's inherent electrostatic properties to perceive
human behaviors. This paper presents a focused overview of passive HBC sensing,
including its underlying principles, historical evolution, hardware
architectures, and applications across research domains. Key challenges, such
as susceptibility to environmental variation, are discussed to trigger
mitigation techniques. Future research opportunities in sensor fusion and
hardware enhancement are highlighted. To support continued innovation, this
work provides open-source resources and aims to empower researchers and
developers to leverage passive electrostatic sensing for next-generation
wearable and ambient intelligence systems.

</details>


### [3] [Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks](https://arxiv.org/abs/2507.13526)
*Gedeon Ghislain Nkwewo Ngoufo,Khaled Humadi,Elham Baladi,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 研究探讨了在集成传感与通信（ISAC）系统中使用空间移位键控（SSK）调制的性能，比较了正弦和啁啾雷达波形，验证了SSK的有效性，并强调了波形选择对传感能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）中空间碎片的增加对轨道安全构成挑战，ISAC系统通过同时实现环境传感和数据通信提供解决方案。

Method: 研究采用SSK调制，结合正弦和啁啾雷达波形，评估其性能，重点关注误码率（BER）和传感能力。

Result: 两种波形在SSK下误码率性能相当，但啁啾波形在距离估计和速度检测精度上表现更优。

Conclusion: SSK是ISAC的有效调制方案，波形选择对优化传感能力至关重要，为轨道碎片监测等空间应用提供了设计支持。

Abstract: The proliferation of space debris in low Earth orbit (LEO) presents critical
challenges for orbital safety, particularly for satellite constellations.
Integrated sensing and communication (ISAC) systems provide a promising dual
function solution by enabling both environmental sensing and data
communication. This study explores the use of space shift keying (SSK)
modulation within ISAC frameworks, evaluating its performance when combined
with sinusoidal and chirp radar waveforms. SSK is particularly attractive due
to its low hardware complexity and robust communication performance. Our
results demonstrate that both waveforms achieve comparable bit error rate (BER)
performance under SSK, validating its effectiveness for ISAC applications.
However, waveform selection significantly affects sensing capability: while the
sinusoidal waveform supports simpler implementation, its high ambiguity limits
range detection. In contrast, the chirp waveform enables range estimation and
provides a modest improvement in velocity detection accuracy. These findings
highlight the strength of SSK as a modulation scheme for ISAC and emphasize the
importance of selecting appropriate waveforms to optimize sensing accuracy
without compromising communication performance. This insight supports the
design of efficient and scalable ISAC systems for space applications,
particularly in the context of orbital debris monitoring.

</details>


### [4] [Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System](https://arxiv.org/abs/2507.13554)
*Meles Weldegebriel,Zihan Li,Dustin Maas,Greg Hellbourg,Ning Zhang,Neal Patwari*

Main category: eess.SP

TL;DR: StopSec是一种隐私保护协议，能快速识别并阻止次级用户（SU）对主用户（PU）的干扰，通过轻量级水印方法和实时反馈机制实现。


<details>
  <summary>Details</summary>
Motivation: 解决频谱共享中次级用户对主用户的干扰问题，确保快速自动停止干扰。

Method: 引入轻量级水印方法标记SU的OFDM数据包，利用数据库反馈机制实现干扰检测与停止。

Result: 实验显示干扰检测和停止延迟低于150毫秒，水印方法对SU数据链路无负面影响，且适应时变信道。

Conclusion: StopSec是一种高效的频谱共享协议，适用于需要快速自动停止干扰的场景。

Abstract: We present the design and validation of Stoppable Secondary Use (StopSec), a
privacy-preserving protocol with the capability to identify a secondary user
(SU) causing interference to a primary user (PU) and to act quickly to stop the
interference. All users are served by a database that provides a feedback
mechanism from a PU to an interfering SU. We introduce a new lightweight and
robust method to watermark an SU's OFDM packet. Through extensive over-the-air
real-time experiments, we evaluate StopSec in terms of interference detection,
identification, and stopping latency, as well as impact on SUs. We show that
the watermarking method avoids negative impact to the secondary data link and
is robust to real-world time-varying channels. Interfering SUs can be stopped
in under 150 milliseconds, and when multiple users are simultaneously
interfering, they can all be stopped. Even when the interference is 10 dB lower
than the noise power, StopSec successfully stops interfering SUs within a few
seconds of their appearance in the channel. StopSec can be an effective
spectrum sharing protocol for cases when interference to a PU must be quickly
and automatically stopped.

</details>


### [5] [Towards channel foundation models (CFMs): Motivations, methodologies and opportunities](https://arxiv.org/abs/2507.13637)
*Jun Jiang,Yuan Gao,Xinyi Wu,Shugong Xu*

Main category: eess.SP

TL;DR: 本文提出了一种名为“信道基础模型（CFMs）”的新框架，通过预训练的通用信道特征提取器解决多种信道相关任务，克服传统AI模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在无线通信系统中存在依赖标注数据、泛化能力有限和任务特定设计等问题，CFMs旨在通过自监督学习和大规模未标注数据解决这些问题。

Method: 利用先进的AI架构和自监督学习技术，CFMs通过预训练的通用特征提取器处理多种信道任务，无需大量人工标注。

Result: CFMs能够有效利用未标注数据，减少对标注数据的依赖，并提升泛化能力。

Conclusion: CFMs为信道相关任务提供了统一框架，未来研究方向包括模型架构创新和高质量多样化数据集的构建。

Abstract: Artificial intelligence (AI) has emerged as a pivotal enabler for
next-generation wireless communication systems. However, conventional AI-based
models encounter several limitations, such as heavy reliance on labeled data,
limited generalization capability, and task-specific design. To address these
challenges, this paper introduces, for the first time, the concept of channel
foundation models (CFMs)-a novel and unified framework designed to tackle a
wide range of channel-related tasks through a pretrained, universal channel
feature extractor. By leveraging advanced AI architectures and self-supervised
learning techniques, CFMs are capable of effectively exploiting large-scale
unlabeled data without the need for extensive manual annotation. We further
analyze the evolution of AI methodologies, from supervised learning and
multi-task learning to self-supervised learning, emphasizing the distinct
advantages of the latter in facilitating the development of CFMs. Additionally,
we provide a comprehensive review of existing studies on self-supervised
learning in this domain, categorizing them into generative, discriminative and
the combined paradigms. Given that the research on CFMs is still at an early
stage, we identify several promising future research directions, focusing on
model architecture innovation and the construction of high-quality, diverse
channel datasets.

</details>


### [6] [Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets](https://arxiv.org/abs/2507.13748)
*Patrick Matalla,Joel Dittmer,Md Salek Mahmud,Christian Koos,Sebastian Randel*

Main category: eess.SP

TL;DR: 提出一种弹性缓冲设计，支持全数字时钟恢复，实现自由运行的接收器时钟，并允许正负时钟频率偏移。


<details>
  <summary>Details</summary>
Motivation: 解决传统时钟恢复方法无法处理正负频率偏移的问题。

Method: 采用弹性缓冲设计，结合自由运行的接收器时钟。

Result: 在-400 ppm至+400 ppm范围内实现无误码实时数据传输。

Conclusion: 该设计为全数字时钟恢复提供了一种高效解决方案。

Abstract: We present an elastic buffer design that enables all-digital clock recovery
implementation with free-running receiver clock featuring negative and positive
clock frequency offsets. Error-free real-time data transmission is demonstrated
from -400 ppm to +400 ppm.

</details>


### [7] [ISAC: From Human to Environmental Sensing](https://arxiv.org/abs/2507.13766)
*Kai Wu,Zhongqin Wang,Shu-Lin Chen,J. Andrew Zhang,Y. Jay Guo*

Main category: eess.SP

TL;DR: 本文综述了集成感知与通信（ISAC）在6G无线通信系统中的潜力，统一分析了其在人类活动和环境监测中的应用，包括信号机制、感知特征及实际可行性。


<details>
  <summary>Details</summary>
Motivation: 探索ISAC在6G系统中的潜力，解决现有研究在人类活动和环境监测领域的碎片化问题。

Method: 通过分析无线信号传播中的物理现象（如人类活动和环境变化）对信道状态信息（CSI）、多普勒谱和信号统计的影响，提出统一的信号级机制。

Result: 实验结果表明，在非视距（NLOS）条件下，ISAC在基础设施有限场景中具有可行性。

Conclusion: 未来研究需解决信号融合、领域适应和通用感知架构等挑战，以实现可扩展和自主的ISAC。

Abstract: Integrated Sensing and Communications (ISAC) is poised to become one of the
defining capabilities of the sixth generation (6G) wireless communications
systems, enabling the network infrastructure to jointly support high-throughput
communications and situational awareness. While recent advances have explored
ISAC for both human-centric applications and environmental monitoring, existing
research remains fragmented across these domains. This paper provides the first
unified review of ISAC-enabled sensing for both human activities and
environment, focusing on signal-level mechanisms, sensing features, and
real-world feasibility. We begin by characterising how diverse physical
phenomena, ranging from human vital sign and motion to precipitation and flood
dynamics, impact wireless signal propagation, producing measurable signatures
in channel state information (CSI), Doppler profiles, and signal statistics. A
comprehensive analysis is then presented across two domains: human sensing
applications including localisation, activity recognition, and vital sign
monitoring; and environmental sensing for rainfall, soil moisture, and water
level. Experimental results from Long-Term Evolution (LTE) sensing under
non-line-of-sight (NLOS) conditions are incorporated to highlight the
feasibility in infrastructure-limited scenarios. Open challenges in signal
fusion, domain adaptation, and generalisable sensing architectures are
discussed to facilitate future research toward scalable and autonomous ISAC.

</details>


### [8] [Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis](https://arxiv.org/abs/2507.13826)
*Kimitaka Sumi,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出了一种基于深度相机获取的人体几何和位移数据模拟呼吸监测中频率调制连续波雷达信号的方法，相比传统模型，显著提高了雷达图像、位移和频谱图的相关系数。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖简化的人体几何或位移模型，无法准确模拟高频散射中心，本研究旨在通过真实深度相机数据提升模拟精度。

Method: 利用深度相机获取的人体形状和运动数据建模高频散射中心，并在不同条件下进行实验验证。

Result: 相比传统方法，相关系数在雷达图像、位移和频谱图上分别提高了7.5%、58.2%和3.2%。

Conclusion: 该方法通过模拟生成雷达生理数据集，提高了非接触传感的准确性，并加深了对影响因素的理解。

Abstract: This study proposes a method for simulating signals received by
frequency-modulated continuous-wave radar during respiratory monitoring, using
human body geometry and displacement data acquired via a depth camera. Unlike
previous studies that rely on simplified models of body geometry or
displacement, the proposed approach models high-frequency scattering centers
based on realistic depth-camera-measured body shapes and motions. Experiments
were conducted with six participants under varying conditions, including
varying target distances, seating orientations, and radar types, with
simultaneous acquisition from the radar and depth camera. Relative to
conventional model-based methods, the proposed technique achieved improvements
of 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,
displacements, and spectrograms, respectively. This work contributes to the
generation of radar-based physiological datasets through simulation and
enhances our understanding of factors affecting the accuracy of non-contact
sensing.

</details>


### [9] [On two fundamental properties of the zeros of spectrograms of noisy signals](https://arxiv.org/abs/2507.13829)
*Arnaud Poinas,Rémi Bardenet*

Main category: eess.SP

TL;DR: 论文研究了信号加入高斯白噪声后，谱图零点的空间分布变化，发现零点会勾勒出信号的支撑区域，并在干扰下形成确定性结构。通过简单信号的计算，论文用零点密度和Rouché定理解释了这种现象。


<details>
  <summary>Details</summary>
Motivation: 探讨信号在噪声背景下谱图零点的分布变化，以及零点如何勾勒信号支撑区域和形成确定性结构，填补了现有方法缺乏理论支持的空白。

Method: 通过简单信号的计算，结合零点密度和Rouché定理，分析零点分布的变化及其与信号参数（如信噪比）的关系。

Result: 研究发现，即使在干扰下，零点也能形成易于检测的确定性结构，尤其是对于叠加的线性调频信号。

Conclusion: 零点密度和Rouché定理为谱图零点的分布现象提供了理论支持，为信号检测提供了新思路。

Abstract: The spatial distribution of the zeros of the spectrogram is significantly
altered when a signal is added to white Gaussian noise. The zeros tend to
delineate the support of the signal, and deterministic structures form in the
presence of interference, as if the zeros were trapped. While sophisticated
methods have been proposed to detect signals as holes in the pattern of
spectrogram zeros, few formal arguments have been made to support the
delineation and trapping effects. Through detailed computations for simple toy
signals, we show that two basic mathematical arguments, the intensity of zeros
and Rouch\'e's theorem, allow discussing delineation and trapping, and the
influence of parameters like the signal-to-noise ratio. In particular,
interfering chirps, even nearly superimposed, yield an easy-to-detect
deterministic structure among zeros.

</details>


### [10] [Device-Free Localization Using Commercial UWB Transceivers](https://arxiv.org/abs/2507.13938)
*Hyun Seok Lee*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的粒子滤波方法，用于提高超宽带（UWB）设备自由定位的准确性，解决了低信噪比和环境干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有UWB设备自由定位技术在低信噪比和复杂环境中难以准确估计目标位置，需要一种更高效的方法。

Method: 结合深度学习（DL）和粒子滤波，首先分析信道脉冲响应（CIR）方差，利用注意力U-Net提取目标反射成分并抑制噪声，最后通过粒子滤波估计位置。

Result: 实验结果显示，该方法在RMSE约为15厘米、平均处理时间4毫秒的情况下表现优异，优于现有方法。

Conclusion: 该方法为物联网和汽车应用提供了一种实用且经济高效的解决方案。

Abstract: Recently, commercial ultra-wideband (UWB) transceivers have enabled not only
measuring device-to-device distance but also tracking the position of a
pedestrian who does not carry a UWB device. UWB-based device-free localization
that does not require dedicated radar equipment is compatible with existing
anchor infrastructure and can be reused to reduce hardware deployment costs.
However, it is difficult to estimate the target's position accurately in
real-world scenarios due to the low signal-to-noise ratio (SNR) and the
cluttered environment. In this paper, we propose a deep learning (DL)-assisted
particle filter to overcome these challenges. First, the channel impulse
response (CIR) variance is analyzed to capture the variability induced by the
target's movement. Then, a DL-based one-dimensional attention U-Net is used to
extract only the reflection components caused by the target and suppress the
noise components within the CIR variance profile. Finally, multiple
preprocessed CIR variance profiles are used as input to a particle filter to
estimate the target's position. Experimental results demonstrate that the
proposed system is a practical and cost-effective solution for IoT and
automotive applications with a root mean square error (RMSE) of about 15 cm and
an average processing time of 4 ms. Furthermore, comparisons with existing
state-of-the-art methods show that the proposed method provides the best
performance with reasonable computational costs.

</details>


### [11] [Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication](https://arxiv.org/abs/2507.14018)
*Zeyuan Zhang,Yue Xiu,Phee Lep Yeoh,Guangyi Liu,Zixing Wu,Ning Wei*

Main category: eess.SP

TL;DR: 论文研究了带有非线性功率放大失真的部分连接混合波束成形发射机，用于集成感知与通信（ISAC），提出了一种失真感知的混合波束成形设计方法。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中非线性功率放大失真对通信速率和感知互信息的影响。

Method: 通过流形优化（MO）和闭式解交替求解三个子问题，得到全数字波束成形矩阵，再通过分解算法获得模拟和数字波束成形矩阵。

Result: 数值结果表明，所提算法相比传统波束成形方法能提升ISAC整体性能。

Conclusion: 提出的失真感知混合波束成形设计有效提升了ISAC系统的性能。

Abstract: This paper investigates a practical partially-connected hybrid beamforming
transmitter for integrated sensing and communication (ISAC) with distortion
from nonlinear power amplification. For this ISAC system, we formulate a
communication rate and sensing mutual information maximization problem driven
by our distortion-aware hybrid beamforming design. To address this non-convex
problem, we first solve for a fully digital beamforming matrix by alternatively
solving three sub-problems using manifold optimization (MO) and our derived
closed-form solutions. The analog and digital beamforming matrices are then
obtained through a decomposition algorithm. Numerical results demonstrate that
the proposed algorithm can improve overall ISAC performance compared to
traditional beamforming methods.

</details>


### [12] [Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming](https://arxiv.org/abs/2507.14035)
*Sai Xu,Kai-Kit Wong,Yanan Du,Hanjiang Hong,Chan-Byoung Chae,Baiyang Liu,Kin-Fai Tong*

Main category: eess.SP

TL;DR: 本文提出了一种硬件-软件协同设计方法，用于优化流体天线系统中的波束成形和端口选择。通过结合图神经网络和随机端口选择，以及基于FPGA的深度学习加速器，实现了高效性能和低延迟。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统（FASs）在波束成形和端口选择方面的优化需求，以及如何通过硬件-软件协同设计提升效率。

Method: 1. 建模FA-enabled多小区MIMO网络，并构建加权和速率最大化问题；2. 提出结合GNN和RPS的方法优化波束成形和端口选择；3. 开发基于FPGA的深度学习加速器以减少推理延迟；4. 引入调度算法减少冗余计算和空闲时间。

Result: 仿真显示GNN-RPS方法在通信性能上具有竞争力；实验评估表明FPGA加速器在低延迟下同时执行多端口选择的波束成形推理。

Conclusion: 硬件-软件协同设计方法在流体天线系统中有效优化了波束成形和端口选择，同时实现了高性能和低延迟。

Abstract: This paper proposes a hardware-software co-design approach to efficiently
optimize beamforming and port selection in fluid antenna systems (FASs). To
begin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input
multiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)
maximization problem is formulated. Second, a method that integrates graph
neural networks (GNNs) with random port selection (RPS) is proposed to jointly
optimize beamforming and port selection, while also assessing the benefits and
limitations of random selection. Third, an instruction-driven deep learning
accelerator based on a field-programmable gate array (FPGA) is developed to
minimize inference latency. To further enhance efficiency, a scheduling
algorithm is introduced to reduce redundant computations and minimize the idle
time of computing cores. Simulation results demonstrate that the proposed
GNN-RPS approach achieves competitive communication performance. Furthermore,
experimental evaluations indicate that the FPGA-based accelerator maintains low
latency while simultaneously executing beamforming inference for multiple port
selections.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [13] [Round-Preserving Asymptotic Compression of Prior-Free Interactive Protocols](https://arxiv.org/abs/2507.13464)
*Gurleen Padda,Dave Touchette*

Main category: cs.IT

TL;DR: 本文研究了通信复杂性和信息复杂性之间的关系，提供了一个更自然的证明方法，并改进了Braverman的结果，实现了轮次保留和有限共享随机性。


<details>
  <summary>Details</summary>
Motivation: 探索通信复杂性和信息复杂性之间的关系，特别是在无先验和交互式设置下，提供一个更自然的证明方法。

Method: 通过估计输入的联合类型或经验分布，设计协议实现无先验反向香农定理，并推广到交互式设置。

Result: 改进了Braverman的结果，实现了轮次保留和有限共享随机性。

Conclusion: 本文提供了一个更自然的证明方法，并改进了现有结果，为通信复杂性和信息复杂性的关系研究提供了新视角。

Abstract: There is a close relationship between the communication complexity and
information complexity of communication problems, as demonstrated by results
such as Shannon's noiseless source coding theorem, and the Slepian-Wolf
theorem. Here, we study this relationship in the prior-free and interactive
setting, where we provide an alternate proof for the result of Braverman [SIAM
Review, vol. 59, no. 4, 2017], that the amortized communication complexity of
simulating a prior-free interactive communication protocol, is equal to its
prior-free information cost. While this is a known result, our approach
addresses the need for a more natural proof of it. We also improve on the
result by achieving round preservation, and using a bounded quantity of shared
randomness. We do this by showing that the communicating parties can produce a
reliable estimate of the joint type, or empirical distribution, of their
inputs. This estimate is then used in our protocol for the prior-free reverse
Shannon theorem with side information at the receiver. These results are then
generalized to the interactive setting to obtain our main result.

</details>


### [14] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: 论文提出了一个框架，用于对Kolmogorov结构函数进行对偶化，并引入统计力学类比，证明了Legendre-Fenchel对偶性，验证了模型复杂度与泛化能力的相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过计算复杂度代理对Kolmogorov结构函数进行对偶化，并探索信息论与统计力学之间的数学类比。

Method: 引入分区函数和自由能泛函，证明Legendre-Fenchel对偶性，并通过Metropolis核的详细平衡解释接受概率。

Result: 理论预测通过线性和树基回归模型实验验证，展示了模型复杂度与泛化能力的相互作用。

Conclusion: 框架成功将信息论与统计力学联系起来，为模型复杂度与泛化能力的研究提供了新视角。

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


### [15] [Efficient Decoding of Double-circulant and Wozencraft Codes from Square-root Errors](https://arxiv.org/abs/2507.13548)
*Oren Dubin,Noam Oz,Noga Ron-Zewi*

Main category: cs.IT

TL;DR: 提出了两种双循环码的高效解码算法，并讨论了其转化为Wozencraft码的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究双循环码的解码效率及其转化为其他码的能力。

Method: 基于Sidon集和循环码的双循环码解码算法，并分析其转化为Wozencraft码的可行性。

Result: 成功将基于Sidon集的双循环码转化为高效解码的Wozencraft码，但基于循环码的转化存在限制。

Conclusion: 双循环码的解码算法高效，且部分可转化为Wozencraft码，但需注意转化限制。

Abstract: We present efficient decoding algorithms from square-root errors for two
known families of double-circulant codes: A construction based on Sidon sets
(Bhargava, Taveres, and Shiva, \emph{IEEE IT 74}; Calderbank, \emph{IEEE IT
83}; Guruswami and Li, \emph{IEEE IT 2025}), and a construction based on cyclic
codes (Chen, Peterson, and Weldon, \emph{Information and Control 1969}). We
further observe that the work of Guruswami and Li implicitly gives a
transformation from double-circulant codes of certain block lengths to
Wozencraft codes which preserves that distance of the codes, and we show that
this transformation also preserves efficiency of decoding. By instantiating
this transformation with the first family of double-circulant codes based on
Sidon sets, we obtain an explicit construction of a Wozencraft code that is
efficiently decodable from square-root errors. We also discuss limitations on
instantiating this transformation with the second family of double-circulant
codes based on cyclic codes.

</details>


### [16] [Density Evolution Analysis of Sparse-Block IDMA](https://arxiv.org/abs/2507.13689)
*Jean-Francois Chamberland,Gianluigi Liva,Krishna Narayanan*

Main category: cs.IT

TL;DR: SB-IDMA是一种新型无源多址协议，旨在提升5G新空口标准的两步随机接入协议性能，并通过密度进化分析其接收机性能。


<details>
  <summary>Details</summary>
Motivation: 改进3GPP 5G新空口标准中的无授权两步随机接入协议性能。

Method: 采用密度进化分析SB-IDMA的连续干扰消除接收机。

Result: 提供了SB-IDMA接收机性能的理论表征。

Conclusion: SB-IDMA通过理论分析验证了其在提升5G随机接入性能方面的潜力。

Abstract: Sparse block interleaver division multiple access (SB-IDMA) is a recently
introduced unsourced multiple access protocol that aims to improve the
performance of the grant-free two-step random access transmission protocol of
the 3GPP 5G New Radio standard. We introduced a density evolution analysis of
the successive interference cancellation receiver of SB-IDMA, providing a
theoretical characterization of its performance.

</details>


### [17] [Asymptotically Optimal Codes Correcting One Substring Edit](https://arxiv.org/abs/2507.13808)
*Yuting Li,Yuanyuan Tang,Hao Lou,Ryan Gabrys,Farzad Farnoud*

Main category: cs.IT

TL;DR: 本文构造了纠正一个子串编辑错误的编码，其冗余度为log n + O(log log n)，达到了渐近最优。


<details>
  <summary>Details</summary>
Motivation: 研究子串编辑错误的纠正问题，目标是减少编码冗余度，使其接近理论下限。

Method: 通过构造一种新的编码方案，能够纠正一个子串编辑错误，其中子串长度受限于常数k。

Result: 提出的编码方案冗余度为log n + O(log log n)，优于之前的结果log n + k。

Conclusion: 该编码方案在冗余度上达到了渐近最优，为子串编辑错误的纠正提供了高效解决方案。

Abstract: The substring edit error is the operation of replacing a substring $u$ of $x$
with another string $v$, where the lengths of $u$ and $v$ are bounded by a
given constant $k$. It encompasses localized insertions, deletions, and
substitutions within a window. Codes correcting one substring edit have
redundancy at least $\log n+k$. In this paper, we construct codes correcting
one substring edit with redundancy $\log n+O(\log \log n)$, which is
asymptotically optimal.

</details>


### [18] [Secretive Hotplug Coded Caching](https://arxiv.org/abs/2507.13961)
*Mallikharjuna Chinnapadamala,Charul Rajput,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 本文研究了热插拔编码缓存模型中的保密性问题，提出了两种针对已知HpPDA类别的保密方案，并在某些内存区域中优于基线方案。


<details>
  <summary>Details</summary>
Motivation: 研究热插拔编码缓存系统中用户离线时的保密性问题，确保用户无法通过缓存内容或服务器传输获取非请求文件的信息。

Method: 提出了两种针对已知HpPDA类别的保密方案，并与基线方案（基于经典编码缓存模型的保密方案）进行比较。

Result: 数值结果表明，在某些内存区域中，提出的方案优于基线方案。

Conclusion: 本文提出的保密方案在热插拔编码缓存系统中具有优势，特别是在特定内存条件下表现更佳。

Abstract: In this work, we consider a coded caching model called \textit{hotplug coded
caching}, in which some users are offline during the delivery phase. The
concept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching
systems has been introduced in the literature, and two classes of HpPDAs are
known. In this paper, we consider a secrecy constraint in hotplug coded caching
setup, where users should not learn anything about any file from their cache
content, and active users should not gain any information about files other
than their demanded file from either their cache content or the server
transmissions. We propose two secretive schemes for the two classes of HpPDAs
and compare them with a baseline scheme, which is a secretive scheme using PDAs
for the classical coded caching setup and can be trivially adapted for the
hotplug coded caching setup. We numerically show that our schemes outperform
the baseline scheme in certain memory regions.

</details>


### [19] [Bounds and Constructions of High-Memory Spatially-Coupled Codes](https://arxiv.org/abs/2507.14064)
*Lei Huang*

Main category: cs.IT

TL;DR: 本文利用Clique Lovász局部引理，为空间耦合（SC）码中移除有害组合结构提供了充分条件，并提出了一种基于Moser-Tardos算法的构造性方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升空间耦合码的解码性能，通过移除有害组合结构并确保性能可预测。

Method: 应用Clique Lovász局部引理，结合Moser-Tardos算法构造性方法，分析有害结构间的依赖关系。

Result: 给出了移除有害结构后剩余结构概率变化的上界，例如移除4-cycles会使6-cycles活跃概率最多增加e^(8/3)倍。

Conclusion: 通过理论分析和算法实现，为空间耦合码的性能优化提供了有效工具。

Abstract: In this paper, we apply the Clique Lov\'asz Local Lemma to provide sufficient
conditions on memory and lifting degree for removing certain harmful
combinatorial structures in spatially-coupled (SC) codes that negatively impact
decoding performance. Additionally, we present, for the first time, a
constructive algorithm based on the Moser-Tardos algorithm that ensures
predictable performance. Furthermore, leveraging the properties of
LLL-distribution and M-T-distribution, we establish the dependencies among the
harmful structures during the construction process. We provide upper bounds on
the probability change of remaining harmful structures after eliminating some
of them. In particular, the elimination of 4-cycles increases the probability
of 6-cycles becoming active by at most a factor of $e^{8/3}$.

</details>


### [20] [Error Correcting Codes for Segmented Burst-Deletion Channels](https://arxiv.org/abs/2507.14070)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 研究了分段突发删除信道，开发了适用于任意字母表的纠错码，冗余度与段长度对数成正比。


<details>
  <summary>Details</summary>
Motivation: 现实中的同步错误通常以突发形式出现，因此需要研究分段突发删除信道。

Method: 利用现有的一突发删除码对每段输入子序列编码，并添加约束以帮助解码器识别段边界。

Result: 所提出的代码冗余度为O(log b)，其中b为段长度。

Conclusion: 成功开发了适用于分段突发删除信道的纠错码，冗余度优化。

Abstract: We study segmented burst-deletion channels motivated by the observation that
synchronization errors commonly occur in a bursty manner in real-world
settings. In this channel model, transmitted sequences are implicitly divided
into non-overlapping segments, each of which may experience at most one burst
of deletions. In this paper, we develop error correction codes for segmented
burst-deletion channels over arbitrary alphabets under the assumption that each
segment may contain only one burst of t-deletions. The main idea is to encode
the input subsequence corresponding to each segment using existing one-burst
deletion codes, with additional constraints that enable the decoder to identify
segment boundaries during the decoding process from the received sequence. The
resulting codes achieve redundancy that scales as O(log b), where b is the
length of each segment.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [21] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: 本文研究了Vision Mamba模型中图像扫描顺序对MRI分割性能的影响，提出了Multi-Scan 2D模块，并通过大规模实验验证了扫描顺序的显著性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，扫描顺序可能影响模型性能，但此前未被系统研究。

Method: 引入Multi-Scan 2D模块，评估21种扫描策略在三个公共数据集上的表现。

Result: 扫描顺序对性能有显著影响（Friedman检验：p=0.0016），性能差异可达27 Dice点。

Conclusion: 扫描顺序是无需额外成本的重要超参数，推荐使用空间连续的扫描路径。

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [22] [Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning](https://arxiv.org/abs/2507.13394)
*Akhil John Thomas,Christiaan Boerkamp*

Main category: eess.IV

TL;DR: 该研究提出了一种基于DeepLabV3的优化分割方法，通过自动阈值微调和参数优化，显著提高了神经分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经分割在医学影像中对精确识别神经结构至关重要，但现有方法存在精度不足的问题。

Method: 采用DeepLabV3架构，结合自动阈值微调和参数优化，改进预处理步骤。

Result: 在超声神经影像上实现了Dice Score 0.78、IoU 0.70和Pixel Accuracy 0.95，显著优于基线模型。

Conclusion: 研究表明，定制化参数选择对自动化神经检测至关重要。

Abstract: Nerve segmentation is crucial in medical imaging for precise identification
of nerve structures. This study presents an optimized DeepLabV3-based
segmentation pipeline that incorporates automated threshold fine-tuning to
improve segmentation accuracy. By refining preprocessing steps and implementing
parameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a
Pixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate
significant improvements over baseline models and highlight the importance of
tailored parameter selection in automated nerve detection.

</details>


### [23] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: 本文介绍了一种通过合成多样化数据训练深度学习模型的方法，以解决神经影像分析中模型泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 神经影像分析中，训练数据集的局限性导致模型泛化能力差，尤其是在MRI等图像外观差异大的领域。

Method: 采用域随机化策略，通过合成随机强度和内容的图像训练模型，无需重新训练或微调即可处理未见过的图像类型。

Result: 该方法在多种影像模态中表现出色，包括MRI、CT、PET等，并扩展到非神经影像领域。

Conclusion: 合成驱动训练范式提高了模型的泛化能力和抗过拟合性，但需权衡计算资源需求，有望加速通用工具的研发。

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [24] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: BreastSegNet是一种多标签分割算法，用于乳腺MRI，覆盖九种解剖结构，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺MRI分割方法仅关注少数解剖结构，限制了定量分析的实用性。

Method: 开发了BreastSegNet，并手动标注了1123张MRI切片，评估了九种分割模型。

Result: nnU-Net ResEncM表现最佳，平均Dice得分为0.694，部分结构得分接近0.90。

Conclusion: BreastSegNet显著提升了乳腺MRI分割的全面性和准确性，代码和数据将公开。

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [25] [Converting T1-weighted MRI from 3T to 7T quality using deep learning](https://arxiv.org/abs/2507.13782)
*Malo Gicquel,Ruoyi Zhao,Anika Wuestefeld,Nicola Spotorno,Olof Strandberg,Kalle Åström,Yu Xiao,Laura EM Wisse,Danielle van Westen,Rik Ossenkoppele,Niklas Mattsson-Carlgren,David Berron,Oskar Hansson,Gabrielle Flood,Jacob Vogel*

Main category: eess.IV

TL;DR: 利用深度学习模型从3T MRI合成7T MRI，提升图像质量和分割效果，且不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 7T MRI提供更高分辨率和组织对比度，但普及性较低。通过合成7T MRI，可以弥补这一不足。

Method: 训练了两种模型：专用U-Net和集成GAN的U-Net，使用172名参与者的配对3T和7T T1加权图像。

Result: 合成7T图像在细节上接近真实7T图像，主观视觉质量更优，且自动分割效果更好。下游任务性能与真实3T图像相似。

Conclusion: 合成7T MRI可提升图像质量和分割效果，未来有望用于临床。

Abstract: Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides
detailed anatomical views, offering better signal-to-noise ratio, resolution
and tissue contrast than 3T MRI, though at the cost of accessibility. We
present an advanced deep learning model for synthesizing 7T brain MRI from 3T
brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172
participants (124 cognitively unimpaired, 48 impaired) from the Swedish
BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:
a specialized U-Net, and a U-Net integrated with a generative adversarial
network (GAN U-Net). Our models outperformed two additional state-of-the-art
3T-to-7T models in image-based evaluation metrics. Four blinded MRI
professionals judged our synthetic 7T images as comparable in detail to real 7T
images, and superior in subjective visual quality to 7T images, apparently due
to the reduction of artifacts. Importantly, automated segmentations of the
amygdalae of synthetic GAN U-Net 7T images were more similar to manually
segmented amygdalae (n=20), than automated segmentations from the 3T images
that were used to synthesize the 7T images. Finally, synthetic 7T images showed
similar performance to real 3T images in downstream prediction of cognitive
status using MRI derivatives (n=3,168). In all, we show that synthetic
T1-weighted brain images approaching 7T quality can be generated from 3T
images, which may improve image quality and segmentation, without compromising
performance in downstream tasks. Future directions, possible clinical use
cases, and limitations are discussed.

</details>


### [26] [Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation](https://arxiv.org/abs/2507.13830)
*Maximilian Rokuss,Benjamin Hamm,Yannick Kirchhoff,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: 首个公开的乳腺MRI数据集，包含超过13,000例标注的左右乳腺分割标签，并附带一个深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 填补乳腺MRI分析中的关键空白，为女性健康领域提供资源。

Method: 提供标注数据集并训练深度学习模型用于左右乳腺分割。

Result: 数据集和模型已公开，可用于开发高级工具。

Conclusion: 该研究为乳腺MRI分析提供了重要资源和工具。

Abstract: We introduce the first publicly available breast MRI dataset with explicit
left and right breast segmentation labels, encompassing more than 13,000
annotated cases. Alongside this dataset, we provide a robust deep-learning
model trained for left-right breast segmentation. This work addresses a
critical gap in breast MRI analysis and offers a valuable resource for the
development of advanced tools in women's health. The dataset and trained model
are publicly available at: www.github.com/MIC-DKFZ/BreastDivider

</details>


### [27] [Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive](https://arxiv.org/abs/2507.13901)
*Lei Xu,Torkel B Brismar*

Main category: eess.IV

TL;DR: AnatomyArchive是一个基于TotalSegmentator的CT图像分析工具，提供自动目标体积选择、解剖结构管理、体素特征提取等功能，支持2D和3D分析，并开源供研究和教育使用。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效、自动化的CT图像分析工具，以支持精准的身体成分分析和医学图像数据库管理。

Method: 基于知识图谱的解剖结构管理，结合GPU加速的算法，实现自动体积裁剪、特征提取和可视化。

Result: 提供了功能全面的软件架构，支持从图像处理到统计分析的完整工作流程。

Conclusion: AnatomyArchive是一个强大的工具，适用于医学图像分析和机器学习模型开发，未来将开源供研究和教育使用。

Abstract: We have developed a novel CT image analysis package named AnatomyArchive,
built on top of the recent full body segmentation model TotalSegmentator. It
provides automatic target volume selection and deselection capabilities
according to user-configured anatomies for volumetric upper- and lower-bounds.
It has a knowledge graph-based and time efficient tool for anatomy segmentation
mask management and medical image database maintenance. AnatomyArchive enables
automatic body volume cropping, as well as automatic arm-detection and
exclusion, for more precise body composition analysis in both 2D and 3D
formats. It provides robust voxel-based radiomic feature extraction, feature
visualization, and an integrated toolchain for statistical tests and analysis.
A python-based GPU-accelerated nearly photo-realistic segmentation-integrated
composite cinematic rendering is also included. We present here its software
architecture design, illustrate its workflow and working principle of
algorithms as well provide a few examples on how the software can be used to
assist development of modern machine learning models. Open-source codes will be
released at https://github.com/lxu-medai/AnatomyArchive for only research and
educational purposes.

</details>


### [28] [Blind Super Resolution with Reference Images and Implicit Degradation Representation](https://arxiv.org/abs/2507.13915)
*Huu-Phu Do,Po-Chih Hu,Hao-Chien Hsueh,Che-Kai Liu,Vu-Hoang Tran,Ching-Chun Huang*

Main category: eess.IV

TL;DR: 论文提出了一种基于参考图像的方法，通过结合模糊核和缩放因子，改进盲超分辨率任务。


<details>
  <summary>Details</summary>
Motivation: 现有盲超分辨率方法仅从低分辨率图像估计退化核，忽略了缩放因子的影响，导致性能受限。

Method: 利用高分辨率参考图像与目标低分辨率图像，自适应学习退化过程，生成额外的LR-HR对以提升性能。

Result: 该方法在训练和零样本盲超分辨率任务中均优于现有方法。

Conclusion: 结合模糊核、缩放因子和参考图像的方法显著提升了盲超分辨率的效果。

Abstract: Previous studies in blind super-resolution (BSR) have primarily concentrated
on estimating degradation kernels directly from low-resolution (LR) inputs to
enhance super-resolution. However, these degradation kernels, which model the
transition from a high-resolution (HR) image to its LR version, should account
for not only the degradation process but also the downscaling factor. Applying
the same degradation kernel across varying super-resolution scales may be
impractical. Our research acknowledges degradation kernels and scaling factors
as pivotal elements for the BSR task and introduces a novel strategy that
utilizes HR images as references to establish scale-aware degradation kernels.
By employing content-irrelevant HR reference images alongside the target LR
image, our model adaptively discerns the degradation process. It is then
applied to generate additional LR-HR pairs through down-sampling the HR
reference images, which are keys to improving the SR performance. Our
reference-based training procedure is applicable to proficiently trained blind
SR models and zero-shot blind SR methods, consistently outperforming previous
methods in both scenarios. This dual consideration of blur kernels and scaling
factors, coupled with the use of a reference image, contributes to the
effectiveness of our approach in blind super-resolution tasks.

</details>


### [29] [Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images](https://arxiv.org/abs/2507.13974)
*Jiaqi Lv,Yijie Zhu,Carmen Guadalupe Colin Tenorio,Brinder Singh Chohan,Mark Eastwood,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的新方法，用于黑色素瘤H&E图像中五种组织类别的分割，结合病理基础模型Virchow2和Efficient-UNet，取得了PUMA Grand Challenge的第一名。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤的组织形态学特征对预后和治疗至关重要，但手动分割耗时且易受主观影响，因此需要可靠的自动化方法。

Method: 使用Virchow2病理基础模型作为特征提取器，融合原始RGB图像，再通过Efficient-UNet编码器-解码器网络进行分割。

Result: 模型在PUMA Grand Challenge的组织分割任务中排名第一，表现出色且具有泛化能力。

Conclusion: 结合病理基础模型的分割网络能有效加速计算病理学工作流程。

Abstract: Melanoma is an aggressive form of skin cancer with rapid progression and high
metastatic potential. Accurate characterisation of tissue morphology in
melanoma is crucial for prognosis and treatment planning. However, manual
segmentation of tissue regions from haematoxylin and eosin (H&E) stained
whole-slide images (WSIs) is labour-intensive and prone to inter-observer
variability, this motivates the need for reliable automated tissue segmentation
methods. In this study, we propose a novel deep learning network for the
segmentation of five tissue classes in melanoma H&E images. Our approach
leverages Virchow2, a pathology foundation model trained on 3.1 million
histopathology images as a feature extractor. These features are fused with the
original RGB images and subsequently processed by an encoder-decoder
segmentation network (Efficient-UNet) to produce accurate segmentation maps.
The proposed model achieved first place in the tissue segmentation task of the
PUMA Grand Challenge, demonstrating robust performance and generalizability.
Our results show the potential and efficacy of incorporating pathology
foundation models into segmentation networks to accelerate computational
pathology workflows.

</details>


### [30] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: OrthoInsight是一个多模态深度学习框架，用于肋骨骨折诊断和报告生成，结合了YOLOv9、医学知识图谱和LLaVA语言模型，性能优于GPT-4和Claude-3。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据增长需要自动化诊断工具，手动分析耗时且易出错。

Method: 整合YOLOv9检测骨折、医学知识图谱提供临床背景、LLaVA生成报告，结合视觉和文本数据。

Result: 在28,675张CT图像上表现优异，平均得分4.28，超越GPT-4和Claude-3。

Conclusion: 多模态学习在医学影像分析中潜力巨大，可为放射科医生提供有效支持。

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [31] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: 提出了一种名为D2IP的新框架，用于加速3D时间序列成像，通过三种策略提升计算效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督学习方法（如DIP）在3D或时间序列成像中计算成本高的问题。

Method: 结合UPWS、TPP和3D-FastResUNet三种策略，加速收敛并保持时间一致性。

Result: 在模拟和临床数据上表现优异，图像质量提升（MSSIM增加24.8%，ERR降低8.1%），计算时间减少7.1倍。

Conclusion: D2IP在动态肺部成像中具有临床应用潜力。

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [32] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: UGPL是一种不确定性引导的渐进学习框架，通过全局到局部分析提升CT图像分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理病理特征的细微和空间多样性，UGPL旨在通过聚焦关键区域改进分类。

Method: 采用证据深度学习量化预测不确定性，通过非极大值抑制机制提取信息丰富的区域，并结合自适应融合机制。

Result: 在三个CT数据集上，UGPL的准确率分别提升了3.29%、2.46%和8.08%。

Conclusion: UGPL通过不确定性引导和渐进学习显著提升了CT图像分类性能。

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>
