{"id": "2507.13384", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13384", "abs": "https://arxiv.org/abs/2507.13384", "authors": ["Osama Hardan", "Omar Elshenhabi", "Tamer Khattab", "Mohamed Mabrok"], "title": "Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation", "comment": "Submitted to the 2025 IEEE International Conference on Future Machine\n  Learning and Data Science (FMLDS)", "summary": "Vision Mamba models promise transformer-level performance at linear\ncomputational cost, but their reliance on serializing 2D images into 1D\nsequences introduces a critical, yet overlooked, design choice: the patch scan\norder. In medical imaging, where modalities like brain MRI contain strong\nanatomical priors, this choice is non-trivial. This paper presents the first\nsystematic study of how scan order impacts MRI segmentation. We introduce\nMulti-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures\nthat facilitates exploring diverse scan paths without additional computational\ncost. We conduct a large-scale benchmark of 21 scan strategies on three public\ndatasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our\nanalysis shows conclusively that scan order is a statistically significant\nfactor (Friedman test: $\\chi^{2}_{20}=43.9, p=0.0016$), with performance\nvarying by as much as 27 Dice points. Spatially contiguous paths -- simple\nhorizontal and vertical rasters -- consistently outperform disjointed diagonal\nscans. We conclude that scan order is a powerful, cost-free hyperparameter, and\nprovide an evidence-based shortlist of optimal paths to maximize the\nperformance of Mamba models in medical imaging.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Vision Mamba\u6a21\u578b\u4e2d\u56fe\u50cf\u626b\u63cf\u987a\u5e8f\u5bf9MRI\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86Multi-Scan 2D\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u626b\u63cf\u987a\u5e8f\u7684\u663e\u8457\u6027\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\uff0c\u626b\u63cf\u987a\u5e8f\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u6b64\u524d\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5f15\u5165Multi-Scan 2D\u6a21\u5757\uff0c\u8bc4\u4f3021\u79cd\u626b\u63cf\u7b56\u7565\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u626b\u63cf\u987a\u5e8f\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff08Friedman\u68c0\u9a8c\uff1ap=0.0016\uff09\uff0c\u6027\u80fd\u5dee\u5f02\u53ef\u8fbe27 Dice\u70b9\u3002", "conclusion": "\u626b\u63cf\u987a\u5e8f\u662f\u65e0\u9700\u989d\u5916\u6210\u672c\u7684\u91cd\u8981\u8d85\u53c2\u6570\uff0c\u63a8\u8350\u4f7f\u7528\u7a7a\u95f4\u8fde\u7eed\u7684\u626b\u63cf\u8def\u5f84\u3002"}}
{"id": "2507.13394", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13394", "abs": "https://arxiv.org/abs/2507.13394", "authors": ["Akhil John Thomas", "Christiaan Boerkamp"], "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning", "comment": null, "summary": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeepLabV3\u7684\u4f18\u5316\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u9608\u503c\u5fae\u8c03\u548c\u53c2\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecf\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u795e\u7ecf\u5206\u5272\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5bf9\u7cbe\u786e\u8bc6\u522b\u795e\u7ecf\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528DeepLabV3\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u52a8\u9608\u503c\u5fae\u8c03\u548c\u53c2\u6570\u4f18\u5316\uff0c\u6539\u8fdb\u9884\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u8d85\u58f0\u795e\u7ecf\u5f71\u50cf\u4e0a\u5b9e\u73b0\u4e86Dice Score 0.78\u3001IoU 0.70\u548cPixel Accuracy 0.95\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5b9a\u5236\u5316\u53c2\u6570\u9009\u62e9\u5bf9\u81ea\u52a8\u5316\u795e\u7ecf\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.13458", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13458", "abs": "https://arxiv.org/abs/2507.13458", "authors": ["Malte Hoffmann"], "title": "Domain-randomized deep learning for neuroimage analysis", "comment": "12 pages, 6 figures, 2 tables, deep learning, domain generalization,\n  domain randomization, neuroimaging, medical image analysis, accepted for\n  publication in IEEE Signal Processing Magazine", "summary": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5c24\u5176\u662f\u5728MRI\u7b49\u56fe\u50cf\u5916\u89c2\u5dee\u5f02\u5927\u7684\u9886\u57df\u3002", "method": "\u91c7\u7528\u57df\u968f\u673a\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5408\u6210\u968f\u673a\u5f3a\u5ea6\u548c\u5185\u5bb9\u7684\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u5904\u7406\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u7c7b\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5f71\u50cf\u6a21\u6001\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ecMRI\u3001CT\u3001PET\u7b49\uff0c\u5e76\u6269\u5c55\u5230\u975e\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u3002", "conclusion": "\u5408\u6210\u9a71\u52a8\u8bad\u7ec3\u8303\u5f0f\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6297\u8fc7\u62df\u5408\u6027\uff0c\u4f46\u9700\u6743\u8861\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u6709\u671b\u52a0\u901f\u901a\u7528\u5de5\u5177\u7684\u7814\u53d1\u3002"}}
{"id": "2507.13604", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13604", "abs": "https://arxiv.org/abs/2507.13604", "authors": ["Qihang Li", "Jichen Yang", "Yaqian Chen", "Yuwen Chen", "Hanxue Gu", "Lars J. Grimm", "Maciej A. Mazurowski"], "title": "BreastSegNet: Multi-label Segmentation of Breast MRI", "comment": null, "summary": "Breast MRI provides high-resolution imaging critical for breast cancer\nscreening and preoperative staging. However, existing segmentation methods for\nbreast MRI remain limited in scope, often focusing on only a few anatomical\nstructures, such as fibroglandular tissue or tumors, and do not cover the full\nrange of tissues seen in scans. This narrows their utility for quantitative\nanalysis. In this study, we present BreastSegNet, a multi-label segmentation\nalgorithm for breast MRI that covers nine anatomical labels: fibroglandular\ntissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and\nimplant. We manually annotated a large set of 1123 MRI slices capturing these\nstructures with detailed review and correction from an expert radiologist.\nAdditionally, we benchmark nine segmentation models, including U-Net, SwinUNet,\nUNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among\nthem, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across\nall labels. It performs especially well on heart, liver, muscle, FGT, and bone,\nwith Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All\nmodel code and weights are publicly available, and we plan to release the data\nat a later date.", "AI": {"tldr": "BreastSegNet\u662f\u4e00\u79cd\u591a\u6807\u7b7e\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u4e73\u817aMRI\uff0c\u8986\u76d6\u4e5d\u79cd\u89e3\u5256\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e73\u817aMRI\u5206\u5272\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5c11\u6570\u89e3\u5256\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5b9a\u91cf\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86BreastSegNet\uff0c\u5e76\u624b\u52a8\u6807\u6ce8\u4e861123\u5f20MRI\u5207\u7247\uff0c\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5206\u5272\u6a21\u578b\u3002", "result": "nnU-Net ResEncM\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747Dice\u5f97\u5206\u4e3a0.694\uff0c\u90e8\u5206\u7ed3\u6784\u5f97\u5206\u63a5\u8fd10.90\u3002", "conclusion": "BreastSegNet\u663e\u8457\u63d0\u5347\u4e86\u4e73\u817aMRI\u5206\u5272\u7684\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.13463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13463", "abs": "https://arxiv.org/abs/2507.13463", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections", "comment": null, "summary": "Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large\nantenna arrays at high frequencies, transitioning communication paradigm into\nthe radiative near-field (NF), where spherical wavefronts enable full-vector\nestimation of both target location and velocity. However, location and motion\nparameters become inherently coupled in this regime, making their joint\nestimation computationally demanding. To overcome this, we propose a novel\napproach that projects the received two-dimensional space-time signal onto the\nangle-Doppler domain using a two-dimensional discrete Fourier transform\n(2D-DFT). Our analysis reveals that the resulting angular spread is centered at\nthe target's true angle, with its width determined by the target's range.\nSimilarly, transverse motion induces a Doppler spread centered at the true\nradial velocity, with the width of Doppler spread proportional to the\ntransverse velocity. Exploiting these spectral characteristics, we develop a\nlow-complexity algorithm that provides coarse estimates of angle, range, and\nvelocity, which are subsequently refined using one-dimensional multiple signal\nclassification (MUSIC) applied independently to each parameter. The proposed\nmethod enables accurate and efficient estimation of NF target motion\nparameters. Simulation results demonstrate a normalized mean squared error\n(NMSE) of -40 dB for location and velocity estimates compared to maximum\nlikelihood estimation, while significantly reducing computational complexity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u5229\u75282D-DFT\u548cMUSIC\u65b9\u6cd5\u8054\u5408\u4f30\u8ba1\u8fd1\u573aUM-MIMO\u7cfb\u7edf\u4e2d\u7684\u76ee\u6807\u4f4d\u7f6e\u548c\u901f\u5ea6\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u8fd1\u573aUM-MIMO\u7cfb\u7edf\u4e2d\uff0c\u76ee\u6807\u4f4d\u7f6e\u548c\u901f\u5ea6\u53c2\u6570\u8026\u5408\uff0c\u4f20\u7edf\u8054\u5408\u4f30\u8ba1\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc72D-DFT\u5c06\u63a5\u6536\u4fe1\u53f7\u6295\u5f71\u5230\u89d2\u5ea6-\u591a\u666e\u52d2\u57df\uff0c\u5229\u7528\u8c31\u7279\u6027\u63d0\u53d6\u7c97\u7565\u4f30\u8ba1\uff0c\u518d\u901a\u8fc7\u4e00\u7ef4MUSIC\u65b9\u6cd5\u7ec6\u5316\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u4e0a\u7684NMSE\u8fbe\u5230-40 dB\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fd1\u573aUM-MIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u76ee\u6807\u8fd0\u52a8\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2507.13464", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.13464", "abs": "https://arxiv.org/abs/2507.13464", "authors": ["Gurleen Padda", "Dave Touchette"], "title": "Round-Preserving Asymptotic Compression of Prior-Free Interactive Protocols", "comment": null, "summary": "There is a close relationship between the communication complexity and\ninformation complexity of communication problems, as demonstrated by results\nsuch as Shannon's noiseless source coding theorem, and the Slepian-Wolf\ntheorem. Here, we study this relationship in the prior-free and interactive\nsetting, where we provide an alternate proof for the result of Braverman [SIAM\nReview, vol. 59, no. 4, 2017], that the amortized communication complexity of\nsimulating a prior-free interactive communication protocol, is equal to its\nprior-free information cost. While this is a known result, our approach\naddresses the need for a more natural proof of it. We also improve on the\nresult by achieving round preservation, and using a bounded quantity of shared\nrandomness. We do this by showing that the communicating parties can produce a\nreliable estimate of the joint type, or empirical distribution, of their\ninputs. This estimate is then used in our protocol for the prior-free reverse\nShannon theorem with side information at the receiver. These results are then\ngeneralized to the interactive setting to obtain our main result.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u4fe1\u590d\u6742\u6027\u548c\u4fe1\u606f\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u81ea\u7136\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86Braverman\u7684\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u8f6e\u6b21\u4fdd\u7559\u548c\u6709\u9650\u5171\u4eab\u968f\u673a\u6027\u3002", "motivation": "\u63a2\u7d22\u901a\u4fe1\u590d\u6742\u6027\u548c\u4fe1\u606f\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u65e0\u5148\u9a8c\u548c\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u4e0b\uff0c\u63d0\u4f9b\u4e00\u4e2a\u66f4\u81ea\u7136\u7684\u8bc1\u660e\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u8f93\u5165\u7684\u8054\u5408\u7c7b\u578b\u6216\u7ecf\u9a8c\u5206\u5e03\uff0c\u8bbe\u8ba1\u534f\u8bae\u5b9e\u73b0\u65e0\u5148\u9a8c\u53cd\u5411\u9999\u519c\u5b9a\u7406\uff0c\u5e76\u63a8\u5e7f\u5230\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u3002", "result": "\u6539\u8fdb\u4e86Braverman\u7684\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u8f6e\u6b21\u4fdd\u7559\u548c\u6709\u9650\u5171\u4eab\u968f\u673a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u81ea\u7136\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u7ed3\u679c\uff0c\u4e3a\u901a\u4fe1\u590d\u6742\u6027\u548c\u4fe1\u606f\u590d\u6742\u6027\u7684\u5173\u7cfb\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.13782", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle \u00c5str\u00f6m", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides\ndetailed anatomical views, offering better signal-to-noise ratio, resolution\nand tissue contrast than 3T MRI, though at the cost of accessibility. We\npresent an advanced deep learning model for synthesizing 7T brain MRI from 3T\nbrain MRI. Paired 7T and 3T T1-weighted images were acquired from 172\nparticipants (124 cognitively unimpaired, 48 impaired) from the Swedish\nBioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:\na specialized U-Net, and a U-Net integrated with a generative adversarial\nnetwork (GAN U-Net). Our models outperformed two additional state-of-the-art\n3T-to-7T models in image-based evaluation metrics. Four blinded MRI\nprofessionals judged our synthetic 7T images as comparable in detail to real 7T\nimages, and superior in subjective visual quality to 7T images, apparently due\nto the reduction of artifacts. Importantly, automated segmentations of the\namygdalae of synthetic GAN U-Net 7T images were more similar to manually\nsegmented amygdalae (n=20), than automated segmentations from the 3T images\nthat were used to synthesize the 7T images. Finally, synthetic 7T images showed\nsimilar performance to real 3T images in downstream prediction of cognitive\nstatus using MRI derivatives (n=3,168). In all, we show that synthetic\nT1-weighted brain images approaching 7T quality can be generated from 3T\nimages, which may improve image quality and segmentation, without compromising\nperformance in downstream tasks. Future directions, possible clinical use\ncases, and limitations are discussed.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece3T MRI\u5408\u62107T MRI\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u4e14\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "7T MRI\u63d0\u4f9b\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\uff0c\u4f46\u666e\u53ca\u6027\u8f83\u4f4e\u3002\u901a\u8fc7\u5408\u62107T MRI\uff0c\u53ef\u4ee5\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u8bad\u7ec3\u4e86\u4e24\u79cd\u6a21\u578b\uff1a\u4e13\u7528U-Net\u548c\u96c6\u6210GAN\u7684U-Net\uff0c\u4f7f\u7528172\u540d\u53c2\u4e0e\u8005\u7684\u914d\u5bf93T\u548c7T T1\u52a0\u6743\u56fe\u50cf\u3002", "result": "\u5408\u62107T\u56fe\u50cf\u5728\u7ec6\u8282\u4e0a\u63a5\u8fd1\u771f\u5b9e7T\u56fe\u50cf\uff0c\u4e3b\u89c2\u89c6\u89c9\u8d28\u91cf\u66f4\u4f18\uff0c\u4e14\u81ea\u52a8\u5206\u5272\u6548\u679c\u66f4\u597d\u3002\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0e\u771f\u5b9e3T\u56fe\u50cf\u76f8\u4f3c\u3002", "conclusion": "\u5408\u62107T MRI\u53ef\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u672a\u6765\u6709\u671b\u7528\u4e8e\u4e34\u5e8a\u3002"}}
{"id": "2507.13520", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13520", "abs": "https://arxiv.org/abs/2507.13520", "authors": ["Sizhen Bian", "Mengxi Liu", "Paul Lukowicz"], "title": "Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing", "comment": null, "summary": "Passive body-area electrostatic field sensing, also referred to as human body\ncapacitance (HBC), is an energy-efficient and non-intrusive sensing modality\nthat exploits the human body's inherent electrostatic properties to perceive\nhuman behaviors. This paper presents a focused overview of passive HBC sensing,\nincluding its underlying principles, historical evolution, hardware\narchitectures, and applications across research domains. Key challenges, such\nas susceptibility to environmental variation, are discussed to trigger\nmitigation techniques. Future research opportunities in sensor fusion and\nhardware enhancement are highlighted. To support continued innovation, this\nwork provides open-source resources and aims to empower researchers and\ndevelopers to leverage passive electrostatic sensing for next-generation\nwearable and ambient intelligence systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u88ab\u52a8\u4eba\u4f53\u7535\u5bb9\uff08HBC\uff09\u4f20\u611f\u7684\u539f\u7406\u3001\u5386\u53f2\u3001\u786c\u4ef6\u67b6\u6784\u53ca\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u73af\u5883\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5229\u7528\u4eba\u4f53\u56fa\u6709\u9759\u7535\u7279\u6027\u5b9e\u73b0\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u5f0f\u884c\u4e3a\u611f\u77e5\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u548c\u667a\u80fd\u73af\u5883\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u7efc\u8ff0\u88ab\u52a8HBC\u4f20\u611f\u7684\u539f\u7406\u3001\u786c\u4ef6\u67b6\u6784\u53ca\u8de8\u9886\u57df\u5e94\u7528\uff0c\u5206\u6790\u73af\u5883\u53d8\u5316\u7684\u6311\u6218\u5e76\u63d0\u51fa\u7f13\u89e3\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\uff0c\u652f\u6301\u4f20\u611f\u5668\u878d\u5408\u548c\u786c\u4ef6\u589e\u5f3a\u7684\u672a\u6765\u7814\u7a76\uff0c\u4fc3\u8fdb\u521b\u65b0\u3002", "conclusion": "\u88ab\u52a8HBC\u4f20\u611f\u5728\u53ef\u7a7f\u6234\u548c\u667a\u80fd\u73af\u5883\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u514b\u670d\u73af\u5883\u654f\u611f\u6027\u5e76\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2507.13543", "categories": ["cs.IT", "cs.AI", "cs.LG", "math-ph", "math.IT", "math.MP", "I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.13543", "abs": "https://arxiv.org/abs/2507.13543", "authors": ["Alexander Kolpakov"], "title": "Loss-Complexity Landscape and Model Structure Functions", "comment": "18 pages, 3 figures; GitHub repository at\n  https://github.com/sashakolpakov/structure-functions", "summary": "We develop a framework for dualizing the Kolmogorov structure function\n$h_x(\\alpha)$, which then allows using computable complexity proxies. We\nestablish a mathematical analogy between information-theoretic constructs and\nstatistical mechanics, introducing a suitable partition function and free\nenergy functional. We explicitly prove the Legendre-Fenchel duality between the\nstructure function and free energy, showing detailed balance of the Metropolis\nkernel, and interpret acceptance probabilities as information-theoretic\nscattering amplitudes. A susceptibility-like variance of model complexity is\nshown to peak precisely at loss-complexity trade-offs interpreted as phase\ntransitions. Practical experiments with linear and tree-based regression models\nverify these theoretical predictions, explicitly demonstrating the interplay\nbetween the model complexity, generalization, and overfitting threshold.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9Kolmogorov\u7ed3\u6784\u51fd\u6570\u8fdb\u884c\u5bf9\u5076\u5316\uff0c\u5e76\u5f15\u5165\u7edf\u8ba1\u529b\u5b66\u7c7b\u6bd4\uff0c\u8bc1\u660e\u4e86Legendre-Fenchel\u5bf9\u5076\u6027\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u590d\u6742\u5ea6\u4ee3\u7406\u5bf9Kolmogorov\u7ed3\u6784\u51fd\u6570\u8fdb\u884c\u5bf9\u5076\u5316\uff0c\u5e76\u63a2\u7d22\u4fe1\u606f\u8bba\u4e0e\u7edf\u8ba1\u529b\u5b66\u4e4b\u95f4\u7684\u6570\u5b66\u7c7b\u6bd4\u3002", "method": "\u5f15\u5165\u5206\u533a\u51fd\u6570\u548c\u81ea\u7531\u80fd\u6cdb\u51fd\uff0c\u8bc1\u660eLegendre-Fenchel\u5bf9\u5076\u6027\uff0c\u5e76\u901a\u8fc7Metropolis\u6838\u7684\u8be6\u7ec6\u5e73\u8861\u89e3\u91ca\u63a5\u53d7\u6982\u7387\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u901a\u8fc7\u7ebf\u6027\u548c\u6811\u57fa\u56de\u5f52\u6a21\u578b\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u6846\u67b6\u6210\u529f\u5c06\u4fe1\u606f\u8bba\u4e0e\u7edf\u8ba1\u529b\u5b66\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.13830", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13830", "abs": "https://arxiv.org/abs/2507.13830", "authors": ["Maximilian Rokuss", "Benjamin Hamm", "Yannick Kirchhoff", "Klaus Maier-Hein"], "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation", "comment": "Accepted at MICCAI 2025 WOMEN", "summary": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider", "AI": {"tldr": "\u9996\u4e2a\u516c\u5f00\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc713,000\u4f8b\u6807\u6ce8\u7684\u5de6\u53f3\u4e73\u817a\u5206\u5272\u6807\u7b7e\uff0c\u5e76\u9644\u5e26\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u586b\u8865\u4e73\u817aMRI\u5206\u6790\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u5973\u6027\u5065\u5eb7\u9886\u57df\u63d0\u4f9b\u8d44\u6e90\u3002", "method": "\u63d0\u4f9b\u6807\u6ce8\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u5de6\u53f3\u4e73\u817a\u5206\u5272\u3002", "result": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u9ad8\u7ea7\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e73\u817aMRI\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u548c\u5de5\u5177\u3002"}}
{"id": "2507.13526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13526", "abs": "https://arxiv.org/abs/2507.13526", "authors": ["Gedeon Ghislain Nkwewo Ngoufo", "Khaled Humadi", "Elham Baladi", "Gunes Karabulut Kurt"], "title": "Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks", "comment": null, "summary": "The proliferation of space debris in low Earth orbit (LEO) presents critical\nchallenges for orbital safety, particularly for satellite constellations.\nIntegrated sensing and communication (ISAC) systems provide a promising dual\nfunction solution by enabling both environmental sensing and data\ncommunication. This study explores the use of space shift keying (SSK)\nmodulation within ISAC frameworks, evaluating its performance when combined\nwith sinusoidal and chirp radar waveforms. SSK is particularly attractive due\nto its low hardware complexity and robust communication performance. Our\nresults demonstrate that both waveforms achieve comparable bit error rate (BER)\nperformance under SSK, validating its effectiveness for ISAC applications.\nHowever, waveform selection significantly affects sensing capability: while the\nsinusoidal waveform supports simpler implementation, its high ambiguity limits\nrange detection. In contrast, the chirp waveform enables range estimation and\nprovides a modest improvement in velocity detection accuracy. These findings\nhighlight the strength of SSK as a modulation scheme for ISAC and emphasize the\nimportance of selecting appropriate waveforms to optimize sensing accuracy\nwithout compromising communication performance. This insight supports the\ndesign of efficient and scalable ISAC systems for space applications,\nparticularly in the context of orbital debris monitoring.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7a7a\u95f4\u79fb\u4f4d\u952e\u63a7\uff08SSK\uff09\u8c03\u5236\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u6b63\u5f26\u548c\u5541\u557e\u96f7\u8fbe\u6ce2\u5f62\uff0c\u9a8c\u8bc1\u4e86SSK\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6ce2\u5f62\u9009\u62e9\u5bf9\u4f20\u611f\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u4e2d\u7a7a\u95f4\u788e\u7247\u7684\u589e\u52a0\u5bf9\u8f68\u9053\u5b89\u5168\u6784\u6210\u6311\u6218\uff0cISAC\u7cfb\u7edf\u901a\u8fc7\u540c\u65f6\u5b9e\u73b0\u73af\u5883\u4f20\u611f\u548c\u6570\u636e\u901a\u4fe1\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528SSK\u8c03\u5236\uff0c\u7ed3\u5408\u6b63\u5f26\u548c\u5541\u557e\u96f7\u8fbe\u6ce2\u5f62\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u8bef\u7801\u7387\uff08BER\uff09\u548c\u4f20\u611f\u80fd\u529b\u3002", "result": "\u4e24\u79cd\u6ce2\u5f62\u5728SSK\u4e0b\u8bef\u7801\u7387\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5541\u557e\u6ce2\u5f62\u5728\u8ddd\u79bb\u4f30\u8ba1\u548c\u901f\u5ea6\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SSK\u662fISAC\u7684\u6709\u6548\u8c03\u5236\u65b9\u6848\uff0c\u6ce2\u5f62\u9009\u62e9\u5bf9\u4f18\u5316\u4f20\u611f\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u8f68\u9053\u788e\u7247\u76d1\u6d4b\u7b49\u7a7a\u95f4\u5e94\u7528\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u652f\u6301\u3002"}}
{"id": "2507.13548", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.13548", "abs": "https://arxiv.org/abs/2507.13548", "authors": ["Oren Dubin", "Noam Oz", "Noga Ron-Zewi"], "title": "Efficient Decoding of Double-circulant and Wozencraft Codes from Square-root Errors", "comment": null, "summary": "We present efficient decoding algorithms from square-root errors for two\nknown families of double-circulant codes: A construction based on Sidon sets\n(Bhargava, Taveres, and Shiva, \\emph{IEEE IT 74}; Calderbank, \\emph{IEEE IT\n83}; Guruswami and Li, \\emph{IEEE IT 2025}), and a construction based on cyclic\ncodes (Chen, Peterson, and Weldon, \\emph{Information and Control 1969}). We\nfurther observe that the work of Guruswami and Li implicitly gives a\ntransformation from double-circulant codes of certain block lengths to\nWozencraft codes which preserves that distance of the codes, and we show that\nthis transformation also preserves efficiency of decoding. By instantiating\nthis transformation with the first family of double-circulant codes based on\nSidon sets, we obtain an explicit construction of a Wozencraft code that is\nefficiently decodable from square-root errors. We also discuss limitations on\ninstantiating this transformation with the second family of double-circulant\ncodes based on cyclic codes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u53cc\u5faa\u73af\u7801\u7684\u9ad8\u6548\u89e3\u7801\u7b97\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u8f6c\u5316\u4e3aWozencraft\u7801\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76\u53cc\u5faa\u73af\u7801\u7684\u89e3\u7801\u6548\u7387\u53ca\u5176\u8f6c\u5316\u4e3a\u5176\u4ed6\u7801\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8eSidon\u96c6\u548c\u5faa\u73af\u7801\u7684\u53cc\u5faa\u73af\u7801\u89e3\u7801\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u8f6c\u5316\u4e3aWozencraft\u7801\u7684\u53ef\u884c\u6027\u3002", "result": "\u6210\u529f\u5c06\u57fa\u4e8eSidon\u96c6\u7684\u53cc\u5faa\u73af\u7801\u8f6c\u5316\u4e3a\u9ad8\u6548\u89e3\u7801\u7684Wozencraft\u7801\uff0c\u4f46\u57fa\u4e8e\u5faa\u73af\u7801\u7684\u8f6c\u5316\u5b58\u5728\u9650\u5236\u3002", "conclusion": "\u53cc\u5faa\u73af\u7801\u7684\u89e3\u7801\u7b97\u6cd5\u9ad8\u6548\uff0c\u4e14\u90e8\u5206\u53ef\u8f6c\u5316\u4e3aWozencraft\u7801\uff0c\u4f46\u9700\u6ce8\u610f\u8f6c\u5316\u9650\u5236\u3002"}}
{"id": "2507.13901", "categories": ["eess.IV", "cs.CV", "62H35, 68U10", "I.4.10; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.13901", "abs": "https://arxiv.org/abs/2507.13901", "authors": ["Lei Xu", "Torkel B Brismar"], "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive", "comment": "24 pages, 7 figures", "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.", "AI": {"tldr": "AnatomyArchive\u662f\u4e00\u4e2a\u57fa\u4e8eTotalSegmentator\u7684CT\u56fe\u50cf\u5206\u6790\u5de5\u5177\uff0c\u63d0\u4f9b\u81ea\u52a8\u76ee\u6807\u4f53\u79ef\u9009\u62e9\u3001\u89e3\u5256\u7ed3\u6784\u7ba1\u7406\u3001\u4f53\u7d20\u7279\u5f81\u63d0\u53d6\u7b49\u529f\u80fd\uff0c\u652f\u63012D\u548c3D\u5206\u6790\uff0c\u5e76\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u6559\u80b2\u4f7f\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684CT\u56fe\u50cf\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u7cbe\u51c6\u7684\u8eab\u4f53\u6210\u5206\u5206\u6790\u548c\u533b\u5b66\u56fe\u50cf\u6570\u636e\u5e93\u7ba1\u7406\u3002", "method": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u89e3\u5256\u7ed3\u6784\u7ba1\u7406\uff0c\u7ed3\u5408GPU\u52a0\u901f\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\u81ea\u52a8\u4f53\u79ef\u88c1\u526a\u3001\u7279\u5f81\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u3002", "result": "\u63d0\u4f9b\u4e86\u529f\u80fd\u5168\u9762\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u652f\u6301\u4ece\u56fe\u50cf\u5904\u7406\u5230\u7edf\u8ba1\u5206\u6790\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "AnatomyArchive\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u6559\u80b2\u4f7f\u7528\u3002"}}
{"id": "2507.13554", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13554", "abs": "https://arxiv.org/abs/2507.13554", "authors": ["Meles Weldegebriel", "Zihan Li", "Dustin Maas", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System", "comment": null, "summary": "We present the design and validation of Stoppable Secondary Use (StopSec), a\nprivacy-preserving protocol with the capability to identify a secondary user\n(SU) causing interference to a primary user (PU) and to act quickly to stop the\ninterference. All users are served by a database that provides a feedback\nmechanism from a PU to an interfering SU. We introduce a new lightweight and\nrobust method to watermark an SU's OFDM packet. Through extensive over-the-air\nreal-time experiments, we evaluate StopSec in terms of interference detection,\nidentification, and stopping latency, as well as impact on SUs. We show that\nthe watermarking method avoids negative impact to the secondary data link and\nis robust to real-world time-varying channels. Interfering SUs can be stopped\nin under 150 milliseconds, and when multiple users are simultaneously\ninterfering, they can all be stopped. Even when the interference is 10 dB lower\nthan the noise power, StopSec successfully stops interfering SUs within a few\nseconds of their appearance in the channel. StopSec can be an effective\nspectrum sharing protocol for cases when interference to a PU must be quickly\nand automatically stopped.", "AI": {"tldr": "StopSec\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u534f\u8bae\uff0c\u80fd\u5feb\u901f\u8bc6\u522b\u5e76\u963b\u6b62\u6b21\u7ea7\u7528\u6237\uff08SU\uff09\u5bf9\u4e3b\u7528\u6237\uff08PU\uff09\u7684\u5e72\u6270\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6c34\u5370\u65b9\u6cd5\u548c\u5b9e\u65f6\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u9891\u8c31\u5171\u4eab\u4e2d\u6b21\u7ea7\u7528\u6237\u5bf9\u4e3b\u7528\u6237\u7684\u5e72\u6270\u95ee\u9898\uff0c\u786e\u4fdd\u5feb\u901f\u81ea\u52a8\u505c\u6b62\u5e72\u6270\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u6c34\u5370\u65b9\u6cd5\u6807\u8bb0SU\u7684OFDM\u6570\u636e\u5305\uff0c\u5229\u7528\u6570\u636e\u5e93\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u5e72\u6270\u68c0\u6d4b\u4e0e\u505c\u6b62\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e72\u6270\u68c0\u6d4b\u548c\u505c\u6b62\u5ef6\u8fdf\u4f4e\u4e8e150\u6beb\u79d2\uff0c\u6c34\u5370\u65b9\u6cd5\u5bf9SU\u6570\u636e\u94fe\u8def\u65e0\u8d1f\u9762\u5f71\u54cd\uff0c\u4e14\u9002\u5e94\u65f6\u53d8\u4fe1\u9053\u3002", "conclusion": "StopSec\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9891\u8c31\u5171\u4eab\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u81ea\u52a8\u505c\u6b62\u5e72\u6270\u7684\u573a\u666f\u3002"}}
{"id": "2507.13689", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.13689", "abs": "https://arxiv.org/abs/2507.13689", "authors": ["Jean-Francois Chamberland", "Gianluigi Liva", "Krishna Narayanan"], "title": "Density Evolution Analysis of Sparse-Block IDMA", "comment": "Presented at the 2025 IEEE Workshop on Signal Processing and\n  Artificial Intelligence for Wireless Communications (SPAWC)", "summary": "Sparse block interleaver division multiple access (SB-IDMA) is a recently\nintroduced unsourced multiple access protocol that aims to improve the\nperformance of the grant-free two-step random access transmission protocol of\nthe 3GPP 5G New Radio standard. We introduced a density evolution analysis of\nthe successive interference cancellation receiver of SB-IDMA, providing a\ntheoretical characterization of its performance.", "AI": {"tldr": "SB-IDMA\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u6e90\u591a\u5740\u534f\u8bae\uff0c\u65e8\u5728\u63d0\u53475G\u65b0\u7a7a\u53e3\u6807\u51c6\u7684\u4e24\u6b65\u968f\u673a\u63a5\u5165\u534f\u8bae\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5bc6\u5ea6\u8fdb\u5316\u5206\u6790\u5176\u63a5\u6536\u673a\u6027\u80fd\u3002", "motivation": "\u6539\u8fdb3GPP 5G\u65b0\u7a7a\u53e3\u6807\u51c6\u4e2d\u7684\u65e0\u6388\u6743\u4e24\u6b65\u968f\u673a\u63a5\u5165\u534f\u8bae\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5bc6\u5ea6\u8fdb\u5316\u5206\u6790SB-IDMA\u7684\u8fde\u7eed\u5e72\u6270\u6d88\u9664\u63a5\u6536\u673a\u3002", "result": "\u63d0\u4f9b\u4e86SB-IDMA\u63a5\u6536\u673a\u6027\u80fd\u7684\u7406\u8bba\u8868\u5f81\u3002", "conclusion": "SB-IDMA\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u53475G\u968f\u673a\u63a5\u5165\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13915", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u7cca\u6838\u548c\u7f29\u653e\u56e0\u5b50\uff0c\u6539\u8fdb\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ec5\u4ece\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4f30\u8ba1\u9000\u5316\u6838\uff0c\u5ffd\u7565\u4e86\u7f29\u653e\u56e0\u5b50\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u9000\u5316\u8fc7\u7a0b\uff0c\u751f\u6210\u989d\u5916\u7684LR-HR\u5bf9\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u96f6\u6837\u672c\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6a21\u7cca\u6838\u3001\u7f29\u653e\u56e0\u5b50\u548c\u53c2\u8003\u56fe\u50cf\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u76f2\u8d85\u5206\u8fa8\u7387\u7684\u6548\u679c\u3002"}}
{"id": "2507.13637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13637", "abs": "https://arxiv.org/abs/2507.13637", "authors": ["Jun Jiang", "Yuan Gao", "Xinyi Wu", "Shugong Xu"], "title": "Towards channel foundation models (CFMs): Motivations, methodologies and opportunities", "comment": "13 pages", "summary": "Artificial intelligence (AI) has emerged as a pivotal enabler for\nnext-generation wireless communication systems. However, conventional AI-based\nmodels encounter several limitations, such as heavy reliance on labeled data,\nlimited generalization capability, and task-specific design. To address these\nchallenges, this paper introduces, for the first time, the concept of channel\nfoundation models (CFMs)-a novel and unified framework designed to tackle a\nwide range of channel-related tasks through a pretrained, universal channel\nfeature extractor. By leveraging advanced AI architectures and self-supervised\nlearning techniques, CFMs are capable of effectively exploiting large-scale\nunlabeled data without the need for extensive manual annotation. We further\nanalyze the evolution of AI methodologies, from supervised learning and\nmulti-task learning to self-supervised learning, emphasizing the distinct\nadvantages of the latter in facilitating the development of CFMs. Additionally,\nwe provide a comprehensive review of existing studies on self-supervised\nlearning in this domain, categorizing them into generative, discriminative and\nthe combined paradigms. Given that the research on CFMs is still at an early\nstage, we identify several promising future research directions, focusing on\nmodel architecture innovation and the construction of high-quality, diverse\nchannel datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4fe1\u9053\u57fa\u7840\u6a21\u578b\uff08CFMs\uff09\u201d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u901a\u7528\u4fe1\u9053\u7279\u5f81\u63d0\u53d6\u5668\u89e3\u51b3\u591a\u79cd\u4fe1\u9053\u76f8\u5173\u4efb\u52a1\uff0c\u514b\u670d\u4f20\u7edfAI\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfAI\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5b58\u5728\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u7b49\u95ee\u9898\uff0cCFMs\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u5148\u8fdb\u7684AI\u67b6\u6784\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0cCFMs\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\u5904\u7406\u591a\u79cd\u4fe1\u9053\u4efb\u52a1\uff0c\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002", "result": "CFMs\u80fd\u591f\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CFMs\u4e3a\u4fe1\u9053\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6a21\u578b\u67b6\u6784\u521b\u65b0\u548c\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u6784\u5efa\u3002"}}
{"id": "2507.13808", "categories": ["cs.IT", "math.IT", "G.2.1"], "pdf": "https://arxiv.org/pdf/2507.13808", "abs": "https://arxiv.org/abs/2507.13808", "authors": ["Yuting Li", "Yuanyuan Tang", "Hao Lou", "Ryan Gabrys", "Farzad Farnoud"], "title": "Asymptotically Optimal Codes Correcting One Substring Edit", "comment": "6 pages, 1 figure", "summary": "The substring edit error is the operation of replacing a substring $u$ of $x$\nwith another string $v$, where the lengths of $u$ and $v$ are bounded by a\ngiven constant $k$. It encompasses localized insertions, deletions, and\nsubstitutions within a window. Codes correcting one substring edit have\nredundancy at least $\\log n+k$. In this paper, we construct codes correcting\none substring edit with redundancy $\\log n+O(\\log \\log n)$, which is\nasymptotically optimal.", "AI": {"tldr": "\u672c\u6587\u6784\u9020\u4e86\u7ea0\u6b63\u4e00\u4e2a\u5b50\u4e32\u7f16\u8f91\u9519\u8bef\u7684\u7f16\u7801\uff0c\u5176\u5197\u4f59\u5ea6\u4e3alog n + O(log log n)\uff0c\u8fbe\u5230\u4e86\u6e10\u8fd1\u6700\u4f18\u3002", "motivation": "\u7814\u7a76\u5b50\u4e32\u7f16\u8f91\u9519\u8bef\u7684\u7ea0\u6b63\u95ee\u9898\uff0c\u76ee\u6807\u662f\u51cf\u5c11\u7f16\u7801\u5197\u4f59\u5ea6\uff0c\u4f7f\u5176\u63a5\u8fd1\u7406\u8bba\u4e0b\u9650\u3002", "method": "\u901a\u8fc7\u6784\u9020\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u65b9\u6848\uff0c\u80fd\u591f\u7ea0\u6b63\u4e00\u4e2a\u5b50\u4e32\u7f16\u8f91\u9519\u8bef\uff0c\u5176\u4e2d\u5b50\u4e32\u957f\u5ea6\u53d7\u9650\u4e8e\u5e38\u6570k\u3002", "result": "\u63d0\u51fa\u7684\u7f16\u7801\u65b9\u6848\u5197\u4f59\u5ea6\u4e3alog n + O(log log n)\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u7ed3\u679clog n + k\u3002", "conclusion": "\u8be5\u7f16\u7801\u65b9\u6848\u5728\u5197\u4f59\u5ea6\u4e0a\u8fbe\u5230\u4e86\u6e10\u8fd1\u6700\u4f18\uff0c\u4e3a\u5b50\u4e32\u7f16\u8f91\u9519\u8bef\u7684\u7ea0\u6b63\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13974", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13974", "abs": "https://arxiv.org/abs/2507.13974", "authors": ["Jiaqi Lv", "Yijie Zhu", "Carmen Guadalupe Colin Tenorio", "Brinder Singh Chohan", "Mark Eastwood", "Shan E Ahmed Raza"], "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images", "comment": "Accepted by MIUA 2025", "summary": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ed1\u8272\u7d20\u7624H&E\u56fe\u50cf\u4e2d\u4e94\u79cd\u7ec4\u7ec7\u7c7b\u522b\u7684\u5206\u5272\uff0c\u7ed3\u5408\u75c5\u7406\u57fa\u7840\u6a21\u578bVirchow2\u548cEfficient-UNet\uff0c\u53d6\u5f97\u4e86PUMA Grand Challenge\u7684\u7b2c\u4e00\u540d\u3002", "motivation": "\u9ed1\u8272\u7d20\u7624\u7684\u7ec4\u7ec7\u5f62\u6001\u5b66\u7279\u5f81\u5bf9\u9884\u540e\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u5206\u5272\u8017\u65f6\u4e14\u6613\u53d7\u4e3b\u89c2\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Virchow2\u75c5\u7406\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u878d\u5408\u539f\u59cbRGB\u56fe\u50cf\uff0c\u518d\u901a\u8fc7Efficient-UNet\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u8fdb\u884c\u5206\u5272\u3002", "result": "\u6a21\u578b\u5728PUMA Grand Challenge\u7684\u7ec4\u7ec7\u5206\u5272\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u5206\u5272\u7f51\u7edc\u80fd\u6709\u6548\u52a0\u901f\u8ba1\u7b97\u75c5\u7406\u5b66\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2507.13748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13748", "abs": "https://arxiv.org/abs/2507.13748", "authors": ["Patrick Matalla", "Joel Dittmer", "Md Salek Mahmud", "Christian Koos", "Sebastian Randel"], "title": "Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets", "comment": null, "summary": "We present an elastic buffer design that enables all-digital clock recovery\nimplementation with free-running receiver clock featuring negative and positive\nclock frequency offsets. Error-free real-time data transmission is demonstrated\nfrom -400 ppm to +400 ppm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f39\u6027\u7f13\u51b2\u8bbe\u8ba1\uff0c\u652f\u6301\u5168\u6570\u5b57\u65f6\u949f\u6062\u590d\uff0c\u5b9e\u73b0\u81ea\u7531\u8fd0\u884c\u7684\u63a5\u6536\u5668\u65f6\u949f\uff0c\u5e76\u5141\u8bb8\u6b63\u8d1f\u65f6\u949f\u9891\u7387\u504f\u79fb\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65f6\u949f\u6062\u590d\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u6b63\u8d1f\u9891\u7387\u504f\u79fb\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f39\u6027\u7f13\u51b2\u8bbe\u8ba1\uff0c\u7ed3\u5408\u81ea\u7531\u8fd0\u884c\u7684\u63a5\u6536\u5668\u65f6\u949f\u3002", "result": "\u5728-400 ppm\u81f3+400 ppm\u8303\u56f4\u5185\u5b9e\u73b0\u65e0\u8bef\u7801\u5b9e\u65f6\u6570\u636e\u4f20\u8f93\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u5168\u6570\u5b57\u65f6\u949f\u6062\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13961", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.13961", "abs": "https://arxiv.org/abs/2507.13961", "authors": ["Mallikharjuna Chinnapadamala", "Charul Rajput", "B. Sundar Rajan"], "title": "Secretive Hotplug Coded Caching", "comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433", "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u70ed\u63d2\u62d4\u7f16\u7801\u7f13\u5b58\u6a21\u578b\u4e2d\u7684\u4fdd\u5bc6\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u5df2\u77e5HpPDA\u7c7b\u522b\u7684\u4fdd\u5bc6\u65b9\u6848\uff0c\u5e76\u5728\u67d0\u4e9b\u5185\u5b58\u533a\u57df\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u70ed\u63d2\u62d4\u7f16\u7801\u7f13\u5b58\u7cfb\u7edf\u4e2d\u7528\u6237\u79bb\u7ebf\u65f6\u7684\u4fdd\u5bc6\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7528\u6237\u65e0\u6cd5\u901a\u8fc7\u7f13\u5b58\u5185\u5bb9\u6216\u670d\u52a1\u5668\u4f20\u8f93\u83b7\u53d6\u975e\u8bf7\u6c42\u6587\u4ef6\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u5df2\u77e5HpPDA\u7c7b\u522b\u7684\u4fdd\u5bc6\u65b9\u6848\uff0c\u5e76\u4e0e\u57fa\u7ebf\u65b9\u6848\uff08\u57fa\u4e8e\u7ecf\u5178\u7f16\u7801\u7f13\u5b58\u6a21\u578b\u7684\u4fdd\u5bc6\u65b9\u6848\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u5185\u5b58\u533a\u57df\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4fdd\u5bc6\u65b9\u6848\u5728\u70ed\u63d2\u62d4\u7f16\u7801\u7f13\u5b58\u7cfb\u7edf\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u5185\u5b58\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2507.13993", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13993", "abs": "https://arxiv.org/abs/2507.13993", "authors": ["Ningyong Wu", "Jinzhi Wang", "Wenhong Zhao", "Chenzhan Yu", "Zhigang Xiu", "Duwei Dai"], "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models", "comment": null, "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.", "AI": {"tldr": "OrthoInsight\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u808b\u9aa8\u9aa8\u6298\u8bca\u65ad\u548c\u62a5\u544a\u751f\u6210\uff0c\u7ed3\u5408\u4e86YOLOv9\u3001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u548cLLaVA\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8eGPT-4\u548cClaude-3\u3002", "motivation": "\u533b\u7597\u5f71\u50cf\u6570\u636e\u589e\u957f\u9700\u8981\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\uff0c\u624b\u52a8\u5206\u6790\u8017\u65f6\u4e14\u6613\u51fa\u9519\u3002", "method": "\u6574\u5408YOLOv9\u68c0\u6d4b\u9aa8\u6298\u3001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e34\u5e8a\u80cc\u666f\u3001LLaVA\u751f\u6210\u62a5\u544a\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u3002", "result": "\u572828,675\u5f20CT\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u5f97\u52064.28\uff0c\u8d85\u8d8aGPT-4\u548cClaude-3\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u53ef\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.13766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13766", "abs": "https://arxiv.org/abs/2507.13766", "authors": ["Kai Wu", "Zhongqin Wang", "Shu-Lin Chen", "J. Andrew Zhang", "Y. Jay Guo"], "title": "ISAC: From Human to Environmental Sensing", "comment": "15 pages, 8 figures", "summary": "Integrated Sensing and Communications (ISAC) is poised to become one of the\ndefining capabilities of the sixth generation (6G) wireless communications\nsystems, enabling the network infrastructure to jointly support high-throughput\ncommunications and situational awareness. While recent advances have explored\nISAC for both human-centric applications and environmental monitoring, existing\nresearch remains fragmented across these domains. This paper provides the first\nunified review of ISAC-enabled sensing for both human activities and\nenvironment, focusing on signal-level mechanisms, sensing features, and\nreal-world feasibility. We begin by characterising how diverse physical\nphenomena, ranging from human vital sign and motion to precipitation and flood\ndynamics, impact wireless signal propagation, producing measurable signatures\nin channel state information (CSI), Doppler profiles, and signal statistics. A\ncomprehensive analysis is then presented across two domains: human sensing\napplications including localisation, activity recognition, and vital sign\nmonitoring; and environmental sensing for rainfall, soil moisture, and water\nlevel. Experimental results from Long-Term Evolution (LTE) sensing under\nnon-line-of-sight (NLOS) conditions are incorporated to highlight the\nfeasibility in infrastructure-limited scenarios. Open challenges in signal\nfusion, domain adaptation, and generalisable sensing architectures are\ndiscussed to facilitate future research toward scalable and autonomous ISAC.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u57286G\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u7edf\u4e00\u5206\u6790\u4e86\u5176\u5728\u4eba\u7c7b\u6d3b\u52a8\u548c\u73af\u5883\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4fe1\u53f7\u673a\u5236\u3001\u611f\u77e5\u7279\u5f81\u53ca\u5b9e\u9645\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22ISAC\u57286G\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5728\u4eba\u7c7b\u6d3b\u52a8\u548c\u73af\u5883\u76d1\u6d4b\u9886\u57df\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65e0\u7ebf\u4fe1\u53f7\u4f20\u64ad\u4e2d\u7684\u7269\u7406\u73b0\u8c61\uff08\u5982\u4eba\u7c7b\u6d3b\u52a8\u548c\u73af\u5883\u53d8\u5316\uff09\u5bf9\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u3001\u591a\u666e\u52d2\u8c31\u548c\u4fe1\u53f7\u7edf\u8ba1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u4fe1\u53f7\u7ea7\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u975e\u89c6\u8ddd\uff08NLOS\uff09\u6761\u4ef6\u4e0b\uff0cISAC\u5728\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u573a\u666f\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3\u4fe1\u53f7\u878d\u5408\u3001\u9886\u57df\u9002\u5e94\u548c\u901a\u7528\u611f\u77e5\u67b6\u6784\u7b49\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u81ea\u4e3b\u7684ISAC\u3002"}}
{"id": "2507.14064", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14064", "abs": "https://arxiv.org/abs/2507.14064", "authors": ["Lei Huang"], "title": "Bounds and Constructions of High-Memory Spatially-Coupled Codes", "comment": "Accepted by ITW2025", "summary": "In this paper, we apply the Clique Lov\\'asz Local Lemma to provide sufficient\nconditions on memory and lifting degree for removing certain harmful\ncombinatorial structures in spatially-coupled (SC) codes that negatively impact\ndecoding performance. Additionally, we present, for the first time, a\nconstructive algorithm based on the Moser-Tardos algorithm that ensures\npredictable performance. Furthermore, leveraging the properties of\nLLL-distribution and M-T-distribution, we establish the dependencies among the\nharmful structures during the construction process. We provide upper bounds on\nthe probability change of remaining harmful structures after eliminating some\nof them. In particular, the elimination of 4-cycles increases the probability\nof 6-cycles becoming active by at most a factor of $e^{8/3}$.", "AI": {"tldr": "\u672c\u6587\u5229\u7528Clique Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\uff0c\u4e3a\u7a7a\u95f4\u8026\u5408\uff08SC\uff09\u7801\u4e2d\u79fb\u9664\u6709\u5bb3\u7ec4\u5408\u7ed3\u6784\u63d0\u4f9b\u4e86\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMoser-Tardos\u7b97\u6cd5\u7684\u6784\u9020\u6027\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u7a7a\u95f4\u8026\u5408\u7801\u7684\u89e3\u7801\u6027\u80fd\uff0c\u901a\u8fc7\u79fb\u9664\u6709\u5bb3\u7ec4\u5408\u7ed3\u6784\u5e76\u786e\u4fdd\u6027\u80fd\u53ef\u9884\u6d4b\u3002", "method": "\u5e94\u7528Clique Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\uff0c\u7ed3\u5408Moser-Tardos\u7b97\u6cd5\u6784\u9020\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u6709\u5bb3\u7ed3\u6784\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7ed9\u51fa\u4e86\u79fb\u9664\u6709\u5bb3\u7ed3\u6784\u540e\u5269\u4f59\u7ed3\u6784\u6982\u7387\u53d8\u5316\u7684\u4e0a\u754c\uff0c\u4f8b\u5982\u79fb\u96644-cycles\u4f1a\u4f7f6-cycles\u6d3b\u8dc3\u6982\u7387\u6700\u591a\u589e\u52a0e^(8/3)\u500d\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4e3a\u7a7a\u95f4\u8026\u5408\u7801\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.14046", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14046", "abs": "https://arxiv.org/abs/2507.14046", "authors": ["Hao Fang", "Hao Yu", "Sihao Teng", "Tao Zhang", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging", "comment": "11 pages, 9 figures", "summary": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD2IP\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f3D\u65f6\u95f4\u5e8f\u5217\u6210\u50cf\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DIP\uff09\u57283D\u6216\u65f6\u95f4\u5e8f\u5217\u6210\u50cf\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408UPWS\u3001TPP\u548c3D-FastResUNet\u4e09\u79cd\u7b56\u7565\uff0c\u52a0\u901f\u6536\u655b\u5e76\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u4e34\u5e8a\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\uff08MSSIM\u589e\u52a024.8%\uff0cERR\u964d\u4f4e8.1%\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c117.1\u500d\u3002", "conclusion": "D2IP\u5728\u52a8\u6001\u80ba\u90e8\u6210\u50cf\u4e2d\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13826", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13826", "abs": "https://arxiv.org/abs/2507.13826", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis", "comment": "10 pages, 9 figures, 6 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a method for simulating signals received by\nfrequency-modulated continuous-wave radar during respiratory monitoring, using\nhuman body geometry and displacement data acquired via a depth camera. Unlike\nprevious studies that rely on simplified models of body geometry or\ndisplacement, the proposed approach models high-frequency scattering centers\nbased on realistic depth-camera-measured body shapes and motions. Experiments\nwere conducted with six participants under varying conditions, including\nvarying target distances, seating orientations, and radar types, with\nsimultaneous acquisition from the radar and depth camera. Relative to\nconventional model-based methods, the proposed technique achieved improvements\nof 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,\ndisplacements, and spectrograms, respectively. This work contributes to the\ngeneration of radar-based physiological datasets through simulation and\nenhances our understanding of factors affecting the accuracy of non-contact\nsensing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u76f8\u673a\u83b7\u53d6\u7684\u4eba\u4f53\u51e0\u4f55\u548c\u4f4d\u79fb\u6570\u636e\u6a21\u62df\u547c\u5438\u76d1\u6d4b\u4e2d\u9891\u7387\u8c03\u5236\u8fde\u7eed\u6ce2\u96f7\u8fbe\u4fe1\u53f7\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u96f7\u8fbe\u56fe\u50cf\u3001\u4f4d\u79fb\u548c\u9891\u8c31\u56fe\u7684\u76f8\u5173\u7cfb\u6570\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u7684\u4eba\u4f53\u51e0\u4f55\u6216\u4f4d\u79fb\u6a21\u578b\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u9ad8\u9891\u6563\u5c04\u4e2d\u5fc3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u771f\u5b9e\u6df1\u5ea6\u76f8\u673a\u6570\u636e\u63d0\u5347\u6a21\u62df\u7cbe\u5ea6\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u83b7\u53d6\u7684\u4eba\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\u6570\u636e\u5efa\u6a21\u9ad8\u9891\u6563\u5c04\u4e2d\u5fc3\uff0c\u5e76\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u76f8\u5173\u7cfb\u6570\u5728\u96f7\u8fbe\u56fe\u50cf\u3001\u4f4d\u79fb\u548c\u9891\u8c31\u56fe\u4e0a\u5206\u522b\u63d0\u9ad8\u4e867.5%\u300158.2%\u548c3.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u751f\u6210\u96f7\u8fbe\u751f\u7406\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u975e\u63a5\u89e6\u4f20\u611f\u7684\u51c6\u786e\u6027\uff0c\u5e76\u52a0\u6df1\u4e86\u5bf9\u5f71\u54cd\u56e0\u7d20\u7684\u7406\u89e3\u3002"}}
{"id": "2507.14070", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14070", "abs": "https://arxiv.org/abs/2507.14070", "authors": ["Yajuan Liu", "Tolga M. Duman"], "title": "Error Correcting Codes for Segmented Burst-Deletion Channels", "comment": null, "summary": "We study segmented burst-deletion channels motivated by the observation that\nsynchronization errors commonly occur in a bursty manner in real-world\nsettings. In this channel model, transmitted sequences are implicitly divided\ninto non-overlapping segments, each of which may experience at most one burst\nof deletions. In this paper, we develop error correction codes for segmented\nburst-deletion channels over arbitrary alphabets under the assumption that each\nsegment may contain only one burst of t-deletions. The main idea is to encode\nthe input subsequence corresponding to each segment using existing one-burst\ndeletion codes, with additional constraints that enable the decoder to identify\nsegment boundaries during the decoding process from the received sequence. The\nresulting codes achieve redundancy that scales as O(log b), where b is the\nlength of each segment.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5206\u6bb5\u7a81\u53d1\u5220\u9664\u4fe1\u9053\uff0c\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u5b57\u6bcd\u8868\u7684\u7ea0\u9519\u7801\uff0c\u5197\u4f59\u5ea6\u4e0e\u6bb5\u957f\u5ea6\u5bf9\u6570\u6210\u6b63\u6bd4\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u540c\u6b65\u9519\u8bef\u901a\u5e38\u4ee5\u7a81\u53d1\u5f62\u5f0f\u51fa\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5206\u6bb5\u7a81\u53d1\u5220\u9664\u4fe1\u9053\u3002", "method": "\u5229\u7528\u73b0\u6709\u7684\u4e00\u7a81\u53d1\u5220\u9664\u7801\u5bf9\u6bcf\u6bb5\u8f93\u5165\u5b50\u5e8f\u5217\u7f16\u7801\uff0c\u5e76\u6dfb\u52a0\u7ea6\u675f\u4ee5\u5e2e\u52a9\u89e3\u7801\u5668\u8bc6\u522b\u6bb5\u8fb9\u754c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u4ee3\u7801\u5197\u4f59\u5ea6\u4e3aO(log b)\uff0c\u5176\u4e2db\u4e3a\u6bb5\u957f\u5ea6\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u5206\u6bb5\u7a81\u53d1\u5220\u9664\u4fe1\u9053\u7684\u7ea0\u9519\u7801\uff0c\u5197\u4f59\u5ea6\u4f18\u5316\u3002"}}
{"id": "2507.14102", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14102", "abs": "https://arxiv.org/abs/2507.14102", "authors": ["Shravan Venkatraman", "Pavan Kumar S", "Rakesh Raj Madavan", "Chandrakala S"], "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography", "comment": "18 pages, 10 figures, 5 tables, 2025 ICCV Workshops", "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL", "AI": {"tldr": "UGPL\u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6e10\u8fdb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u5206\u6790\u63d0\u5347CT\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u75c5\u7406\u7279\u5f81\u7684\u7ec6\u5fae\u548c\u7a7a\u95f4\u591a\u6837\u6027\uff0cUGPL\u65e8\u5728\u901a\u8fc7\u805a\u7126\u5173\u952e\u533a\u57df\u6539\u8fdb\u5206\u7c7b\u3002", "method": "\u91c7\u7528\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u975e\u6781\u5927\u503c\u6291\u5236\u673a\u5236\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u533a\u57df\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2aCT\u6570\u636e\u96c6\u4e0a\uff0cUGPL\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e863.29%\u30012.46%\u548c8.08%\u3002", "conclusion": "UGPL\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u548c\u6e10\u8fdb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.13829", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.13829", "abs": "https://arxiv.org/abs/2507.13829", "authors": ["Arnaud Poinas", "R\u00e9mi Bardenet"], "title": "On two fundamental properties of the zeros of spectrograms of noisy signals", "comment": null, "summary": "The spatial distribution of the zeros of the spectrogram is significantly\naltered when a signal is added to white Gaussian noise. The zeros tend to\ndelineate the support of the signal, and deterministic structures form in the\npresence of interference, as if the zeros were trapped. While sophisticated\nmethods have been proposed to detect signals as holes in the pattern of\nspectrogram zeros, few formal arguments have been made to support the\ndelineation and trapping effects. Through detailed computations for simple toy\nsignals, we show that two basic mathematical arguments, the intensity of zeros\nand Rouch\\'e's theorem, allow discussing delineation and trapping, and the\ninfluence of parameters like the signal-to-noise ratio. In particular,\ninterfering chirps, even nearly superimposed, yield an easy-to-detect\ndeterministic structure among zeros.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4fe1\u53f7\u52a0\u5165\u9ad8\u65af\u767d\u566a\u58f0\u540e\uff0c\u8c31\u56fe\u96f6\u70b9\u7684\u7a7a\u95f4\u5206\u5e03\u53d8\u5316\uff0c\u53d1\u73b0\u96f6\u70b9\u4f1a\u52fe\u52d2\u51fa\u4fe1\u53f7\u7684\u652f\u6491\u533a\u57df\uff0c\u5e76\u5728\u5e72\u6270\u4e0b\u5f62\u6210\u786e\u5b9a\u6027\u7ed3\u6784\u3002\u901a\u8fc7\u7b80\u5355\u4fe1\u53f7\u7684\u8ba1\u7b97\uff0c\u8bba\u6587\u7528\u96f6\u70b9\u5bc6\u5ea6\u548cRouch\u00e9\u5b9a\u7406\u89e3\u91ca\u4e86\u8fd9\u79cd\u73b0\u8c61\u3002", "motivation": "\u63a2\u8ba8\u4fe1\u53f7\u5728\u566a\u58f0\u80cc\u666f\u4e0b\u8c31\u56fe\u96f6\u70b9\u7684\u5206\u5e03\u53d8\u5316\uff0c\u4ee5\u53ca\u96f6\u70b9\u5982\u4f55\u52fe\u52d2\u4fe1\u53f7\u652f\u6491\u533a\u57df\u548c\u5f62\u6210\u786e\u5b9a\u6027\u7ed3\u6784\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7b80\u5355\u4fe1\u53f7\u7684\u8ba1\u7b97\uff0c\u7ed3\u5408\u96f6\u70b9\u5bc6\u5ea6\u548cRouch\u00e9\u5b9a\u7406\uff0c\u5206\u6790\u96f6\u70b9\u5206\u5e03\u7684\u53d8\u5316\u53ca\u5176\u4e0e\u4fe1\u53f7\u53c2\u6570\uff08\u5982\u4fe1\u566a\u6bd4\uff09\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u5e72\u6270\u4e0b\uff0c\u96f6\u70b9\u4e5f\u80fd\u5f62\u6210\u6613\u4e8e\u68c0\u6d4b\u7684\u786e\u5b9a\u6027\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u53e0\u52a0\u7684\u7ebf\u6027\u8c03\u9891\u4fe1\u53f7\u3002", "conclusion": "\u96f6\u70b9\u5bc6\u5ea6\u548cRouch\u00e9\u5b9a\u7406\u4e3a\u8c31\u56fe\u96f6\u70b9\u7684\u5206\u5e03\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u4e3a\u4fe1\u53f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.13938", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13938", "abs": "https://arxiv.org/abs/2507.13938", "authors": ["Hyun Seok Lee"], "title": "Device-Free Localization Using Commercial UWB Transceivers", "comment": "8 pages, 10 figures, preprint", "summary": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u8d85\u5bbd\u5e26\uff08UWB\uff09\u8bbe\u5907\u81ea\u7531\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f4e\u4fe1\u566a\u6bd4\u548c\u73af\u5883\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u6709UWB\u8bbe\u5907\u81ea\u7531\u5b9a\u4f4d\u6280\u672f\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u548c\u7c92\u5b50\u6ee4\u6ce2\uff0c\u9996\u5148\u5206\u6790\u4fe1\u9053\u8109\u51b2\u54cd\u5e94\uff08CIR\uff09\u65b9\u5dee\uff0c\u5229\u7528\u6ce8\u610f\u529bU-Net\u63d0\u53d6\u76ee\u6807\u53cd\u5c04\u6210\u5206\u5e76\u6291\u5236\u566a\u58f0\uff0c\u6700\u540e\u901a\u8fc7\u7c92\u5b50\u6ee4\u6ce2\u4f30\u8ba1\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728RMSE\u7ea6\u4e3a15\u5398\u7c73\u3001\u5e73\u5747\u5904\u7406\u65f6\u95f44\u6beb\u79d2\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7269\u8054\u7f51\u548c\u6c7d\u8f66\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14018", "abs": "https://arxiv.org/abs/2507.14018", "authors": ["Zeyuan Zhang", "Yue Xiu", "Phee Lep Yeoh", "Guangyi Liu", "Zixing Wu", "Ning Wei"], "title": "Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a practical partially-connected hybrid beamforming\ntransmitter for integrated sensing and communication (ISAC) with distortion\nfrom nonlinear power amplification. For this ISAC system, we formulate a\ncommunication rate and sensing mutual information maximization problem driven\nby our distortion-aware hybrid beamforming design. To address this non-convex\nproblem, we first solve for a fully digital beamforming matrix by alternatively\nsolving three sub-problems using manifold optimization (MO) and our derived\nclosed-form solutions. The analog and digital beamforming matrices are then\nobtained through a decomposition algorithm. Numerical results demonstrate that\nthe proposed algorithm can improve overall ISAC performance compared to\ntraditional beamforming methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5931\u771f\u7684\u90e8\u5206\u8fde\u63a5\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u53d1\u5c04\u673a\uff0c\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5931\u771f\u611f\u77e5\u7684\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3ISAC\u7cfb\u7edf\u4e2d\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5931\u771f\u5bf9\u901a\u4fe1\u901f\u7387\u548c\u611f\u77e5\u4e92\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6d41\u5f62\u4f18\u5316\uff08MO\uff09\u548c\u95ed\u5f0f\u89e3\u4ea4\u66ff\u6c42\u89e3\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u5f97\u5230\u5168\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u77e9\u9635\uff0c\u518d\u901a\u8fc7\u5206\u89e3\u7b97\u6cd5\u83b7\u5f97\u6a21\u62df\u548c\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u76f8\u6bd4\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\u80fd\u63d0\u5347ISAC\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5931\u771f\u611f\u77e5\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86ISAC\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14035", "abs": "https://arxiv.org/abs/2507.14035", "authors": ["Sai Xu", "Kai-Kit Wong", "Yanan Du", "Hanjiang Hong", "Chan-Byoung Chae", "Baiyang Liu", "Kin-Fai Tong"], "title": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming", "comment": null, "summary": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u548c\u7aef\u53e3\u9009\u62e9\u3002\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u968f\u673a\u7aef\u53e3\u9009\u62e9\uff0c\u4ee5\u53ca\u57fa\u4e8eFPGA\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\u80fd\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FASs\uff09\u5728\u6ce2\u675f\u6210\u5f62\u548c\u7aef\u53e3\u9009\u62e9\u65b9\u9762\u7684\u4f18\u5316\u9700\u6c42\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u63d0\u5347\u6548\u7387\u3002", "method": "1. \u5efa\u6a21FA-enabled\u591a\u5c0f\u533aMIMO\u7f51\u7edc\uff0c\u5e76\u6784\u5efa\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff1b2. \u63d0\u51fa\u7ed3\u5408GNN\u548cRPS\u7684\u65b9\u6cd5\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u7aef\u53e3\u9009\u62e9\uff1b3. \u5f00\u53d1\u57fa\u4e8eFPGA\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u4ee5\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff1b4. \u5f15\u5165\u8c03\u5ea6\u7b97\u6cd5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u7a7a\u95f2\u65f6\u95f4\u3002", "result": "\u4eff\u771f\u663e\u793aGNN-RPS\u65b9\u6cd5\u5728\u901a\u4fe1\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff1b\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660eFPGA\u52a0\u901f\u5668\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u540c\u65f6\u6267\u884c\u591a\u7aef\u53e3\u9009\u62e9\u7684\u6ce2\u675f\u6210\u5f62\u63a8\u7406\u3002", "conclusion": "\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u5728\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u4e2d\u6709\u6548\u4f18\u5316\u4e86\u6ce2\u675f\u6210\u5f62\u548c\u7aef\u53e3\u9009\u62e9\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u4f4e\u5ef6\u8fdf\u3002"}}
