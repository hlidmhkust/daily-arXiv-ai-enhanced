<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 6]
- [cs.IT](#cs.IT) [Total: 9]
- [eess.SP](#eess.SP) [Total: 13]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Keep the Core: Adversarial Priors for Significance-Preserving Brain MRI Segmentation](https://arxiv.org/abs/2512.15811)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Aoxue Chen,Dayong Ren*

Main category: eess.IV

TL;DR: 提出"Keep the Core"数据增强范式，通过对抗性先验识别关键诊断特征，在保持核心语义的同时进行选择性增强和掩码，提升医学图像分割的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临标注稀疏问题，现有增强方法往往破坏关键诊断语义或未能优先处理重要特征，需要一种能保持诊断重要性的数据增强方法。

Method: 使用SAGE模块离线识别最小化扰动能改变分割边界的关键token，生成token重要性图W；在线KEEP模块基于W进行两种增强：1)语义保持增强（恢复高重要性token原值），2)引导掩码增强（掩码低重要性token进行MAE式重建）。

Result: 在2D医学数据集上实现了最先进的分割鲁棒性和泛化性能，SAGE的结构化先验和KEEP的区域选择机制高度互补。

Conclusion: "Keep the Core"通过对抗性先验引导的显著性保持数据增强，解决了医学图像分割中特征无关增强破坏诊断语义的问题，无需推理开销且与骨干网络无关。

Abstract: Medical image segmentation is constrained by sparse pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce "Keep the Core," a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-preserving manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map $W$ by solving an adversarial optimization problem to maximally degrade performance, while an $\ell_1$ sparsity penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \& Preservation) module uses $W$ for a two-pronged augmentation strategy: (1) Semantic-Preserving Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an $\text{MAE}$-style reconstruction, forcing the model to learn robust representations from preserved critical features. "Keep the Core" is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets.

</details>


### [2] [BioimageAIpub: a toolbox for AI-ready bioimaging data publishing](https://arxiv.org/abs/2512.15820)
*Stefan Dvoretskii,Anwai Archit,Constantin Pape,Josh Moore,Marco Nolden*

Main category: eess.IV

TL;DR: BioimageAIpub：一个简化生物成像数据转换的工作流，可无缝上传到HuggingFace平台


<details>
  <summary>Details</summary>
Motivation: 现代生物图像分析方法需要大量数据，但现有数据存储库（如IDR和BioImage Archive）的内容通常需要大量数据整理才能被图像分析工具直接使用，这耗费研究人员大量时间，阻碍了更强大分析工具的开发。

Method: 引入BioimageAIpub工作流，该工作流简化生物成像数据转换过程，支持将数据无缝上传到广泛使用的机器学习数据集和模型共享平台HuggingFace。

Result: 通过BioimageAIpub工作流，研究人员可以更高效地处理和共享生物成像数据，减少数据整理时间。

Conclusion: BioimageAIpub解决了生物成像数据转换和共享的瓶颈问题，有助于促进更强大的生物图像分析工具的开发。

Abstract: Modern bioimage analysis approaches are data hungry, making it necessary for researchers to scavenge data beyond those collected within their (bio)imaging facilities. In addition to scale, bioimaging datasets must be accompanied with suitable, high-quality annotations and metadata. Although established data repositories such as the Image Data Resource (IDR) and BioImage Archive offer rich metadata, their contents typically cannot be directly consumed by image analysis tools without substantial data wrangling. Such a tedious assembly and conversion of (meta)data can account for a dedicated amount of time investment for researchers, hindering the development of more powerful analysis tools. Here, we introduce BioimageAIpub, a workflow that streamlines bioimaging data conversion, enabling a seamless upload to HuggingFace, a widely used platform for sharing machine learning datasets and models.

</details>


### [3] [SNIC: Synthesized Noisy Images using Calibration](https://arxiv.org/abs/2512.15905)
*Nik Bhatt*

Main category: eess.IV

TL;DR: 该论文提出了一种构建高质量异方差噪声模型的方法，创建了包含6000多张噪声图像的SNIC数据集，在RAW和TIFF格式下都能生成逼真的合成噪声图像，性能优于制造商提供的DNG噪声模型。


<details>
  <summary>Details</summary>
Motivation: 先进的去噪算法需要大规模高质量数据集，而基于物理的统计噪声模型可以通过模拟数字图像中的噪声来创建这样的数据集。但目前缺乏关于如何正确校准和调整这些异方差噪声模型的信息，也缺少使用这些模型的公开数据集。

Method: 探索构建高质量异方差噪声模型的过程，开发能够生成逼真合成噪声图像的方法，支持RAW和TIFF格式。通过校准过程优化噪声模型参数。

Result: 合成的噪声图像在LPIPS指标上达到与真实噪声图像相当的结果，并且在LPIPS和SOTA去噪模型测试中都大大优于制造商提供的DNG噪声模型。创建了SNIC数据集，包含6000多张噪声图像，涵盖4个传感器（包括智能手机、卡片机和单反）的30个场景。

Conclusion: 该方法能够生成高质量的合成噪声图像，SNIC是首个同时提供RAW和TIFF格式的合成噪声图像数据集，为去噪算法开发提供了有价值的资源。

Abstract: Advanced denoising algorithms require large, high-quality datasets. Physics-based, statistical noise models can create such datasets by realistically simulating noise in digital images. However, there is little information on the correct way to calibrate and tune these heteroscedastic models, and a lack of published datasets using them. In this paper, we explore the process of building high-quality heteroscedastic noise models. Our methods produce realistic synthesized noisy images, in both RAW and TIFF formats. Our synthesized noisy images achieve comparable LPIPS results to real noisy images, and greatly outperform those created with manufacturer-provided DNG noise models both in LPIPS and when tested with a state-of-the-art (SOTA) denoising model. Using our approach, we created the Synthesized Noisy Images using Calibration dataset (SNIC) containing over 6000 noisy images, comprising 30 scenes from four sensors, including two smartphone sensors, a point-and-shoot, and a DSLR. SNIC is the first synthesized noisy image dataset provided in both RAW and TIFF format.

</details>


### [4] [In search of truth: Evaluating concordance of AI-based anatomy segmentation models](https://arxiv.org/abs/2512.15921)
*Lena Giebeler,Deepa Krishnaswamy,David Clunie,Jakob Wasserthal,Lalith Kumar Shiyam Sundar,Andres Diaz-Pinto,Klaus H. Maier-Hein,Murong Xu,Bjoern Menze,Steve Pieper,Ron Kikinis,Andrey Fedorov*

Main category: eess.IV

TL;DR: 提出一个用于评估无标注数据上解剖分割模型的框架，通过标准化表示和可视化工具比较多个开源模型在CT图像上的分割效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI解剖分割模型数量增加，如何在缺乏真实标注的数据集上评估这些模型成为挑战。需要一种实用的框架来辅助模型比较和选择。

Method: 将分割结果统一为标准化的互操作表示，扩展3D Slicer工具来加载和比较这些标准化分割，使用交互式汇总图和基于OHIF Viewer的浏览器可视化。在NLST数据集上评估6个开源模型对31个解剖结构的分割效果。

Result: 框架能够自动化加载、结构级检查和跨模型比较。初步结果显示该方法能快速检测和审查问题结果，某些结构（如肺）分割一致性很好，但其他结构（如椎骨、肋骨）存在无效分割。

Conclusion: 开发了包括分割标准化脚本、汇总图和可视化工具在内的资源，帮助在缺乏真实标注的情况下评估模型，最终支持明智的模型选择。

Abstract: Purpose AI-based methods for anatomy segmentation can help automate characterization of large imaging datasets. The growing number of similar in functionality models raises the challenge of evaluating them on datasets that do not contain ground truth annotations. We introduce a practical framework to assist in this task. Approach We harmonize the segmentation results into a standard, interoperable representation, which enables consistent, terminology-based labeling of the structures. We extend 3D Slicer to streamline loading and comparison of these harmonized segmentations, and demonstrate how standard representation simplifies review of the results using interactive summary plots and browser-based visualization using OHIF Viewer. To demonstrate the utility of the approach we apply it to evaluating segmentation of 31 anatomical structures (lungs, vertebrae, ribs, and heart) by six open-source models - TotalSegmentator 1.5 and 2.6, Auto3DSeg, MOOSE, MultiTalent, and CADS - for a sample of Computed Tomography (CT) scans from the publicly available National Lung Screening Trial (NLST) dataset. Results We demonstrate the utility of the framework in enabling automating loading, structure-wise inspection and comparison across models. Preliminary results ascertain practical utility of the approach in allowing quick detection and review of problematic results. The comparison shows excellent agreement segmenting some (e.g., lung) but not all structures (e.g., some models produce invalid vertebrae or rib segmentations). Conclusions The resources developed are linked from https://imagingdatacommons.github.io/segmentation-comparison/ including segmentation harmonization scripts, summary plots, and visualization tools. This work assists in model evaluation in absence of ground truth, ultimately enabling informed model selection.

</details>


### [5] [MCR-VQGAN: A Scalable and Cost-Effective Tau PET Synthesis Approach for Alzheimer's Disease Imaging](https://arxiv.org/abs/2512.15947)
*Jin Young Kim,Jeremy Hudson,Jeongchul Kim,Qing Lyu,Christopher T. Whitlow*

Main category: eess.IV

TL;DR: 提出MCR-VQGAN模型，从结构T1加权MRI合成tau PET图像，以解决tau PET临床应用的局限性，包括辐射暴露、可用性低、工作量大和成本高的问题。


<details>
  <summary>Details</summary>
Motivation: tau PET是阿尔茨海默病诊断的关键工具，但其临床应用受到辐射暴露、设备有限、临床工作量大和成本高昂等限制。需要开发从更易获取的MRI合成tau PET图像的方法。

Method: 提出多尺度CBAM残差向量量化生成对抗网络(MCR-VQGAN)，在标准VQGAN基础上集成多尺度卷积、ResNet块和卷积块注意力模块。使用ADNI的222对T1-MRI和tau PET数据进行训练，并与cGAN、WGAN-GP、CycleGAN和VQGAN比较。

Result: MCR-VQGAN在所有指标上表现最优：MSE 0.0056±0.0061，PSNR 24.39±4.49 dB，SSIM 0.9000±0.0453。基于CNN的AD分类器在真实图像上准确率为63.64%，在合成图像上为65.91%，表明合成图像保留了诊断相关特征。

Conclusion: MCR-VQGAN能够合成高质量的tau PET图像，可作为传统tau PET成像的可靠替代方案，有望提高tau成像生物标志物在AD研究和临床工作流程中的可及性和可扩展性。

Abstract: Tau positron emission tomography (PET) is a critical diagnostic modality for Alzheimer's disease (AD) because it visualizes and quantifies neurofibrillary tangles, a hallmark of AD pathology. However, its widespread clinical adoption is hindered by significant challenges, such as radiation exposure, limited availability, high clinical workload, and substantial financial costs. To overcome these limitations, we propose Multi-scale CBAM Residual Vector Quantized Generative Adversarial Network (MCR-VQGAN) to synthesize high-fidelity tau PET images from structural T1-weighted MRI scans. MCR-VQGAN improves standard VQGAN by integrating three key architectural enhancements: multi-scale convolutions, ResNet blocks, and Convolutional Block Attention Modules (CBAM). Using 222 paired structural T1-weighted MRI and tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI), we trained and compared MCR-VQGAN with cGAN, WGAN-GP, CycleGAN, and VQGAN. Our proposed model achieved superior image synthesis performance across all metrics: MSE of 0.0056 +/- 0.0061, PSNR of 24.39 +/- 4.49 dB, and SSIM of 0.9000 +/- 0.0453. To assess the clinical utility of the synthetic images, we trained and evaluated a CNN-based AD classifier. The classifier achieved comparable accuracy when tested on real (63.64%) and synthetic (65.91%) images. This result indicates that our synthesis process successfully preserves diagnostically relevant features without significant information loss. Our results demonstrate that MCR-VQGAN can offer a reliable and scalable surrogate for conventional tau PET imaging, potentially improving the accessibility and scalability of tau imaging biomarkers for AD research and clinical workflows.

</details>


### [6] [Single-View Tomographic Reconstruction Using Learned Primal Dual](https://arxiv.org/abs/2512.16065)
*Sean Breckling,Matthew Swan,Keith D. Tan,Derek Wingard,Brandon Baldonado,Yoohwan Kim,Ju-Yeon Jo,Evan Scott,Jordan Pillow*

Main category: eess.IV

TL;DR: 研究Learned Primal Dual方法在单视角轴对称目标断层扫描重建中的性能，比较两种X射线成像模式下的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LPD方法在各种断层扫描重建中表现出色，特别是在有限视角或有限视图数等挑战性采集条件下，但需要研究其在更极端情况下的性能：单视角轴对称目标的断层扫描重建。

Method: 研究两种成像模式：1) 低发散或平行X射线模式；2) 锥束X射线成像测试平台。使用封闭形式积分变换或基于物理的光线追踪软件生成训练数据，然后添加模糊和噪声。将结果与常见数值反演方法进行比较。

Result: 论文展示了LPD方法在单视角轴对称目标断层扫描重建中的结果，并与传统数值反演方法进行了对比分析。

Conclusion: 该研究评估了LPD方法在极端单视角断层扫描重建场景下的性能，为该方法在更广泛成像条件下的应用提供了参考。

Abstract: The Learned Primal Dual (LPD) method has shown promising results in various tomographic reconstruction modalities, particularly under challenging acquisition restrictions such as limited viewing angles or a limited number of views. We investigate the performance of LPD in a more extreme case: single-view tomographic reconstructions of axially-symmetric targets. This study considers two modalities: the first assumes low-divergence or parallel X-rays. The second models a cone-beam X-ray imaging testbed. For both modalities, training data is generated using closed-form integral transforms, or physics-based ray-tracing software, then corrupted with blur and noise. Our results are then compared against common numerical inversion methodologies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [7] [Information theory and discriminative sampling for model discovery](https://arxiv.org/abs/2512.16000)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.IT

TL;DR: 该论文将费舍尔信息矩阵（FIM）与稀疏识别非线性动力学（SINDy）数据驱动框架结合，通过可视化混沌与非混沌系统的信息模式，展示了基于信息的分析如何通过优先处理信息量更大的数据来提高采样效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 费舍尔信息和香农熵是理解分析动力系统的互补工具，但如何将它们与数据驱动的模型发现框架结合，以量化信息内容、指导采样策略、提高学习效率并减少数据需求，是本研究的核心动机。

Method: 将费舍尔信息矩阵（FIM）整合到稀疏识别非线性动力学（SINDy）数据驱动框架中，可视化混沌和非混沌系统的信息模式，分析单轨迹和多初始条件的信息分布，并通过FIM的谱分析阐明统计装袋的益处。

Result: 展示了基于信息的分析能够提高采样效率和增强模型性能，通过优先处理信息量更大的数据；在三种场景（单轨迹、可调控制参数、多轨迹自由初始化）中证明了费舍尔信息和熵度量能够促进数据效率。

Conclusion: 在数据驱动模型发现日益重要的背景下，基于可量化信息度量的原则性采样策略为提高学习效率和减少数据需求提供了强大方法，费舍尔信息分析为优化数据收集和模型训练提供了系统框架。

Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.

</details>


### [8] [Optimal Key Rates for Decentralized Secure Aggregation with Arbitrary Collusion and Heterogeneous Security Constraints](https://arxiv.org/abs/2512.16112)
*Zhou Li,Xiang Zhang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 研究去中心化安全聚合中的异构安全约束问题，通过安全集合和共谋集合的设定，优化通信和密钥速率，降低密钥开销。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化安全聚合需要大量密钥来保护除输入总和和共谋用户信息外的所有信息。为降低密钥开销，研究具有任意共谋和异构安全约束的去中心化安全聚合。

Method: 定义安全集合和共谋集合，用户子集的安全需求不同。通过线性规划方法表征最优通信和源密钥速率，找到每输入比特所需的最小共享密钥位数。

Result: 表征了最优源密钥速率，即实现具有任意共谋和异构安全约束的去中心化安全聚合所需的最小共享密钥位数。该表征通常归结为求解线性规划问题。

Conclusion: 通过引入异构安全约束，优化了去中心化安全聚合的密钥开销，为不同安全需求场景提供了更高效的解决方案。

Abstract: Decentralized secure aggregation (DSA) considers a fully-connected network of $K$ users, where each pair of users can communicate bidirectionally over an error-free channel. Each user holds a private input, and the goal is for each user to compute the sum of all inputs without revealing any additional information, even in the presence of collusion among up to $T$ users. Traditional DSA typically requires large key sizes to protect all information except for the input sum and the information of colluding users. To mitigate the source keys overhead, we study decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints. In this setting, the inputs of a predefined collection of user subsets, called the \emph{security set} $\bm{\mathcal{S}}$, must be protected from another predefined collection, the \emph{collusion set} $\bm{\mathcal{T}}$. For an arbitrary security set $\mathcal{S}\in \bm{\mathcal{S}}$ and an arbitrary collusion set $\mathcal{T}\in \bm{\mathcal{T}}$, we characterize the optimal communication and source key rates. A key contribution of this work is the characterization of the optimal source key rate, i.e., the minimum number of key bits per input bit that must be shared among users for decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints to be feasible. In general, this characterization reduces to solving a linear program.

</details>


### [9] [New Quantum Stabilizer Codes from generalized Monomial-Cartesian Codes constructed using two different generalized Reed-Solomon codes](https://arxiv.org/abs/2512.16482)
*Oisin Campion,Fernando Hernando,Gary McGuire*

Main category: cs.IT

TL;DR: 本文定义了广义单项式笛卡尔码(GMCC)，作为广义Reed-Solomon码的自然扩展，通过组合两个不同的广义RS码来构建GMCC，并建立了GMCC为Hermitian自正交的充分条件，从而获得新的量子码构造。


<details>
  <summary>Details</summary>
Motivation: 扩展广义Reed-Solomon码的概念，构建更一般的编码结构，并探索其在量子纠错码中的应用潜力。

Method: 1. 定义广义单项式笛卡尔码(GMCC)；2. 展示如何通过组合两个不同的广义Reed-Solomon码来构造GMCC；3. 建立GMCC为Hermitian自正交的充分条件。

Result: 提出了GMCC的新定义和构造方法，并证明了在特定条件下GMCC具有Hermitian自正交性，从而可以用于构造新的量子纠错码。

Conclusion: GMCC为广义Reed-Solomon码的有意义扩展，其Hermitian自正交特性为量子码构造提供了新的工具和可能性。

Abstract: In this work, we define Generalized Monomial Cartesian Codes (GMCC), which constitute a natural extension of generalized Reed-Solomon codes. We describe how two different generalized Reed-Solomon codes can be combined to construct one GMCC. We further establish sufficient conditions ensuring that the GMCC are Hermitian self-orthogonal, thus leading to new constructions of quantum codes.

</details>


### [10] [Novel Inconsistency Results for Partial Information Decomposition](https://arxiv.org/abs/2512.16662)
*Philip Hendrik Matthias,Abdullah Makkeh,Michael Wibral,Aaron J. Gutknecht*

Main category: cs.IT

TL;DR: PID的三个核心属性（非负性、链式法则、可逆变换不变性）在PID框架中无法同时满足，任何PID方法都必须牺牲至少一个经典信息论的基本性质。


<details>
  <summary>Details</summary>
Motivation: 尽管部分信息分解（PID）在理论和应用上有广泛发展，但缺乏统一的标准框架，存在多种竞争性方案。通过建立不一致性结果来澄清可能性边界，揭示必须做出的基本选择。

Method: 利用最近发展的部分信息分解的纯粹学方法，建立新颖的不一致性结果。主要定理证明三个经典信息论基石性质（非负性、链式法则、可逆变换不变性）在PID设置中相互不兼容。

Result: 证明了任何PID框架都必须至少牺牲一个经典信息论的基本性质。同时强化了Rauh等人的经典结果，表明非负性、恒等性质和Williams-Beer公理无法共存。

Conclusion: 部分信息分解领域存在根本性的权衡，无法同时满足所有理想性质。这迫使研究人员明确选择哪些信息论原则在PID背景下应该被保留或放弃，为未来的理论发展提供了清晰的边界。

Abstract: Partial Information Decomposition (PID) seeks to disentangle how information about a target variable is distributed across multiple sources, separating redundant, unique, and synergistic contributions. Despite extensive theoretical development and applications across diverse fields, the search for a unique, universally accepted solution remains elusive, with numerous competing proposals offering different decompositions. A promising but underutilized strategy for making progress is to establish inconsistency results, proofs that certain combinations of intuitively appealing axioms cannot be simultaneously satisfied. Such results clarify the landscape of possibilities and force us to recognize where fundamental choices must be made. In this work, we leverage the recently developed mereological approach to PID to establish novel inconsistency results with far-reaching implications. Our main theorem demonstrates that three cornerstone properties of classical information theory, namely non-negativity, the chain rule, and invariance under invertible transformations, become mutually incompatible when extended to the PID setting. This result reveals that any PID framework must sacrifice at least one property that seems fundamental to information theory itself. Additionally, we strengthen the classical result of Rauh et al., which showed that non-negativity, the identity property, and the Williams and Beer axioms cannot coexist.

</details>


### [11] [Confusions and Erasures of Error-Bounded Block Decoders with Finite Blocklength](https://arxiv.org/abs/2512.16665)
*Bin Han,Yao Zhu,Rafael F. Schaefer,Giuseppe Caire,Anke Schmeink,H. Vincent Poor,Hans D. Schotten*

Main category: cs.IT

TL;DR: 该论文在有限块长(FBL)下分析AWGN信道中块解码器的两类错误：未检测错误(混淆)和擦除，首次在FBL下系统研究混淆与擦除概率，验证上层协议假设物理层错误表现为擦除而非混淆的合理性。


<details>
  <summary>Details</summary>
Motivation: 上层协议普遍假设物理层错误表现为数据包擦除而非未检测的损坏，但这一假设缺乏物理层严格验证。块错误率(BLER)无法区分混淆和擦除，而两者对跨层协议设计影响显著不同。

Method: 在BLER约束的最大似然解码下，通过球体填充分析，提供块混淆和擦除概率的解析界，并推导这些界对块长和信噪比的敏感性。

Result: 首次在FBL下分析混淆和擦除，提供理论界和敏感性分析。发现对于实际FBL码，块混淆相比块擦除可忽略，特别是在大块长和高SNR下。

Conclusion: 验证了MAC和网络层协议常用的块擦除信道抽象假设的合理性，确认实际FBL码中混淆可忽略，为跨层协议设计提供理论依据。

Abstract: This paper investigates two distinct types of block errors - undetected errors (confusions) and erasures - in additive white Gaussian noise (AWGN) channels with error-bounded block decoders operating in the finite blocklength (FBL) regime. While block error rate (BLER) is a common metric, it does not distinguish between confusions and erasures, which can have significantly different impacts in cross-layer protocol design, despite upper-layer protocols universally assuming physical (PHY) errors manifest as packet erasures rather than undetected corruptions - an assumption lacking rigorous PHY-layer validation. We present a systematic analysis of confusions and erasures under BLER-constrained maximum likelihood (ML) decoding. Through sphere-packing analysis, we provide analytical bounds for both block confusion and erasure probabilities, and derive the sensitivities of these bounds to blocklength and signal-to-noise ratio (SNR). To the best of our knowledge, this is the first study on this topic in the FBL regime. Our findings provide theoretical validation for the block erasure channel abstraction commonly assumed in medium access control (MAC) and network layer protocols, confirming that, for practical FBL codes, block confusions are negligible compared to block erasures, especially at large blocklengths and high SNR.

</details>


### [12] [Secure Event-triggered MolecularvCommunication - Information Theoretic Perspective and Optimal Performance](https://arxiv.org/abs/2512.16761)
*Wafa Labidi,Vida Gholamian,Yaning Zhao,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 论文研究分子通信中的随机识别和安全随机识别，针对离散时间泊松信道，采用识别框架而非传统香农传输，分析容量公式以提高能效和安全性。


<details>
  <summary>Details</summary>
Motivation: 分子通信作为新兴研究领域，关注人体细胞间的通信机制和医疗应用。由于细胞通过释放分子传递信息，通常用泊松信道建模。传统香农通信不适合事件驱动的分子通信，而识别框架能显著提高能效和硬件要求。同时，受生物纳米物联网概念影响，体内通信需要安全保护。

Method: 采用离散时间泊松信道模型，使用Ahlswede和Dueck提出的识别框架。首先分析随机识别，然后扩展到安全随机识别。推导两种场景下的容量公式。

Result: 推导出随机识别和安全随机识别的容量公式，为分子通信的性能和安全影响提供了全面理解。

Conclusion: 识别框架比传统香农传输更适合分子通信，能实现双指数级增长，显著提高能效。安全随机识别为体内通信提供了必要的安全保护，具有重要的医疗应用价值。

Abstract: Molecular Communication (MC) is an emerging field of research focused on understanding how cells in the human body communicate and exploring potential medical applications. In theoretical analysis, the goal is to investigate cellular communication mechanisms and develop nanomachine-assisted therapies to combat diseases. Since cells transmit information by releasing molecules at varying intensities, this process is commonly modeled using Poisson channels. In our study, we consider a discrete-time Poisson channel (DTPC). MC is often event-driven, making traditional Shannon communication an unsuitable performance metric. Instead, we adopt the identification framework introduced by Ahlswede and Dueck. In this approach, the receiver is only concerned with detecting whether a specific message of interest has been transmitted. Unlike Shannon transmission codes, the size of identification (ID) codes for a discrete memoryless channel (DMC) increases doubly exponentially with blocklength when using randomized encoding. This remarkable property makes the ID paradigm significantly more efficient than classical Shannon transmission in terms of energy consumption and hardware requirements. Another critical aspect of MC, influenced by the concept of the Internet of Bio-NanoThings, is security. In-body communication must be protected against potential eavesdroppers. To address this, we first analyze the DTPC for randomized identification (RI) and then extend our study to secure randomized identification (SRI). We derive capacity formulas for both RI and SRI, providing a comprehensive understanding of their performance and security implications.

</details>


### [13] [Thermodynamics a la Souriau on Kähler Non Compact Symmetric Spaces for Cartan Neural Networks](https://arxiv.org/abs/2512.16772)
*Pietro G. Fré,Alexander S. Sorin,Mario Trigiante*

Main category: cs.IT

TL;DR: 本文澄清了非紧对称空间上热力学的抽象几何表述问题，证明了仅Kähler对称空间支持Gibbs分布，解决了温度空间确定问题，并建立了信息几何与热力学几何的统一性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在澄清Cartan神经网络新范式中隐藏层的数学模型——非紧对称空间上热力学抽象几何表述的几个问题，特别是区分动力系统相关的广义热力学与Souriau风格的Gibbs概率分布。

Method: 采用抽象几何方法研究非紧对称空间U/H上的热力学，证明仅Kähler对称空间支持Gibbs分布，利用伴随作用确定温度空间，并通过Paint群对称性将Poincaré和Siegel平面的构造推广到Calabi-Vesentini流形类。

Result: 主要结果是证明仅Kähler对称空间U/H支持Gibbs分布，确定了广义温度空间是U的伴随作用下Cartan子代数中正性域的轨道，建立了信息几何与热力学几何的统一性，证明了Gibbs分布在U对称群作用下的协变性。

Conclusion: 本文建立了非紧对称空间上热力学的完整几何框架，证明了Gibbs分布仅存在于Kähler对称空间，确定了温度空间结构，统一了信息几何与热力学几何，为Cartan神经网络提供了理论基础。

Abstract: In this paper, we clarify several issues concerning the abstract geometrical formulation of thermodynamics on non compact symmetric spaces $\mathrm{U/H}$ that are the mathematical model of hidden layers in the new paradigm of Cartan Neural Networks. We introduce a distinction between the generalized thermodynamics associated with Dynamical Systems and the challenging proposal of Gibbs probability distributions on $\mathrm{U/H}$ provided by generalized thermodynamics {à} la Souriau. Main result is the proof that $\mathrm{U/H}$.s supporting Gibbs distributions are only the Kähler ones. For the latter, we solve the problem of determining the space of temperatures, namely of Lie algebra elements for which the partition function converges. The space of generalized temperatures is the orbit under the adjoint action of $\mathrm{U}$ of a positivity domain in the Cartan subalgebra $C_c\subset\mathbb{H}$ of the maximal compact subalgebra $\mathbb{H}\subset\mathbb{U}$. We illustrate how our explicit constructions for the Poincaré and Siegel planes might be extended to the whole class of Calabi-Vesentini manifolds utilizing Paint Group symmetry. Furthermore we claim that Rao's, Chentsov's, Amari's Information Geometry and the thermodynamical geometry of Ruppeiner and Lychagin are the very same thing. The most important property of the Gibbs probability distributions provided by the here introduced setup is their covariance with respect to the action of the full group of symmetries $\mathrm{U}$. The partition function is invariant against $\mathrm{U}$ transformations and the set of its arguments, namely the generalized temperatures, can be always reduced to a minimal set whose cardinality is equal to the rank of the compact denominator group $\mathrm{H}\subset \mathrm{U}$.

</details>


### [14] [An Extension of Enumerative Sphere Shaping for Arbitrary Channel Input Distributions](https://arxiv.org/abs/2512.16808)
*Frederik Ritter,Andrej Rode,Laurent Schmalen*

Main category: cs.IT

TL;DR: 本文提出了一种广义枚举球面成形（ESS）方法，能够生成任意离散信道输入分布，相比传统ESS仅支持高斯类分布，扩展了其适用性到更多信道类型。


<details>
  <summary>Details</summary>
Motivation: 传统ESS算法虽然比CCDM等算法能利用更多信道输入符号序列，但其产生的信道输入符号分布是固定的，只能用于具有高斯类容量最优输入分布的信道，限制了ESS在大多数信道上的应用。

Method: 通过将ESS内部使用的固定权重替换为依赖于期望信道输入分布的权重，从而将ESS推广到能够产生任意离散信道输入分布。使用广义ESS结合概率幅度成形（PAS）进行数值仿真。

Result: 在256符号序列的简化无放大相干光链路模型仿真中，广义ESS相比CCDM在误帧率低于10^{-4}时，最大传输速率提高了0.0425比特/符号。

Conclusion: 广义ESS扩展了传统ESS的应用范围，使其能够适用于大多数信道类型，并在非高斯容量最优输入分布的信道上取得了显著的性能提升。

Abstract: A non-uniform channel input distribution is key for achieving the capacity of arbitrary channels. However, message bits are generally assumed to follow a uniform distribution which must first be transformed to a non-uniform distribution by using a distribution matching algorithm. One such algorithm is enumerative sphere shaping (ESS). Compared to algorithms such as constant composition distribution matching (CCDM), ESS can utilize more channel input symbol sequences, allowing it to achieve a comparably low rate loss. However, the distribution of channel input symbols produced by ESS is fixed, restricting the utility of ESS to channels with Gaussian-like capacity-achieving input distributions. In this paper, we generalize ESS to produce arbitrary discrete channel input distributions, making it usable on most channels. Crucially, our generalization replaces fixed weights used internally by ESS with weights depending on the desired channel input distribution. We present numerical simulations using generalized ESS with probabilistic amplitude shaping (PAS) to transmit sequences of 256 symbols over a simplified model of an unamplified coherent optical link, a channel with a distinctly non-Gaussian capacity-achieving input distribution. In these simulations, we found that generalized ESS improves the maximum transmission rate by 0.0425 bit/symbol at a frame error rate below 10^{-4} compared to CCDM.

</details>


### [15] [Toward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks](https://arxiv.org/abs/2512.16860)
*Emirhan Zor,Bora Bozkurt,Ferkan Yilmaz*

Main category: cs.IT

TL;DR: 提出了一种结合CRC辅助GRAND解码与SIC的两用户下行功率域NOMA方案，相比传统SIC方法显著改善了误码率性能


<details>
  <summary>Details</summary>
Motivation: 传统SIC方法在用户间功率差异较小时容易受到错误传播的影响，限制了NOMA系统的性能。需要一种更鲁棒的解码方案来减少错误传播并提高吞吐量。

Method: 提出CRC辅助的GRAND解码与SIC结合的NOMA框架：1) 利用GRAND的噪声中心策略系统性地排序和测试候选错误模式；2) CRC不仅用于错误检测，还辅助解码过程；3) 强用户通过GRAND增强的弱用户信号解码来应用SIC，减少错误传播。

Result: 在AWGN和瑞利衰落信道下的仿真结果表明，相比现有NOMA解码技术，CRC辅助GRAND-NOMA方法显著改善了BER性能，减少了系统开销，提高了吞吐量。

Conclusion: 将GRAND等通用解码方法集成到干扰受限的多用户环境中具有巨大潜力，可为未来鲁棒的无线网络提供更可靠的通信方案。

Abstract: Non-Orthogonal Multiple Access (NOMA) technology has emerged as a promising technology to enable massive connectivity and enhanced spectral efficiency in next-generation wireless networks. In this study, we propose a novel two-user downlink power-domain NOMA framework that integrates a Cyclic Redundancy Check (CRC)-aided Guessing Random Additive Noise Decoding (GRAND) with successive interference cancellation (SIC). Unlike conventional SIC methods, which are susceptible to error propagation when there is low power disparity between users, the proposed scheme leverages GRAND's noise-centric strategy to systematically rank and test candidate error patterns until the correct codeword is identified. In this architecture, CRC is utilized not only to detect errors but also to aid the decoding process, effectively eliminating the need for separate Forward Error Correction (FEC) codes and reducing overall system overhead. Furthermore, the strong user enhances its decoding performance by applying SIC that is reinforced by GRAND-based decoding of the weaker user's signals, thereby minimizing error propagation and increasing throughput. Comprehensive simulation results over both Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, under varying power allocations and user distances, show that the CRC-aided GRAND-NOMA approach significantly improves the Bit Error Rate (BER) performance compared to state-of-the-art NOMA decoding techniques. These findings underscore the potential of integrating universal decoding methods like GRAND into interference-limited multiuser environments for robust future wireless networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [16] [Multiple Source Localization via Local Radio Map Construction in Urban Environments](https://arxiv.org/abs/2512.15724)
*Qilu Zhang,Hongying Tang,Wen Chen,Ziyi Song,Jiang Wang*

Main category: eess.SP

TL;DR: LRM-MSL：一种基于局部无线电地图的多源定位通用框架，通过二值化和连通分量分析实现多源分离，再使用坐标回归网络进行单源定位，在VaryTxLoc数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市环境中的多源定位问题在认知无线电等领域至关重要，但现有方法要么无法有效利用接收信号强度信息，要么缺乏对任意数量源的泛化能力。

Method: 提出LRM-MSL框架：1) 构建仅保留源周围RSS信息的局部无线电地图并进行二值化；2) 应用连通分量分析实现多源分离，将MSL问题转化为一系列单源定位任务；3) 设计数值坐标回归网络执行单源定位。

Result: 由于缺乏公开的MSL RSS数据集，构建了VaryTxLoc数据集进行评估。实验结果表明LRM-MSL是一种准确有效的方法，优于最先进的方法。

Conclusion: LRM-MSL是一个通用的多源定位框架，能够处理任意数量的源，通过局部无线电地图和连通分量分析有效解决了多源分离问题，在性能上超越了现有方法。

Abstract: Accurately and efficiently addressing the multiple source localization (MSL) problem in urban environments, particularly designing a general method adaptable to an arbitrary number of sources, plays a crucial role in various fields such as cognitive radio (CR).
  Existing methods either fail to effectively utilize received signal strength (RSS) information without redundancy or lack generalizability to an arbitrary number of sources.
  In this work, we propose the Local Radio Map-Aided Multiple Source Localization Framework (LRM-MSL), which is a general method capable of handling an arbitrary number of sources.
  First, this framework constructs a local radio map that retains only the RSS information around the sources and binarizes it. Then, the connected component analysis tool is applied to the binarized map, which implements multi-source separation, transforming the MSL problem into a series of single-source localization (SSL) tasks.
  Finally, we design a numerical coordinate regression network to perform the SSL tasks.
  Since there is no publicly available RSS dataset for MSL, we construct the VaryTxLoc dataset to evaluate the performance of LRM-MSL. Experimental results demonstrate that LRM-MSL is an accurate and effective method, outperforming state-of-the-art approaches. Our code and dataset can be downloaded from https://github.com/hereis77/LRM-MSL.

</details>


### [17] [TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge](https://arxiv.org/abs/2512.15729)
*Matteo Fasulo,Giusy Spacone,Thorir Mar Ingolfsson,Yawei Li,Luca Benini,Andrea Cossettini*

Main category: eess.SP

TL;DR: TinyMyo：一个轻量级表面肌电信号基础模型，基于Transformer编码器架构，仅3.6M参数，通过自监督预训练实现多任务泛化，并在超低功耗微控制器上部署成功。


<details>
  <summary>Details</summary>
Motivation: 表面肌电信号在多个领域有广泛应用，但现有方法在跨被试、设备和协议的泛化能力上存在挑战。现有肌电基础模型局限于单一任务且难以在嵌入式平台部署。

Method: 提出TinyMyo轻量级基础模型，基于Transformer编码器架构，使用公开数据集进行自监督预训练。通过最小化任务特定头部适配，同一骨干网络可处理多种下游任务。

Result: 模型在多个数据集上达到或超越SOTA：NinaPro DB5 (89.4±0.16%)、UCI-EMG (97.56±0.32%)、EPN-612 (96.74±0.09%)。首次在超低功耗微控制器(GAP9)上部署肌电基础模型，平均功耗36.45mW。

Conclusion: TinyMyo为肌电社区提供了灵活的基础资源，能够加速未来研究，并展示了在资源受限设备上部署基础模型的可行性，推动了肌电信号处理的实际应用。

Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.

</details>


### [18] [HiLTS: Human in the Loop Therapeutic System: A Wireless-enabled Precision Medicine Platform for Brainwave Entrainment](https://arxiv.org/abs/2512.15807)
*Arfan Ghani*

Main category: eess.SP

TL;DR: 研究开发了一种最小化数字定制芯片，能够产生稳定的6Hz振荡来干扰癫痫发作活动，为低功耗可穿戴癫痫干预设备提供了概念验证。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作源于异常的神经同步活动，全球影响超过5000万人。尽管药物治疗有所进展，但仍有大量患者无法控制发作，需要替代的神经调节策略。现有的节律性神经夹带系统大多依赖复杂的模拟电子设备或高功率刺激硬件。

Method: 使用公开的EEG癫痫数据集，提取并平均模拟癫痫发作波形，将其数字化以模拟神经前端，并直接与芯片的数字输出记录接口。芯片脉冲序列经过重采样和低通重构，产生模拟6Hz波形，允许直接比较癫痫形态、其数字化表示和夹带输出。

Result: 频域和时域分析表明，芯片施加的窄带6Hz节律覆盖了癫痫活动的宽带频谱特征。这为低功耗数字定制夹带提供了概念验证。

Conclusion: 该研究为简化、可穿戴的癫痫中断设备提供了潜在途径，可用于精准医疗和未来医疗设备。

Abstract: Epileptic seizures arise from abnormally synchronised neural activity and remain a major global health challenge, affecting more than 50 million people worldwide. Despite advances in pharmacological interventions, a significant proportion of patients continue to experience uncontrolled seizures, underscoring the need for alternative neuromodulation strategies. Rhythmic neural entrainment has recently emerged as a promising mechanism for disrupting pathological synchrony, but most existing systems rely on complex analogue electronics or high-power stimulation hardware. This study investigates a minimal digital custom-designed chip that generates a stable 6 Hz oscillation capable of entraining epileptic seizure activity. Using a publicly available EEG seizure dataset, we extracted and averaged analogue seizure waveforms, digitised them to emulate neural front-ends, and directly interfaced the digitised signals with digital output recordings acquired from the chip using a Saleae Logic analyser. The chip pulse train was resampled and low-pass-reconstructed to produce an analogue 6 Hz waveform, allowing direct comparison between seizure morphology, its digitised representation, and the entrained output. Frequency-domain and time-domain analyses demonstrate that the chip imposes a narrow-band 6 Hz rhythm that overrides the broadband spectral profile of seizure activity. These results provide a proof-of-concept for low-power digital custom-designed entrainment as a potential pathway toward simplified, wearable seizure-interruption devices for precision medicine and future healthcare devices.

</details>


### [19] [Concurrence: A dependence criterion for time series, applied to biological data](https://arxiv.org/abs/2512.16001)
*Evangelos Sariyanidi,John D. Herrington,Lisa Yankowitz,Pratik Chaudhari,Theodore D. Satterthwaite,Casey J. Zampella,Jeffrey S. Morris,Edward Gunning,Robert T. Schultz,Russell T. Shinohara,Birkan Tunc*

Main category: eess.SP

TL;DR: 提出"concurrence"依赖度量标准，通过分类器区分时间序列对齐与错位片段来检测复杂非线性依赖关系


<details>
  <summary>Details</summary>
Motivation: 生物系统常表现出复杂的非线性相互作用，现有方法需要先验知识或大数据集，难以捕捉这些依赖关系

Method: 引入concurrence准则：如果能够构建分类器区分时间序列的对齐与错位片段，则认为两个时间序列存在依赖关系

Result: 该方法理论上与依赖关系相关，能跨学科应用，可检测fMRI、生理和行为数据等多种信号的关系，无需参数调优或大数据

Conclusion: concurrence可作为科学分析的标准方法，广泛检测复杂非线性依赖关系，促进跨学科发现

Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.

</details>


### [20] [Simultaneous Secrecy and Covert Communications (SSACC) in Mobility-Aware RIS-Aided Networks](https://arxiv.org/abs/2512.16224)
*Yanyu Cheng,Yujian Hu,Haoran Liu,Hua Zhong,Wei Wang,Pan Li,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出一种RIS辅助网络中结合保密通信和隐蔽通信的方案，使用生成扩散模型和深度强化学习优化功率分配，在用户移动性下平衡安全性和容量性能。


<details>
  <summary>Details</summary>
Motivation: 在RIS辅助网络中，需要同时实现保密通信（防止窃听）和隐蔽通信（防止检测），传统方法难以在用户移动性下平衡这两种安全需求。

Method: 1) 提出SSACC方案，最大化保密容量和检测错误概率；2) 在最坏情况下推导AMDEP和ASC的闭式表达式；3) 提出新性能指标，基于GDM和DRL设计算法优化功率分配。

Result: 仿真结果显示，相比传统DDPG方法，所提算法收敛更快、性能更优，能有效平衡安全性和容量性能。

Conclusion: 提出的SSACC方案和GDM-DRL算法在RIS辅助网络中能有效实现保密和隐蔽通信的平衡，为移动用户提供更好的安全保障。

Abstract: In this paper, we propose a simultaneous secrecy and covert communications (SSACC) scheme in a reconfigurable intelligent surface (RIS)-aided network with a cooperative jammer. The scheme enhances communication security by maximizing the secrecy capacity and the detection error probability (DEP). Under a worst-case scenario for covert communications, we consider that the eavesdropper can optimally adjust the detection threshold to minimize the DEP. Accordingly, we derive closedform expressions for both average minimum DEP (AMDEP) and average secrecy capacity (ASC). To balance AMDEP and ASC, we propose a new performance metric and design an algorithm based on generative diffusion models (GDM) and deep reinforcement learning (DRL). The algorithm maximizes data rates under user mobility while ensuring high AMDEP and ASC by optimizing power allocation. Simulation results demonstrate that the proposed algorithm achieves faster convergence and superior performance compared to conventional deep deterministic policy gradient (DDPG) methods, thereby validating its effectiveness in balancing security and capacity performance.

</details>


### [21] [Fast Collaborative Inference via Distributed Speculative Decoding](https://arxiv.org/abs/2512.16273)
*Ce Zheng,Ke Zhang,Sun Chen,Wenqi Zhang,Qiong Liu,Angesom Ataklity Tesfay*

Main category: eess.SP

TL;DR: 提出TSLT方法，通过传输截断稀疏对数来减少AI-RAN中分布式推测解码的上行链路通信开销，同时保持接受率和推理质量。


<details>
  <summary>Details</summary>
Motivation: 在AI原生无线接入网络中，设备-边缘协同推理使用推测解码时，现有分布式方案在每个步骤传输完整词汇表对数，导致显著的上行链路开销。

Method: 提出截断稀疏对数传输策略，只传输截断候选集的logits和索引；扩展到多候选情况以提高接受概率；提供理论保证证明接受率在TSLT下得以保持。

Result: 实验显示TSLT显著减少上行链路通信，同时保持端到端推理延迟和模型质量，证明其在未来AI-RAN系统中实现可扩展、通信高效分布式LLM推理的有效性。

Conclusion: TSLT方法为AI-RAN系统中的分布式LLM推理提供了通信高效的解决方案，通过稀疏化传输策略在保持性能的同时大幅降低通信开销。

Abstract: Speculative decoding accelerates large language model (LLM) inference by allowing a small draft model to predict multiple future tokens for verification by a larger target model. In AI-native radio access networks (AI-RAN), this enables device-edge collaborative inference but introduces significant uplink overhead, as existing distributed speculative decoding schemes transmit full vocabulary logits at every step. We propose a sparsify-then-sample strategy, Truncated Sparse Logits Transmission (TSLT), which transmits only the logits and indices of a truncated candidate set. We provide theoretical guarantees showing that the acceptance rate is preserved under TSLT. TSLT is further extended to multi-candidate case, where multiple draft candidates per step increase acceptance probability. Experiments show that TSLT significantly reduces uplink communication while maintaining end-to-end inference latency and model quality, demonstrating its effectiveness for scalable, communication-efficient distributed LLM inference in future AI-RAN systems.

</details>


### [22] [CPMamba: Selective State Space Models for MIMO Channel Prediction in High-Mobility Environments](https://arxiv.org/abs/2512.16315)
*Sheng Luo,Jiashu Xie,Yueling Che,Junmei Yao,Jian Tian,Daquan Feng,Kaishun Wu*

Main category: eess.SP

TL;DR: CPMamba：基于选择性状态空间模型的高效MIMO-OFDM信道预测框架，在保持线性计算复杂度的同时实现最先进的预测精度


<details>
  <summary>Details</summary>
Motivation: 现有信道预测方法存在高复杂度问题，且无法准确建模快速时变信道的时序变化特性，特别是在高移动性场景下，信道预测对于抵抗信道老化和保证通信质量至关重要

Method: 提出CPMamba框架：1）使用专门设计的特征提取和嵌入网络从历史CSI中提取特征；2）采用堆叠的残差Mamba模块进行时序建模；3）利用输入相关的选择性机制动态调整状态转移，有效捕获CSI间的长程依赖关系

Result: 在3GPP标准信道模型下的仿真结果表明：CPMamba在所有场景下都达到了最先进的预测精度，具有优异的泛化能力和鲁棒性；相比现有基线模型，参数量减少约50%，同时性能相当或更好

Conclusion: CPMamba通过选择性状态空间模型实现了高效的信道预测，在保持线性计算复杂度的同时显著提升了预测精度，大幅降低了实际部署的门槛

Abstract: Channel prediction is a key technology for improving the performance of various functions such as precoding, adaptive modulation, and resource allocation in MIMO-OFDM systems. Especially in high-mobility scenarios with fast time-varying channels, it is crucial for resisting channel aging and ensuring communication quality. However, existing methods suffer from high complexity and the inability to accurately model the temporal variations of channels. To address this issue, this paper proposes CPMamba -- an efficient channel prediction framework based on the selective state space model. The proposed CPMamba architecture extracts features from historical channel state information (CSI) using a specifically designed feature extraction and embedding network and employs stacked residual Mamba modules for temporal modeling. By leveraging an input-dependent selective mechanism to dynamically adjust state transitions, it can effectively capture the long-range dependencies between the CSIs while maintaining a linear computational complexity. Simulation results under the 3GPP standard channel model demonstrate that CPMamba achieves state-of-the-art prediction accuracy across all scenarios, along with superior generalization and robustness. Compared to existing baseline models, CPMamba reduces the number of parameters by approximately 50 percent while achieving comparable or better performance, thereby significantly lowering the barrier for practical deployment.

</details>


### [23] [An active-set algorithm for spectral unmixing](https://arxiv.org/abs/2512.16432)
*Nils Foix-Colonier,Sébastien Bourguignon*

Main category: eess.SP

TL;DR: 提出一种针对线性光谱解混问题的专用算法，支持最小丰度约束，相比通用求解器具有计算优势


<details>
  <summary>Details</summary>
Motivation: 线性光谱解混在非负性和和为1约束下是凸优化问题，现有算法众多。实践中（特别是监督解混，即使用大字典时），由于丰度的非负性，解往往稀疏，这促使使用主动集求解器。考虑到问题的特定特征，设计专用算法相比通用求解器可以获得计算性能优势。

Method: 提出一种专用算法，将非负性约束扩展到更广泛的最小丰度约束，设计针对性的求解策略

Result: 未在摘要中明确说明具体结果，但暗示该算法相比通用求解器具有计算性能优势

Conclusion: 针对线性光谱解混问题设计专用算法是必要的，特别是当需要处理最小丰度约束时，可以获得更好的计算性能

Abstract: Linear spectral unmixing under nonnegativity and sum-to-one constraints is a convex optimization problem for which many algorithms were proposed. In practice, especially for supervised unmixing (i.e., with a large dictionary), solutions tend to be sparse due to the nonnegativity of the abundances, thereby motivating the use of an active-set solver. Given the problem specific features, it seems advantageous to design a dedicated algorithm in order to gain computational performance compared to generic solvers. In this paper, we propose to derive such a specific algorithm, while extending the nonnegativity constraints to broader minimum abundance constraints.

</details>


### [24] [Robust 6G OFDM High-Mobility Communications Using Delay-Doppler Superimposed Pilots](https://arxiv.org/abs/2512.16496)
*Mauro Marchese,Pietro Savazzi*

Main category: eess.SP

TL;DR: 提出一种用于6G高移动性场景的OFDM接收机架构，采用延迟-多普勒叠加导频方案进行信道估计，考虑ICI、分数延迟和多普勒频移，实现鲁棒通信性能


<details>
  <summary>Details</summary>
Motivation: 6G高移动性场景（速度高达1000 km/h）对OFDM系统带来挑战，现有延迟-多普勒叠加导频研究未充分考虑ICI、分数延迟和多普勒频移的影响

Method: 1) 在延迟-多普勒域添加单个导频的叠加导频方案；2) 推导分离分数延迟-多普勒估计算法；3) 提出基于Landweber迭代的低复杂度均衡方法，利用固有信道结构

Result: 仿真结果显示，所提接收机架构在各种移动条件下（速度高达1000 km/h）实现鲁棒通信性能，相比现有方法提高了有效吞吐量

Conclusion: 该研究为6G高移动性OFDM通信提供了一种有效的接收机架构，通过考虑实际信道效应并采用低复杂度算法，实现了高性能和高吞吐量

Abstract: In this work, a novel receiver architecture for orthogonal frequency division multiplexing (OFDM) communications in 6G high-mobility scenarios is developed. In particular, a delay-Doppler superimposed pilot (SP) scheme is used for channel estimation (CE) by adding a single pilot in the delay-Doppler domain. Unlike previous research on delay-Doppler superimposed pilots in OFDM systems, intercarrier interference (ICI) effects, fractional delays, and Doppler shifts are considered. Consequently, a disjoint fractional delay-Doppler estimation algorithm is derived, and a reduced-complexity equalization method based on the Landweber iteration, which exploits intrinsic channel structure, is proposed. Simulation results reveal that the proposed receiver architecture achieves robust communication performance across various mobility conditions, with speeds of up to 1000 km/h, and increases the effective throughput compared to existing methods.

</details>


### [25] [Efficient Precoding for LEO Satellites: A Low-Complexity Matrix Inversion Method via Woodbury Matrix Identity and arSVD](https://arxiv.org/abs/2512.16543)
*Mohammad Momani,Thomas Delamotte,Andreas Knopp*

Main category: eess.SP

TL;DR: 提出结合Woodbury公式与自适应随机奇异值分解的低复杂度预编码框架，用于LEO卫星大规模天线阵列，相比传统RZF预编码计算量减少61%，仅轻微损失和速率性能。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星大规模天线阵列需要计算高效且自适应的预编码技术，以应对动态信道变化并提升频谱效率。传统正则化迫零预编码的Gram矩阵求逆计算复杂，限制了实时实现。

Method: 开发低复杂度框架，整合Woodbury公式与自适应随机奇异值分解。Woodbury公式利用低秩扰动降低求逆复杂度，arSVD动态提取主导奇异分量，进一步提升计算效率，实现Gram矩阵逆的实时更新。

Result: 蒙特卡洛仿真表明，相比传统RZF预编码的全矩阵求逆，该方法计算量减少高达61%，仅带来和速率性能的轻微下降。

Conclusion: WB-arSVD为下一代卫星通信提供了可扩展且高效的解决方案，适用于功率受限环境的实时部署。

Abstract: The increasing deployment of massive active antenna arrays in low Earth orbit (LEO) satellites necessitates computationally efficient and adaptive precoding techniques to mitigate dynamic channel variations and enhance spectral efficiency. Regularized zero-forcing (RZF) precoding is widely used in multi-user MIMO systems; however, its real-time implementation is limited by the computationally intensive inversion of the Gram matrix. In this work, we develop a low-complexity framework that integrates the Woodbury (WB) formula with adaptive randomized singular value decomposition (arSVD) to efficiently update the Gram matrix inverse as the satellite moves along its orbit. By leveraging low-rank perturbations, the WB formula reduces inversion complexity, while arSVD dynamically extracts dominant singular components, further enhancing computational efficiency. Monte Carlo simulations demonstrate that the proposed method achieves computational savings of up to 61\% compared to conventional RZF precoding with full matrix inversion, while incurring only a modest degradation in sum-rate performance. These results demonstrate that WB-arSVD offers a scalable and efficient solution for next-generation satellite communications, facilitating real-time deployment in power-constrained environments.

</details>


### [26] [Channel State Information Preprocessing for CSI-based Physical-Layer Authentication Using Reconciliation](https://arxiv.org/abs/2512.16719)
*Atsu Kokuvi Angelo Passah,Rodrigo C. de Lamare,Arsenia Chorti*

Main category: eess.SP

TL;DR: 提出自适应RPCA预处理方法，提升CSI物理层认证精度，通过信息协调框架显著降低错误概率


<details>
  <summary>Details</summary>
Motivation: 传统CSI-PLA面临信道状态信息在时域上的变化和不一致问题，需要自适应预处理技术来提升认证准确性

Method: 基于鲁棒主成分分析开发自适应RPCA预处理方法，结合基于信息协调的PLA框架，利用Polar码的高斯近似设计短码长Slepian Wolf解码器

Result: 相比无预处理和无协调的基线方案，A-RPCA显著降低协调后错误概率，在LOS和NLOS场景下检测概率均达到1，优于PCA、鲁棒PCA、自编码器和ReProCS等现有方法

Conclusion: 自适应RPCA预处理方法能有效缓解CSI时域变化问题，显著提升物理层认证性能，在合成和真实数据集中均表现出优越性能

Abstract: This paper introduces an adaptive preprocessing technique to enhance the accuracy of channel state information-based physical layer authentication (CSI-PLA) alleviating CSI variations and inconsistencies in the time domain. To this end, we develop an adaptive robust principal component analysis (A-RPCA) preprocessing method based on robust principal component analysis (RPCA). The performance evaluation is then conducted using a PLA framework based on information reconciliation, in which Gaussian approximation (GA) for Polar codes is leveraged for the design of short codelength Slepian Wolf decoders. Furthermore, an analysis of the proposed A-RPCA methods is carried out. Simulation results show that compared to a baseline scheme without preprocessing and without reconciliation, the proposed A-RPCA method substantially reduces the error probability after reconciliation and also substantially increases the detection probabilities that is also 1 in both line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios. We have compared against state-of the-art preprocessing schemes in both synthetic and real datasets, including principal component analysis (PCA) and robust PCA, autoencoders and the recursive projected compressive sensing (ReProCS) framework and we have validated the superior performance of the proposed approach.

</details>


### [27] [Misspecified Crame-Rao Bound for AoA Estimation at a ULA under a Spoofing Attack](https://arxiv.org/abs/2512.16735)
*Sotiris Skaperas,Arsenia Chorti*

Main category: eess.SP

TL;DR: 提出一个基于误设定Cramér-Rao界(MCRB)的框架，用于分析主动攻击对基于到达角(AoA)的物理层认证的影响，推导出闭式MCRB表达式并揭示攻击引入的惩罚项。


<details>
  <summary>Details</summary>
Motivation: 研究主动攻击对基于到达角的物理层认证系统的影响，传统CRB无法准确评估攻击场景下的性能界限，需要建立考虑攻击者存在的理论分析框架。

Method: 使用误设定Cramér-Rao界(MCRB)理论框架，分析单天线用户与M天线ULA验证器之间的AoA认证系统，考虑具有L天线的欺骗攻击者，假设确定性导频信号。

Result: 推导出闭式MCRB表达式，发现攻击引入的惩罚项不依赖于信噪比，而是取决于攻击者位置、阵列几何结构和攻击者预编码向量。

Conclusion: MCRB框架有效分析了主动攻击对物理层认证的影响，攻击引入的惩罚项揭示了系统性能下降的关键因素，为抗攻击认证系统设计提供了理论指导。

Abstract: A framework is presented for analyzing the impact of active attacks to location-based physical layer authentication (PLA) using the machinery of misspecified Cramér--Rao bound (MCRB). In this work, we focus on the MCRB in the angle-of-arrival (AoA) based authentication of a single antenna user when the verifier posseses an $M$ antenna element uniform linear array (ULA), assuming deterministic pilot signals; in our system model the presence of a spoofing adversary with an arbitrary number $L$ of antenna elements is assumed. We obtain a closed-form expression for the MCRB and demonstrate that the attack introduces in it a penalty term compared to the classic CRB, which does not depend on the signal-to-noise ratio (SNR) but on the adversary's location, the array geometry and the attacker precoding vector.

</details>


### [28] [Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer](https://arxiv.org/abs/2512.16786)
*Chenyu Zhu,Zeyang Li,Ziyi Xie,Jie Zhang*

Main category: eess.SP

TL;DR: 提出一种集成复数变分模态分解算法，结合时序卷积网络和空间注意力机制，在少量符号数据下实现96%准确率的特定发射器识别


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的特定发射器识别方法通常需要大量数据或先验信息，在实际场景中面临标记数据有限的挑战

Method: 1) 集成复数变分模态分解算法分解和重构复数信号以逼近原始发射信号；2) 使用时序卷积网络建模序列信号特征；3) 引入空间注意力机制自适应加权信息丰富的信号段；4) 分支网络利用预训练权重减少对辅助数据集的需求

Result: 在模拟数据上的消融实验验证了各模块的有效性，在公开数据集上仅使用10个符号且无需任何先验知识就达到了96%的准确率

Conclusion: 该方法在有限标记数据场景下实现了高性能的特定发射器识别，为物理层安全提供了有效的解决方案

Abstract: Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.

</details>
