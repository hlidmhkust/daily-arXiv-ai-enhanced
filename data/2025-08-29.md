<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 8]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.IT](#cs.IT) [Total: 10]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [A Machine Learning Approach to Volumetric Computations of Solid Pulmonary Nodules](https://arxiv.org/abs/2508.20127)
*Yihan Zhou,Haocheng Huang,Yue Yu,Jianhui Shang*

Main category: eess.IV

TL;DR: 提出了一种结合多尺度3D CNN和亚型特异性偏差校正的肺结节体积估计框架，相比传统方法误差降低17个百分点，处理速度提升3倍


<details>
  <summary>Details</summary>
Motivation: 早期肺癌检测依赖于CT扫描中肺结节的精确体积评估，传统方法如CTR和球形近似由于结节形状和密度变化导致估计不一致

Method: 使用多尺度3D卷积神经网络结合亚型特异性偏差校正进行精确体积估计，在364例上海胸科医院数据集上训练和评估

Result: 平均绝对偏差为8.0%，推理时间低于20秒/扫描，相比现有深度学习和半自动化流程（误差25-30%，处理时间60+秒）有显著提升

Conclusion: 该方法为临床肺结节筛查和监测提供了高精度、高效且可扩展的工具，有望改善早期肺癌检测

Abstract: Early detection of lung cancer is crucial for effective treatment and relies
on accurate volumetric assessment of pulmonary nodules in CT scans. Traditional
methods, such as consolidation-to-tumor ratio (CTR) and spherical
approximation, are limited by inconsistent estimates due to variability in
nodule shape and density. We propose an advanced framework that combines a
multi-scale 3D convolutional neural network (CNN) with subtype-specific bias
correction for precise volume estimation. The model was trained and evaluated
on a dataset of 364 cases from Shanghai Chest Hospital. Our approach achieved a
mean absolute deviation of 8.0 percent compared to manual nonlinear regression,
with inference times under 20 seconds per scan. This method outperforms
existing deep learning and semi-automated pipelines, which typically have
errors of 25 to 30 percent and require over 60 seconds for processing. Our
results show a reduction in error by over 17 percentage points and a threefold
acceleration in processing speed. These advancements offer a highly accurate,
efficient, and scalable tool for clinical lung nodule screening and monitoring,
with promising potential for improving early lung cancer detection.

</details>


### [2] [Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads](https://arxiv.org/abs/2508.20135)
*Andrew Yarovoi,Christopher R. Valenta*

Main category: eess.IV

TL;DR: 提出了一种数据高效的点云分割框架，通过两阶段训练（多数据集预训练+小样本微调）在仅50个标注点云的情况下，将mIoU从33.5%提升到51.8%


<details>
  <summary>Details</summary>
Motivation: 解决在有限标注数据情况下实现鲁棒3D语义分割的挑战，特别是在非铺装道路等复杂场景中

Method: 两阶段训练框架：先在混合公共数据集和小规模领域数据集上预训练投影式CNN，然后仅用领域数据微调轻量级预测头；探索了Point Prompt Training、Manifold Mixup正则化和直方图归一化环境光等技术

Result: 仅使用50个标注点云，mIoU从33.5%提升到51.8%，整体准确率从85.5%提升到90.8%

Conclusion: 跨多个数据集的预训练是提高泛化能力的关键，在有限监督下也能实现鲁棒分割，为低数据场景下的3D语义分割提供了实用框架

Abstract: In this case study, we present a data-efficient point cloud segmentation
pipeline and training framework for robust segmentation of unimproved roads and
seven other classes. Our method employs a two-stage training framework: first,
a projection-based convolutional neural network is pre-trained on a mixture of
public urban datasets and a small, curated in-domain dataset; then, a
lightweight prediction head is fine-tuned exclusively on in-domain data. Along
the way, we explore the application of Point Prompt Training to batch
normalization layers and the effects of Manifold Mixup as a regularizer within
our pipeline. We also explore the effects of incorporating histogram-normalized
ambients to further boost performance. Using only 50 labeled point clouds from
our target domain, we show that our proposed training approach improves mean
Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%
to 90.8%, when compared to naive training on the in-domain data. Crucially, our
results demonstrate that pre-training across multiple datasets is key to
improving generalization and enabling robust segmentation under limited
in-domain supervision. Overall, this study demonstrates a practical framework
for robust 3D semantic segmentation in challenging, low-data scenarios. Our
code is available at: https://github.com/andrewyarovoi/MD-FRNet.

</details>


### [3] [Global Motion Corresponder for 3D Point-Based Scene Interpolation under Large Motion](https://arxiv.org/abs/2508.20136)
*Junru Lin,Chirag Vashist,Mikaela Angelina Uy,Colton Stearns,Xuan Luo,Leonidas Guibas,Ke Li*

Main category: eess.IV

TL;DR: GMC是一种处理大运动场景插值的新方法，通过SE(3)映射到共享规范空间，显著优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有动态场景插值方法假设相邻时间步之间的运动足够小，但实践中即使轻微偏离这个假设也会导致传统技术失败

Method: 学习一元势场预测SE(3)映射到共享规范空间，平衡对应关系、空间和语义平滑性以及局部刚性

Result: 在经历大全局运动的3D场景插值任务上显著优于现有基线方法，并具备其他基线方法无法实现的外推能力

Conclusion: GMC方法能够鲁棒地处理大运动并实现平滑过渡，为动态场景插值提供了更强大的解决方案

Abstract: Existing dynamic scene interpolation methods typically assume that the motion
between consecutive timesteps is small enough so that displacements can be
locally approximated by linear models. In practice, even slight deviations from
this small-motion assumption can cause conventional techniques to fail. In this
paper, we introduce Global Motion Corresponder (GMC), a novel approach that
robustly handles large motion and achieves smooth transitions. GMC learns unary
potential fields that predict SE(3) mappings into a shared canonical space,
balancing correspondence, spatial and semantic smoothness, and local rigidity.
We demonstrate that our method significantly outperforms existing baselines on
3D scene interpolation when the two states undergo large global motions.
Furthermore, our method enables extrapolation capabilities where other baseline
methods cannot.

</details>


### [4] [Is the medical image segmentation problem solved? A survey of current developments and future directions](https://arxiv.org/abs/2508.20139)
*Guoping Xu,Jayaram K. Udupa,Jax Luo,Songlin Zhao,Yajun Yu,Scott B. Raymond,Hao Peng,Lipeng Ning,Yogesh Rathi,Wei Liu,You Zhang*

Main category: eess.IV

TL;DR: 这篇论文对过去十年医学图像分割领域进行了深度综述，涵盖了从监督学习到半监督/无监督学习、从器官分割到病灶分割、从2D到3D/4D分割等七个关键维度的进展。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在过去二十年中推动医学图像分割快速发展，需要系统评估当前模型在多大程度上解决了持续存在的挑战，以及还存在哪些差距，为未来研究提供指导。

Method: 通过深入回顾医学图像分割的发展历程，分析多尺度分析、注意力机制和先验知识集成等核心原理在网络编码器、瓶颈、跳跃连接和解码器组件中的应用。

Result: 论文提供了医学图像分割领域的全面概述，识别了七个关键发展趋势，并建立了持续更新的文献和开源资源库以支持后续研究。

Conclusion: 该综述为深度学习医学图像分割的发展轨迹提供了整体视角，旨在激发未来创新，并通过维护更新的资源库支持持续的研究工作。

Abstract: Medical image segmentation has advanced rapidly over the past two decades,
largely driven by deep learning, which has enabled accurate and efficient
delineation of cells, tissues, organs, and pathologies across diverse imaging
modalities. This progress raises a fundamental question: to what extent have
current models overcome persistent challenges, and what gaps remain? In this
work, we provide an in-depth review of medical image segmentation, tracing its
progress and key developments over the past decade. We examine core principles,
including multiscale analysis, attention mechanisms, and the integration of
prior knowledge, across the encoder, bottleneck, skip connections, and decoder
components of segmentation networks. Our discussion is organized around seven
key dimensions: (1) the shift from supervised to semi-/unsupervised learning,
(2) the transition from organ segmentation to lesion-focused tasks, (3)
advances in multi-modality integration and domain adaptation, (4) the role of
foundation models and transfer learning, (5) the move from deterministic to
probabilistic segmentation, (6) the progression from 2D to 3D and 4D
segmentation, and (7) the trend from model invocation to segmentation agents.
Together, these perspectives provide a holistic overview of the trajectory of
deep learning-based medical image segmentation and aim to inspire future
innovation. To support ongoing research, we maintain a continually updated
repository of relevant literature and open-source resources at
https://github.com/apple1986/medicalSegReview

</details>


### [5] [UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases](https://arxiv.org/abs/2508.20141)
*Ruowei Tang,Pengfei Zhao,Xiaoguang Li,Ning Xu,Yue Cheng,Mengshi Zhang,Zhixiang Wang,Zhengyu Zhang,Hongxia Yin,Heyu Ding,Shusheng Gong,Yuhe Liu,Zhenchang Wang*

Main category: eess.IV

TL;DR: 本文介绍了UltraEar数据库的建立，这是一个大规模、多中心的0.1mm等向性超高分辨率耳部CT图像数据库，包含丰富的临床数据，用于耳科疾病研究和AI算法开发。


<details>
  <summary>Details</summary>
Motivation: 耳部疾病影响全球数十亿人，CT在准确诊断和治疗规划中起关键作用，但缺乏大规模、高质量的耳部影像数据库来支持研究和AI发展。

Method: 从11家三级医院招募患者（2020-2035年），收集超高分辨率CT图像、结构化报告和完整临床信息，开发标准化预处理流程进行几何校准、图像标注和多结构分割。

Result: 建立了包含多种耳科疾病的大型数据库，涵盖中耳炎、胆脂瘤、听骨链畸形等多种疾病，数据经过匿名化处理并安全存储。

Conclusion: UltraEar数据库为耳科影像研究提供了前所未有的高质量资源，具有推动放射学研究、AI算法开发、教育培训和多机构合作研究的巨大潜力。

Abstract: Ear diseases affect billions of people worldwide, leading to substantial
health and socioeconomic burdens. Computed tomography (CT) plays a pivotal role
in accurate diagnosis, treatment planning, and outcome evaluation. The
objective of this study is to present the establishment and design of UltraEar
Database, a large-scale, multicentric repository of isotropic 0.1 mm
ultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated
to ear diseases. UltraEar recruits patients from 11 tertiary hospitals between
October 2020 and October 2035, integrating U-HRCT images, structured CT
reports, and comprehensive clinical information, including demographics,
audiometric profiles, surgical records, and pathological findings. A broad
spectrum of otologic disorders is covered, such as otitis media, cholesteatoma,
ossicular chain malformation, temporal bone fracture, inner ear malformation,
cochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus
bony deficiency. Standardized preprocessing pipelines have been developed for
geometric calibration, image annotation, and multi-structure segmentation. All
personal identifiers in DICOM headers and metadata are removed or anonymized to
ensure compliance with data privacy regulation. Data collection and curation
are coordinated through monthly expert panel meetings, with secure storage on
an offline cloud system. UltraEar provides an unprecedented
ultra-high-resolution reference atlas with both technical fidelity and clinical
relevance. This resource has significant potential to advance radiological
research, enable development and validation of AI algorithms, serve as an
educational tool for training in otologic imaging, and support
multi-institutional collaborative studies. UltraEar will be continuously
updated and expanded, ensuring long-term accessibility and usability for the
global otologic research community.

</details>


### [6] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 使用iPhone 15 Pro Max的LiDAR深度感应器和颜色摄像头进行实时背景移除和合成，充分利用深度信息不受光照影响的优势


<details>
  <summary>Details</summary>
Motivation: 解决传统背景移除技术（如蓝幕搭键）和AI模型在光照条件下表现不稳定的问题，提供一种更可靠的方案

Method: 集成iPhone 15 Pro Max的LiDAR和颜色摄像头，使用SwiftUI和Swift框架开发用户界面和后端，采用Metal Shader Language进行GPU加速的实时图像处理，达到60fps的游戏级帧率

Result: 系统能够在低光和光线充足环境下都表现稳定，深度信息不受主体光照影响，目前深度地图分辨率为320x240

Conclusion: LiDAR技术在移动设备上有期成为视频和摄影应用中领先的背景移除方法，主要限制是深度数据分辨率和某些材质的反射性能

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile
devices can be used as a replacement for traditional background removal and
compositing techniques. Unlike approaches such as chroma keying and trained AI
models, LiDAR's depth information is independent of subject lighting, and
performs equally well in low-light and well-lit environments. We integrate the
LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image
processing. We use Apple's SwiftUI and Swift frameworks for user interface and
backend development, and Metal Shader Language (MSL) for realtime image
enhancement at the standard iPhone streaming frame rate of 60 frames per
second. The only meaningful limitations of the technology are the streaming
bandwidth of the depth data, which currently reduces the depth map resolution
to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect
accurate depth from some materials. If the LiDAR resolution on a mobile device
like the iPhone can be improved to match the color image resolution, LiDAR
could feasibly become the preeminent method of background removal for video
applications and photography.

</details>


### [7] [GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction](https://arxiv.org/abs/2508.20600)
*Kian Anvari Hamedani,Narges Razizadeh,Shahabedin Nabavi,Mohsen Ebrahimi Moghaddam*

Main category: eess.IV

TL;DR: GENRE-CMR是一个基于GAN的生成对抗网络架构，采用残差深度展开重建框架，通过边缘感知区域损失和统计分布对齐损失来提高心脏磁共振图像重建的保真度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 加速心脏磁共振(CMR)图像重建在扫描时间和图像质量之间存在权衡挑战，特别是在不同采集设置下的泛化能力方面存在困难。

Method: 采用生成对抗网络(GAN)架构，使用残差深度展开重建框架，将迭代优化展开为卷积子网络级联，并整合边缘感知区域(EAR)损失和统计分布对齐(SDA)损失两种损失函数。

Result: 在训练数据和未见数据上均超越最先进方法，在未见数据分布上达到0.9552 SSIM和38.90 dB PSNR，在各种加速因子和采样轨迹下表现优异。

Conclusion: 该框架为高质量CMR重建提供了统一且鲁棒的解决方案，为在不同采集协议下的临床适应性部署铺平了道路。

Abstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction
remains a critical challenge due to the trade-off between scan time and image
quality, particularly when generalizing across diverse acquisition settings. We
propose GENRE-CMR, a generative adversarial network (GAN)-based architecture
employing a residual deep unrolled reconstruction framework to enhance
reconstruction fidelity and generalization. The architecture unrolls iterative
optimization into a cascade of convolutional subnetworks, enriched with
residual connections to enable progressive feature propagation from shallow to
deeper stages. To further improve performance, we integrate two loss functions:
(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on
structurally informative regions and helps prevent common reconstruction
blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which
regularizes the feature space across diverse data distributions via a symmetric
KL divergence formulation. Extensive experiments confirm that GENRE-CMR
surpasses state-of-the-art methods on training and unseen data, achieving
0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various
acceleration factors and sampling trajectories. Ablation studies confirm the
contribution of each proposed component to reconstruction quality and
generalization. Our framework presents a unified and robust solution for
high-quality CMR reconstruction, paving the way for clinically adaptable
deployment across heterogeneous acquisition protocols.

</details>


### [8] [Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025](https://arxiv.org/abs/2508.21041)
*Guillaume Balezo,Raphaël Bourgade,Thomas Walter*

Main category: eess.IV

TL;DR: DINOv3-H+视觉变换器通过LoRA微调和数据增强，在MIDOG 2025挑战中实现了0.8871的平衡准确率，为异常有丝分裂分类提供了强基线


<details>
  <summary>Details</summary>
Motivation: 异常有丝分裂图形(AMFs)是预后不良的重要标志物，但由于出现频率低、形态细微和观察者间差异，检测难度很大

Method: 使用在自然图像上预训练的DINOv3-H+视觉变换器，采用低秩适应(LoRA)方法进行参数高效微调(仅65万可训练参数)，并配合广泛的数据增强策略

Result: 尽管存在领域差距，DINOv3在组织病理学图像上表现出良好的迁移能力，在初步测试集上达到0.8871的平衡准确率

Conclusion: DINOv3预训练模型具有强大的鲁棒性，结合参数高效微调方法，为MIDOG 2025挑战中的异常有丝分裂分类任务提供了强有力的基准方案

Abstract: Atypical mitotic figures (AMFs) are markers of abnormal cell division
associated with poor prognosis, yet their detection remains difficult due to
low prevalence, subtle morphology, and inter-observer variability. The MIDOG
2025 challenge introduces a benchmark for AMF classification across multiple
domains. In this work, we evaluate the recently published DINOv3-H+ vision
transformer, pretrained on natural images, which we fine-tuned using low-rank
adaptation (LoRA, 650k trainable parameters) and extensive augmentation.
Despite the domain gap, DINOv3 transfers effectively to histopathology,
achieving a balanced accuracy of 0.8871 on the preliminary test set. These
results highlight the robustness of DINOv3 pretraining and show that, when
combined with parameter-efficient fine-tuning, it provides a strong baseline
for atypical mitosis classification in MIDOG 2025.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [9] [Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels](https://arxiv.org/abs/2508.20277)
*Xiaoyan Ma,Shahryar Zehtabi,Taejoon Kim,Christopher G. Brinton*

Main category: eess.SP

TL;DR: 本文研究了基于OFDM的空中联邦学习系统，分析了移动设备高速移动导致的信道估计不完美和模型参数不对齐问题，推导了单轮和多轮全局模型更新的闭式表达式和误差界限。


<details>
  <summary>Details</summary>
Motivation: 移动设备（如无人机）的高移动性导致信道估计不完美，造成模型参数传输不同步和时间变化的上传信道，这些因素在OTA-FL训练过程中引起失真但尚未得到充分研究。

Method: 首先推导了单轮全局模型更新的闭式表达式，然后扩展到多轮全局更新分析，得到了OTA-FL累积误差的界限，并通过大量数值模拟验证理论结果。

Result: 获得了考虑信道不完美因素的全局模型更新数学表达式和误差界限，数值模拟结果与理论分析一致。

Conclusion: 本文成功量化了移动性引起的信道不完美对OTA-FL系统的影响，为理解和改进移动环境下的联邦学习性能提供了理论依据。

Abstract: This paper investigates an OFDM-based over-the-air federated learning
(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles
(UAVs), transmit local machine learning (ML) models to a central parameter
server (PS) for global model aggregation. The high mobility of local devices
results in imperfect channel estimation, leading to a misalignment problem,
i.e., the model parameters transmitted from different local devices do not
arrive at the central PS simultaneously. Moreover, the mobility introduces
time-varying uploading channels, which further complicates the aggregation
process. All these factors collectively cause distortions in the OTA-FL
training process which are underexplored. To quantify these effects, we first
derive a closed-form expression for a single-round global model update in terms
of these channel imperfections. We then extend our analysis to capture multiple
rounds of global updates, yielding a bound on the accumulated error in OTA-FL.
We validate our theoretical results via extensive numerical simulations, which
corroborate our derived analysis.

</details>


### [10] [Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design](https://arxiv.org/abs/2508.20531)
*Chaoying Huang,Wen Chen,Qingqing Wu,Xusheng Zhu,Zhendong Li,Ying Wang,Jinhong Yuan*

Main category: eess.SP

TL;DR: 提出一种新型双智能反射面辅助的SWIPT系统，采用独立功率分配策略，在近场和混合场信道模型下优化能量收集性能


<details>
  <summary>Details</summary>
Motivation: 传统SWIPT系统在信息传输和能量收集之间存在权衡限制，需要更精确的信道建模和优化方法来提升系统性能

Method: 建立近场和混合场IRS反射链路模型，采用交替优化算法结合拉格朗日对偶方法和DC规划求解非凸问题

Result: 在混合场情况下发现AP-IRS-用户信道增益对双IRS相位偏移不变的重要特性，使优化问题转化为凸问题，推导出组合信道增益的渐近性能闭式解

Conclusion: 数值结果验证了双IRS辅助SWIPT系统采用独立功率分配策略相比基准方案的性能优势，为实际系统设计提供了理论指导

Abstract: This paper proposes a novel dual-intelligent reflecting surface (IRS) aided
interference-limited simultaneous wireless information and power transfer
(SWIPT) system with independent power splitting (PS), where each receiving
antenna applies different PS factors to offer an advantageous trade-off between
the useful information and harvested energy. We separately establish the near-
and hybrid-field channel models for IRS-reflected links to evaluate the
performance gain more precisely and practically. Specifically, we formulate an
optimization problem of maximizing the harvested power by jointly optimizing
dual-IRS phase shifts, independent PS ratio, and receive beamforming vector in
both near- and hybrid-field cases. In the near-field case, the alternating
optimization algorithm is proposed to solve the non-convex problem by applying
the Lagrange duality method and the difference-of-convex (DC) programming. In
the hybrid-field case, we first present an interesting result that the
AP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which
allows the optimization problem to be transformed into a convex one. Then, we
derive the asymptotic performance of the combined channel gains in closed-form
and analyze the characteristics of the dual-IRS. Numerical results validate our
analysis and indicate the performance gains of the proposed scheme that
dual-IRS-aided SWIPT with independent PS over other benchmark schemes.

</details>


### [11] [Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders](https://arxiv.org/abs/2508.20535)
*Annika Stiehl,Nicolas Weeger,Christian Uhl,Dominic Bechtold,Nicole Ille,Stefan Geißelsöder*

Main category: eess.SP

TL;DR: 提出基于深度卷积自编码器(DCAE)的癫痫发作检测方法，通过结合时域和频域损失函数来提取EEG信号的低维表征，解决了现有方法在敏感性和误报率之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫是常见神经系统疾病，需要可靠的发作检测方法。现有深度学习模型难以同时实现高敏感性和低误报率，且在EEG信号的最佳输入表征（时域或频域）方面缺乏一致性。

Method: 使用深度卷积自编码器提取EEG信号的低维潜在表征，通过比较基于时域和频域的重构误差来评估模型保留关键特征的能力。训练了多种具有不同时域和频域损失函数的自编码器。

Result: 同时考虑时域和频域损失的DCAE模型获得了最佳重构性能，表明单一表征的深度神经网络可能无法保留所有相关信号特性。

Conclusion: 该研究揭示了深度学习模型处理EEG数据的方式，验证了在时域信号输入时频域信息是否被有效捕获，为癫痫自动检测提供了新的思路。

Abstract: Epilepsy is one of the most common neurological disorders. This disease
requires reliable and efficient seizure detection methods.
Electroencephalography (EEG) is the gold standard for seizure monitoring, but
its manual analysis is a time-consuming task that requires expert knowledge. In
addition, there are no well-defined features that allow fully automated
analysis. Existing deep learning-based approaches struggle to achieve high
sensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack
consistency in the optimal EEG input representation, whether in the time or
frequency domain. To address these issues, we propose a Deep Convolutional
Autoencoder (DCAE) to extract low-dimensional latent representations that
preserve essential EEG signal features. The ability of the model to preserve
relevant information was evaluated by comparing reconstruction errors based on
both time series and frequency-domain representations. Several autoencoders
with different loss functions based on time and frequency were trained and
evaluated to determine their effectiveness in reconstructing EEG features. Our
results show that the DCAE model taking both time series and frequency losses
into account achieved the best reconstruction performance. This indicates that
Deep Neural Networks with a single representation might not preserve the
relevant signal properties. This work provides insight into how deep learning
models process EEG data and examines whether frequency information is captured
when time series signals are used as input.

</details>


### [12] [Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis](https://arxiv.org/abs/2508.20602)
*Matthieu Correa,Nicolas Vignais,Isabelle A. Siegler,Maxime Projetti*

Main category: eess.SP

TL;DR: 本研究提出了一种基于完整集成经验模态分解的适应性筛波方法，用于消除肌肉活动记录中的运动伪影，在动态条件下显示出比传统带通筛波更好的性能。


<details>
  <summary>Details</summary>
Motivation: 肌肉活动记录(MMG)在实际应用中存在对运动伪影敏感的问题，限制了其在现场测量中的应用。

Method: 采用基于完整集成经验模态分解(CEEMDAN)的适应性筛波方法，结合适应性噪声和谱模糊熵权来分离运动伪影。

Result: 与传统带通筛波技术相比，新方法在肩肌和脊肌的运动重构方面显示更优的结果(R² = 0.907和0.842)，能够在5-20Hz带宽内动态筛除运动伪影。

Conclusion: 该创新方法能够动态筛除运动伪影，但对于踏步或跑步时胸部和下肢肌肉的MMG信号解释仍需静慎，因为冲击相关加速度仍存在且需要量化。

Abstract: Mechanomyography (MMG) is a promising tool for measuring muscle activity in
the field but its sensitivity to motion artifacts limits its application. In
this study, we proposed an adaptative filtering method for MMG accelerometers
based on the complete ensemble empirical mode decomposition, with adaptative
noise and spectral fuzzy entropy, to isolate motions artefacts from the MMG
signal in dynamic conditions. We compared our method with the traditional
band-pass filtering technique, demonstrating better results concerning motion
recomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and
0.842). Thus, this innovative method allows the filtering of motion artifacts
dynamically in the 5-20 Hz bandwidth, which is not achievable with traditional
method. However, the interpretation of accelerometric MMG signals from the
trunk and lower-limb muscles during walking or running should be approached
with great caution as impact-related accelerations are still present, though
their exact quantity still needs to be quantified.

</details>


### [13] [Weighted Bayesian Cram$\acute{\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation](https://arxiv.org/abs/2508.20761)
*Yaniv Mazor,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 该论文针对混合分辨率系统开发了加权贝叶斯克拉美罗下界(WBCRB)，提出了基于区域划分的MSE近似方法，在存在量化误差时能准确预测MSE的非单调行为。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率架构在通信和雷达系统中广泛应用以降低硬件成本，但粗量化数据会引入参数估计的非平凡权衡，需要开发新的性能下界和分析方法。

Method: 开发了具有一般权重函数的WBCRB，包括经典BCRB、BFIM逆加权WBCRB和最优权重WBCRB。提出将估计问题划分为信息区域和饱和区域，分别应用区域特定的WBCRB近似来获得准确的复合MSE估计。

Result: 在LGO模型中的仿真结果表明，WBCRB优于BCRB，BFIM逆加权版本接近最优WBCRB。基于WBCRB的MSE近似更紧致，能准确预测量化误差存在时MSE的非单调行为。

Conclusion: 所提出的WBCRB和MSE近似方法为混合分辨率系统的性能分析提供了有效的理论工具，特别适用于存在量化误差的参数估计场景。

Abstract: Mixed-resolution architectures, combining high-resolution (analog) data with
coarsely quantized (e.g., 1-bit) data, are widely employed in emerging
communication and radar systems to reduce hardware costs and power consumption.
However, the use of coarsely quantized data introduces non-trivial tradeoffs in
parameter estimation tasks. In this paper, we investigate the derivation of
lower bounds for such systems. In particular, we develop the weighted Bayesian
Cramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight
function. We demonstrate the special cases of: (i) the classical BCRB; (ii) the
WBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse
weighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight
function. Based on the developed WBCRB, we propose a new method to approximate
the mean-squared-error (MSE) by partitioning the estimation problem into two
regions: (a) where the 1-bit quantized data is informative; and (b) where it is
saturated. We apply region-specific WBCRB approximations in these regions to
achieve an accurate composite MSE estimate. We derive the bounds and MSE
approximation for the linear Gaussian orthonormal (LGO) model, which is
commonly used in practical signal processing applications. Our simulation
results demonstrate the use of the proposed bounds and approximation method in
the LGO model with a scalar unknown parameter. It is shown that the WBCRB
outperforms the BCRB, where the BFIM-Inverse weighting version approaches the
optimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is
tighter and accurately predicts the non-monotonic behavior of the MSE in the
presence of quantization errors.

</details>


### [14] [Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar](https://arxiv.org/abs/2508.20864)
*Ehsan Sadeghi,Paul Havinga*

Main category: eess.SP

TL;DR: 本文提出基于毫米波FMCW雷达的非接触式生命体征监测方法，通过改进Prony和MUSIC算法显著提升了心率和呼吸率检测精度，MAE分别达到0.81/1.8和0.8/1.01。


<details>
  <summary>Details</summary>
Motivation: 传统接触式生命体征监测方法在临床和急救场景中存在局限性，需要开发更可靠的非侵入式监测解决方案。

Method: 采用毫米波FMCW雷达技术，针对性地改进了Prony和MUSIC算法，增强信号处理能力以捕捉细微生理变化，并有效抑制噪声和谐波干扰。

Result: 改进算法在心率检测中的平均绝对误差：Prony为0.81，MUSIC为1.8；呼吸率检测中：Prony为0.8，MUSIC为1.01，表现出优异的监测性能。

Conclusion: FMCW雷达结合改进的信号处理算法为医疗环境提供了可靠的非接触式连续生命体征监测方案，特别适用于传统接触式监测不实用的临床和急救场景。

Abstract: This paper explores the deployment of mm-wave Frequency Modulated Continuous
Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus
on overcoming the limitations of traditional sensing methods by enhancing
signal processing techniques to capture subtle physiological changes
effectively. Our study introduces novel adaptations of the Prony and MUSIC
algorithms tailored for real-time heart and respiration rate monitoring,
significantly advancing the accuracy and reliability of non-contact vital sign
monitoring using radar technologies. Notably, these algorithms demonstrate a
robust ability to suppress noise and harmonic interference. For instance, the
mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8
and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,
respectively. These results underscore the potential of FMCW radar as a
reliable, non-invasive solution for continuous vital sign monitoring in
healthcare settings, particularly in clinical and emergency scenarios where
traditional contact-based monitoring is impractical.

</details>


### [15] [A Correction for the Paper "Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis"](https://arxiv.org/abs/2508.20990)
*Hong-Yan Zhang,Haoting Liu,Rui-Jia Lin,Yu Zhou*

Main category: eess.SP

TL;DR: 本文指出了SGMD方法在轨迹矩阵形式扩展后未同步更新对角平均原理的局限性，并通过回拉定理修复了该bug


<details>
  <summary>Details</summary>
Motivation: SGMD方法在扩展奇异谱分析的轨迹矩阵形式时，未同步更新对角平均原理(DAP)，存在计算时间序列分量的缺陷

Method: 提出回拉定理(pulling back theorem)来计算轨迹矩阵对应分量的时间序列分量

Result: 修复了SGMD方法中的bug，完善了时间序列分解的计算方法

Conclusion: 通过回拉定理成功解决了SGMD方法中对角平均原理未更新的问题，提高了时间序列分解的准确性

Abstract: The symplectic geometry mode decomposition (SGMD) is a powerful method for
decomposing time series, which is based on the diagonal averaging principle
(DAP) inherited from the singular spectrum analysis (SSA). Although the authors
of SGMD method generalized the form of the trajectory matrix in SSA, the DAP is
not updated simultaneously. In this work, we pointed out the limitations of the
SGMD method and fixed the bugs with the pulling back theorem for computing the
given component of time series from the corresponding component of trajectory
matrix.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [16] [Flexible XL-MIMO via Array Configuration Codebook: Codebook Design and Array Configuration Training](https://arxiv.org/abs/2508.20369)
*Haiquan Lu,Hongqi Min,Yong Zeng,Shaodan Ma*

Main category: cs.IT

TL;DR: 提出阵列配置码本(ACC)概念，通过动态像素激活实现灵活的大规模MIMO架构，相比传统天线选择方案能提升系统性能并降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 解决XL-MIMO技术中高硬件成本和功耗问题，实现经济高效的大规模MIMO部署。

Method: 设计包含多种经典阵列配置的码本，采用两级扫描方案（阵列级和像素级）进行配置训练，并提出了贪婪天线选择算法。

Result: 仿真结果表明，在多用户通信和无线定位场景下，码本优化方案能有效提升系统性能。

Conclusion: ACC概念为XL-MIMO提供了灵活且经济高效的实现方案，在通信和定位应用中均表现出优越性能。

Abstract: XL-MIMO emerges as a promising technology to achieve unprecedented
enhancements in spectral efficiency and spatial resolution, via
orders-of-magnitude increase in the antenna array size. However, the practical
issues of high hardware cost and power consumption pose great challenges
towards the cost-effective implementation of XL-MIMO. To address such
challenges, this paper proposes a novel concept called array configuration
codebook (ACC), which enables flexible XL-MIMO cost-effectively and improves
the system performance compared with conventional antenna selection (AS)
schemes with limited number of RF chains. Specifically, ACC refers to a set of
pre-designed array configuration codewords, where each codeword specifies the
positions of activated antenna pixels. Then, flexible XL-MIMO architecture can
be enabled via dynamical pixel activation based on the designed ACC, without
having to exhaustively try all possible combinations of the antenna pixels
activations. As an illustration, we give a specific codebook design,
encompassing the classic compact array (CA), uniform sparse array (USA),
modular array (MoA), nested array (NA), and co-prime array (CPA), and each
codeword is specified by one array configuration parameter. With the designed
ACC, array configuration training is considered for multi-UE communication to
maximize the sum rate. To reduce the training overhead of exhaustive scanning,
a two-stage scanning scheme is proposed, including the array- and pixel-level
scanning. For comparison, the greedy AS scheme is proposed, where the resulting
incremental SINR expression by activating antenna pixel sequentially is derived
in closed-form. Subsequently, array configuration training is extended to the
wireless localization scenario. Simulation results demonstrate the
effectiveness of codeword optimization for scenarios of multi-UE communication
and wireless localization.

</details>


### [17] [Secure Satellite Communications via Multiple Aerial RISs: Joint Optimization of Reflection, Association, and Deployment](https://arxiv.org/abs/2508.20455)
*Zhaole Wang,Naijin Liu,Xiao Tang,Shuai Yuan,Chenxi Wang,Zhi Zhai,Qinghe Du,Jinxin Liu*

Main category: cs.IT

TL;DR: 本文研究了基于空中可重构智能表面(ARIS)辅助的多波束多组卫星安全通信，通过联合优化传输反射波束成形、ARIS-组关联和ARIS部署，最大化组间多播速率总和并约束窃听速率。


<details>
  <summary>Details</summary>
Motivation: 卫星通信是未来6G网络的关键使能技术，但其广覆盖和高链路衰减特性给物理层安全带来重大挑战，需要新的安全通信解决方案。

Method: 采用块坐标下降框架，将联合优化问题分解为多个子问题并迭代求解，处理混合整数和非凸的优化问题。

Result: 仿真结果表明，所提出的ARIS辅助多波束卫星系统在各种网络场景下都能显著提升安全通信性能。

Conclusion: 该研究为未来安全卫星网络中智能表面的部署和优化提供了有价值的见解，证明了ARIS在增强卫星通信安全方面的有效性。

Abstract: Satellite communication is envisioned as a key enabler of future 6G networks,
yet its wide coverage with high link attenuation poses significant challenges
for physical layer security. In this paper, we investigate secure multi-beam,
multi-group satellite communications assisted by aerial reconfigurable
intelligent surfaces (ARISs). To maximize the sum of achievable multicast rates
among the groups while constraining wiretap rates, we formulate a joint
optimization problem involving transmission and reflection beamforming,
ARIS-group association, and ARIS deployment. Due to the mixed-integral and
non-convex nature of the formulated problem, we propose to decompose the
problem and employ the block coordinate descent framework that iteratively
solves the subproblems. Simulation results demonstrate that the proposed
ARIS-assisted multi-beam satellite system provides a notable improvement in
secure communication performance under various network scenarios, offering
useful insights into the deployment and optimization of intelligent surfaces in
future secure satellite networks.

</details>


### [18] [Precoded Polar Product Decoder Based on Soft-Output SCL Decoding and Maximization of Generalized Mutual Information](https://arxiv.org/abs/2508.20580)
*Nicolás Alvarez Prado,Andreas Straßhofer*

Main category: cs.IT

TL;DR: 结合码本概率的软信息生成和GMI优化的缩放系数，显著提升了预编码极化码乘积码的迭代译码性能


<details>
  <summary>Details</summary>
Motivation: 传统启发式缩放和仅基于候选列表的软信息生成方法性能有限，需要更精确的软信息处理和优化策略来提升乘积码的迭代译码效果

Method: 1) 基于码本概率生成比特级软信息，近似考虑SCL译码器所有有效路径的辅助量；2) 使用离线计算的GMI最大化系数在消息传递过程中缩放软信息；3) 提出SCL译码器的外推版本用于蒙特卡洛密度演化分析

Result: 仿真结果显示相比启发式缩放和仅基于候选列表的方法，纠错性能显著提升；计算得到的译码阈值能准确预测译码器性能

Conclusion: 该方法通过精确的软信息生成和优化的缩放策略，有效提升了预编码极化码乘积码的迭代译码性能，且理论分析与实际性能吻合良好

Abstract: We combine two approaches to optimize the iterative decoding of product codes
with precoded polar component codes. On one side, we generate bitwise soft
messages based on the codebook probability, an approximation of an auxiliary
quantity that considers all valid decoding paths of a successive cancellation
list (SCL) decoder. On the other side, we scale the soft information during
message passing with offline-computed coefficients, which maximize the
generalized mutual information (GMI) between the channel input and the outgoing
message in each half iteration. Simulation results show significant improvement
of the error-correcting performance compared to heuristic scaling and soft
information generation based solely on the candidate list of the decoder.
Moreover, we present an extrinsic version of the SCL decoder, which we use in a
Monte Carlo density evolution analysis to derive decoding thresholds. The
computed thresholds accurately predict the performance of the decoder.

</details>


### [19] [Polar subcodes for MIMO systems](https://arxiv.org/abs/2508.20684)
*Liudmila Karakchieva,Peter Trifonov*

Main category: cs.IT

TL;DR: 提出了用于MIMO系统的极化码联合列表解码方法，包括QR和MMSE检测器，以及具有跨天线动态冻结约束的极化子码构造


<details>
  <summary>Details</summary>
Motivation: 研究极化码在MIMO系统中的性能，通过联合解码和特殊约束设计来提升系统性能

Method: 开发联合列表解码算法，推导近似和精确路径度量，设计具有跨天线动态冻结约束的极化子码构造方法

Result: 所获得的极化子码相比相同速率分配的LDPC编码MIMO系统提供了显著的性能增益

Conclusion: 提出的极化码联合解码方法和特殊约束设计能有效提升MIMO系统性能，优于传统LDPC编码方案

Abstract: Polar-coded multiple-input multiple-output systems are investigated. An
advanced receiver implementing joint list decoding of polar codes and QR- and
MMSE-based detectors is proposed. The approximate and exact path metrics are
derived for joint list decoder of polar codes. A construction of polar subcodes
for MIMO systems with cross-antenna dynamic freezing constraints is proposed.
The obtained polar subcodes provide significant performance gain compared to
LDPC-coded MIMO systems with the same rate allocation.

</details>


### [20] [Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free Massive MIMO](https://arxiv.org/abs/2508.20704)
*Wei Jiang,Hans D Schotten*

Main category: cs.IT

TL;DR: 本文提出了分层无蜂窝大规模MIMO系统，通过用中央基站替代部分接入点来降低部署成本，同时保持性能。研究发现集中式迫零合并方案在性能和成本效率之间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 传统无蜂窝大规模MIMO系统部署成本高，需要解决成本与性能之间的权衡问题。

Method: 开发统一的分析框架支持任意合并方案，并提出针对分层两阶架构的新型分层合并方法。

Result: 通过分析用户公平性、系统容量、前传需求和计算复杂度，发现分层无蜂窝系统在性能和成本效率方面表现优异。

Conclusion: 分层无蜂窝大规模MIMO使用集中式迫零合并方案能够实现性能与成本效率的最佳平衡，是未来无线通信系统的有前景的架构。

Abstract: Cell-free (CF) massive MIMO offers uniform service via distributed access
points (APs), which impose high deployment costs. A novel design called
hierarchical cell-free (HCF) addresses this problem by replacing some APs with
a central base station, thereby lowering the costs of fronthaul network
(wireless sites and fiber cables) while preserving performance. To identify the
optimal uplink configuration in HCF massive MIMO, this paper provides the first
comprehensive analysis, benchmarking it against cellular and CF systems. We
develop a unified analytical framework for spectral efficiency that supports
arbitrary combining schemes and introduce a novel hierarchical combining
approach tailored to HCF two-tier architecture. Through analysis and evaluation
of user fairness, system capacity, fronthaul requirements, and computational
complexity, this paper identifies that HCF using centralized zero-forcing
combining achieves the optimal balance between performance and cost-efficiency.

</details>


### [21] [What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?](https://arxiv.org/abs/2508.20708)
*Wei Jiang,Hans D. Schotten*

Main category: cs.IT

TL;DR: 这篇论文研究了元空间大规模MIMO系统的最优上行链路技术，通过统一分析框架和优化策略，识别出实际部署中的最佳方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在方法论分裂和偏差（如单/多天线设备、理想/相关通信渡），需要统一框架来进行公平比较。

Method: 建立统一分析框架（支持中央化/分布式处理），发展最小-最大功率控制优化策略，并综合评估四项关键指标。

Result: 通过分析和评估，确定了实际元空间大规模MIMO部署的最优上行技术方案。

Conclusion: 论文提供了一个统一的分析框架和优化方法，为实际元空间大规模MIMO系统的上行链路技术选择提供了重要依据。

Abstract: This paper seeks to determine the most efficient uplink technique for
cell-free massive MIMO systems. Despite offering great advances, existing works
suffer from fragmented methodologies and inconsistent assumptions (e.g.,
single- vs. multi-antenna access points, ideal vs. spatially correlated
channels). To address these limitations, we: (1) establish a unified analytical
framework compatible with centralized/distributed processing and diverse
combining schemes; (2) develop a universal optimization strategy for max-min
power control; and (3) conduct a holistic study among four critical metrics:
worst-case user spectral efficiency (fairness), system capacity, fronthaul
signaling, and computational complexity. Through analyses and evaluation, this
work ultimately identifies the optimal uplink technique for practical cell-free
deployments.

</details>


### [22] [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](https://arxiv.org/abs/2508.20806)
*Moriba Jah,Van Haslett*

Main category: cs.IT

TL;DR: 提出了基于可能性理论和认知谦逊的非贝叶斯滤波框架ESPF，通过兼容性加权支持更新、惊奇感知剪枝和自适应离散化来维护状态空间的合理性区域，而非传统后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法依赖概率假设，在稀疏或对抗性传感环境中容易过度自信，需要新的框架来处理认知不确定性和信息不足的情况。

Method: 使用可能性理论构建ESPF框架，采用兼容性加权支持更新、惊奇感知剪枝和稀疏网格积分进行自适应离散化，使用Choquet积分融合竞争假设。

Result: 开发了一个能够根据信息结构动态收缩或扩展信念支持的推理引擎，无需先验统计校准，在缺乏先验或先验误导的情况下实现鲁棒估计。

Conclusion: 这项工作在如何协调推理、证据和无知方面提出了基础性转变，为缺乏先验信息或先验不可靠的场景提供了鲁棒的状态估计解决方案。

Abstract: Traditional state estimation methods rely on probabilistic assumptions that
often collapse epistemic uncertainty into scalar beliefs, risking
overconfidence in sparse or adversarial sensing environments. We introduce the
Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework
fully grounded in possibility theory and epistemic humility. ESPF redefines the
evolution of belief over state space using compatibility-weighted support
updates, surprisalaware pruning, and adaptive dispersion via sparse grid
quadrature. Unlike conventional filters, ESPF does not seek a posterior
distribution, but rather maintains a structured region of plausibility or
non-rejection, updated using ordinal logic rather than integration. For
multi-model inference, we employ the Choquet integral to fuse competing
hypotheses based on a dynamic epistemic capacity function, generalizing
classical winner-take-all strategies. The result is an inference engine capable
of dynamically contracting or expanding belief support in direct response to
information structure, without requiring prior statistical calibration. This
work presents a foundational shift in how inference, evidence, and ignorance
are reconciled, supporting robust estimation where priors are unavailable,
misleading, or epistemically unjustified.

</details>


### [23] [On the non-existence of perfect codes in the sum-rank metric](https://arxiv.org/abs/2508.20940)
*Giuseppe Del Prete,Antonio Roccolano,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文研究了和秩度量中的完美码，这是汉明度量和秩度量的推广，在多跳网络编码和空时编码中很重要。论文分析和秩度量球的几何特性，推导体积界限，并针对两区块和多区块空间建立了完美码存在性和非存在性的条件。


<details>
  <summary>Details</summary>
Motivation: 和秩度量是汉明度量和秩度量的重要推广，在多跳网络编码和空时编码中有重要应用。虽然汉明度量和秩度量中的完美码已被完全分类，但和秩度量中非平凡完美码的存在性仍然是一个开放问题，需要深入研究。

Method: 论文分析和秩度量球的几何特性，推导球体积的界限，研究球填充界限的应用。针对两区块空间，确定完美码存在的显式参数约束；针对多区块空间，基于最小距离、可除性条件和码维数建立非存在性结果，并提供基于度量球体积同余条件的计算证据。

Result: 研究得出了两区块空间中完美码存在的参数约束条件，以及多区块空间中各种参数范围内的非存在性结果。通过度量球体积的同余条件分析，为完美码的存在性提供了计算证据支持。

Conclusion: 论文在和秩度量完美码研究方面取得了重要进展，为这一开放问题提供了新的理论工具和结果，推动了对和秩度量编码理论的深入理解，为后续研究奠定了基础。

Abstract: We study perfect codes in the sum-rank metric, a generalization of both the
Hamming and rank metrics relevant in multishot network coding and space-time
coding. A perfect code attains equality in the sphere-packing bound,
corresponding to a partition of the ambient space into disjoint metric balls.
While perfect codes in the Hamming and rank metrics are completely classified,
the existence of nontrivial perfect codes in the sum-rank metric remains
largely open. In this paper, we investigate linear perfect codes in the
sum-rank metric. We analyze the geometry of balls and derive bounds on their
volumes, showing how the sphere-packing bound applies. For two-block spaces, we
determine explicit parameter constraints for the existence of perfect codes.
For multiple-block spaces, we establish non-existence results for various
ranges of minimum distance, divisibility conditions, and code dimensions. We
further provide computational evidence based on congruence conditions imposed
by the volume of metric balls.

</details>


### [24] [On Secrecy Capacity of Binary Beampointing Channels with Block Memory and Feedback](https://arxiv.org/abs/2508.20980)
*Siyao Li,Mingzhe Chen,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了具有块内存储和反馈的二进制捕捌通道的保密容量，提出了联合通信与适应性感知方案，并证明其效率随块长度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 为模拟毫米波系统中的材料传输和反射反馈，需要研究具有块内存储和反馈的二进制捕捌通道的保密性能力。

Method: 提出了一种联合通信与适应性感知(JCAS)方案，利用无噪反馈信息来平衡正当用户感知和防止恶意监听者信息泄漏的需求。

Result: 建立了保密容量的闭式上界，并求解了JCAS方案的可达保密速率。模拟结果显示内外界间隔随块长度增加而紧紧。

Conclusion: JCAS方案通过战略性利用反馈信息，在毫米波系统中有效地平衡了通信与感知的需求，提高了系统的保密性能。

Abstract: This paper investigates the secrecy capacity of the binary beampointing (BBP)
channel with block memory and feedback, a simplified yet insightful model for
millimeter-wave (mmWave) systems with beamformed transmissions and backscatter
feedback. We consider a system where a legitimate receiver and a passive
eavesdropper experience independent and uniformly distributed angular
directions over transmission blocks, with the base station receiving noiseless,
unit-delayed feedback from both, under the per-symbol input cost constraints.
We establish a closed-form upper bound on the secrecy capacity, which is based
on the main channel between the base station and the legitimate receiver.
Moreover, we propose a joint communication and adaptive sensing (JCAS) scheme
and derive its achievable secrecy rate. Simulation results show that the gap
between the inner and outer bounds narrows as the number of block length
increases. This reveals the efficiency of this JCAS scheme, which strategically
leverages feedback to balance the demands of sensing the legitimate user and
preventing information leakage to the eavesdropper.

</details>


### [25] [On the Sensing Capacity of Gaussian "Beam-Pointing" Channels with Block Memory and Feedback](https://arxiv.org/abs/2508.20997)
*Siyao Li,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究5G/6G系统中的高斯波束指向信道，提出联合通信与感知方案，分析感知容量并推导上下界，在Q=1时达到最优感知速率。


<details>
  <summary>Details</summary>
Motivation: 为满足5G/6G高频无线通信需求（如毫米波、亚太赫兹），需要解决波束指向问题，其中信道状态包含未知的离开角，需要联合通信与感知来提升传输效率。

Method: 提出联合通信与感知方案，使用严格因果反馈来估计离开角，通过动态规划推导感知容量的上界，并提出可达的内界作为优化问题。

Result: 推导了感知容量的上界和可达内界，在Q=1的特殊情况下，所提传输方案达到了最优感知速率，并揭示了感知与通信性能之间的权衡关系。

Conclusion: 该研究为高频无线通信系统中的波束指向问题提供了有效的联合通信感知解决方案，在特定条件下达到最优性能，揭示了通信与感知之间的基本权衡。

Abstract: Driven by the demands of high-frequency wireless communications in 5G and 6G
systems (e.g., mmWave, sub-THz), we explore a state-dependent {\em Gaussian
beam-pointing} (GBP) channel. In this model, the channel state defines an
unknown angle of departure (AoD), which remains constant within each coherence
block of $Q$ time slots but changes independently across blocks. The
transmitter receives strictly causal feedback which may originate from a radar
detection system or explicit feedback from the receiver at the end of each slot
and estimates the AoD at the end of each block. To enhance transmission
efficiency, we propose a joint communication and sensing scheme. While the
communication capacity of the GBP channel has been previously analyzed by the
authors, this work focuses on sensing capacity, characterized by the mutual
information between the channel state and the feedback conditioned on the
transmitted signal. We derive an upper bound using dynamic programming and
propose an achievable inner bound on the sensing capacity, both formulated as
optimization problems. For the special case of $Q=1$, the proposed transmission
scheme achieves the optimal sensing rate and highlights the inherent trade-off
between sensing and communication performance.

</details>
