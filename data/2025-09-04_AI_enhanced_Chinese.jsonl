{"id": "2509.02585", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02585", "abs": "https://arxiv.org/abs/2509.02585", "authors": ["Zhuoyan Shen", "Esther B\u00e4r", "Maria Hawkins", "Konstantin Br\u00e4utigam", "Charles-Antoine Collins-Fekete"], "title": "Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge", "comment": null, "summary": "This report details our submission to the Mitotic Domain Generalization\n(MIDOG) 2025 challenge, which addresses the critical task of mitotic figure\ndetection in histopathology for cancer prognostication. Following the \"Bitter\nLesson\"\\cite{sutton2019bitterlesson} principle that emphasizes data scale over\nalgorithmic novelty, we have publicly released two new datasets to bolster\ntraining data for both conventional \\cite{Shen2024framework} and atypical\nmitoses \\cite{shen_2025_16780587}. Besides, we implement up-to-date training\nmethodologies for both track and reach a Track-1 F1-Score of 0.8407 on our test\nset, as well as a Track-2 balanced accuracy of 0.9107 for atypical mitotic cell\nclassification.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MIDOG 2025\u6311\u6218\u8d5b\u7684\u63d0\u4ea4\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u75c5\u7406\u7ec4\u7ec7\u5b66\u4e2d\u7684\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\uff0c\u901a\u8fc7\u53d1\u5e03\u65b0\u6570\u636e\u96c6\u548c\u91c7\u7528\u6700\u65b0\u8bad\u7ec3\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u764c\u75c7\u9884\u540e\u8bc4\u4f30\u4e2d\u5173\u952e\u7684\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u4efb\u52a1\uff0c\u9075\u5faa\"Bitter Lesson\"\u539f\u5219\uff0c\u5f3a\u8c03\u6570\u636e\u89c4\u6a21\u800c\u975e\u7b97\u6cd5\u65b0\u9896\u6027\u3002", "method": "\u516c\u5f00\u53d1\u5e03\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08\u5e38\u89c4\u548c\u975e\u5e38\u89c4\u6709\u4e1d\u5206\u88c2\uff09\uff0c\u91c7\u7528\u6700\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u548c\u4f18\u5316\u6280\u672f\u3002", "result": "Track-1 F1\u5206\u6570\u8fbe\u52300.8407\uff0cTrack-2\u975e\u5e38\u89c4\u6709\u4e1d\u5206\u88c2\u7ec6\u80de\u5206\u7c7b\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u52300.9107\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u548c\u73b0\u4ee3\u8bad\u7ec3\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u5728\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u89c4\u6a21\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.02586", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02586", "abs": "https://arxiv.org/abs/2509.02586", "authors": ["Esha Sadia Nasir", "Jiaqi Lv", "Mostafa Jahanifer", "Shan E Ahmed Raza"], "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping", "comment": null, "summary": "Automated detection and classification of mitotic figures especially\ndistinguishing atypical from normal remain critical challenges in computational\npathology. We present MitoDetect++, a unified deep learning pipeline designed\nfor the MIDOG 2025 challenge, addressing both mitosis detection and atypical\nmitosis classification. For detection (Track 1), we employ a U-Net-based\nencoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced\nwith attention modules, and trained via combined segmentation losses. For\nclassification (Track 2), we leverage the Virchow2 vision transformer,\nfine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource\nconsumption. To improve generalization and mitigate domain shifts, we integrate\nstrong augmentations, focal loss, and group-aware stratified 5-fold\ncross-validation. At inference, we deploy test-time augmentation (TTA) to boost\nrobustness. Our method achieves a balanced accuracy of 0.892 across validation\ndomains, highlighting its clinical applicability and scalability across tasks.", "AI": {"tldr": "MitoDetect++\u662f\u4e00\u4e2a\u7528\u4e8eMIDOG 2025\u6311\u6218\u8d5b\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u901a\u8fc7U-Net\u67b6\u6784\u8fdb\u884c\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\uff0c\u4f7f\u7528Vision Transformer\u8fdb\u884c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff0c\u5728\u9a8c\u8bc1\u57df\u4e0a\u8fbe\u52300.892\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\uff0c\u7279\u522b\u662f\u533a\u5206\u975e\u5178\u578b\u548c\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\uff0c\u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u7edf\u4e00\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528U-Net\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08EfficientNetV2-L\u9aa8\u5e72\u7f51\u7edc\uff09\u8fdb\u884c\u68c0\u6d4b\uff0c\u4f7f\u7528Virchow2\u89c6\u89c9\u53d8\u6362\u5668\uff08LoRA\u5fae\u8c03\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u6a21\u5757\u3001\u5f3a\u6570\u636e\u589e\u5f3a\u3001\u7126\u70b9\u635f\u5931\u548c\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u5728\u9a8c\u8bc1\u57df\u4e0a\u5b9e\u73b0\u4e860.892\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u4e34\u5e8a\u9002\u7528\u6027\u548c\u8de8\u4efb\u52a1\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MitoDetect++\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u5206\u7c7b\u4efb\u52a1\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.02588", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02588", "abs": "https://arxiv.org/abs/2509.02588", "authors": ["Maxime W. Lafarge", "Viktor H. Koelzer"], "title": "Sequential Hard Mining: a data-centric approach for Mitosis Detection", "comment": null, "summary": "With a continuously growing availability of annotated datasets of mitotic\nfigures in histology images, finding the best way to optimally use with this\nunprecedented amount of data to optimally train deep learning models has become\na new challenge. Here, we build upon previously proposed approaches with a\nfocus on efficient sampling of training data inspired by boosting techniques\nand present our candidate solutions for the two tracks of the MIDOG 2025\nchallenge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91c7\u7528\u53d7\u52b3\u52a0\u6280\u672f\u542f\u53d1\u7684\u9ad8\u6548\u6837\u672c\u91c7\u6837\u65b9\u6cd5\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5229\u7528\u5927\u91cf\u6709\u6807\u6ce8\u7684\u6709\u4e1d\u5206\u88c2\u56fe\u8c61\u6570\u636e\u96c6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u4e3aMIDOG 2025\u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u6709\u4e1d\u5206\u88c2\u56fe\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u5982\u4f55\u6700\u4f18\u5730\u5229\u7528\u8fd9\u4e9b\u5927\u91cf\u6570\u636e\u6765\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6210\u4e3a\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u4e4b\u524d\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u53d7\u52b3\u52a0\u6280\u672f\u542f\u53d1\u7684\u9ad8\u6548\u6837\u672c\u91c7\u6837\u65b9\u6cd5\uff0c\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u4f18\u5316\u9009\u62e9\u3002", "result": "\u4e3aMIDOG 2025\u6311\u6218\u8d5b\u7684\u4e24\u4e2a\u8d5b\u9053\u63d0\u4f9b\u4e86\u501f\u9274\u52b3\u52a0\u6280\u672f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u91c7\u6837\u7b56\u7565\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5229\u7528\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u6765\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6709\u4e1d\u5206\u88c2\u56fe\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.02589", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02589", "abs": "https://arxiv.org/abs/2509.02589", "authors": ["Xuan Qi", "Dominic Labella", "Thomas Sanford", "Maxwell Lee"], "title": "Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer", "comment": "for grandchallenge midog 2025 track 2 abstract", "summary": "We tackle atypical versus normal mitosis classification in the MIDOG 2025\nchallenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for\naccuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer\ntypes (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To\nassess domain generalization, we applied leave-one-cancer-type-out\ncross-validation with 5-fold ensembles, using stain-deconvolution for image\naugmentation. For challenge submissions, we trained an ensemble with the same\n5-fold split but on all cancer types. In the preliminary evaluation phase, this\nmodel achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy\nof 0.85, demonstrating competitive and well-balanced performance across\nmetrics.", "AI": {"tldr": "\u4f7f\u7528EfficientViT-L2\u6df7\u5408\u67b6\u6784\u5728MIDOG 2025\u6311\u6218\u4e2d\u5b9e\u73b0\u975e\u5178\u578b\u4e0e\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff0c\u572813,938\u4e2a\u764c\u7ec6\u80de\u6838\u6570\u636e\u96c6\u4e0a\u83b7\u5f970.859\u5e73\u8861\u51c6\u786e\u7387\u548c0.942 ROC AUC", "motivation": "\u89e3\u51b3\u764c\u75c7\u75c5\u7406\u56fe\u50cf\u4e2d\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u4e0e\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\u7684\u81ea\u52a8\u5206\u7c7b\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u764c\u75c7\u8bca\u65ad\u548c\u5206\u7ea7\u81f3\u5173\u91cd\u8981", "method": "\u91c7\u7528EfficientViT-L2\u6df7\u5408CNN-ViT\u67b6\u6784\uff0c\u4f7f\u7528\u7559\u4e00\u764c\u75c7\u7c7b\u578b\u4ea4\u53c9\u9a8c\u8bc1\u548c5\u6298\u96c6\u6210\uff0c\u901a\u8fc7\u67d3\u8272\u89e3\u5377\u79ef\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a", "result": "\u5728\u521d\u6b65\u8bc4\u4f30\u9636\u6bb5\u83b7\u5f97\u5e73\u8861\u51c6\u786e\u73870.859\u3001ROC AUC 0.942\u548c\u539f\u59cb\u51c6\u786e\u73870.85\uff0c\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u4e14\u5747\u8861\u7684\u6027\u80fd", "conclusion": "EfficientViT-L2\u67b6\u6784\u5728\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u548c\u5e73\u8861\u7684\u6027\u80fd\u6307\u6807"}}
{"id": "2509.03034", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03034", "abs": "https://arxiv.org/abs/2509.03034", "authors": ["Xiaofeng Liu", "Jun Zhang", "Fang-Wei Fu"], "title": "On a class of twisted elliptic curve codes", "comment": null, "summary": "Motivated by the studies of twisted generalized Reed-Solomon (TGRS) codes, we\ninitiate the study of twisted elliptic curve codes (TECCs) in this paper. In\nparticular, we study a class of TECCs with one twist. The parity-check matrices\nof the TECCs are explicitly given by computing the Weil differentials. Then the\nsufficient and necessary conditions of self-duality are presented. The minimum\ndistances of the TECCs are also determined. Moreover, examples of MDS, AMDS,\nself-dual and MDS self-dual TECCs are given. Finally, we calculate the\ndimensions of the Schur squares of TECCs and show the non-equivalence between\nTECCs and ECCs/GRS codes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u626d\u66f2\u7684\u692d\u5706\u66f2\u7ebf\u7801(TECCs)\uff0c\u7279\u522b\u5173\u6ce8\u5355\u626d\u66f2\u60c5\u51b5\uff0c\u7ed9\u51fa\u4e86\u5947\u5076\u6821\u9a8c\u77e9\u9635\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5efa\u7acb\u4e86\u81ea\u5bf9\u5076\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u786e\u5b9a\u4e86\u6700\u5c0f\u8ddd\u79bb\uff0c\u5e76\u63d0\u4f9b\u4e86MDS\u3001AMDS\u3001\u81ea\u5bf9\u5076\u548cMDS\u81ea\u5bf9\u5076TECCs\u7684\u5b9e\u4f8b\u3002", "motivation": "\u53d7\u626d\u66f2\u5e7f\u4e49Reed-Solomon(TGRS)\u7801\u7814\u7a76\u7684\u542f\u53d1\uff0c\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u626d\u66f2\u692d\u5706\u66f2\u7ebf\u7801(TECCs)\uff0c\u65e8\u5728\u6269\u5c55\u7f16\u7801\u7406\u8bba\u5728\u692d\u5706\u66f2\u7ebf\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97Weil\u5fae\u5206\u663e\u5f0f\u7ed9\u51faTECCs\u7684\u5947\u5076\u6821\u9a8c\u77e9\u9635\uff0c\u5efa\u7acb\u81ea\u5bf9\u5076\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u786e\u5b9a\u6700\u5c0f\u8ddd\u79bb\uff0c\u5e76\u6784\u9020\u5177\u4f53\u5b9e\u4f8b\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86TECCs\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7ed9\u51fa\u4e86\u81ea\u5bf9\u5076\u6027\u7684\u5b8c\u6574\u5224\u636e\uff0c\u786e\u5b9a\u4e86\u6700\u5c0f\u8ddd\u79bb\uff0c\u63d0\u4f9b\u4e86\u591a\u79cd\u7c7b\u578bTECCs\u7684\u5b9e\u4f8b\uff0c\u5e76\u8bc1\u660e\u4e86TECCs\u4e0eECCs/GRS\u7801\u7684\u975e\u7b49\u4ef7\u6027\u3002", "conclusion": "TECCs\u4e3a\u7f16\u7801\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u4ef7\u503c\u548c\u6f5c\u5728\u5e94\u7528\u524d\u666f\uff0c\u7279\u522b\u662f\u5728\u6784\u9020MDS\u81ea\u5bf9\u5076\u7801\u65b9\u9762\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2509.02568", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02568", "abs": "https://arxiv.org/abs/2509.02568", "authors": ["Mohammad Mehedi Hasan", "Pedro G. Lind", "Hernando Ombao", "Anis Yazidi", "Rabindra Khadka"], "title": "EEG-MSAF: An Interpretable Microstate Framework uncovers Default-Mode Decoherence in Early Neurodegeneration", "comment": "Dementia, EEG, Microstates, Explainable, SHAP", "summary": "Dementia (DEM) is a growing global health challenge, underscoring the need\nfor early and accurate diagnosis. Electroencephalography (EEG) provides a\nnon-invasive window into brain activity, but conventional methods struggle to\ncapture its transient complexity. We present the \\textbf{EEG Microstate\nAnalysis Framework (EEG-MSAF)}, an end-to-end pipeline that leverages EEG\nmicrostates discrete, quasi-stable topographies to identify DEM-related\nbiomarkers and distinguish DEM, mild cognitive impairment (MCI), and normal\ncognition (NC). EEG-MSAF comprises three stages: (1) automated microstate\nfeature extraction, (2) classification with machine learning (ML), and (3)\nfeature ranking using Shapley Additive Explanations (SHAP) to highlight key\nbiomarkers. We evaluate on two EEG datasets: the public Chung-Ang University\nEEG (CAUEEG) dataset and a clinical cohort from Thessaloniki Hospital. Our\nframework demonstrates strong performance and generalizability. On CAUEEG,\nEEG-MSAF-SVM achieves \\textbf{89\\% $\\pm$ 0.01 accuracy}, surpassing the deep\nlearning baseline CEEDNET by \\textbf{19.3\\%}. On the Thessaloniki dataset, it\nreaches \\textbf{95\\% $\\pm$ 0.01 accuracy}, comparable to EEGConvNeXt. SHAP\nanalysis identifies mean correlation and occurrence as the most informative\nmetrics: disruption of microstate C (salience/attention network) dominates DEM\nprediction, while microstate F, a novel default-mode pattern, emerges as a key\nearly biomarker for both MCI and DEM. By combining accuracy, generalizability,\nand interpretability, EEG-MSAF advances EEG-based dementia diagnosis and sheds\nlight on brain dynamics across the cognitive spectrum.", "AI": {"tldr": "EEG-MSAF\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684EEG\u5fae\u72b6\u6001\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548cSHAP\u7279\u5f81\u89e3\u91ca\uff0c\u5728\u75f4\u5446\u75c7\u65e9\u671f\u8bca\u65ad\u4e2d\u8fbe\u523089-95%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u75f4\u5446\u75c7\u662f\u5168\u7403\u6027\u5065\u5eb7\u6311\u6218\uff0c\u9700\u8981\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u3002\u4f20\u7edfEEG\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8111\u7535\u6d3b\u52a8\u7684\u77ac\u65f6\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5206\u6790\u6846\u67b6\u6765\u8bc6\u522b\u75f4\u5446\u76f8\u5173\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "EEG-MSAF\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1)\u81ea\u52a8\u5fae\u72b6\u6001\u7279\u5f81\u63d0\u53d6\uff1b2)\u673a\u5668\u5b66\u4e60\u5206\u7c7b\uff1b3)SHAP\u7279\u5f81\u6392\u5e8f\u8bc6\u522b\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002\u4f7f\u7528SVM\u7b49\u5206\u7c7b\u5668\u5728\u4e24\u4e2aEEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728CAUEEG\u6570\u636e\u96c6\u4e0a\u8fbe\u523089%\u00b10.01\u51c6\u786e\u7387\uff0c\u6bd4CEEDNET\u57fa\u7ebf\u63d0\u534719.3%\uff1b\u5728Thessaloniki\u6570\u636e\u96c6\u4e0a\u8fbe\u523095%\u00b10.01\u51c6\u786e\u7387\uff0c\u4e0eEEGConvNeXt\u76f8\u5f53\u3002SHAP\u5206\u6790\u8bc6\u522b\u51fa\u5fae\u72b6\u6001C\u548cF\u4f5c\u4e3a\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "EEG-MSAF\u7ed3\u5408\u4e86\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u8fdb\u4e86\u57fa\u4e8eEEG\u7684\u75f4\u5446\u75c7\u8bca\u65ad\uff0c\u5e76\u63ed\u793a\u4e86\u8de8\u8ba4\u77e5\u8c31\u7cfb\u7684\u8111\u52a8\u529b\u5b66\u7279\u5f81\uff0c\u5fae\u72b6\u6001F\u4f5c\u4e3a\u65b0\u7684\u9ed8\u8ba4\u6a21\u5f0f\u6a21\u5f0f\u662fMCI\u548cDEM\u7684\u65e9\u671f\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2509.02591", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02591", "abs": "https://arxiv.org/abs/2509.02591", "authors": ["Mieko Ochi", "Bae Yuan"], "title": "Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical Mitosis Classification", "comment": null, "summary": "Mitotic figures are classified into typical and atypical variants, with\natypical counts correlating strongly with tumor aggressiveness. Accurate\ndifferentiation is therefore essential for patient prognostication and resource\nallocation, yet remains challenging even for expert pathologists. Here, we\nleveraged Pathology Foundation Models (PFMs) pre-trained on large\nhistopathology datasets and applied parameter-efficient fine-tuning via\nlow-rank adaptation. During training, we employ a fisheye transform to\nemphasize mitoses and Fourier Domain Adaptation using ImageNet target images.\nFinally, we ensembled multiple PFMs to integrate complementary morphological\ninsights, achieving a high balanced accuracy on the Preliminary Evaluation\nPhase dataset.", "AI": {"tldr": "\u5229\u7528\u75c5\u7406\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6548\u7387\u5fae\u8c03\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u7cbe\u51c6\u533a\u5206\u6709\u7ebf\u5206\u88c2\u7684\u5178\u578b\u548c\u975e\u5178\u578b\u53d8\u4f53\uff0c\u63d0\u9ad8\u809d\u80bf\u7624\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u6709\u7ebf\u5206\u88c2\u7684\u975e\u5178\u578b\u53d8\u4f53\u6570\u91cf\u4e0e\u80bf\u7624\u6076\u6027\u7a0b\u5ea6\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u5374\u662f\u751f\u7269\u68c0\u9a8c\u5e08\u8bc6\u522b\u7684\u96be\u70b9\uff0c\u9700\u8981\u7cbe\u51c6\u533a\u5206\u6765\u652f\u6491\u60a3\u8005\u9884\u540e\u8bc4\u4f30\u548c\u8d44\u6e90\u5206\u914d", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u7ec4\u7ec7\u75c5\u7406\u6570\u636e\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u4f4e\u79e9\u9002\u914d\u8fdb\u884c\u53c2\u6570\u6548\u7387\u5fae\u8c03\uff0c\u8bad\u7ec3\u4e2d\u4f7f\u7528\u9c7c\u773c\u53d8\u6362\u5f3a\u8c03\u6709\u7ebf\u5206\u88c2\uff0c\u5e76\u901a\u8fc7\u5085\u91cc\u53f6\u57df\u9002\u914d\u5229\u7528ImageNet\u76ee\u6807\u56fe\u50cf\uff0c\u6700\u540e\u96c6\u6210\u591a\u4e2a\u6a21\u578b\u6574\u5408\u8865\u5145\u6027\u5f62\u6001\u89c1\u89e3", "result": "\u5728\u9884\u8bc4\u9636\u6bb5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u9ad8\u5e73\u8861\u51c6\u786e\u5ea6", "conclusion": "\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u7684\u6548\u7387\u5fae\u8c03\u548c\u591a\u6a21\u578b\u96c6\u6210\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u6709\u7ebf\u5206\u88c2\u53d8\u4f53\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u809d\u80bf\u7624\u9884\u540e\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u5dee\u5f02\u5316\u6307\u6807"}}
{"id": "2509.03128", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03128", "abs": "https://arxiv.org/abs/2509.03128", "authors": ["Zichang Ren", "Chunhang Zheng", "Dou Li", "Yuping Zhao"], "title": "Successive Cancellation Decoding For General Monotone Chain Polar Codes", "comment": null, "summary": "Monotone chain polar codes generalize classical polar codes to multivariate\nsettings, offering a flexible approach for achieving the entire admissible rate\nregion in the distributed lossless coding problem. However, this flexibility\nalso introduces significant challenges for existing successive cancellation\n(SC) based decoding schemes. Motivated by the need for a general SC decoding\nsolution, we present a comprehensive decoding strategy for monotone chain polar\ncodes that can handle arbitrary numbers of terminals, non-binary alphabets, and\ndecoding along arbitrary monotone chains. Specifically, we formulate the SC\ndecoding task as a series of inference subtasks over the polar transform and\npropose a computational graph framework based on probability propagation\nprinciples. This approach highlights the impact of variable switching during\ndecoding and shows that time complexity varies between $O(N\\log{N})$ and\n$O(N^2)$, depending on the specific chain structure. Moreover, we demonstrate\nthat the widely used $O(N)$ space optimization is not universally applicable to\nmonotone chain polar codes, which prompts us to introduce a constant-time\ndecoder forking strategy based on the proposed logical computation graphs. This\nstrategy enables time-efficient list decoding without relying on $O(N)$-space\ntechniques. Numerical results verify the superior performance of the proposed\nscheme compared with the classical lazy-copy scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5355\u8c03\u94fe\u6781\u5316\u7801\u7684\u901a\u7528SC\u89e3\u7801\u6846\u67b6\uff0c\u652f\u6301\u591a\u7ec8\u7aef\u3001\u975e\u4e8c\u8fdb\u5236\u548c\u4efb\u610f\u5355\u8c03\u94fe\u89e3\u7801\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u5728O(NlogN)\u5230O(N\u00b2)\u4e4b\u95f4\uff0c\u5e76\u5f15\u5165\u4e86\u5e38\u6570\u65f6\u95f4\u5206\u53c9\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u5217\u8868\u89e3\u7801\u3002", "motivation": "\u5355\u8c03\u94fe\u6781\u5316\u7801\u5728\u5206\u5e03\u5f0f\u65e0\u635f\u7f16\u7801\u4e2d\u5177\u6709\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709SC\u89e3\u7801\u65b9\u6848\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u901a\u7528\u7684\u89e3\u7801\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u4efb\u610f\u7ec8\u7aef\u6570\u3001\u975e\u4e8c\u8fdb\u5236\u5b57\u6bcd\u8868\u548c\u4efb\u610f\u5355\u8c03\u94fe\u3002", "method": "\u5c06SC\u89e3\u7801\u4efb\u52a1\u5236\u5b9a\u4e3a\u6781\u5316\u53d8\u6362\u4e0a\u7684\u63a8\u7406\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u4f20\u64ad\u539f\u7406\u7684\u8ba1\u7b97\u56fe\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u5e38\u6570\u65f6\u95f4\u89e3\u7801\u5668\u5206\u53c9\u7b56\u7565\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u65b9\u6848\u76f8\u6bd4\u7ecf\u5178lazy-copy\u65b9\u6848\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u65f6\u95f4\u6548\u7387\u9ad8\u4e14\u4e0d\u4f9d\u8d56O(N)\u7a7a\u95f4\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u8ba1\u7b97\u56fe\u6846\u67b6\u548c\u5206\u53c9\u7b56\u7565\u4e3a\u5355\u8c03\u94fe\u6781\u5316\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u901a\u7528SC\u89e3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.02724", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02724", "abs": "https://arxiv.org/abs/2509.02724", "authors": ["Xiang-Gen Xia"], "title": "Recall Gabor Communication Theory and Joint Time-Frequency Analysis", "comment": null, "summary": "In this article, we first briefly recall Gabor's communication theory and\nthen Gabor transform and expansion, and also its connection with joint time\nfrequency analysis.", "AI": {"tldr": "\u672c\u6587\u56de\u987eGabor\u901a\u4fe1\u7406\u8bba\u3001Gabor\u53d8\u6362\u548c\u5c55\u5f00\uff0c\u53ca\u5176\u4e0e\u8054\u5408\u65f6\u9891\u5206\u6790\u7684\u8054\u7cfb", "motivation": "\u91cd\u65b0\u5ba1\u89c6Gabor\u7684\u901a\u4fe1\u7406\u8bba\u57fa\u7840\u7406\u8bba\uff0c\u63a2\u8ba8\u5176\u65f6\u9891\u5206\u6790\u65b9\u6cd5\u5728\u73b0\u4ee3\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e94\u7528\u4ef7\u503c", "method": "\u91c7\u7528\u7406\u8bba\u56de\u987e\u548c\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406Gabor\u901a\u4fe1\u7406\u8bba\u3001Gabor\u53d8\u6362\u548c\u5c55\u5f00\u7684\u6570\u5b66\u6846\u67b6", "result": "\u5efa\u7acb\u4e86Gabor\u7406\u8bba\u4e0e\u8054\u5408\u65f6\u9891\u5206\u6790\u4e4b\u95f4\u7684\u660e\u786e\u8054\u7cfb\uff0c\u4e3a\u73b0\u4ee3\u65f6\u9891\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840", "conclusion": "Gabor\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u65f6\u9891\u5206\u6790\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5176\u53d8\u6362\u548c\u5c55\u5f00\u65b9\u6cd5\u5728\u8054\u5408\u65f6\u9891\u5206\u6790\u4e2d\u5177\u6709\u6301\u7eed\u7684\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.02593", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02593", "abs": "https://arxiv.org/abs/2509.02593", "authors": ["Rapha\u00ebl Bourgade", "Guillaume Balezo", "Thomas Walter"], "title": "Robust Pan-Cancer Mitotic Figure Detection with YOLOv12", "comment": null, "summary": "Mitotic figures represent a key histoprognostic feature in tumor pathology,\nproviding crucial insights into tumor aggressiveness and proliferation.\nHowever, their identification remains challenging, subject to significant\ninter-observer variability, even among experienced pathologists. To address\nthis issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the\nthird edition of an international competition aiming to develop robust mitosis\ndetection algorithms. In this paper, we present a mitotic figures detection\napproach based on the YOLOv12 object detection architecture, achieving a\n$F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge,\nwithout relying on external data.", "AI": {"tldr": "\u57fa\u4e8eYOLOv12\u7684\u6709\u6770\u56fe\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u5f970.801\u7684F1\u5206\u6570\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u6570\u636e", "motivation": "\u6709\u6770\u56fe\u662f\u80bf\u7624\u8d28\u9884\u540e\u7684\u5173\u952e\u7ec4\u7ec7\u5b66\u6807\u5fd7\uff0c\u4f46\u8bc6\u522b\u5b58\u5728\u663e\u8457\u7684\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\uff0c\u9700\u8981\u53d1\u5c55\u7a33\u5065\u7684\u81ea\u52a8\u68c0\u6d4b\u7b97\u6cd5", "method": "\u91c7\u7528YOLOv12\u7269\u4f53\u68c0\u6d4b\u67b6\u6784\u5f00\u53d1\u6709\u6770\u56fe\u68c0\u6d4b\u65b9\u6cd5", "result": "\u5728MIDOG 2025\u6311\u6218\u8d5b\u9884\u6d4b\u8bd5\u9a8c\u96c6\u4e0a\u83b7\u5f97F1\u5206\u65700.801", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f7f\u7528\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6709\u6770\u56fe\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2509.03337", "categories": ["cs.IT", "math.IT", "94B05, 94B65"], "pdf": "https://arxiv.org/pdf/2509.03337", "abs": "https://arxiv.org/abs/2509.03337", "authors": ["Liren Lin", "Guanghui Zhang", "Bocong Chen", "Hongwei Liu"], "title": "New Bounds for Linear Codes with Applications", "comment": "15 pages", "summary": "Bounds on linear codes play a central role in coding theory, as they capture\nthe fundamental trade-off between error-correction capability (minimum\ndistance) and information rate (dimension relative to length). Classical\nresults characterize this trade-off solely in terms of the parameters $n$, $k$,\n$d$ and $q$. In this work we derive new bounds under the additional assumption\nthat the code contains a nonzero codeword of weight $w$.By combining\nresidual-code techniques with classical results such as the Singleton and\nGriesmer bounds,we obtain explicit inequalities linking $n$, $k$, $d$, $q$ and\n$w$. These bounds impose sharper restrictions on admissible codeword weights,\nparticularly those close to the minimum distance or to the code length.\nApplications include refined constraints on the weights of MDS codes, numerical\nrestrictions on general linear codes, and excluded weight ranges in the weight\ndistribution. Numerical comparisons across standard parameter sets demonstrate\nthat these $w$-aware bounds strictly enlarge known excluded weight ranges and\nsharpen structural limitations on linear codes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u4f20\u7edf\u7ebf\u6027\u7801\u8fb9\u754c\u7406\u8bba\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u96f6\u7801\u5b57\u6743\u91cdw\u7684\u9644\u52a0\u5047\u8bbe\uff0c\u7ed3\u5408\u6b8b\u5dee\u7801\u6280\u672f\u548c\u7ecf\u5178\u8fb9\u754c\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u4e86\u8fde\u63a5\u53c2\u6570n\u3001k\u3001d\u3001q\u548cw\u7684\u663e\u5f0f\u4e0d\u7b49\u5f0f\uff0c\u83b7\u5f97\u4e86\u6bd4\u4f20\u7edf\u8fb9\u754c\u66f4\u4e25\u683c\u7684\u9650\u5236\u6761\u4ef6\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u7801\u8fb9\u754c\u7406\u8bba\u4ec5\u57fa\u4e8e\u53c2\u6570n\u3001k\u3001d\u548cq\uff0c\u5ffd\u7565\u4e86\u7801\u5b57\u6743\u91cd\u4fe1\u606f\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5229\u7528\u7801\u4e2d\u5305\u542b\u7279\u5b9a\u6743\u91cd\u7801\u5b57\u7684\u9644\u52a0\u4fe1\u606f\uff0c\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u8fb9\u754c\u9650\u5236\u3002", "method": "\u7ed3\u5408\u6b8b\u5dee\u7801\u6280\u672f\u548c\u7ecf\u5178\u8fb9\u754c\u65b9\u6cd5\uff08\u5982Singleton\u8fb9\u754c\u548cGriesmer\u8fb9\u754c\uff09\uff0c\u63a8\u5bfc\u51fa\u8fde\u63a5\u7801\u957fn\u3001\u7ef4\u5ea6k\u3001\u6700\u5c0f\u8ddd\u79bbd\u3001\u57df\u5927\u5c0fq\u548c\u7801\u5b57\u6743\u91cdw\u7684\u663e\u5f0f\u4e0d\u7b49\u5f0f\u3002", "result": "\u83b7\u5f97\u4e86\u6bd4\u4f20\u7edf\u8fb9\u754c\u66f4\u4e25\u683c\u7684\u6743\u91cd\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u63a5\u8fd1\u6700\u5c0f\u8ddd\u79bb\u6216\u7801\u957f\u7684\u6743\u91cd\u8303\u56f4\u5185\u3002\u6570\u503c\u6bd4\u8f83\u663e\u793a\u8fd9\u4e9bw\u611f\u77e5\u8fb9\u754c\u663e\u8457\u6269\u5927\u4e86\u5df2\u77e5\u7684\u6392\u9664\u6743\u91cd\u8303\u56f4\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7801\u5b57\u6743\u91cd\u4fe1\u606f\uff0c\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u7ebf\u6027\u7801\u7ed3\u6784\u66f4\u7cbe\u7ec6\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728MDS\u7801\u6743\u91cd\u7ea6\u675f\u3001\u4e00\u822c\u7ebf\u6027\u7801\u6570\u503c\u9650\u5236\u548c\u6743\u91cd\u5206\u5e03\u6392\u9664\u8303\u56f4\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.02797", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02797", "abs": "https://arxiv.org/abs/2509.02797", "authors": ["Sagnik Bhattacharya", "Abhiram Rao Gorle", "John M. Cioffi"], "title": "minPIC: Towards Optimal Power Allocation in Multi-User Interference Channels", "comment": "To appear in IEEE GLOBECOM 2025", "summary": "6G envisions massive cell-free networks with spatially nested multiple access\n(MAC) and broadcast (BC) channels without centralized coordination. This makes\noptimal resource allocation across power, subcarriers, and decoding orders\ncrucial for interference channels (ICs), where neither transmitters nor\nreceivers can cooperate. Current orthogonal multiple access (OMA) methods, as\nwell as non-orthogonal (NOMA) and rate-splitting (RSMA) schemes, rely on fixed\nheuristics for interference management, leading to suboptimal rates, power\ninefficiency, and scalability issues. This paper proposes a novel minPIC\nframework for optimal power, subcarrier, and decoding order allocation in\ngeneral multi-user ICs. Unlike existing methods, minPIC eliminates heuristic\nSIC order assumptions. Despite the convexity of the IC capacity region, fixing\nan SIC order induces non-convexity in resource allocation, traditionally\nrequiring heuristic approximations. We instead introduce a dual-variable-guided\nsorting criterion to identify globally optimal SIC orders, followed by convex\noptimization with auxiliary log-det constraints, efficiently solved via binary\nsearch. We also demonstrate that minPIC could potentially meet the stringent\nhigh-rate, low-power targets of immersive XR and other 6G applications. To the\nbest of our knowledge, minPIC is the first algorithmic realisation of the\nPareto boundary of the SIC-achievable rate region for Gaussian ICs, opening the\ndoor to scalable interference management in cell-free networks.", "AI": {"tldr": "minPIC\u6846\u67b6\uff1a\u9996\u4e2a\u5b9e\u73b0\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u5e15\u7d2f\u6258\u8fb9\u754c\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u53d8\u91cf\u5f15\u5bfc\u6392\u5e8f\u548c\u51f8\u4f18\u5316\u89e3\u51b36G\u65e0\u8702\u7a9d\u7f51\u7edc\u4e2d\u529f\u7387\u3001\u5b50\u8f7d\u6ce2\u548c\u89e3\u7801\u987a\u5e8f\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898", "motivation": "6G\u65e0\u8702\u7a9d\u7f51\u7edc\u9700\u8981\u5904\u7406\u7a7a\u95f4\u5d4c\u5957\u7684\u591a\u5740\u63a5\u5165\u548c\u5e7f\u64ad\u4fe1\u9053\uff0c\u73b0\u6709OMA\u3001NOMA\u548cRSMA\u65b9\u6848\u4f9d\u8d56\u56fa\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u5e72\u6270\u7ba1\u7406\uff0c\u5bfc\u81f4\u901f\u7387\u6b21\u4f18\u3001\u529f\u7387\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898", "method": "\u63d0\u51faminPIC\u6846\u67b6\uff0c\u5f15\u5165\u53cc\u53d8\u91cf\u5f15\u5bfc\u6392\u5e8f\u51c6\u5219\u8bc6\u522b\u5168\u5c40\u6700\u4f18SIC\u987a\u5e8f\uff0c\u7136\u540e\u901a\u8fc7\u5e26\u6709\u8f85\u52a9\u5bf9\u6570\u884c\u5217\u5f0f\u7ea6\u675f\u7684\u51f8\u4f18\u5316\u8fdb\u884c\u6c42\u89e3\uff0c\u91c7\u7528\u4e8c\u5206\u641c\u7d22\u9ad8\u6548\u5b9e\u73b0", "result": "minPIC\u80fd\u591f\u5b9e\u73b0\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u6709\u671b\u6ee1\u8db3\u6c89\u6d78\u5f0fXR\u7b496G\u5e94\u7528\u7684\u9ad8\u901f\u7387\u3001\u4f4e\u529f\u8017\u8981\u6c42", "conclusion": "minPIC\u662f\u9996\u4e2a\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u5e15\u7d2f\u6258\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u4e3a\u65e0\u8702\u7a9d\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u5e72\u6270\u7ba1\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2509.02595", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02595", "abs": "https://arxiv.org/abs/2509.02595", "authors": ["Hana Feki", "Alice Blondel", "Thomas Walter"], "title": "ConvNeXt with Histopathology-Specific Augmentations for Mitotic Figure Classification", "comment": null, "summary": "Accurate mitotic figure classification is crucial in computational pathology,\nas mitotic activity informs cancer grading and patient prognosis.\nDistinguishing atypical mitotic figures (AMFs), which indicate higher tumor\naggressiveness, from normal mitotic figures (NMFs) remains challenging due to\nsubtle morphological differences and high intra-class variability. This task is\nfurther complicated by domain shifts, including variations in organ, tissue\ntype, and scanner, as well as limited annotations and severe class imbalance.\nTo address these challenges in Track 2 of the MIDOG 2025 Challenge, we propose\na solution based on the lightweight ConvNeXt architecture, trained on all\navailable datasets (AMi-Br, AtNorM-Br, AtNorM-MD, and OMG-Octo) to maximize\ndomain coverage. Robustness is enhanced through a histopathology-specific\naugmentation pipeline, including elastic and stain-specific transformations,\nand balanced sampling to mitigate class imbalance. A grouped 5-fold\ncross-validation strategy ensures reliable evaluation. On the preliminary\nleaderboard, our model achieved a balanced accuracy of 0.8961, ranking among\nthe top entries. These results highlight that broad domain exposure combined\nwith targeted augmentation strategies is key to building accurate and\ngeneralizable mitotic figure classifiers.", "AI": {"tldr": "\u57fa\u4e8e\u8f7b\u91cf\u7ea7ConvNeXt\u67b6\u6784\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u75c5\u7406\u5b66\u7279\u5f02\u6027\u6570\u636e\u589e\u5f3a\uff0c\u5728MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e860.8961\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u5e7f\u6cdb\u9886\u57df\u66b4\u9732\u548c\u9488\u5bf9\u6027\u589e\u5f3a\u7b56\u7565\u5bf9\u6784\u5efa\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u7c7b\u5668\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u51c6\u786e\u5206\u7c7b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5bf9\u8ba1\u7b97\u75c5\u7406\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u533a\u5206\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf(AMFs)\u548c\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\u56fe\u50cf(NMFs)\u5b58\u5728\u6311\u6218\uff0c\u5305\u62ec\u5f62\u6001\u5dee\u5f02\u7ec6\u5fae\u3001\u7c7b\u5185\u53d8\u5f02\u5927\u3001\u9886\u57df\u504f\u79fb\u3001\u6807\u6ce8\u6709\u9650\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7ConvNeXt\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6(AMi-Br, AtNorM-Br, AtNorM-MD, OMG-Octo)\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4ee5\u6700\u5927\u5316\u9886\u57df\u8986\u76d6\uff1b\u91c7\u7528\u75c5\u7406\u5b66\u7279\u5f02\u6027\u6570\u636e\u589e\u5f3a\u7ba1\u9053(\u5305\u62ec\u5f39\u6027\u548c\u67d3\u8272\u7279\u5f02\u6027\u53d8\u6362)\u548c\u5e73\u8861\u91c7\u6837\u6765\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b\u4f7f\u7528\u5206\u7ec45\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\u3002", "result": "\u5728\u521d\u6b65\u6392\u884c\u699c\u4e0a\uff0c\u6a21\u578b\u8fbe\u5230\u4e860.8961\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u6392\u540d\u9760\u524d\u3002", "conclusion": "\u5e7f\u6cdb\u9886\u57df\u66b4\u9732\u7ed3\u5408\u9488\u5bf9\u6027\u589e\u5f3a\u7b56\u7565\u662f\u6784\u5efa\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5173\u952e\u3002"}}
{"id": "2509.03481", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03481", "abs": "https://arxiv.org/abs/2509.03481", "authors": ["Lorenzo Talamanca", "Julian Trouillon"], "title": "PoolPy: Flexible Group Testing Design for Large-Scale Screening", "comment": null, "summary": "In large screening campaigns, group testing can greatly reduce the number of\ntests needed when compared to testing each sample individually. However,\nchoosing and applying an appropriate group testing method remains challenging\ndue to the wide variety in design and performance across methods, and the lack\nof accessible tools. Here, we present PoolPy, a unified framework for designing\nand selecting optimal group testing strategies across ten different methods\naccording to user-defined constraints, such as time, cost or sample dilution.\nBy computing over 10,000 group testing designs made available through a web\ninterface, we identified key trade-offs, such as minimizing test number or\ngroup size, that define applicability to specific use cases. Overall, we show\nthat no single method is universally optimal, and provide clear indications for\nmethod choice on a case-by-case basis.", "AI": {"tldr": "PoolPy\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7fa4\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u63d0\u4f9b10\u79cd\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u5316\u7b56\u7565\u9009\u62e9\uff0c\u901a\u8fc7web\u754c\u9762\u8ba1\u7b97\u8d85\u8fc710,000\u79cd\u8bbe\u8ba1\u65b9\u6848\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u65f6\u95f4\u3001\u6210\u672c\u6216\u6837\u672c\u7a00\u91ca\u7b49\u7ea6\u675f\u6761\u4ef6\u9009\u62e9\u6700\u4f18\u7fa4\u4f53\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u7b5b\u67e5\u6d3b\u52a8\u4e2d\uff0c\u7fa4\u4f53\u68c0\u6d4b\u76f8\u6bd4\u5355\u4e2a\u6837\u672c\u68c0\u6d4b\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u68c0\u6d4b\u6b21\u6570\uff0c\u4f46\u7531\u4e8e\u65b9\u6cd5\u591a\u6837\u6027\u548c\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6613\u7528\u5de5\u5177\uff0c\u9009\u62e9\u548c\u5b9e\u65bd\u5408\u9002\u7684\u7fa4\u4f53\u68c0\u6d4b\u65b9\u6cd5\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1PoolPy\u7edf\u4e00\u6846\u67b6\uff0c\u96c6\u621010\u79cd\u7fa4\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u65f6\u95f4\u3001\u6210\u672c\u6216\u6837\u672c\u7a00\u91ca\u7b49\u7ea6\u675f\u6761\u4ef6\uff0c\u8ba1\u7b97\u548c\u6bd4\u8f83\u8d85\u8fc710,000\u79cd\u7fa4\u4f53\u68c0\u6d4b\u8bbe\u8ba1\u65b9\u6848\u3002", "result": "\u8bc6\u522b\u4e86\u5173\u952e\u6743\u8861\u56e0\u7d20\uff08\u5982\u6700\u5c0f\u5316\u68c0\u6d4b\u6b21\u6570\u6216\u7ec4\u5927\u5c0f\uff09\uff0c\u786e\u5b9a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u662f\u6700\u4f18\u7684\u3002", "conclusion": "\u7fa4\u4f53\u68c0\u6d4b\u65b9\u6cd5\u7684\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u6848\u4f8b\u8fdb\u884c\uff0cPoolPy\u6846\u67b6\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6307\u5bfc\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u9009\u62e9\u6700\u9002\u5408\u7684\u7fa4\u4f53\u68c0\u6d4b\u7b56\u7565\u3002"}}
{"id": "2509.02819", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02819", "abs": "https://arxiv.org/abs/2509.02819", "authors": ["Sameer Mathad", "Taejoon Kim", "David J. Love"], "title": "Protecting Legacy Wireless Systems Against Interference: Precoding and Codebook Approaches Using Massive MIMO and Region Constraints", "comment": null, "summary": "The ever-increasing demand for high-speed wireless communication has\ngenerated significant interest in utilizing frequency bands that are adjacent\nto those occupied by legacy wireless systems. Since the legacy wireless systems\nwere designed based on often decades-old assumptions about wireless\ninterference, utilizing these new bands will result in interference with the\nexisting legacy users. Many of these legacy wireless devices are used by\ncritical infrastructure networks upon which society depends. There is an urgent\nneed to develop schemes that can protect legacy users from such interference.\nFor many applications, legacy users are located within\ngeographically-constrained regions. Several studies have proposed mitigating\ninterference through the implementation of exclusion zones near these\ngeographically-constrained regions. In contrast to solutions based on\ngeographic exclusion zones, this paper presents a communication theory-based\nsolution. By leveraging knowledge of these geographically-constrained regions,\nwe aim to reduce the interference impact on legacy users. We achieve this by\nincorporating received power constraints, termed as region constraints, in our\nmassive multiple-input multiple-output (MIMO) system design. We perform a\ncapacity analysis of single-user massive MIMO and a sum-rate analysis of the\nmulti-user massive MIMO system with transmit power and region constraints. We\npresent a precoding design method that allows for the utilization of new\nfrequency bands while protecting legacy users.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u4fe1\u7406\u8bba\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u52a0\u5165\u533a\u57df\u7ea6\u675f\u6765\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u514d\u53d7\u5e72\u6270\uff0c\u800c\u4e0d\u662f\u91c7\u7528\u5730\u7406\u6392\u9664\u533a\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u9ad8\u901f\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u5229\u7528\u4f20\u7edf\u7cfb\u7edf\u6bb5\u6bb5\u7684\u90bb\u8fd1\u9891\u6bb5\uff0c\u4f46\u8fd9\u4f1a\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\u4e2d\u7684\u4f20\u7edf\u7528\u6237\u9020\u6210\u5e72\u6270\u3002\u73b0\u6709\u7684\u5730\u7406\u6392\u9664\u533a\u65b9\u6848\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u66f4\u597d\u7684\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u5728\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u6dfb\u52a0\u63a5\u6536\u529f\u7387\u7ea6\u675f\uff08\u533a\u57df\u7ea6\u675f\uff09\uff0c\u5bf9\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u7cfb\u7edf\u8fdb\u884c\u5bb9\u91cf\u5206\u6790\u548c\u603b\u901f\u7387\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7f16\u7801\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5229\u7528\u65b0\u9891\u6bb5\u8d44\u6e90\u7684\u540c\u65f6\uff0c\u6709\u6548\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u514d\u53d7\u5e72\u6270\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u901a\u4fe1\u7406\u8bba\u57fa\u7840\u7684\u533a\u57df\u7ea6\u675f\u65b9\u6848\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u65b0\u65e7\u7cfb\u7edf\u5171\u5b58\u65f6\u7684\u5e72\u6270\u95ee\u9898\uff0c\u4e3a\u9ad8\u901f\u65e0\u7ebf\u901a\u4fe1\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.02597", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02597", "abs": "https://arxiv.org/abs/2509.02597", "authors": ["Shuting Xu", "Runtong Liu", "Zhixuan Chen", "Junlin Hou", "Hao Chen"], "title": "Solutions for Mitotic Figure Detection and Atypical Classification in MIDOG 2025", "comment": null, "summary": "Deep learning has driven significant advances in mitotic figure analysis\nwithin computational pathology. In this paper, we present our approach to the\nMitosis Domain Generalization (MIDOG) 2025 Challenge, which consists of two\ndistinct tasks, i.e., mitotic figure detection and atypical mitosis\nclassification. For the mitotic figure detection task, we propose a two-stage\ndetection-classification framework that first localizes candidate mitotic\nfigures and subsequently refines the predictions using a dedicated\nclassification module. For the atypical mitosis classification task, we employ\nan ensemble strategy that integrates predictions from multiple state-of-the-art\ndeep learning architectures to improve robustness and accuracy. Extensive\nexperiments demonstrate the effectiveness of our proposed methods across both\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u6790\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b-\u5206\u7c7b\u6846\u67b6\u548c\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728MIDOG 2025\u6311\u6218\u8d5b\u7684\u4e24\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u7684\u57df\u6cdb\u5316\u95ee\u9898", "method": "\u5bf9\u4e8e\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u4efb\u52a1\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5148\u5b9a\u4f4d\u5019\u9009\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\uff0c\u518d\u7528\u4e13\u7528\u5206\u7c7b\u6a21\u5757\u7cbe\u70bc\u9884\u6d4b\uff1b\u5bf9\u4e8e\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4efb\u52a1\u91c7\u7528\u96c6\u6210\u7b56\u7565\uff0c\u6574\u5408\u591a\u4e2a\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u9884\u6d4b", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u5747\u6709\u6548", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b-\u5206\u7c7b\u6846\u67b6\u548c\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.03038", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03038", "abs": "https://arxiv.org/abs/2509.03038", "authors": ["Ruihong Jiang", "Ruichen Zhang", "Yanqing Xu", "Huimin Hu", "Yang Lu", "Dusit Niyato"], "title": "Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS Blockage", "comment": "5 pages, 4 figures", "summary": "This paper considers a power-splitting (PS)-based simultaneous wireless\ninformation and power transfer (SWIPT) system employing a reconfigurable\npinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We\nformulate a joint optimization of the PA position and the PS ratio to maximize\nthe average signal-to-noise ratio (SNR) at a user, subject to its average\nenergy harvesting (EH) and PA placement limits. We derive a closed-form optimal\nsolution. Results demonstrate that the EH requirement has a deterministic\nimpact on the optimal PA position as well as its feasible region, requiring\ndeployment of the PA as close to the user as possible to maximize average\nchannel gain. This spatial adaptation, combined with dynamic PS, enables robust\nSWIPT performance in the presence of probabilistic LoS blockage, revealing that\nmechanical reconfigurability primarily enhances sustainability by ensuring\nenergy feasibility in dynamic environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6982\u7387\u6027\u89c6\u7ebf\u963b\u585e\u73af\u5883\u4e0b\uff0c\u4f7f\u7528\u53ef\u91cd\u914d\u5236\u62c9\u6293\u5929\u7ebf\u7684\u529b\u91cf\u5206\u5272\u5782\u76f4SWIPT\u7cfb\u7edf\uff0c\u901a\u8fc7\u5173\u8054\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u529b\u91cf\u5206\u5272\u6bd4\u6765\u6700\u5927\u5316\u5e73\u5747\u4fe1\u566a\u6bd4\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\uff0c\u5e94\u5bf9\u6982\u7387\u6027\u89c6\u7ebf\u963b\u585e\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u673a\u68b0\u53ef\u91cd\u914d\u5236\u6027\u63d0\u5347\u7cfb\u7edf\u7684\u7eed\u822a\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4f7f\u7528\u53ef\u91cd\u914d\u5236\u62c9\u6293\u5929\u7ebf\u7684PS-SWIPT\u7cfb\u7edf\u6a21\u578b\uff0c\u5f62\u6210\u5173\u4e8e\u5929\u7ebf\u4f4d\u7f6e\u548c\u529b\u91cf\u5206\u5272\u6bd4\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u6c42\u89e3\u51fa\u95ed\u5f62\u6700\u4f18\u89e3\u3002", "result": "\u80fd\u91cf\u6536\u96c6\u8981\u6c42\u5bf9\u6700\u4f18\u5929\u7ebf\u4f4d\u7f6e\u53ca\u5176\u53ef\u884c\u57df\u4ea7\u751f\u786e\u5b9a\u6027\u5f71\u54cd\uff0c\u9700\u5c06\u5929\u7ebf\u90e8\u7f72\u5728\u8fd1\u7528\u6237\u5904\u4ee5\u6700\u5927\u5316\u5e73\u5747\u901a\u9053\u589e\u76ca\uff1b\u7a7a\u95f4\u9002\u914d\u4e0e\u52a8\u6001\u529b\u91cf\u5206\u5272\u7ed3\u5408\u53ef\u5728\u6982\u7387\u6027LoS\u963b\u585e\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684SWIPT\u6027\u80fd\u3002", "conclusion": "\u673a\u68b0\u53ef\u91cd\u914d\u5236\u6027\u4e3b\u8981\u901a\u8fc7\u786e\u4fdd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u80fd\u91cf\u53ef\u884c\u6027\u6765\u63d0\u5347\u7cfb\u7edf\u7684\u7eed\u822a\u80fd\u529b\uff0c\u4e3a\u5e94\u5bf9\u6982\u7387\u6027\u89c6\u7ebf\u963b\u585e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02598", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02598", "abs": "https://arxiv.org/abs/2509.02598", "authors": ["Andrew Broad", "Jason Keighley", "Lucy Godson", "Alex Wright"], "title": "MIDOG 2025: Mitotic Figure Detection with Attention-Guided False Positive Correction", "comment": null, "summary": "We present a novel approach which extends the existing Fully Convolutional\nOne-Stage Object Detector (FCOS) for mitotic figure detection. Our composite\nmodel adds a Feedback Attention Ladder CNN (FAL-CNN) model for classification\nof normal versus abnormal mitotic figures, feeding into a fusion network that\nis trained to generate adjustments to bounding boxes predicted by FCOS. Our\nnetwork aims to reduce the false positive rate of the FCOS object detector, to\nimprove the accuracy of object detection and enhance the generalisability of\nthe network. Our model achieved an F1 score of 0.655 for mitosis detection on\nthe preliminary evaluation dataset.", "AI": {"tldr": "\u901a\u8fc7\u5728FCOS\u68c0\u6d4b\u5668\u57fa\u7840\u4e0a\u589e\u6dfbFAL-CNN\u5206\u7c7b\u5668\u548c\u878d\u5408\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u5076\u7136\u68c0\u6d4b\u9519\u8bef\u7387\u7684\u6709\u7ec6\u80de\u68c0\u6d4b\u65b9\u6cd5", "motivation": "\u51cf\u5c11FCOS\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u5076\u7136\u68c0\u6d4b\u9519\u8bef\u7387\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u7f51\u7edc\u7684\u666e\u9002\u6027", "method": "\u5728FCOS\u68c0\u6d4b\u5668\u57fa\u7840\u4e0a\uff0c\u589e\u6dfbFAL-CNN\u6a21\u578b\u7528\u4e8e\u5f02\u5e38\u6709\u7ec6\u80de\u5206\u7c7b\uff0c\u901a\u8fc7\u878d\u5408\u7f51\u7edc\u751f\u6210\u68c0\u6d4b\u6846\u8c03\u6574", "result": "\u5728\u9884\u6d4b\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86F1\u5206\u65700.655\u7684\u6709\u7ec6\u80de\u68c0\u6d4b\u7ed3\u679c", "conclusion": "\u8be5\u590d\u5408\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6709\u7ec6\u80de\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2509.03066", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03066", "abs": "https://arxiv.org/abs/2509.03066", "authors": ["Huaicheng Zhang", "Ruoxin Wang", "Chenlian Zhou", "Jiguang Shi", "Yue Ge", "Zhoutong Li", "Sheng Chang", "Hao Wang", "Jin He", "Qijun Huang"], "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG", "comment": null, "summary": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis.", "AI": {"tldr": "S2M2ECG\u662f\u4e00\u4e2a\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u7684\u5fc3\u7535\u56fe\u5206\u6790\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u7ea7\u878d\u5408\u673a\u5236\u5904\u7406\u591a\u5bfc\u8054ECG\u4fe1\u53f7\uff0c\u5728\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7279\u5f81\u878d\u5408\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u591a\u5bfc\u8054\u5fc3\u7535\u56fe\u4fe1\u53f7\u5206\u6790\u9762\u4e34\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u591a\u6e90\u7279\u5f81\u878d\u5408\u7684\u5e73\u8861\u6311\u6218\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u7684\u9ad8\u6548\u8ba1\u7b97\u7279\u6027\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u4f7f\u5176\u7279\u522b\u9002\u5408ECG\u8fd9\u7c7b\u4f4e\u7ef4\u6570\u636e\u3002", "method": "\u63d0\u51faS2M2ECG\u67b6\u6784\uff0c\u5305\u542b\u4e09\u7ea7\u878d\u5408\u673a\u5236\uff1a1)\u65f6\u7a7a\u53cc\u5411SSM\u4e0e\u5206\u6bb5\u6807\u8bb0\u5316\u7684\u4f4e\u5c42\u4fe1\u53f7\u878d\u5408\uff1b2)\u5bfc\u8054\u5185\u53cc\u5411\u626b\u63cf\u7684\u65f6\u95f4\u4fe1\u606f\u878d\u5408\uff1b3)\u8de8\u5bfc\u8054\u7279\u5f81\u4ea4\u4e92\u7684\u7a7a\u95f4\u4fe1\u606f\u878d\u5408\u3002\u91c7\u7528\u591a\u5206\u652f\u8bbe\u8ba1\u548c\u5bfc\u8054\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u5404\u5bfc\u8054\u72ec\u7acb\u5206\u6790\u7684\u540c\u65f6\u786e\u4fdd\u65e0\u7f1d\u96c6\u6210\u3002", "result": "S2M2ECG\u5728\u8282\u5f8b\u3001\u5f62\u6001\u548c\u4e34\u5e8a\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5176\u8f7b\u91cf\u7ea7\u67b6\u6784\u53c2\u6570\u6570\u91cf\u51e0\u4e4e\u662f\u6700\u5c11\u7684\uff0c\u975e\u5e38\u9002\u5408\u9ad8\u6548\u63a8\u7406\u548c\u4fbf\u6377\u90e8\u7f72\u3002", "conclusion": "S2M2ECG\u5728\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548cECG\u7279\u5f02\u6027\u7279\u5f81\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u9ad8\u6027\u80fd\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.02599", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02599", "abs": "https://arxiv.org/abs/2509.02599", "authors": ["Piotr Giedziun", "Jan So\u0142tysik", "Mateusz G\u00f3rczany", "Norbert Ropiak", "Marcin Przymus", "Piotr Krajewski", "Jaros\u0142aw Kwiecie\u0144", "Artur Bartczak", "Izabela Wasiak", "Mateusz Maniewski"], "title": "RF-DETR for Robust Mitotic Figure Detection: A MIDOG 2025 Track 1 Approach", "comment": "Challenge report for MIDOG 2025 Track 1", "summary": "Mitotic figure detection in histopathology images remains challenging due to\nsignificant domain shifts across different scanners, staining protocols, and\ntissue types. This paper presents our approach for the MIDOG 2025 challenge\nTrack 1, focusing on robust mitotic figure detection across diverse\nhistological contexts. While we initially planned a two-stage approach\ncombining high-recall detection with subsequent classification refinement, time\nconstraints led us to focus on optimizing a single-stage detection pipeline. We\nemployed RF-DETR (Roboflow Detection Transformer) with hard negative mining,\ntrained on MIDOG++ dataset. On the preliminary test set, our method achieved an\nF1 score of 0.789 with a recall of 0.839 and precision of 0.746, demonstrating\neffective generalization across unseen domains. The proposed solution offers\ninsights into the importance of training data balance and hard negative mining\nfor addressing domain shift challenges in mitotic figure detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRF-DETR\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u8d1f\u6837\u672c\u6316\u6398\u6280\u672f\u5728MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e86\u8de8\u57df\u7684\u6709\u7a0b\u5206\u88c2\u56fe\u68c0\u6d4b\uff0c\u5728\u9884\u6d4b\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f970.789\u7684F1\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u7531\u4e8e\u626b\u63cf\u4eea\u3001\u67d3\u8272\u534f\u8bae\u548c\u7ec4\u7ec7\u7c7b\u578b\u5dee\u5f02\u5bfc\u81f4\u7684\u663e\u8457\u57df\u5047\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u6709\u7a0b\u5206\u88c2\u56fe\u68c0\u6d4b\u7684\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528RF-DETR\uff08Roboflow Detection Transformer\uff09\u68af\u5f62\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u786c\u8d1f\u6837\u672c\u6316\u6398\u6280\u672f\uff0c\u5728MIDOG++\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u539f\u8ba1\u5212\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6848\uff08\u9ad8\u56de\u6536\u7387\u68c0\u6d4b+\u5206\u7c7b\u7cbe\u70bc\uff09\uff0c\u4f46\u56f4\u4e8e\u65f6\u95f4\u9650\u5236\u6539\u4e3a\u5355\u9636\u6bb5\u68c0\u6d4b\u7ba1\u7ebf\u3002", "result": "\u5728\u9884\u6d4b\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97F1\u5206\u65700.789\uff08\u56de\u6536\u73870.839\uff0c\u7cbe\u786e\u73870.746\uff09\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u672a\u89c1\u57df\u4e0a\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u7684\u5e73\u8861\u6027\u548c\u786c\u8d1f\u6837\u672c\u6316\u6398\u6280\u672f\u5bf9\u4e8e\u89e3\u51b3\u6709\u7a0b\u5206\u88c2\u56fe\u68c0\u6d4b\u4e2d\u7684\u57df\u5047\u79fb\u6311\u6218\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5355\u9636\u6bb5\u68c0\u6d4b\u65b9\u6848\u5728\u65f6\u95f4\u7d27\u5f20\u60c5\u51b5\u4e0b\u4ecd\u80fd\u83b7\u5f97\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2509.03311", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03311", "abs": "https://arxiv.org/abs/2509.03311", "authors": ["Penggao Yan", "Li-Ta Hsu"], "title": "Credible Uncertainty Quantification under Noise and System Model Mismatch", "comment": "This manuscript has been submitted to IEEE Signal Processing Letters", "summary": "State estimators often provide self-assessed uncertainty metrics, such as\ncovariance matrices, whose reliability is critical for downstream tasks.\nHowever, these self-assessments can be misleading due to underlying modeling\nviolations like noise or system model mismatch. This letter addresses the\nproblem of estimator credibility by introducing a unified, multi-metric\nevaluation framework. We construct a compact credibility portfolio that\nsynergistically combines traditional metrics like the Normalized Estimation\nError Squared (NEES) and the Noncredibility Index (NCI) with proper scoring\nrules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our\nkey contributions are a novel energy distance-based location test to robustly\ndetect system model misspecification and a method that leverages the asymmetric\nsensitivities of NLL and ES to distinguish optimism covariance scaling from\nsystem bias. Monte Carlo simulations across six distinct credibility scenarios\ndemonstrate that our proposed method achieves high classification accuracy\n(80-100%), drastically outperforming single-metric baselines which consistently\nfail to provide a complete and correct diagnosis. This framework provides a\npractical tool for turning patterns of credibility indicators into actionable\ndiagnoses of model deficiencies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6307\u6807\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u72b6\u6001\u4f30\u8ba1\u5668\u7684\u53ef\u4fe1\u5ea6\uff0c\u7ed3\u5408\u4f20\u7edf\u6307\u6807\u548c\u9002\u5f53\u8bc4\u5206\u89c4\u5219\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u6a21\u578b\u7f3a\u9677", "motivation": "\u72b6\u6001\u4f30\u8ba1\u5668\u63d0\u4f9b\u7684\u81ea\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u6307\u6807\uff08\u5982\u534f\u65b9\u5dee\u77e9\u9635\uff09\u53ef\u80fd\u7531\u4e8e\u566a\u58f0\u6216\u7cfb\u7edf\u6a21\u578b\u4e0d\u5339\u914d\u7b49\u5efa\u6a21\u8fdd\u89c4\u800c\u5177\u6709\u8bef\u5bfc\u6027\uff0c\u9700\u8981\u53ef\u9760\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5", "method": "\u6784\u5efa\u7d27\u51d1\u7684\u53ef\u4fe1\u5ea6\u7ec4\u5408\uff0c\u7ed3\u5408\u5f52\u4e00\u5316\u4f30\u8ba1\u8bef\u5dee\u5e73\u65b9(NEES)\u3001\u4e0d\u53ef\u4fe1\u6307\u6570(NCI)\u7b49\u4f20\u7edf\u6307\u6807\u4e0e\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u548c\u80fd\u91cf\u8bc4\u5206(ES)\u7b49\u9002\u5f53\u8bc4\u5206\u89c4\u5219\uff0c\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u8ddd\u79bb\u7684\u4f4d\u7f6e\u68c0\u9a8c\u6765\u68c0\u6d4b\u7cfb\u7edf\u6a21\u578b\u8bef\u8bbe\uff0c\u5229\u7528NLL\u548cES\u7684\u4e0d\u5bf9\u79f0\u654f\u611f\u6027\u533a\u5206\u4e50\u89c2\u534f\u65b9\u5dee\u7f29\u653e\u548c\u7cfb\u7edf\u504f\u5dee", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u53ef\u4fe1\u5ea6\u573a\u666f\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u7387(80-100%)\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6307\u6807\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u80fd\u591f\u5c06\u53ef\u4fe1\u5ea6\u6307\u6807\u7684\u6a21\u5f0f\u8f6c\u5316\u4e3a\u5bf9\u6a21\u578b\u7f3a\u9677\u7684\u53ef\u64cd\u4f5c\u8bca\u65ad"}}
{"id": "2509.03070", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03070", "abs": "https://arxiv.org/abs/2509.03070", "authors": ["Po-Heng Chou", "Wei-Lung Mao", "Ru-Ping Lin"], "title": "YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform", "comment": "5 pages, 2 figures, 2 tables", "summary": "This letter proposes a YOLO-based framework for spatial bearing fault\ndiagnosis using time-frequency spectrograms derived from continuous wavelet\ntransform (CWT). One-dimensional vibration signals are first transformed into\ntime-frequency spectrograms using Morlet wavelets to capture transient fault\nsignatures. These spectrograms are then processed by YOLOv9, v10, and v11\nmodels to classify fault types. Evaluated on three benchmark datasets,\nincluding Case Western Reserve University (CWRU), Paderborn University (PU),\nand Intelligent Maintenance System (IMS), the proposed CWT--YOLO pipeline\nachieves significantly higher accuracy and generalizability than the baseline\nMCNN--LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%\n(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism\nenables direct visualization of fault locations in spectrograms, offering a\npractical solution for condition monitoring in rotating machinery.", "AI": {"tldr": "\u57fa\u4e8eYOLO\u7684\u65f6\u9891\u8c31\u56fe\u8f74\u627f\u6545\u969c\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u5c06\u632f\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a\u65f6\u9891\u8c31\u56fe\uff0c\u5229\u7528YOLOv9/v10/v11\u6a21\u578b\u8fdb\u884c\u6545\u969c\u5206\u7c7b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89c6\u5316\u6545\u969c\u4f4d\u7f6e\u3002", "motivation": "\u89e3\u51b3\u65cb\u8f6c\u673a\u68b0\u8f74\u627f\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u8f6c\u6361\u6545\u969c\u7279\u5f81\u6355\u6349\u56f0\u96be\u548c\u6a21\u578b\u901a\u7528\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u6545\u969c\u4f4d\u7f6e\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Morlet\u5c0f\u6ce2\u5c06\u4e00\u7ef4\u632f\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a\u65f6\u9891\u8c31\u56fe\uff0c\u7136\u540e\u91c7\u7528YOLOv9\u3001v10\u3001v11\u6a21\u578b\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u68c0\u6d4b\u3002", "result": "\u5728CWRU\u3001PU\u3001IMS\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff1aYOLOv11\u5206\u522b\u83b7\u5f9799.4%\u300197.8%\u300199.5%\u7684mAP\u5206\u6570\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u51c6MCNN-LSTM\u6a21\u578b\u3002", "conclusion": "CWT-YOLO\u6d41\u6c34\u7ebf\u4e3a\u65cb\u8f6c\u673a\u68b0\u72b6\u6001\u76d1\u63a7\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u901a\u7528\u6027\u548c\u53ef\u89c6\u5316\u6545\u969c\u4f4d\u7f6e\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02600", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02600", "abs": "https://arxiv.org/abs/2509.02600", "authors": ["Tengyou Xu", "Haochen Yang", "Xiang 'Anthony' Chen", "Hongyan Gu", "Mohammad Haeri"], "title": "Team Westwood Solution for MIDOG 2025 Challenge", "comment": "2 pages, 2 figures", "summary": "This abstract presents our solution (Team Westwood) for mitosis detection and\natypical mitosis classification in the MItosis DOmain Generalization (MIDOG)\n2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial\nmitosis candidate screening with high sensitivity, followed by a random forest\nclassifier ensembling predictions of three convolutional neural networks\n(CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the\natypical mitosis classification, we trained another random forest classifier\nensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and\nInceptionV3. On the preliminary test set, our solution achieved an F1 score of\n0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for\ntrack 2 atypical mitosis classification.", "AI": {"tldr": "\u56e2\u961fWestwood\u5728MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u63d0\u51fa\u7684\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528nnUNetV2\u8fdb\u884c\u521d\u59cb\u5019\u9009\u68c0\u6d4b\uff0c\u7ed3\u5408\u591a\u4e2aCNN\u6a21\u578b\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8fdb\u884c\u96c6\u6210\u9884\u6d4b", "motivation": "\u89e3\u51b3\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u7684\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "method": "\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\uff1annUNetV2\u8fdb\u884c\u521d\u59cb\u5019\u9009\u7b5b\u67e5 + EfficientNet-b3/b5/V2-s\u4e09\u4e2aCNN + \u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u96c6\u6210\uff1b\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff1aEfficientNet-b3/b5 + InceptionV3\u4e09\u4e2aCNN + \u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u96c6\u6210", "result": "\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6709\u4e1d\u5206\u88c2\u68c0\u6d4bF1\u5f97\u52060.7450\uff0c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u5e73\u8861\u51c6\u786e\u73870.8722", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u578b\u96c6\u6210\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2509.03333", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03333", "abs": "https://arxiv.org/abs/2509.03333", "authors": ["Tianfu Qi", "Jun Wang"], "title": "Baseband Model, Cutoff Rate Bounds and Constellation Shaping for Mixed Gaussian-Impulsive Noise", "comment": null, "summary": "Mixed noise, composed of white Gaussian noise (WGN) and impulsive noise (IN),\nappears in numerous communication scenarios and can severely degrade system\nperformance. In this paper, we address this issue by optimizing the transmitted\nconstellation under mixed noise based on a theoretical analysis of the cutoff\nrate (CR). First, starting from the passband model of the mixed noise, we\nderive its corresponding baseband representation. Due to the complexity of the\nCR, an exact analytic expression is generally intractable. Therefore, the\nbaseband noise model is employed to obtain closed-form lower and upper bounds\nof the CR. A piecewise linear approximation is applied to derive efficient\nbounds by exploiting the algebraic properties of the integral terms. These\nbounds are then used as criteria to optimize the transmitted constellation\npoints in both geometric and probabilistic distributions. The projected\ngradient method is employed to solve the optimization problem, and the\nconvergence and properties of the solutions are analyzed. Numerical results\ndemonstrate that the proposed CR bounds are tight and exhibit the expected\nasymptotic behavior. Furthermore, the optimized constellation scheme achieves a\nsignificant rate improvement compared to baselines.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6df7\u5408\u566a\u58f0\uff08\u9ad8\u65af\u767d\u566a\u58f0+\u8109\u51b2\u566a\u58f0\uff09\u4e0b\u7684\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u622a\u6b62\u7387\u6765\u4f18\u5316\u53d1\u9001\u661f\u5ea7\u56fe\u8bbe\u8ba1\u3002", "motivation": "\u6df7\u5408\u566a\u58f0\u5728\u591a\u79cd\u901a\u4fe1\u573a\u666f\u4e2d\u51fa\u73b0\uff0c\u4f1a\u4e25\u91cd\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u627e\u5230\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u901a\u4fe1\u901f\u7387\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u6df7\u5408\u566a\u58f0\u7684\u57fa\u5e26\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u5f97\u5230\u622a\u6b62\u7387\u7684\u95ed\u5f0f\u4e0a\u4e0b\u754c\uff0c\u6700\u540e\u5229\u7528\u6295\u5f71\u68af\u5ea6\u6cd5\u4f18\u5316\u661f\u5ea7\u56fe\u7684\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u622a\u6b62\u7387\u8fb9\u754c\u7d27\u5bc6\u4e14\u5177\u6709\u9884\u671f\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u4f18\u5316\u540e\u7684\u661f\u5ea7\u65b9\u6848\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u622a\u6b62\u7387\u5206\u6790\u7684\u661f\u5ea7\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6df7\u5408\u566a\u58f0\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u6240\u63d0\u51fa\u7684\u8fb9\u754c\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4ef7\u503c\u548c\u5b9e\u7528\u610f\u4e49\u3002"}}
{"id": "2509.03077", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03077", "abs": "https://arxiv.org/abs/2509.03077", "authors": ["Ogechukwu Kanu", "Ashkan Eshaghbeigi", "Hatem Abou-Zeid"], "title": "Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?", "comment": "7 pages, 7 figures, IEEE international conference on communication\n  2025", "summary": "Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.\nHowever, a key challenge in developing AI-powered solutions is the extensive\ndata collection and labeling efforts required to train supervised deep learning\nmodels. To overcome this, self-supervised learning (SSL) approaches have\nrecently demonstrated remarkable success across various domains by leveraging\nlarge volumes of unlabeled data to achieve near-supervised performance. In this\npaper, we propose an effective SSL scheme for radio signal representation\nlearning using momentum contrast. By applying contrastive learning, our method\nextracts robust, transferable representations from a large real-world dataset.\nWe assess the generalizability of these learned representations across two\nwireless communications tasks: angle of arrival (AoA) estimation and automatic\nmodulation classification (AMC). Our results show that carefully designed\naugmentations and diverse data enable contrastive learning to produce\nhigh-quality, invariant latent representations. These representations are\neffective even with frozen encoder weights, and fine-tuning further enhances\nperformance, surpassing supervised baselines. To the best of our knowledge,\nthis is the first work to propose and demonstrate the effectiveness of\nself-supervised learning for radio signals across multiple tasks. Our findings\nhighlight the potential of self-supervised learning to transform AI for\nwireless communications by reducing dependence on labeled data and improving\nmodel generalization - paving the way for scalable foundational 6G AI models\nand solutions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u91cf\u5bf9\u6bd4\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u65e0\u7ebf\u7535\u4fe1\u53f7\u8868\u793a\u5b66\u4e60\uff0c\u5728AoA\u4f30\u8ba1\u548cAMC\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf", "motivation": "\u89e3\u51b36G AI\u53d1\u5c55\u4e2d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a", "method": "\u4f7f\u7528\u52a8\u91cf\u5bf9\u6bd4\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u548c\u591a\u6837\u5316\u6570\u636e\u63d0\u53d6\u4e0d\u53d8\u6027\u6f5c\u5728\u8868\u793a", "result": "\u5b66\u4e60\u5230\u7684\u8868\u793a\u5373\u4f7f\u5728\u51bb\u7ed3\u7f16\u7801\u5668\u6743\u91cd\u65f6\u4e5f\u6709\u6548\uff0c\u5fae\u8c03\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u8d85\u8d8a\u76d1\u7763\u57fa\u7ebf", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a6G AI\u57fa\u7840\u6a21\u578b\u94fa\u5e73\u9053\u8def"}}
{"id": "2509.02601", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02601", "abs": "https://arxiv.org/abs/2509.02601", "authors": ["Piotr Giedziun", "Jan So\u0142tysik", "Mateusz G\u00f3rczany", "Norbert Ropiak", "Marcin Przymus", "Piotr Krajewski", "Jaros\u0142aw Kwiecie\u0144", "Artur Bartczak", "Izabela Wasiak", "Mateusz Maniewski"], "title": "Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies", "comment": null, "summary": "We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary\nclassification of normal mitotic figures (NMFs) versus atypical mitotic figures\n(AMFs). The approach leverages pathology-specific foundation model H-optimus-0,\nselected based on recent cross-domain generalization benchmarks and our\nempirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp\naugmentation. Implementation includes soft labels based on multi-expert\nconsensus, hard negative mining, and adaptive focal loss, metric learning and\ndomain adaptation. The method demonstrates both the promise and challenges of\napplying foundation models to this complex classification task, achieving\nreasonable performance in the preliminary evaluation phase.", "AI": {"tldr": "\u57fa\u4e8eH-optimus-0\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528LoRA\u5fae\u8c03\u548cMixUp\u589e\u5f3a\u7b49\u65b9\u6cd5\u89e3\u51b3\u6b63\u5e38\u4e0e\u5f02\u5e38\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u7684\u4e8c\u5206\u7c7b\u95ee\u9898", "motivation": "\u89e3\u51b3MIDOG 2025\u6311\u6218\u8d5b\u7b2c\u4e8c\u8d5b\u9053\u4e2d\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\u56fe\u50cf(NMFs)\u4e0e\u5f02\u5e38\u6709\u4e1d\u5206\u88c2\u56fe\u50cf(AMFs)\u7684\u590d\u6742\u4e8c\u5206\u7c7b\u95ee\u9898", "method": "\u4f7f\u7528H-optimus-0\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408LoRA\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u3001MixUp\u6570\u636e\u589e\u5f3a\u3001\u57fa\u4e8e\u591a\u4e13\u5bb6\u5171\u8bc6\u7684\u8f6f\u6807\u7b7e\u3001\u96be\u8d1f\u6837\u672c\u6316\u6398\u3001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u3001\u5ea6\u91cf\u5b66\u4e60\u548c\u57df\u9002\u5e94\u6280\u672f", "result": "\u5728\u521d\u6b65\u8bc4\u4f30\u9636\u6bb5\u53d6\u5f97\u4e86\u5408\u7406\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8fd9\u4e00\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u524d\u666f\u548c\u6311\u6218"}}
{"id": "2509.03111", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03111", "abs": "https://arxiv.org/abs/2509.03111", "authors": ["Hao Yang", "Guang Ouyang"], "title": "Handwriting Imagery EEG Classification based on Convolutional Neural Networks", "comment": null, "summary": "Handwriting imagery has emerged as a promising paradigm for brain-computer\ninterfaces (BCIs) aimed at translating brain activity into text output.\nCompared with invasively recorded electroencephalography (EEG), non-invasive\nrecording offers a more practical and feasible approach to capturing brain\nsignals for BCI. This study explores the limit of decoding non-invasive EEG\nassociated with handwriting imagery into English letters using deep neural\nnetworks. To this end, five participants were instructed to imagine writing the\n26 English letters with their EEG being recorded from the scalp. A measurement\nof EEG similarity across letters was conducted to investigate letter-specific\npatterns in the dataset. Subsequently, four convolutional neural network (CNN)\nmodels were trained for EEG classification. Descriptively, the EEG data clearly\nexhibited letter-specific patterns serving as a proof-of-concept for\nEEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN\nclassifiers trained on each participant reached the highest limit of around\n20%. This study marks the first attempt to decode non-invasive EEG associated\nwith handwriting imagery. Although the achieved accuracy is not sufficient for\na usable brain-to-text BCI, the model's performance is noteworthy in revealing\nthe potential for translating non-invasively recorded brain signals into text\noutputs and establishing a baseline for future research.", "AI": {"tldr": "\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5934\u76ae\u7535\u8111\u7535\u4f4d\u56fe\u50cf\u624b\u5199\u60f3\u8c61\u7684\u975e\u4fb5\u5165\u6027EEG\u4fe1\u53f7\uff0c\u8bc6\u522b26\u4e2a\u82f1\u6587\u5b57\u6bcd\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe20%\uff0c\u4e3a\u8111-\u6587\u672cBCI\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u7ebf", "motivation": "\u624b\u5199\u60f3\u8c61\u4f5c\u4e3a\u8111-\u673a\u63a5\u53e3\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u975e\u4fb5\u5165\u6027EEG\u8bb0\u5f55\u6bd4\u4fb5\u5165\u6027\u65b9\u6cd5\u66f4\u5b9e\u7528\u53ef\u884c\uff0c\u9700\u8981\u63a2\u7d22\u5176\u5728\u6587\u672c\u8f93\u51fa\u65b9\u9762\u7684\u89e3\u7801\u6781\u9650", "method": "\u4f7f\u75285\u540d\u53c2\u4e0e\u8005\u60f3\u8c61\u624b\u519926\u4e2a\u82f1\u6587\u5b57\u6bcd\uff0c\u8bb0\u5f55\u5934\u76aeEEG\u4fe1\u53f7\uff0c\u8fdb\u884c\u5b57\u6bcd\u95f4EEG\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u7136\u540e\u8bad\u7ec3\u56db\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u5206\u7c7b", "result": "EEG\u6570\u636e\u663e\u793a\u51fa\u660e\u663e\u7684\u5b57\u6bcd\u7279\u5f02\u6a21\u5f0f\uff0cCNN\u5206\u7c7b\u5668\u5728\u968f\u673a\u6c34\u5e733.85%\u7684\u57fa\u7840\u4e0a\u8fbe\u5230\u7ea620%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e86\u5c06\u975e\u4fb5\u5165\u6027\u8111\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6587\u672c\u8f93\u51fa\u7684\u6f5c\u529b", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u89e3\u7801\u624b\u5199\u60f3\u8c61\u76f8\u5173\u7684\u975e\u4fb5\u5165\u6027EEG\uff0c\u867d\u7136\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u652f\u6491\u53ef\u7528\u7684\u8111-\u6587\u672cBCI\uff0c\u4f46\u6a21\u578b\u8868\u73b0\u4e3a\u672a\u6765\u7814\u7a76\u786e\u7acb\u4e86\u57fa\u51c6\u7ebf\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u5411\u7684\u7814\u7a76\u6f5c\u529b"}}
{"id": "2509.02602", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.02602", "abs": "https://arxiv.org/abs/2509.02602", "authors": ["Moona Mazher", "Steven A Niederer", "Abdul Qayyum"], "title": "Masked Autoencoder Pretraining and BiXLSTM ResNet Architecture for PET/CT Tumor Segmentation", "comment": null, "summary": "The accurate segmentation of lesions in whole-body PET/CT imaging is\nes-sential for tumor characterization, treatment planning, and response\nassess-ment, yet current manual workflows are labor-intensive and prone to\ninter-observer variability. Automated deep learning methods have shown promise\nbut often remain limited by modality specificity, isolated time points, or\nin-sufficient integration of expert knowledge. To address these challenges, we\npresent a two-stage lesion segmentation framework developed for the fourth\nAutoPET Challenge. In the first stage, a Masked Autoencoder (MAE) is em-ployed\nfor self-supervised pretraining on unlabeled PET/CT and longitudinal CT scans,\nenabling the extraction of robust modality-specific representations without\nmanual annotations. In the second stage, the pretrained encoder is fine-tuned\nwith a bidirectional XLSTM architecture augmented with ResNet blocks and a\nconvolutional decoder. By jointly leveraging anatomical (CT) and functional\n(PET) information as complementary input channels, the model achieves improved\ntemporal and spatial feature integration. Evalua-tion on the AutoPET Task 1\ndataset demonstrates that self-supervised pre-training significantly enhances\nsegmentation accuracy, achieving a Dice score of 0.582 compared to 0.543\nwithout pretraining. These findings high-light the potential of combining\nself-supervised learning with multimodal fu-sion for robust and generalizable\nPET/CT lesion segmentation. Code will be available at\nhttps://github.com/RespectKnowledge/AutoPet_2025_BxLSTM_UNET_Segmentation", "AI": {"tldr": "\u57fa\u4e8eMAE\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u53cc\u5411XLSTM\u7ed3\u6784\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7PET/CT\u591a\u6a21\u6001\u878d\u5408\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u5168\u8eab\u75c5\u53d8\u5206\u5272", "motivation": "\u89e3\u51b3\u5168\u8eabPET/CT\u75c5\u53d8\u5206\u5272\u4e2d\u624b\u5de5\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\u3001\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u6a21\u6001\u7279\u5f02\u6027\u3001\u65f6\u95f4\u70b9\u9694\u79bb\u548c\u4e13\u5bb6\u77e5\u8bc6\u6574\u5408\u4e0d\u8db3\u7b49\u9650\u5236", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Masked Autoencoder(MAE)\u5728\u65e0\u6807\u6ce8PET/CT\u548c\u7eb5\u5411CT\u626b\u63cf\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u53cc\u5411XLSTM\u7ed3\u6784\u589e\u5f3aResNet\u5757\u548c\u5377\u79ef\u89e3\u7801\u5668\u5bf9\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u8fdb\u884c\u5fae\u8c03", "result": "\u5728AutoPET Task 1\u6570\u636e\u96c6\u4e0a\uff0c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\uff0cDice\u5206\u6570\u4ece0.543\u63d0\u5347\u52300.582", "conclusion": "\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u878d\u5408\u7684\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u4e14\u53ef\u63a8\u5e7f\u7684PET/CT\u75c5\u53d8\u5206\u5272"}}
{"id": "2509.03193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03193", "abs": "https://arxiv.org/abs/2509.03193", "authors": ["Maximilian Neidhardt", "Sarah Latus", "Tim Eixmann", "Gereon H\u00fcttmann", "Alexander Schlaefer"], "title": "Deep Learning for High Speed Optical Coherence Elastography with a Fiber Scanning Endoscope", "comment": null, "summary": "Tissue stiffness is related to soft tissue pathologies and can be assessed\nthrough palpation or via clinical imaging systems, e.g., ultrasound or magnetic\nresonance imaging. Typically, the image based approaches are not suitable\nduring interventions, particularly for minimally invasive surgery. To this end,\nwe present a miniaturized fiber scanning endoscope for fast and localized\nelastography. Moreover, we propose a deep learning based signal processing\npipeline to account for the intricate data and the need for real-time\nestimates. Our elasticity estimation approach is based on imaging complex and\ndiffuse wave fields that encompass multiple wave frequencies and propagate in\nvarious directions. We optimize the probe design to enable different scan\npatterns. To maximize temporal sampling while maintaining three-dimensional\ninformation we define a scan pattern in a conical shape with a temporal\nfrequency of 5.05 kHz. To efficiently process the image sequences of complex\nwave fields we consider a spatio-temporal deep learning network. We train the\nnetwork in an end-to-end fashion on measurements from phantoms representing\nmultiple elasticities. The network is used to obtain localized and robust\nelasticity estimates, allowing to create elasticity maps in real-time. For 2D\nscanning, our approach results in a mean absolute error of 6.31+-5.76 kPa\ncompared to 11.33+-12.78 kPa for conventional phase tracking. For scanning\nwithout estimating the wave direction, the novel 3D method reduces the error to\n4.48+-3.63 kPa compared to 19.75+-21.82 kPa for the conventional 2D method.\nFinally, we demonstrate feasibility of elasticity estimates in ex-vivo porcine\ntissue.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5c0f\u578b\u5316\u5149\u7ea4\u626b\u63cf\u5185\u955c\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5fae\u4f30\u624b\u672f\u4e2d\u5b9e\u65f6\u8bc4\u4f30\u7ec6\u80f6\u7ec6\u80f6\u7f29\u786c\u5ea6\uff0c\u63d0\u9ad8\u4e86\u7cbe\u786e\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u57fa\u4e8e\u7ec6\u80f6\u7f29\u786c\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5728\u5fae\u4f30\u624b\u672f\u4e2d\u4e0d\u9002\u7528\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4ecb\u5165\u6027\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u5feb\u901f\u3001\u5c40\u90e8\u5316\u786c\u5ea6\u6d4b\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u5c0f\u578b\u5316\u5149\u7ea4\u626b\u63cf\u5185\u955c\uff0c\u91c7\u7528\u9525\u5f62\u626b\u63cf\u6a21\u5f0f\uff085.05 kHz\uff09\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4-\u65f6\u95f4\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5904\u7406\u590d\u6742\u6ce2\u573a\u56fe\u50cf\u5e8f\u5217\uff0c\u5728\u6a21\u62df\u4e0d\u540c\u5f39\u6027\u7684\u5e7b\u5f71\u6837\u54c1\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u57282D\u626b\u63cf\u4e2d\uff0c\u65b0\u65b9\u6cd5\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a6.31\u00b15.76 kPa\uff0c\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u76f8\u4f4d\u8ddf\u8e2a\u65b9\u6cd5\u768411.33\u00b112.78 kPa\u3002\u57283D\u626b\u63cf\u4e2d\uff0c\u8bef\u5dee\u964d\u4f4e\u52304.48\u00b13.63 kPa\uff0c\u800c\u4f20\u7edf2D\u65b9\u6cd5\u4e3a19.75\u00b121.82 kPa\u3002\u8fd8\u5728\u732a\u7ec4\u7ec7\u4e2d\u8bc1\u660e\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u6d41\u6c34\u7ebf\u548c\u5c0f\u578b\u5316\u5185\u955c\u7cfb\u7edf\u80fd\u591f\u5728\u5fae\u4f30\u624b\u672f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u7ec6\u80f6\u7f29\u786c\u5ea6\u8bc4\u4f30\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u6668\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.02607", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02607", "abs": "https://arxiv.org/abs/2509.02607", "authors": ["Nisanth Kumar Panneerselvam", "Guneet Mummaneni", "Emilie Roncali"], "title": "Towards Digital Twins for Optimal Radioembolization", "comment": null, "summary": "Radioembolization is a localized liver cancer treatment that delivers\nradioactive microspheres (30 micron) to tumors via a catheter inserted in the\nhepatic arterial tree. The goal is to maximize therapeutic efficacy while\nminimizing damage to healthy liver tissue. However, optimization is challenging\ndue to complex hepatic artery anatomy, variable blood flow, and uncertainty in\nmicrosphere transport. The creation of dynamic, patient-specific digital twins\nmay provide a transformative solution to these challenges. This work outlines a\nframework for a liver radioembolization digital twin using high-fidelity\ncomputational fluid dynamics (CFD) and/or recent physics-informed machine\nlearning approaches. The CFD approach involves microsphere transport\ncalculations in the hepatic arterial tree with individual patient data, which\nenables personalized treatment planning. Although accurate, traditional CFD is\ncomputationally expensive and limits clinical applicability.\n  To accelerate simulations, physics-informed neural networks (PINNs) and their\ngenerative extensions play an increasingly important role. PINNs integrate\ngoverning equations, such as the Navier-Stokes equations, directly into the\nneural network training process, enabling mesh-free, data-efficient\napproximation of blood flow and microsphere transport. Physics-informed\ngenerative adversarial networks (PI-GANs), diffusion models (PI-DMs), and\ntransformer-based architectures further enable uncertainty-aware, temporally\nresolved predictions with reduced computational cost. These AI surrogates not\nonly maintain physical fidelity but also support rapid sampling of diverse flow\nscenarios, facilitating real-time decision support.\n  Together, CFD and physics-informed AI methods form the foundation of dynamic,\npatient-specific digital twin to optimize radioembolization planning and\nultimately improve clinical outcomes.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\u548c\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u7684\u52a8\u6001\u6570\u5b57\u53cc\u80ce\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5e72\u813e\u7535\u5c04\u585e\u6cbb\u7597\u7684\u4e2a\u4eba\u5316\u8ba1\u5212\u3002", "motivation": "\u5e72\u813e\u7535\u5c04\u585e\u6cbb\u7597\u9762\u4e34\u590d\u6742\u7684\u809a\u52a8\u8109\u89e3\u5256\u7ed3\u6784\u3001\u53d8\u5316\u7684\u8840\u6d41\u548c\u5fae\u7403\u8fd0\u8f93\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5927\u5316\u6cbb\u7597\u6548\u679c\u5e76\u6700\u5c0f\u5316\u5065\u5eb7\u7ec4\u7ec7\u635f\u4f24\u7684\u4e2a\u4eba\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9ad8\u4fdd\u771f\u5ea6\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66(CFD)\u548c\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5(PINNs\u3001PI-GANs\u3001PI-DMs\u7b49)\u76f8\u7ed3\u5408\u7684\u65b9\u6848\u3002CFD\u8fdb\u884c\u5fae\u7403\u8fd0\u8f93\u8ba1\u7b97\uff0c\u800cAI\u6a21\u578b\u901a\u8fc7\u5c06\u63a7\u5236\u65b9\u7a0b\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\uff0c\u5b9e\u73b0\u7f51\u683c\u514d\u9700\u7684\u9ad8\u6548\u4eff\u771f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u6301\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5927\u5927\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u5feb\u901f\u91d1\u77f3\u591a\u79cd\u6d41\u52a8\u573a\u666f\u7684\u91c7\u6837\uff0c\u4e3a\u5b9e\u65f6\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "CFD\u4e0e\u7269\u7406\u4fe1\u606fAI\u65b9\u6cd5\u7684\u7ed3\u5408\u6784\u6210\u4e86\u52a8\u6001\u6570\u5b57\u53cc\u80ce\u7684\u57fa\u7840\uff0c\u6709\u671b\u4f18\u5316\u5e72\u813e\u7535\u5c04\u585e\u6cbb\u7597\u7684\u4e2a\u4eba\u5316\u8ba1\u5212\uff0c\u6700\u7ec8\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u3002"}}
{"id": "2509.03273", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03273", "abs": "https://arxiv.org/abs/2509.03273", "authors": ["Zeyuan Zhang", "Yue Xiu", "Zheng Dong", "Jiacheng Yin", "Maurice J. Khabbaz", "Chadi Assi", "Ning Wei"], "title": "Crosstalk-Resilient Beamforming for Movable Antenna Enabled Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a movable antenna (MA) enabled integrated sensing and\ncommunication (ISAC) system under the influence of antenna crosstalk. First, it\ngeneralizes the antenna crosstalk model from the conventional fixed-position\nantenna (FPA) system to the MA scenario. Then, a Cramer-Rao bound (CRB)\nminimization problem driven by joint beamforming and antenna position design is\npresented. Specifically, to address this highly non-convex flexible beamforming\nproblem, we deploy a deep reinforcement learning (DRL) approach to train a\nflexible beamforming agent. To ensure stability during training, a Twin Delayed\nDeep Deterministic Policy Gradient (TD3) algorithm is adopted to balance\nexploration with reward maximization for efficient and reliable learning.\nNumerical results demonstrate that the proposed crosstalk-resilient (CR)\nalgorithm enhances the overall ISAC performance compared to other benchmark\nschemes.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5929\u7ebf\u4ea4\u8c61\u5f71\u54cd\u4e0b\u7684\u53ef\u79fb\u52a8\u5929\u7ebf\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u805a\u7126\u548c\u5929\u7ebf\u4f4d\u7f6e\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u7684\u4ea4\u8c61\u5f39\u6027\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u7cfb\u7edf\u4e2d\u7684\u5929\u7ebf\u4ea4\u8c61\u95ee\u9898\u5728\u53ef\u79fb\u52a8\u5929\u7ebf\u573a\u666f\u4e0b\u9700\u8981\u91cd\u65b0\u5efa\u6a21\uff0c\u4ee5\u4f18\u5316\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528Twin Delayed Deep Deterministic Policy Gradient (TD3)\u7b97\u6cd5\u8bad\u7ec3\u7075\u6d3b\u805a\u7126\u7ee7\u7535\u5668\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5956\u52b1\u6700\u5927\u5316\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u4ea4\u8c61\u5f39\u6027\u7b97\u6cd5\u5728\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u5176\u4ed6\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u8c61\u5f71\u54cd\u51cf\u7f13\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2509.02612", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02612", "abs": "https://arxiv.org/abs/2509.02612", "authors": ["Leire Benito-Del-Valle", "Pedro A. Moreno-S\u00e1nchez", "Itziar Egusquiza", "Itsaso Vitoria", "Artzai Pic\u00f3n", "Cristina L\u00f3pez-Saratxaga", "Adrian Galdran"], "title": "Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition", "comment": "version 0, to be updated; submitted to midog 2025", "summary": "The MIDOG 2025 challenge extends prior work on mitotic figure detection by\nintroducing a new Track 2 on atypical mitosis classification. This task aims to\ndistinguish normal from atypical mitotic figures in histopathology images, a\nclinically relevant but highly imbalanced and cross-domain problem. We\ninvestigated two complementary backbones: (i) ConvNeXt-Small, pretrained on\nImageNet, and (ii) a histopathology-specific ViT from Lunit trained via\nself-supervision. To address the strong prevalence imbalance (9408 normal vs.\n1741 atypical), we synthesized additional atypical examples to approximate\nclass balance and compared models trained with real-only vs. real+synthetic\ndata. Using five-fold cross-validation, both backbones reached strong\nperformance (mean AUROC approximately 95 percent), with ConvNeXt achieving\nslightly higher peaks while Lunit exhibited greater fold-to-fold stability.\nSynthetic balancing, however, did not lead to consistent improvements. On the\norganizers' preliminary hidden test set, explicitly designed as an\nout-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4\npercent), whereas Lunit remained competitive on balanced accuracy. These\nfindings suggest that both ImageNet and domain-pretrained backbones are viable\nfor atypical mitosis classification, with domain-pretraining conferring\nrobustness and ImageNet pretraining reaching higher peaks, while naive\nsynthetic balancing has limited benefit. Full hidden test set results will be\nreported upon challenge completion.", "AI": {"tldr": "MIDOG 2025\u6311\u6218\u8d5b\u65b0\u589e\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4efb\u52a1\uff0c\u7814\u7a76\u6bd4\u8f83\u4e86ImageNet\u9884\u8bad\u7ec3\u7684ConvNeXt\u548c\u75c5\u7406\u5b66\u81ea\u76d1\u7763ViT\u6a21\u578b\uff0c\u4e24\u8005\u5728AUROC\u7ea695%\u8868\u73b0\u4f18\u5f02\uff0c\u5408\u6210\u6570\u636e\u5e73\u8861\u6548\u679c\u6709\u9650\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u6b63\u5e38\u4e0e\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u7ec6\u80de\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e34\u5e8a\u76f8\u5173\u4f46\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u8de8\u57df\u6311\u6218\u3002", "method": "\u4f7f\u7528ConvNeXt-Small\uff08ImageNet\u9884\u8bad\u7ec3\uff09\u548cLunit ViT\uff08\u75c5\u7406\u5b66\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff09\u4e24\u79cd\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6bd4\u8f83\u6027\u80fd\uff0c\u5e76\u5c1d\u8bd5\u5408\u6210\u6570\u636e\u5e73\u8861\u7c7b\u522b\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5747\u8fbe\u5230\u7ea695%\u7684\u5e73\u5747AUROC\uff0cConvNeXt\u5cf0\u503c\u7565\u9ad8\u4f46Lunit\u66f4\u7a33\u5b9a\u3002\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0aConvNeXt\u8fbe\u523095.4% AUROC\uff0c\u5408\u6210\u5e73\u8861\u672a\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "ImageNet\u548c\u9886\u57df\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u90fd\u9002\u7528\u4e8e\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff0c\u9886\u57df\u9884\u8bad\u7ec3\u63d0\u4f9b\u9c81\u68d2\u6027\u800cImageNet\u9884\u8bad\u7ec3\u80fd\u8fbe\u5230\u66f4\u9ad8\u5cf0\u503c\uff0c\u7b80\u5355\u7684\u5408\u6210\u5e73\u8861\u65b9\u6cd5\u6548\u76ca\u6709\u9650\u3002"}}
{"id": "2509.02627", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02627", "abs": "https://arxiv.org/abs/2509.02627", "authors": ["Jie Xiao", "Mengye Lyu", "Shaojun Liu"], "title": "A Two-Stage Strategy for Mitosis Detection Using Improved YOLO11x Proposals and ConvNeXt Classification", "comment": null, "summary": "MIDOG 2025 Track 1 requires mitosis detection in whole-slide images (WSIs)\ncontaining non-tumor, inflamed, and necrotic regions. Due to the complicated\nand heterogeneous context, as well as possible artifacts, there are often false\npositives and false negatives, thus degrading the detection F1-score. To\naddress this problem, we propose a two-stage framework. Firstly, an improved\nYOLO11x, integrated with EMA attention and LSConv, is employed to generate\nmitosis candidates. We use a low confidence threshold to generate as many\nproposals as possible, ensuring the detection recall. Then, a ConvNeXt-Tiny\nclassifier is employed to filter out the false positives, ensuring the\ndetection precision. Consequently, the proposed two-stage framework can\ngenerate a high detection F1-score. Evaluated on a fused dataset comprising\nMIDOG++, MITOS_WSI_CCMCT, and MITOS_WSI_CMC, our framework achieves an F1-score\nof 0.882, which is 0.035 higher than the single-stage YOLO11x baseline. This\nperformance gain is produced by a significant precision improvement, from 0.762\nto 0.839, and a comparable recall. The code is available at\nhttps://github.com/xxiao0304/MIDOG-2025-Track-1-of-SZTU.", "AI": {"tldr": "\u901a\u8fc7\u4e24\u6b65\u6846\u67b6\uff08YOLO11x\u6536\u96c6\u5206\u88c2\u5012\u4f53\u5019\u9009 + ConvNeXt-Tiny\u5206\u7c7b\u5668\u8fc7\u6ee4\uff09\u63d0\u9ad8\u5168\u7247\u56fe\u8c31\u5206\u88c2\u68c0\u6d4b\u7684F1\u5206\u6570\uff0c\u5728\u590d\u6742\u75c5\u7406\u73af\u5883\u4e2d\u51cf\u5c11\u5047\u963b\u6027\u548c\u5047\u963b\u6027", "motivation": "\u5168\u7247\u56fe\u8c31\u5206\u88c2\u68c0\u6d4b\u9047\u5230\u975e\u809d\u80bf\u3001\u708e\u75c7\u3001\u574f\u6b7b\u533a\u57df\u7b49\u590d\u6742\u73af\u5883\u5bfc\u81f4\u5047\u963b\u6027\u548c\u5047\u963b\u6027\u589e\u591a\uff0c\u5f71\u54cdF1\u5206\u6570", "method": "1. \u6539\u8fdbYOLO11x\uff08\u96c6\u6210EMA\u6ce8\u610f\u529b\u548cLSConv\uff09\u4f7f\u7528\u4f4e\u4fe1\u5fc3\u9608\u503c\u751f\u6210\u5927\u91cf\u5206\u88c2\u5012\u4f53\u5019\u9009\uff0c\u4fdd\u8bc1\u68c0\u51fa\u7387\n2. ConvNeXt-Tiny\u5206\u7c7b\u5668\u8fc7\u6ee4\u5047\u963b\u6027\uff0c\u63d0\u9ad8\u7cbe\u786e\u5ea6", "result": "\u5728MIDOG++\u3001MITOS_WSI_CCMCT\u548cMITOS_WSI_CMC\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.882\u7684F1\u5206\u6570\uff0c\u6bd4\u5355\u6b65YOLO11x\u57fa\u7ebf\u63d0\u9ad80.035\uff0c\u7cbe\u786e\u5ea6\u4ece0.762\u63d0\u5347\u52300.839\uff0c\u68c0\u51fa\u7387\u4fdd\u6301\u76f8\u4f3c", "conclusion": "\u4e24\u6b65\u6846\u67b6\u901a\u8fc7\u5148\u6536\u96c6\u540e\u8fc7\u6ee4\u7684\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u68c0\u6d4b\u7684\u68c0\u51fa\u7387\u548c\u7cbe\u786e\u5ea6\uff0c\u5728\u590d\u6742\u75c5\u7406\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5206\u88c2\u5012\u4f53\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2509.02630", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02630", "abs": "https://arxiv.org/abs/2509.02630", "authors": ["Euiseop Song", "Jaeyoung Park", "Jaewoo Park"], "title": "Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection", "comment": null, "summary": "Mitotic figure detection remains a challenging task in computational\npathology due to domain variability and morphological complexity. This paper\ndescribes our participation in the MIDOG 2025 challenge, focusing on robust\nmitotic figure detection across diverse tissue domains. We developed a\ntwo-stage pipeline combining Faster R-CNN for candidate detection with an\nensemble of three classifiers (DenseNet-121, EfficientNet-v2,\nInceptionResNet-v2) for false positive reduction. Our best submission achieved\nF1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN\ntrained solely on MIDOG++ dataset. While our high recall demonstrates effective\nmitotic figure detection, the critically low precision (12.67%) reveals\nfundamental challenges in distinguishing true mitoses from morphologically\nsimilar imposters across diverse domains. Analysis of six submission variants\nshowed that subsequent optimization attempts were counterproductive,\nhighlighting the omplexity of domain generalization in histopathology. This\nwork provides valuable insights into the practical challenges of developing\nrobust mitotic figure detection algorithms and emphasizes the importance of\neffective false positive suppression strategies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u53c2\u4e0eMIDOG 2025\u6311\u6218\u8d5b\u7684\u4e24\u9636\u6bb5\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528Faster R-CNN\u8fdb\u884c\u5019\u9009\u68c0\u6d4b\u548c\u96c6\u6210\u5206\u7c7b\u5668\u8fdb\u884c\u5047\u9633\u6027\u6291\u5236\uff0c\u83b7\u5f97\u4e86\u9ad8\u53ec\u56de\u7387\u4f46\u4f4e\u7cbe\u5ea6\u7684\u7ed3\u679c", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u6709\u4e1d\u5206\u88c2\u56fe\u5f62\u68c0\u6d4b\u9762\u4e34\u7684\u9886\u57df\u53d8\u5f02\u6027\u548c\u5f62\u6001\u590d\u6742\u6027\u6311\u6218\uff0c\u5f00\u53d1\u8de8\u4e0d\u540c\u7ec4\u7ec7\u9886\u57df\u7684\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5", "method": "\u4e24\u9636\u6bb5\u7ba1\u9053\uff1aFaster R-CNN\u7528\u4e8e\u5019\u9009\u68c0\u6d4b\uff0cDenseNet-121\u3001EfficientNet-v2\u3001InceptionResNet-v2\u4e09\u79cd\u5206\u7c7b\u5668\u96c6\u6210\u8fdb\u884c\u5047\u9633\u6027\u51cf\u5c11", "result": "\u6700\u4f73\u63d0\u4ea4\u83b7\u5f97F1-score 0.2237\uff08\u53ec\u56de\u73870.9528\uff0c\u7cbe\u786e\u73870.1267\uff09\uff0c\u9ad8\u53ec\u56de\u7387\u663e\u793a\u6709\u6548\u68c0\u6d4b\u4f46\u6781\u4f4e\u7cbe\u786e\u7387\u63ed\u793a\u8de8\u9886\u57df\u533a\u5206\u771f\u4f2a\u6709\u4e1d\u5206\u88c2\u7684\u6839\u672c\u6311\u6218", "conclusion": "\u540e\u7eed\u4f18\u5316\u5c1d\u8bd5\u9002\u5f97\u5176\u53cd\uff0c\u7a81\u663e\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u9886\u57df\u6cdb\u5316\u7684\u590d\u6742\u6027\uff0c\u5f3a\u8c03\u4e86\u6709\u6548\u5047\u9633\u6027\u6291\u5236\u7b56\u7565\u7684\u91cd\u8981\u6027"}}
{"id": "2509.03488", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03488", "abs": "https://arxiv.org/abs/2509.03488", "authors": ["Miguel Rivas-Costa", "Carlos Mosquera"], "title": "Efficient DoA Estimation with Hybrid Linear and Rectangular Arrays Using Compact DFT Codebook", "comment": null, "summary": "Hybrid Analog and Digital (HAD) architectures provide a cost-effective\nalternative for large-scale antenna arrays, but accurate Direction-of-Arrival\n(DoA) estimation remains challenging due to limited digital dimensionality and\nconstrained beamforming design. In this work, we propose a HAD architecture\nthat employs Butler matrices to synthesize DFT beams over a uniform linear\narray. By exploiting the Cauchy-like displacement structure of the beamformed\nsignal, we introduce a second-order statistics estimation algorithm that\nachieves near-optimal accuracy, approaching the Cram\\'er-Rao Lower Bound (CRLB)\nand outperforming state-of-the-art methods in simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Butler\u77e9\u9635\u7684\u6df7\u5408\u6a21\u62df\u6570\u5b57\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u6ce2\u675f\u6210\u5f62\u4fe1\u53f7\u7684\u67ef\u897f\u7c7b\u4f4d\u79fb\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1CRLB\u4e0b\u754c\u7684\u8fd1\u6700\u4f18DoA\u4f30\u8ba1\u7cbe\u5ea6", "motivation": "\u6df7\u5408\u6a21\u62df\u6570\u5b57\u67b6\u6784\u5728\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\uff0c\u4f46\u7531\u4e8e\u6709\u9650\u7684\u6570\u5b57\u7ef4\u5ea6\u548c\u53d7\u9650\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u51c6\u786e\u7684\u6ce2\u8fbe\u65b9\u5411\u4f30\u8ba1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u91c7\u7528Butler\u77e9\u9635\u5728\u5747\u5300\u7ebf\u6027\u9635\u5217\u4e0a\u5408\u6210DFT\u6ce2\u675f\uff0c\u5229\u7528\u6ce2\u675f\u6210\u5f62\u4fe1\u53f7\u7684\u67ef\u897f\u7c7b\u4f4d\u79fb\u7ed3\u6784\uff0c\u63d0\u51fa\u4e8c\u9636\u7edf\u8ba1\u91cf\u4f30\u8ba1\u7b97\u6cd5", "result": "\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c(CRLB)\u7684\u8fd1\u6700\u4f18\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u51fa\u7684HAD\u67b6\u6784\u548c\u4f30\u8ba1\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u4e2d\u7684DoA\u4f30\u8ba1\u6311\u6218\uff0c\u8fbe\u5230\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u6027\u80fd"}}
{"id": "2509.02637", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02637", "abs": "https://arxiv.org/abs/2509.02637", "authors": ["Yasemin Topuz", "M. Taha G\u00f6kcan", "Serdar Y\u0131ld\u0131z", "Song\u00fcl Varl\u0131"], "title": "A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection", "comment": null, "summary": "Mitotic figure detection is a crucial task in computational pathology, as\nmitotic activity serves as a strong prognostic marker for tumor aggressiveness.\nHowever, domain variability that arises from differences in scanners, tissue\ntypes, and staining protocols poses a major challenge to the robustness of\nautomated detection methods. In this study, we introduce SDF-YOLO (Single\nDetect Focused YOLO), a lightweight yet domain-robust detection framework\ndesigned specifically for small, rare targets such as mitotic figures. The\nmodel builds on YOLOv11 with task-specific modifications, including a single\ndetection head aligned with mitotic figure scale, coordinate attention to\nenhance positional sensitivity, and improved cross-channel feature mixing.\nExperiments were conducted on three datasets that span human and canine tumors:\nMIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary\ncarcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025\nchallenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a\nprecision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of\n5.793, demonstrating both competitive accuracy and computational efficiency.\nThese results indicate that SDF-YOLO provides a reliable and efficient\nframework for robust mitotic figure detection across diverse domains.", "AI": {"tldr": "SDF-YOLO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u5177\u6709\u9886\u57df\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u5c0f\u4e14\u7f55\u89c1\u7684\u76ee\u6807\u5982\u6838\u5206\u88c2\u8c61\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6838\u5206\u88c2\u8c61\u68c0\u6d4b\u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u626b\u63cf\u4eea\u3001\u7ec4\u7ec7\u7c7b\u578b\u548c\u67d3\u8272\u65b9\u6848\u7684\u5dee\u5f02\u5bfc\u81f4\u7684\u9886\u57df\u53d8\u5f02\u6027\u5bf9\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "\u57fa\u4e8eYOLOv11\u6784\u5efa\uff0c\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u4fee\u6539\uff1a\u5355\u68c0\u6d4b\u5934\u4e0e\u6838\u5206\u88c2\u8c61\u5c3a\u5ea6\u5bf9\u9f50\u3001\u5750\u6807\u6ce8\u610f\u529b\u589e\u5f3a\u4f4d\u7f6e\u654f\u611f\u6027\u3001\u6539\u8fdb\u7684\u8de8\u901a\u9053\u7279\u5f81\u6df7\u5408\u3002", "result": "\u5728MIDOG2025\u6311\u6218\u8d5b\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97AP 0.799\uff0c\u7cbe\u786e\u73870.758\uff0c\u53ec\u56de\u73870.775\uff0cF1\u5206\u65700.766\uff0cFROC-AUC 5.793\uff0c\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SDF-YOLO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u6838\u5206\u88c2\u8c61\u68c0\u6d4b\u3002"}}
{"id": "2509.02640", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02640", "abs": "https://arxiv.org/abs/2509.02640", "authors": ["Biwen Meng", "Xi Long", "Jingxin Liu"], "title": "Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge", "comment": null, "summary": "Atypical mitotic figures (AMFs) are clinically relevant indicators of\nabnormal cell division, yet their reliable detection remains challenging due to\nmorphological ambiguity and scanner variability. In this work, we investigated\nthree variants of adapting the pathology foundation model UNI2-h for the\nMIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that\nvisual prompt tuning (VPT) substantially improved generalization, and that\nfurther integrating test-time augmentation (TTA) with Vahadane and Macenko\nstain normalization provided the best robustness. Our final submission achieved\na balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary\nleaderboard, ranking within the top 10 teams. These results demonstrate that\nprompt-based adaptation combined with stain-normalization TTA offers an\neffective strategy for atypical mitosis classification under diverse imaging\nconditions.", "AI": {"tldr": "\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u8c03\u6574(VPT)\u5484\u67d3\u8272\u6807\u51c6\u5316\u6d4b\u8bd5\u65f6\u589e\u5f3a(TTA)\u7b49\u6280\u672f\uff0c\u57fa\u4e8eUNI2-h\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u5728MIDOG2025\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u7684\u975e\u5178\u578b\u6709\u4e1d\u5207\u88c2\u68c0\u6d4b", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5207\u88c2(AMFs)\u4f5c\u4e3a\u5f02\u5e38\u7ec6\u80de\u5206\u88c2\u7684\u4e34\u5e8a\u6307\u6807\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u56e0\u5f62\u6001\u5a92\u7cca\u6027\u5484\u626b\u63cf\u4eea\u53d8\u5f02\u6027\u5bfc\u81f4\u68c0\u6d4b\u56f0\u96be", "method": "\u7814\u7a76\u4e86\u4e09\u79cdUNI2-h\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u9002\u914d\u65b9\u6848\uff1aLoRA\u57fa\u51c6\u7ebf\u3001\u89c6\u89c9\u63d0\u793a\u8c03\u6574(VPT)\u3001\u4ee5\u53caVPT\u7ed3\u5408Vahadane\u5484Macenko\u67d3\u8272\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a(TTA)", "result": "\u6700\u7ec8\u63d0\u4ea4\u5728\u9884\u6d4b\u6392\u884c\u699c\u4e0a\u83b7\u5f970.8837\u7684\u5e73\u8861\u51c6\u786e\u7387\u54840.9513\u7684ROC-AUC\uff0c\u6392\u540d\u524d10\u56e2\u961f", "conclusion": "\u63d0\u793a\u57fa\u9002\u914d\u7ed3\u5408\u67d3\u8272\u6807\u51c6\u5316TTA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u5728\u591a\u6837\u5316\u6210\u50cf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u975e\u5178\u578b\u6709\u4e1d\u5207\u88c2\u5206\u7c7b"}}
{"id": "2509.02957", "categories": ["eess.IV", "cs.CV", "68T07", "I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.02957", "abs": "https://arxiv.org/abs/2509.02957", "authors": ["Navya Sri Kelam", "Akash Parekh", "Saikiran Bonthu", "Nitin Singhal"], "title": "Ensemble YOLO Framework for Multi-Domain Mitotic Figure Detection in Histopathology Images", "comment": "3pages, MIDOG25 Challenge", "summary": "Accurate detection of mitotic figures in whole slide histopathological images\nremains a challenging task due to their scarcity, morphological heterogeneity,\nand the variability introduced by tissue preparation and staining protocols.\nThe MIDOG competition series provides standardized benchmarks for evaluating\ndetection approaches across diverse domains, thus motivating the development of\ngeneralizable deep learning models. In this work, we investigate the\nperformance of two modern one-stage detectors, YOLOv5 and YOLOv8, trained on\nMIDOG++, CMC, and CCMCT datasets. To enhance robustness, training incorporated\nstain-invariant color perturbations and texture preserving augmentations. In\ninternal validation, YOLOv5 achieved superior precision, while YOLOv8 provided\nimproved recall, reflecting architectural trade-offs between anchor-based and\nanchor-free detection. To capitalize on these complementary strengths, we\nemployed an ensemble of the two models, which improved sensitivity without a\nmajor reduction in precision. These findings highlight the effectiveness of\nensemble strategies built upon contemporary object detectors to advance\nautomated mitosis detection in digital pathology.", "AI": {"tldr": "YOLOv5\u548cYOLOv8\u5728\u75c5\u7406\u56fe\u50cf\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u63d0\u5347\u68c0\u6d4b\u6548\u679c", "motivation": "\u89e3\u51b3\u5168\u73bb\u7247\u75c5\u7406\u56fe\u50cf\u4e2d\u6709\u4e1d\u5206\u88c2\u7ec6\u80de\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5305\u62ec\u7a00\u7f3a\u6027\u3001\u5f62\u6001\u5f02\u8d28\u6027\u548c\u67d3\u8272\u53d8\u5f02\u95ee\u9898\uff0c\u5229\u7528MIDOG\u7ade\u8d5b\u57fa\u51c6\u63a8\u52a8\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53d1\u5c55", "method": "\u4f7f\u7528YOLOv5\u548cYOLOv8\u4e24\u79cd\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u5728MIDOG++\u3001CMC\u548cCCMCT\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528\u67d3\u8272\u4e0d\u53d8\u6027\u989c\u8272\u6270\u52a8\u548c\u7eb9\u7406\u4fdd\u6301\u589e\u5f3a\u6280\u672f\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u6700\u540e\u96c6\u6210\u4e24\u79cd\u6a21\u578b", "result": "\u5185\u90e8\u9a8c\u8bc1\u4e2dYOLOv5\u7cbe\u5ea6\u66f4\u9ad8\uff0cYOLOv8\u53ec\u56de\u7387\u66f4\u597d\uff0c\u96c6\u6210\u6a21\u578b\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u7075\u654f\u5ea6", "conclusion": "\u57fa\u4e8e\u73b0\u4ee3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u96c6\u6210\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u6709\u4e1d\u5206\u88c2\u81ea\u52a8\u68c0\u6d4b\u7684\u6027\u80fd"}}
{"id": "2509.03173", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03173", "abs": "https://arxiv.org/abs/2509.03173", "authors": ["Mingfeng Lin"], "title": "Deep Self-knowledge Distillation: A hierarchical supervised learning for coronary artery segmentation", "comment": null, "summary": "Coronary artery disease is a leading cause of mortality, underscoring the\ncritical importance of precise diagnosis through X-ray angiography. Manual\ncoronary artery segmentation from these images is time-consuming and\ninefficient, prompting the development of automated models. However, existing\nmethods, whether rule-based or deep learning models, struggle with issues like\npoor performance and limited generalizability. Moreover, current knowledge\ndistillation methods applied in this field have not fully exploited the\nhierarchical knowledge of the model, leading to certain information waste and\ninsufficient enhancement of the model's performance capabilities for\nsegmentation tasks. To address these issues, this paper introduces Deep\nSelf-knowledge Distillation, a novel approach for coronary artery segmentation\nthat leverages hierarchical outputs for supervision. By combining Deep\nDistribution Loss and Pixel-wise Self-knowledge Distillation Loss, our method\nenhances the student model's segmentation performance through a hierarchical\nlearning strategy, effectively transferring knowledge from the teacher model.\nOur method combines a loosely constrained probabilistic distribution vector\nwith tightly constrained pixel-wise supervision, providing dual regularization\nfor the segmentation model while also enhancing its generalization and\nrobustness. Extensive experiments on XCAD and DCA1 datasets demonstrate that\nour approach outperforms the dice coefficient, accuracy, sensitivity and IoU\ncompared to other models in comparative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6df1\u5ea6\u81ea\u6211\u77e5\u8bc6\u840c\u8358\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u8f93\u51fa\u76d1\u7763\u548c\u53cc\u91cd\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51a0\u72b6\u52a8\u8109\u5206\u5272\u7684\u6027\u80fd\u548c\u6a21\u578b\u6f14\u8fdb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u51a0\u72b6\u52a8\u8109\u5206\u5272\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u666e\u9002\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u5f53\u524d\u7684\u77e5\u8bc6\u840c\u8358\u65b9\u6cd5\u6ca1\u6709\u5145\u5206\u5229\u7528\u6a21\u578b\u7684\u5c42\u6b21\u77e5\u8bc6\uff0c\u5bfc\u81f4\u4fe1\u606f\u6d6a\u8d39\u548c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u6df1\u5ea6\u81ea\u6211\u77e5\u8bc6\u840c\u8358\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5206\u5e03\u635f\u5931\u548c\u50cf\u7d20\u7ea7\u81ea\u6211\u77e5\u8bc6\u840c\u8358\u635f\u5931\uff0c\u901a\u8fc7\u5c42\u6b21\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u5b66\u751f\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u677e\u5f1b\u7ea6\u675f\u7684\u6982\u7387\u5206\u5e03\u5411\u91cf\u548c\u4e25\u683c\u7ea6\u675f\u7684\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "result": "\u5728XCAD\u548cDCA1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728dice\u7cfb\u6570\u3001\u51c6\u786e\u7387\u3001\u654f\u611f\u6027\u548cIoU\u6307\u6807\u4e0a\u90fd\u8d85\u8fc7\u4e86\u5176\u4ed6\u6bd4\u8f83\u6a21\u578b\u3002", "conclusion": "\u6df1\u5ea6\u81ea\u6211\u77e5\u8bc6\u840c\u8358\u65b9\u6cd5\u901a\u8fc7\u5c42\u6b21\u77e5\u8bc6\u5229\u7528\u548c\u53cc\u91cd\u76d1\u7763\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u51a0\u72b6\u52a8\u8109\u5206\u5272\u7684\u6027\u80fd\u548c\u6a21\u578b\u6f14\u8fdb\u80fd\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03188", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03188", "abs": "https://arxiv.org/abs/2509.03188", "authors": ["Hania Ghouse", "Muzammil Behzad"], "title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images", "comment": null, "summary": "Segmentation of small and irregularly shaped abdominal organs, such as the\nadrenal glands in CT imaging, remains a persistent challenge due to severe\nclass imbalance, poor spatial context, and limited annotated data. In this\nwork, we propose a unified framework that combines variational reconstruction,\nsupervised segmentation, and adversarial patch-based feedback to address these\nlimitations in a principled and scalable manner. Our architecture is built upon\na VAE-UNet backbone that jointly reconstructs input patches and generates\nvoxel-level segmentation masks, allowing the model to learn disentangled\nrepresentations of anatomical structure and appearance. We introduce a\npatch-based training pipeline that selectively injects synthetic patches\ngenerated from the learned latent space, and systematically study the effects\nof varying synthetic-to-real patch ratios during training. To further enhance\noutput fidelity, the framework incorporates perceptual reconstruction loss\nusing VGG features, as well as a PatchGAN-style discriminator for adversarial\nsupervision over spatial realism. Comprehensive experiments on the BTCV dataset\ndemonstrate that our approach improves segmentation accuracy, particularly in\nboundary-sensitive regions, while maintaining strong reconstruction quality.\nOur findings highlight the effectiveness of hybrid generative-discriminative\ntraining regimes for small-organ segmentation and provide new insights into\nbalancing realism, diversity, and anatomical consistency in data-scarce\nscenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53d8\u5206\u91cd\u5efa\u3001\u76d1\u7763\u5206\u5272\u548c\u5bf9\u6297\u6027\u8865\u4e01\u53cd\u9988\u6765\u89e3\u51b3CT\u5f62\u6001\u5c0f\u800c\u4e0d\u89c4\u5219\u8179\u90e8\u5668\u5b98\u7684\u5206\u5272\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u8179\u90e8\u5c0f\u5668\u5b98\u5206\u5272\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u3001\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u8f83\u5c11\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528VAE-UNet\u57fa\u7840\u67b6\u6765\u8054\u5408\u91cd\u5efa\u8f93\u5165\u8865\u4e01\u548c\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u7ed3\u5408\u4e86\u53d8\u5206\u91cd\u5efa\u3001\u76d1\u7763\u5206\u5272\u3001\u5bf9\u6297\u6027\u8865\u4e01\u53cd\u9988\u3001VGG\u611f\u77e5\u91cd\u5efa\u635f\u5931\u548cPatchGAN\u8fa8\u522b\u5668\u3002", "result": "\u5728BTCV\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b8c\u6574\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u754c\u654f\u611f\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u6df7\u5408\u751f\u6210-\u8fa8\u522b\u8bad\u7ec3\u65b9\u6848\u5bf9\u5c0f\u5668\u5b98\u5206\u5272\u6709\u6548\uff0c\u4e3a\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5e73\u8861\u771f\u5b9e\u6027\u3001\u591a\u6837\u6027\u548c\u89e3\u5256\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.03421", "categories": ["eess.IV", "cs.CV", "J.3; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.03421", "abs": "https://arxiv.org/abs/2509.03421", "authors": ["Yukun Zhou", "Paul Nderitu", "Jocelyn Hui Lin Goh", "Justin Engelmann", "Siegfried K. Wagner", "Anran Ran", "Hongyang Jiang", "Lie Ju", "Ke Zou", "Sahana Srinivasan", "Hyunmin Kim", "Takahiro Ninomiya", "Zheyuan Wang", "Gabriel Dawei Yang", "Eden Ruffell", "Dominic Williamson", "Rui Santos", "Gabor Mark Somfai", "Carol Y. Cheung", "Tien Yin Wong", "Daniel C. Alexander", "Yih Chung Tham", "Pearse A. Keane"], "title": "Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics", "comment": "39 pages, 8 Figures", "summary": "Medical foundation models, pre-trained with large-scale clinical data,\ndemonstrate strong performance in diverse clinically relevant applications.\nRETFound, trained on nearly one million retinal images, exemplifies this\napproach in applications with retinal images. However, the emergence of\nincreasingly powerful and multifold larger generalist foundation models such as\nDINOv2 and DINOv3 raises the question of whether domain-specific pre-training\nremains essential, and if so, what gap persists. To investigate this, we\nsystematically evaluated the adaptability of DINOv2 and DINOv3 in retinal image\napplications, compared to two specialist RETFound models, RETFound-MAE and\nRETFound-DINOv2. We assessed performance on ocular disease detection and\nsystemic disease prediction using two adaptation strategies: fine-tuning and\nlinear probing. Data efficiency and adaptation efficiency were further analysed\nto characterise trade-offs between predictive performance and computational\ncost. Our results show that although scaling generalist models yields strong\nadaptability across diverse tasks, RETFound-DINOv2 consistently outperforms\nthese generalist foundation models in ocular-disease detection and oculomics\ntasks, demonstrating stronger generalisability and data efficiency. These\nfindings suggest that specialist retinal foundation models remain the most\neffective choice for clinical applications, while the narrowing gap with\ngeneralist foundation models suggests that continued data and model scaling can\ndeliver domain-relevant gains and position them as strong foundations for\nfuture medical foundation models.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u53d1\u73b0\uff0c\u5c3a\u5bf8\u66f4\u5927\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u89c6\u7f51\u819c\u56fe\u50cf\u5e94\u7528\u4e2d\u663e\u793a\u5f3a\u5927\u9002\u5e94\u6027\uff0c\u4f46\u4e13\u4e1a\u7684RETFound-DINOv2\u6a21\u578b\u4ecd\u5728\u773c\u90e8\u75be\u75c5\u68c0\u6d4b\u548c\u773c\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u666e\u9002\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u901a\u7528\u57fa\u7840\u6a21\u578b\u5982DINOv2\u548cDINOv3\u7684\u51fa\u73b0\u5f15\u53d1\u4e86\u95ee\u9898\uff1a\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u662f\u5426\u4ecd\u5fc5\u9700\uff1f\u5982\u679c\u662f\uff0c\u4e13\u4e1a\u6a21\u578b\u4e0e\u901a\u7528\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u6709\u591a\u5927\uff1f", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30DINOv2\u548cDINOv3\u5728\u89c6\u7f51\u819c\u56fe\u50cf\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u4e0e\u4e13\u4e1a\u6a21\u578bRETFound-MAE\u548cRETFound-DINOv2\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528\u7cbe\u8c03\u548c\u7ebf\u6027\u63a2\u9488\u4e24\u79cd\u9002\u5e94\u7b56\u7565\uff0c\u8bc4\u4f30\u773c\u90e8\u75be\u75c5\u68c0\u6d4b\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5c3a\u5bf8\u66f4\u5927\u7684\u901a\u7528\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u4e2d\u663e\u793a\u5f3a\u5927\u9002\u5e94\u6027\uff0c\u4f46RETFound-DINOv2\u5728\u773c\u90e8\u75be\u75c5\u68c0\u6d4b\u548c\u773c\u79d1\u5b66\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8fc7\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u666e\u9002\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u4e13\u4e1a\u89c6\u7f51\u819c\u57fa\u7840\u6a21\u578b\u4ecd\u662f\u4e34\u5e8a\u5e94\u7528\u7684\u6700\u4f73\u9009\u62e9\uff0c\u4f46\u4e0e\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u6301\u7eed\u7684\u6570\u636e\u548c\u6a21\u578b\u6269\u5927\u53ef\u4ee5\u5e26\u6765\u9886\u57df\u76f8\u5173\u6536\u76ca\uff0c\u4f7f\u5f97\u901a\u7528\u6a21\u578b\u6210\u4e3a\u672a\u6765\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u57fa\u7840\u3002"}}
