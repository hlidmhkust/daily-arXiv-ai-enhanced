<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 7]
- [cs.IT](#cs.IT) [Total: 4]
- [eess.SP](#eess.SP) [Total: 6]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI Reconstruction](https://arxiv.org/abs/2509.21531)
*Rohan Sanda,Asad Aali,Andrew Johnston,Eduardo Reis,Jonathan Singh,Gordon Wetzstein,Sara Fridovich-Keil*

Main category: eess.IV

TL;DR: 提出PaDIS-MRI方法，将基于补丁的扩散模型应用于多线圈MRI重建，在小数据集上表现优于现有方法，在放射科医生盲测中获得91.7%的诊断优越性。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间长、成本高、易受运动伪影影响，现有扩散模型需要大量训练数据，而临床环境中数据稀缺。

Method: 扩展基于补丁的扩散逆求解器(PaDIS)到复数多线圈MRI重建，使用小数据集训练（最少25个k空间图像）。

Result: 在7倍欠采样MRI重建中，PaDIS-MRI在图像质量指标、像素级不确定性、跨对比度泛化性和对严重k空间欠采样的鲁棒性方面均优于FastMRI-EDM。

Conclusion: 基于补丁的扩散先验在数据稀缺的临床环境中具有高保真MRI重建潜力，对诊断置信度至关重要。

Abstract: Magnetic resonance imaging (MRI) requires long acquisition times, raising
costs, reducing accessibility, and making scans more susceptible to motion
artifacts. Diffusion probabilistic models that learn data-driven priors can
potentially assist in reducing acquisition time. However, they typically
require large training datasets that can be prohibitively expensive to collect.
Patch-based diffusion models have shown promise in learning effective
data-driven priors over small real-valued datasets, but have not yet
demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse
Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it
against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x
undersampled MRI reconstruction on the FastMRI brain dataset. We show that
PaDIS-MRI models trained on small datasets of as few as 25 k-space images
outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE),
pixel-level uncertainty, cross-contrast generalization, and robustness to
severe k-space undersampling. In a blinded study with three radiologists,
PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of
cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex
reconstruction with wavelet sparsity. These findings highlight the potential of
patch-based diffusion priors for high-fidelity MRI reconstruction in
data-scarce clinical settings where diagnostic confidence matters.

</details>


### [2] [Transabdominal Fetal Oximetry via Diffuse Optics: Principled Analysis and Demonstration in Pregnant Ovine Models](https://arxiv.org/abs/2509.21594)
*Weitai Qian,Rishad Raiyan Joarder,Randall Fowler,Begum Kasap,Mahya Saffarpour,Kourosh Vali,Tailai Lihe,Aijun Wang,Diana Farmer,Soheil Ghiasi*

Main category: eess.IV

TL;DR: 提出了一种基于漫射光学的经腹胎儿血氧饱和度监测方法，通过指数脉动比特征和多探测器信息融合，显著提高了胎儿血氧饱和度的估计精度。


<details>
  <summary>Details</summary>
Motivation: 漫射光学技术有望通过连续测量胎儿血氧饱和度来显著改善胎儿健康监测，但需要提高传感精度并阐明经腹胎儿血氧测定的基本限制。

Method: 提出了理论推导和综合处理流程，引入指数脉动比作为关键特征，开发机器学习模型融合多个探测器的信息。

Result: 在模拟数据集和活体实验数据集中，该方法分别实现了4.81%和6.85%的平均绝对误差，皮尔逊相关系数分别为0.81和0.71。

Conclusion: 该方法优于现有方法，提高了胎儿血氧饱和度估计的准确性，证明了其作为分娩期胎儿监测补充技术的可行性。

Abstract: Diffuse optics has the potential to offer a substantial advancement in fetal
health monitoring via enabling continuous measurement of fetal blood oxygen
saturation (fSpO$_2$). Aiming to enhance the sensing accuracy and to elucidate
the foundational limits of Transabdominal Fetal Oximetry (TFO) via diffuse
optics, we introduce a theoretical derivation, and a comprehensive pipeline for
fSpO$_2$ estimation from non-invasively sensed diffuse light intensity values,
which are leveraged to analyze datasets obtained through both simulations and
in-vivo experiments in gold standard large animal model of pregnancy. We
propose the Exponential Pulsation Ratio (EPR) as a key feature, and develop
machine-learning models to fuse the information collected across multiple
detectors. Our proposed method demonstrates a Mean Absolute Error (MAE) of
4.81% and 6.85% with a Pearson's r correlation of 0.81 (p<0.001) and 0.71
(p<0.001) for estimation of fSpO$_2$ in simulated dataset and in-vivo dataset,
respectively. Across both datasets, our method outperforms the existing
approaches, enhancing the accuracy of the fSpO$_2$ estimation and demonstrates
its viability as a supplemental technology for intrapartum fetal monitoring.

</details>


### [3] [Multicollinearity-Aware Parameter-Free Strategy for Hyperspectral Band Selection: A Dependence Measures-Based Approach](https://arxiv.org/abs/2509.21973)
*Dibyabha Deb,Ujjwal Verma*

Main category: eess.IV

TL;DR: 提出一种结合ABC、MI和VIF三种依赖度量的无参数高光谱波段选择方法，通过VIF预选减少多共线性，再基于ABC和MI聚类选择最优波段子集


<details>
  <summary>Details</summary>
Motivation: 高光谱波段维度高、冗余大，现有波段选择方法存在对初始化敏感、参数调优困难和计算成本高等问题

Method: 使用VIF进行波段预选减少多共线性，然后结合ABC（线性相关性）和MI（与标签的不确定性减少）进行聚类选择最优波段子集

Result: 在四个标准数据集上验证，所选波段与其他方法有显著重叠，SVM分类验证VIF剪枝通过最小化多共线性提升分类性能

Conclusion: 该方法完全无参数，结合ABC和MI能产生鲁棒且具有判别性的波段子集，VIF驱动的剪枝能有效提升分类性能

Abstract: Hyperspectral bands offer rich spectral and spatial information; however,
their high dimensionality poses challenges for efficient processing. Band
selection (BS) methods aim to extract a smaller subset of bands to reduce
spectral redundancy. Existing approaches, such as ranking-based,
clustering-based, and iterative methods, often suffer from issues like
sensitivity to initialization, parameter tuning, and high computational cost.
This work introduces a BS strategy integrating three dependence measures:
Average Band Correlation (ABC) and Mutual Information (MI), and Variance
Inflation Factor (VIF). ABC quantifies linear correlations between spectral
bands, while MI measures uncertainty reduction relative to ground truth labels.
To address multicollinearity and reduce the search space, the approach first
applies a VIF-based pre-selection of spectral bands. Subsequently, a clustering
algorithm is used to identify the optimal subset of bands based on the ABC and
MI values. Unlike previous methods, this approach is completely parameter-free
for hyperspectral band selection, eliminating the need for optimal parameter
estimation. The proposed method is evaluated on four standard benchmark
datasets: WHU-Hi-LongKou, Pavia University, Salinas, and Oil Spill datasets,
and is compared to existing state-of-the-art approaches. There is significant
overlap between the bands identified by our proposed method and those selected
by other methods, indicating that our approach effectively captures the most
relevant spectral features. Further, support vector machine (SVM)
classification validates that VIF-driven pruning enhances classification by
minimizing multicollinearity. Ablation studies confirm that combining ABC with
MI yields robust, discriminative band subsets.

</details>


### [4] [Comparative Analysis of GAN and Diffusion for MRI-to-CT translation](https://arxiv.org/abs/2509.22049)
*Emily Honey,Anders Helbo,Jens Petersen*

Main category: eess.IV

TL;DR: 比较两种MRI到CT图像转换方法：条件生成对抗网络(cGAN)和条件去噪扩散概率模型(cDDPM)，发现cDDPM在多通道条件输入下表现更优。


<details>
  <summary>Details</summary>
Motivation: 当CT图像缺失或难以获取时，需要从MRI图像生成合成CT(sCT)图像，因此需要确定最有效的MRI到CT转换策略。

Method: 使用Pix2Pix代表cGAN架构，Palette代表cDDPM架构，将3D转换问题分解为2D横向平面序列转换，研究单切片和多切片条件输入的影响。

Result: MRI到CT生成模型受益于多通道条件输入，且cDDPM架构表现更佳。

Conclusion: cDDPM在MRI到CT转换任务中优于cGAN，多通道条件输入能提升生成质量。

Abstract: Computed tomography (CT) is essential for treatment and diagnostics; In case
CT are missing or otherwise difficult to obtain, methods for generating
synthetic CT (sCT) images from magnetic resonance imaging (MRI) images are
sought after. Therefore, it is valuable to establish a reference for what
strategies are most effective for MRI-to-CT translation. In this paper, we
compare the performance of two frequently used architectures for MRI-to-CT
translation: a conditional generative adversarial network (cGAN) and a
conditional denoising diffusion probabilistic model (cDDPM). We chose
well-established implementations to represent each architecture: Pix2Pix for
cGAN, and Palette for cDDPM. We separate the classical 3D translation problem
into a sequence of 2D translations on the transverse plane, to investigate the
viability of a strategy that reduces the computational cost. We also
investigate the impact of conditioning the generative process on a single MRI
image/slice and on multiple MRI slices. The performance is assessed using a
thorough evaluation protocol, including a novel slice-wise metric Similarity Of
Slices (SIMOS), which measures the continuity between transverse slices when
compiling the sCTs into 3D format. Our comparative analysis revealed that
MRI-to-CT generative models benefit from multi-channel conditional input and
using cDDPM as an architecture.

</details>


### [5] [Fifty Years of SAR Automatic Target Recognition: The Road Forward](https://arxiv.org/abs/2509.22159)
*Jie Zhou,Yongxiang Liu,Li Liu,Weijie Li,Bowen Peng,Yafei Song,Gangyao Kuang,Xiang Li*

Main category: eess.IV

TL;DR: 这篇论文对50年合成孔径雷达自动目标识别(SAR ATR)发展进行了首次全面回顾，分析了从传统方法到现代深度学习框架的演进历程，并提供了系统性整理的公开数据集资源。


<details>
  <summary>Details</summary>
Motivation: 提供SAR ATR领域50年发展的首次全面综述，理清技术演进脉络，区分传统挑战和新兴问题，并为研究者提供实用的数据集资源和未来方向指导。

Method: 系统性回顾和分析SAR ATR发展历程，重点分析传统方法(统计建模、散射中心分析、特征工程)在深度学习框架中的继承与精炼，并整理所有公开可用的SAR数据集。

Result: 清晰区分了深度学习已显著缓解的长期挑战与新出现的障碍，提出了物理引导深度学习的最新进展，并提供了系统组织的公开数据集汇编。

Conclusion: 该工作不仅记录了SAR ATR领域的技术演进，还为研究者和实践者提供了实用资源和前瞻性见解，推动了该领域向更泛化和物理一致的方向发展。

Abstract: This paper provides the first comprehensive review of fifty years of
synthetic aperture radar automatic target recognition (SAR ATR) development,
tracing its evolution from inception to the present day. Central to our
analysis is the inheritance and refinement of traditional methods, such as
statistical modeling, scattering center analysis, and feature engineering,
within modern deep learning frameworks. The survey clearly distinguishes
long-standing challenges that have been substantially mitigated by deep
learning from newly emerging obstacles. We synthesize recent advances in
physics-guided deep learning and propose future directions toward more
generalizable and physically-consistent SAR ATR. Additionally, we provide a
systematically organized compilation of all publicly available SAR datasets,
complete with direct links to support reproducibility and benchmarking. This
work not only documents the technical evolution of the field but also offers
practical resources and forward-looking insights for researchers and
practitioners. A systematic summary of existing literature, code, and datasets
are open-sourced at
\href{https://github.com/JoyeZLearning/SAR-ATR-From-Beginning-to-Present}{https://github.com/JoyeZLearning/SAR-ATR-From-Beginning-to-Present}.

</details>


### [6] [COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics](https://arxiv.org/abs/2509.22240)
*Matt Y. Cheung,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: eess.IV

TL;DR: COMPASS是一个用于医学图像分割的度量不确定性量化框架，通过在模型表示空间中进行校准，为下游度量生成高效的置信区间。


<details>
  <summary>Details</summary>
Motivation: 在临床应用中，分割模型的实用性通常基于下游度量（如器官大小）的准确性，而非像素级分割精度。因此，对这些度量进行不确定性量化对决策至关重要。

Method: COMPASS通过在模型表示空间中扰动中间特征，沿着对目标度量最敏感的低维子空间进行校准，避免将复杂的分割到度量管道视为黑盒。

Result: 在四个医学图像分割任务中，COMPASS产生的区间比传统CP基线显著更紧凑，同时通过估计重要性权重，在协变量偏移下也能恢复目标覆盖。

Conclusion: COMPASS为医学图像分割提供了实用的基于度量的不确定性量化方法，通过利用深度学习模型的归纳偏置实现高效校准。

Abstract: In clinical applications, the utility of segmentation models is often based
on the accuracy of derived downstream metrics such as organ size, rather than
by the pixel-level accuracy of the segmentation masks themselves. Thus,
uncertainty quantification for such metrics is crucial for decision-making.
Conformal prediction (CP) is a popular framework to derive such principled
uncertainty guarantees, but applying CP naively to the final scalar metric is
inefficient because it treats the complex, non-linear segmentation-to-metric
pipeline as a black box. We introduce COMPASS, a practical framework that
generates efficient, metric-based CP intervals for image segmentation models by
leveraging the inductive biases of their underlying deep neural networks.
COMPASS performs calibration directly in the model's representation space by
perturbing intermediate features along low-dimensional subspaces maximally
sensitive to the target metric. We prove that COMPASS achieves valid marginal
coverage under exchangeability and nestedness assumptions. Empirically, we
demonstrate that COMPASS produces significantly tighter intervals than
traditional CP baselines on four medical image segmentation tasks for area
estimation of skin lesions and anatomical structures. Furthermore, we show that
leveraging learned internal features to estimate importance weights allows
COMPASS to also recover target coverage under covariate shifts. COMPASS paves
the way for practical, metric-based uncertainty quantification for medical
image segmentation.

</details>


### [7] [Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss](https://arxiv.org/abs/2509.22394)
*Javier Sequeiro González,Arthur Longuefosse,Miguel Díaz Benito,Álvaro García Martín,Fabien Baldacci*

Main category: eess.IV

TL;DR: 提出基于3D nnUNet的MR到CT和CBCT到CT图像转换方法，使用SynthRAD2025多中心数据集，涵盖头颈、胸部和腹部区域。引入解剖特征优先损失函数，结合残差UNet网络，在保持图像质量的同时提升解剖结构重建精度。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像跨模态转换中解剖结构重建精度不足的问题，特别是在MR到CT和CBCT到CT转换中，需要更好地保留临床相关结构如骨骼和病变区域。

Method: 采用两种网络配置：标准UNet和残差UNet，基于nnUNet框架。引入AFP损失函数，使用紧凑分割网络提取多层特征。采用3D补丁训练，针对不同解剖区域定制补丁大小，使用zscore标准化，训练1000-1500轮，AFP微调500轮。

Result: 残差网络结合AFP损失能产生更清晰的图像重建和更好的解剖保真度，特别是在MR到CT中的骨骼结构和CBCT到CT中的病变区域。仅使用L1损失的网络在强度指标上略优。

Conclusion: 该方法为跨模态医学图像合成提供了稳定解决方案，证明了自动nnUNet管道与残差学习和解剖引导特征损失结合的有效性。

Abstract: We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT
image translation using the multicenter SynthRAD2025 dataset, covering head and
neck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two
main network configurations: a standard UNet and a residual UNet, both adapted
from nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss
was introduced, which compares multilayer features extracted from a compact
segmentation network trained on TotalSegmentator labels, enhancing
reconstruction of clinically relevant structures. Input volumes were normalized
per-case using zscore normalization for MRIs, and clipping plus dataset level
zscore normalization for CBCT and CT. Training used 3D patches tailored to each
anatomical region without additional data augmentation. Models were trained for
1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a
combined L1+AFP objective. During inference, overlapping patches were
aggregated via mean averaging with step size of 0.3, and postprocessing
included reverse zscore normalization. Both network configurations were applied
across all regions, allowing consistent model design while capturing local
adaptations through residual learning and AFP loss. Qualitative and
quantitative evaluation revealed that residual networks combined with AFP
yielded sharper reconstructions and improved anatomical fidelity, particularly
for bone structures in MR to CT and lesions in CBCT to CT, while L1only
networks achieved slightly better intensity-based metrics. This methodology
provides a stable solution for cross modality medical image synthesis,
demonstrating the effectiveness of combining the automatic nnUNet pipeline with
residual learning and anatomically guided feature losses.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [8] [Differentially-Private Decentralized Learning in Heterogeneous Multicast Networks](https://arxiv.org/abs/2509.21688)
*Amir Ziaeddini,Yauhen Yakimenka,Jörg Kliewer*

Main category: cs.IT

TL;DR: 提出了一种功率控制的差分隐私去中心化学习算法，通过联合控制传输功率和注入的高斯噪声水平，在满足隐私和能量预算的同时实现O(log T)的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 针对一组客户端协作训练共同学习模型的需求，在具有行随机邻接矩阵的网络中，需要同时满足隐私保护和能量约束。

Method: 采用功率控制的差分隐私方法，联合控制模型更新的传输功率和注入的高斯噪声水平，确保满足给定的隐私和能量预算。

Result: 算法实现了O(log T)的收敛速率，其中T是遗憾函数中的时间界限，数值结果证实该算法优于现有工作。

Conclusion: 所提出的功率控制差分隐私去中心化学习算法在保证隐私和能量约束的同时，实现了良好的收敛性能，优于现有方法。

Abstract: We propose a power-controlled differentially private decentralized learning
algorithm designed for a set of clients aiming to collaboratively train a
common learning model. The network is characterized by a row-stochastic
adjacency matrix, which reflects different channel gains between the clients.
In our privacy-preserving approach, both the transmit power for model updates
and the level of injected Gaussian noise are jointly controlled to satisfy a
given privacy and energy budget. We show that our proposed algorithm achieves a
convergence rate of O(log T), where T is the horizon bound in the regret
function. Furthermore, our numerical results confirm that our proposed
algorithm outperforms existing works.

</details>


### [9] [Dual and Covering Radii of Extended Algebraic Geometry Codes](https://arxiv.org/abs/2509.21773)
*Yunlong Zhu,Chang-An Zhao*

Main category: cs.IT

TL;DR: 本文研究了扩展代数几何码和Roth-Lempel型代数几何码，包括它们的对偶码和最小距离，并证明了在某些条件下g-MDS码的长度可以达到q+1+2g√q，同时确定了扩展代数几何码的覆盖半径有g+2个可能值。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注扩展Reed-Solomon码及其对偶码和覆盖半径，但对扩展代数几何码（g≥1）的研究较少，因此需要系统研究扩展代数几何码及其相关性质。

Method: 通过研究扩展代数几何码和Roth-Lempel型代数几何码，分析它们的对偶码和最小距离，利用最大曲线构造扩展代数几何码，并计算覆盖半径的可能取值范围。

Result: 证明了对于某些g，g-MDS码的长度可以达到q+1+2g√q，这是通过最大曲线构造的扩展代数几何码实现的；确定了扩展代数几何码的覆盖半径有g+2个可能值；对于g=1的情况，当长度n足够大或存在[n,k+1]MDS椭圆码时，覆盖半径的可能值减少到2个。

Conclusion: 扩展代数几何码在特定条件下可以达到理论上的最大长度，其覆盖半径的取值范围相对较小，特别是在g=1的情况下，覆盖半径的可能值进一步减少，这为代数几何码的设计和应用提供了理论依据。

Abstract: Many literatures consider the extended Reed-Solomon (RS) codes, including
their dual codes and covering radii, but few focus on extended algebraic
geometry (AG) codes of genus $g\ge1$. In this paper, we investigate extended AG
codes and Roth-Lempel type AG codes, including their dual codes and minimum
distances. Moreover, we show that for certain $g$, the length of a $g$-MDS code
over a finite field $\mathbb{F}_q$ can attain $q+1+2g\sqrt{q}$, which is
achieved by an extended AG code from the maximal curves of genus $g$. Notably,
for some small finite fields, this length $q+1+2g\sqrt{q}$ is the largest among
all known $g$-MDS codes. Subsequently, we establish that the covering radius of
an $[n,k]$ extended AG code has $g+2$ possible values. For the case of $g=1$,
we prove that this range reduces to two possible values when the length $n$ is
sufficiently large, or when there exists an $[n,k+1]$ MDS elliptic code.

</details>


### [10] [Radio-PPG: photoplethysmogram digital twin synthesis using deep neural representation of 6G/WiFi ISAC signals](https://arxiv.org/abs/2509.22326)
*Israel Jesus Santos Filho,Muhammad Mahboob Ur Rahman,Taous-Meriem Laleg-Kirati,Tareq Al-Naffouri*

Main category: cs.IT

TL;DR: 提出了一种基于6G/WiFi集成感知与通信系统的非接触式数字孪生光电容积脉搏波合成方法，使用软件定义无线电采集胸部反射信号，构建了包含30名健康受试者的Radio-PPG数据集，并测试了两种AI模型用于DT-PPG信号合成。


<details>
  <summary>Details</summary>
Motivation: 开发非接触式数字孪生生物信号监测方法，实现实时生理过程监测，用于早期疾病诊断和个性化治疗，特别关注COVID-19、心血管疾病和特殊需求人群的健康评估。

Method: 使用5.23 GHz软件定义无线电照射人体胸部并收集反射信号，构建Radio-PPG数据集；测试了两种AI模型：离散余弦变换+多层感知机，以及级联的U-NET模型（近似网络和细化网络）配合自定义损失函数。

Result: U-NET模型在DT-PPG合成中取得了0.194的相对平均绝对误差，ISAC感知开销仅为15.62%；合成DT-PPG的生命体征估计和特征提取精度与参考PPG相当。

Conclusion: 这项工作展示了生成式AI和6G/WiFi ISAC技术在非接触式健康筛查工具开发中的潜力，为COVID-19、心血管疾病和特殊需求人群健康评估奠定了基础。

Abstract: Digital twins for 1D bio-signals enable real-time monitoring of physiological
processes of a person, which enables early disease diagnosis and personalized
treatment. This work introduces a novel non-contact method for digital twin
(DT) photoplethysmogram (PPG) signal synthesis under the umbrella of 6G/WiFi
integrated sensing and communication (ISAC) systems. We employ a
software-defined radio (SDR) operating at 5.23 GHz that illuminates the chest
of a nearby person with a wideband 6G/WiFi signal and collects the reflected
signals. This allows us to acquire Radio-PPG dataset that consists of 300
minutes worth of near synchronous 64-channel radio data, PPG data, along with
the labels (three body vitals) of 30 healthy subjects. With this, we test two
artificial intelligence (AI) models for DT-PPG signal synthesis: i) discrete
cosine transform followed by a multi-layer perceptron, ii) two U-NET models
(Approximation network, Refinement network) in cascade, along with a custom
loss function. Experimental results indicate that U-NET model achieves an
impressive relative mean absolute error of 0.194 with a small ISAC sensing
overhead of 15.62%, for DT-PPG synthesis. Furthermore, we performed quality
assessment of the synthetic DT-PPG by computing the accuracy of DT-PPG-based
vitals estimation and feature extraction, which turned out to be at par with
that of reference PPG-based vitals estimation and feature extraction. This work
highlights the potential of generative AI and 6G/WiFi ISAC technologies and
serves as a foundational step towards the development of non-contact screening
tools for covid-19, cardiovascular diseases and well-being assessment of people
with special needs.

</details>


### [11] [UAV-Enabled Fluid Antenna Systems for Multi-Target Wireless Sensing over LAWCNs](https://arxiv.org/abs/2509.22497)
*Xuhui Zhang,Wenchao Liu,Chunjie Wang,Jinke Ren,Huijun Xing,Shuqiang Wang,Yanyan Shen*

Main category: cs.IT

TL;DR: 提出了一种基于流体天线系统的无人机多目标无线感知方案，通过优化无人机轨迹、天线位置和波束成形来最小化多目标估计的克拉美-罗下界，显著提升了感知精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统能够增强空间灵活性和感知精度，本文旨在利用无人机搭载的FAS技术为低空无线消费网络提供多目标无线感知，支持低空经济任务。

Method: 提出了一种高效的交替优化算法，联合优化无人机轨迹、发射和接收流体天线的位置以及无人机的发射波束成形，以解决非凸优化问题。

Result: 仿真结果表明，与传统固定位置天线方案相比，所提系统在估计精度和感知可靠性方面有显著提升，通过自适应轨迹设计和波束成形实现了增强的感知性能。

Conclusion: 该系统通过灵活的FAS天线重定位有效抑制干扰，在无人机低空无线消费网络中具有实现精确感知的实际潜力。

Abstract: Fluid antenna system (FAS) is emerging as a key technology for enhancing
spatial flexibility and sensing accuracy in future wireless systems. This paper
investigates an unmanned aerial vehicle (UAV)-enabled FAS for multi-target
wireless sensing in low-altitude wireless consumer networks (LAWCNs) for
achieving the low-altitude economy (LAE) missions. We formulate an optimization
problem aimed at minimizing the average Cram\'er-Rao bound (CRB) for multiple
target estimations. To tackle this non-convex problem, an efficient alternating
optimization (AO) algorithm is proposed, which jointly optimizes the UAV
trajectory, the antenna position of the transmit fluid antennas (FAs) and the
receive FAs, and the transmit beamforming at the UAV. Simulation results
demonstrate significant performance improvements in estimation accuracy and
sensing reliability compared to conventional schemes, e.g., the fixed position
antenna scheme. The proposed system achieves enhanced sensing performance
through adaptive trajectory design and beamforming, alongside effective
interference suppression via the flexible FAS antenna repositioning,
underscoring its practical potential for precision sensing in the UAV-enabled
LAWCNs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [12] [Hidden Markov Model Decoding for LDPC Codes](https://arxiv.org/abs/2509.21872)
*Jan C Olivier,Etienne Barnard*

Main category: eess.SP

TL;DR: 提出了一种基于隐马尔可夫模型的LDPC码迭代解码方法，通过前向-后向平滑估计实现高效解码，在短帧长度下性能可与Polar码相媲美。


<details>
  <summary>Details</summary>
Motivation: 为LDPC码提供更高效的自然解码框架，改进传统置信传播算法的解码阈值，特别是在短帧长度场景下。

Method: 使用一阶隐马尔可夫模型，构建基于编码帧比特随机游走的时间齐次模型，隐藏状态包含两个编码比特，将奇偶校验自然融入观测模型。

Result: LDPC解码阈值相比Tanner图上的置信传播显著提升，在512比特或更短的帧长度下，帧错误率和解码阈值与采用SCL-CRC解码的Polar码相当。

Conclusion: 基于HMM的LDPC解码器在短帧场景下表现优异，为LDPC码提供了有竞争力的解码方案。

Abstract: The paper proposes an iterative Hidden Markov Model (HMM) for decoding a Low
Density Parity Check (LDPC) code. It is demonstrated that a first-order HMM
provides a natural framework for the decoder. The HMM is time-homogeneous with
a fixed transition matrix and is based on a random walk through the encoded
frame bits. Each hidden state contains a pair of two encoded bits, and parity
checks are naturally incorporated into the observation model. The paper shows
that by implementing a forward-backward smoothing estimator for the hidden
states, decoding is efficient and requires only a small number of iterations in
most cases. The results show that the LDPC decoding threshold is significantly
improved compared to belief propagation (BP) on a Tanner graph. Numerical
results are presented showing that LDPC codes under the proposed decoder yield
a frame error rate (FER) and decoding threshold comparable to that of a Polar
code where Successive Cancellation List (SCL) - Cyclic Redundancy Check (CRC)
decoding is deployed. This is shown to be achieved even if the frame length is
short (on the order of $512$ bits or less) and a regular LDPC code is used. 1

</details>


### [13] [CRB minimization for PASS Assisted ISAC](https://arxiv.org/abs/2509.22181)
*Haochen Li,Ruikang Zhong,Jiayi Lei,Pan Zhiwen,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种多波导PASS辅助的集成感知与通信系统，采用PASS发射和ULA接收的基站架构，通过交替优化方法最小化目标感知的克拉美罗下界。


<details>
  <summary>Details</summary>
Motivation: 为了解决集成感知与通信系统中感知性能与通信服务质量之间的平衡问题，同时考虑功率预算和天线部署约束。

Method: 采用PASS发射-ULA接收的基站配置，通过交替优化方法解决非凸优化问题，在通信QoS约束、功率预算约束和PA部署约束下最小化目标感知CRB。

Result: 仿真结果表明，所提出的PASS辅助ISAC框架在性能上优于基准方案。

Conclusion: 该PASS辅助的ISAC系统能够有效提升感知性能，同时保证通信服务质量，为集成感知与通信系统提供了有效的解决方案。

Abstract: A multiple waveguide PASS assisted integrated sensing and communication
(ISAC) system is proposed, where the base station (BS) is equipped with
transmitting pinching antennas (PAs) and receiving uniform linear array (ULA)
antennas. The PASS-transmitting-ULA-receiving (PTUR) BS transmits the
communication and sensing signals through the stretched PAs on waveguides and
collects the echo sensing signals with the mounted ULA. Based on this
configuration, a target sensing Cramer Rao Bound (CRB) minimization problem is
formulated under communication quality-of-service (QoS) constraints, power
budget constraints, and PA deployment constraints. An alternating optimization
(AO) method is employed to address the formulated non-convex optimization
problem. Simulation results demonstrate that the proposed PASS assisted ISAC
framework achieves superior performance over benchmark schemes.

</details>


### [14] [A Deep Neural Network Codebook Approach for Near-Field Nulling Control Beam Focusing](https://arxiv.org/abs/2509.22204)
*Mohammadhossein Karimi,Yuanzhe Gong,Tho Le-Ngoc*

Main category: eess.SP

TL;DR: 提出一种用于XL-MIMO近场区域多用户干扰抑制的深度神经网络码本方法，通过分区训练轻量级DNN模型来预测波束赋形权重，在降低计算复杂度的同时实现有效的干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于DNN的零陷控制波束赋形方法面临可扩展性和复杂度挑战，特别是在XL-MIMO系统的近场区域中，需要更高效的干扰抑制方案。

Method: 基于相关性采样对菲涅尔区域进行分区，为每个子区域分配轻量级全连接DNN模型，使用LCMV方法生成的波束赋形权重进行训练，预测能同时优化信号强度和抑制干扰的零陷控制波束聚焦权重。

Result: 训练模型在75个样本子区域上实现平均相位误差0.085弧度、幅度误差0.52 dB，Ansys HFSS全波仿真显示干扰抑制优于31.64 dB，与LCMV方法性能差距在2 dB以内。

Conclusion: 所提出的DNN码本方法在降低计算复杂度的同时，有效缓解了XL-MIMO系统中的多用户干扰问题，验证了其在近场区域干扰抑制中的有效性。

Abstract: This paper proposes a deep neural network (DNN) codebook approach for
multi-user interference (MUI) mitigation in extremely large multiple-input
multiple-output (XL-MIMO) systems operating in the near-field region. Unlike
existing DNN-based nulling control beamforming (NCBF) methods that face
scalability and complexity challenges, the proposed framework partitions the
Fresnel region using correlation-based sampling and assigns a lightweight fully
connected DNN model to each subsection. Each model is trained on beamforming
weights generated using the linearly constrained minimum variance (LCMV)
method, enabling accurate prediction of nulling control beam-focusing weights
that simultaneously optimize the desired signal strength and suppress potential
interference for both collinear and non-collinear user configurations.
Simulation results show that the trained models achieve average phase and
magnitude prediction errors of 0.085 radians and 0.52 dB, respectively, across
75 sample subsections. Full-wave simulations in Ansys HFSS further demonstrate
that the proposed DNN codebook achieves interference suppression better than
31.64 dB, with a performance gap within 2 dB of the LCMV method, thereby
validating its effectiveness in mitigating MUI while reducing computational
complexity.

</details>


### [15] [Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM Communications](https://arxiv.org/abs/2509.22327)
*Zheao Li,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 提出了一种基于OFDM-IM的SIM增强宽带多用户收发器，通过稀疏激活和深度展开优化解决SIM在宽带部署中的结构限制问题。


<details>
  <summary>Details</summary>
Motivation: 解决SIM在宽带部署中的两个结构限制：(i) 单一准静态SIM相位张量需要适应所有子载波；(ii) 多用户调度每帧改变子载波激活模式，需要快速重新配置。

Method: 使用OFDM-IM的稀疏激活将高保真均衡限制在活动音调上，有效扩展可用带宽；提出展开投影梯度下降网络(UPGD-Net)，在SIM层和算法迭代上进行双重展开优化。

Result: 宽带多用户下行链路仿真显示快速单调收敛、明显的层深度最佳点，在最差链路BER和总速率方面获得一致增益。

Conclusion: 通过将结构稀疏性与BER驱动的深度展开优化相结合，该框架直接解决了SIM的关键宽带缺陷。

Abstract: Leveraging the multilayer realization of programmable metasurfaces, stacked
intelligent metasurfaces (SIM) enable fine-grained wave-domain control.
However, their wideband deployment is impeded by two structural factors: (i) a
single, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)
multiuser scheduling changes the subcarrier activation pattern frame by frame,
requiring rapid reconfiguration. To address both challenges, we develop a
SIM-enhanced wideband multiuser transceiver built on orthogonal
frequency-division multiplexing with index modulation (OFDM-IM). The sparse
activation of OFDM-IM confines high-fidelity equalization to the active tones,
effectively widening the usable bandwidth. To make the design
reliability-aware, we directly target the worst-link bit-error rate (BER) and
adopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a
principled surrogate, turning the reliability optimization tractable. For
frame-rate inference and interpretability, we propose an unfolded
projected-gradient-descent network (UPGD-Net) that double-unrolls across the
SIM's layers and algorithmic iterations: each cell computes the analytic
gradient from the cascaded precoder with a learnable per-iteration step size.
Simulations on wideband multiuser downlinks show fast, monotone convergence, an
evident layer-depth sweet spot, and consistent gains in worst-link BER and sum
rate. By combining structural sparsity with a BER-driven, deep-unfolded
optimization backbone, the proposed framework directly addresses the key
wideband deficiencies of SIM.

</details>


### [16] [Specific multi-emitter identification via multi-label learning](https://arxiv.org/abs/2509.22396)
*Yuhao Chen,Boxiang He,Shilian Wang,Jing Lei*

Main category: eess.SP

TL;DR: 提出了一种基于多标签学习的特定多发射器识别方法，用于从重叠信号中识别多个发射器，具有较低复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理多个发射器信号重叠的场景，需要解决多发射器识别问题。

Method: 设计了多发射器指纹提取器来减轻重叠信号的相互干扰，并提出多发射器决策器来分配所有发射器识别。

Result: 实验结果表明，与基线方法相比，所提出的SMEI方案在各种重叠条件下实现了相当的识别精度，同时运行复杂度显著降低。

Conclusion: 该方法能够以较低复杂度从重叠信号中识别多个发射器，具有重要应用价值。

Abstract: Specific emitter identification leverages hardware-induced impairments to
uniquely determine a specific transmitter. However, existing approaches fail to
address scenarios where signals from multiple emitters overlap. In this paper,
we propose a specific multi-emitter identification (SMEI) method via
multi-label learning to determine multiple transmitters. Specifically, the
multi-emitter fingerprint extractor is designed to mitigate the mutual
interference among overlapping signals. Then, the multi-emitter decision maker
is proposed to assign the all emitter identification using the previous
extracted fingerprint. Experimental results demonstrate that, compared with
baseline approach, the proposed SMEI scheme achieves comparable identification
accuracy under various overlapping conditions, while operating at significantly
lower complexity. The significance of this paper is to identify multiple
emitters from overlapped signal with a low complexity.

</details>


### [17] [Approximation of the Range Ambiguity Function in Near-field Sensing Systems](https://arxiv.org/abs/2509.22423)
*Marcin Wachowiak,André Bourdoux,Sofie Pollin*

Main category: eess.SP

TL;DR: 本文研究了近场系统的距离模糊函数，分析了带宽和近场波束聚焦如何共同决定分辨率。推导了匹配滤波模糊函数，提出了基于孔径-带宽乘积的近似准则，并展示了近场波束聚焦相比远场的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究近场系统中带宽和波束聚焦对分辨率的联合影响，探索近场区域相比远场的性能优势，特别是在近距离场景下的分辨率提升。

Method: 推导了通用的匹配滤波模糊函数，引入不同天线阵列几何结构的近场阵列因子，将近场模糊函数近似为距离相关的近场阵列因子与带宽波形模糊函数的乘积，建立了基于孔径-带宽乘积的近似准则。

Result: 近场波束聚焦相比远场情况在分辨率、峰值旁瓣比和积分旁瓣电平方面均有显著改善，但这些增益具有距离依赖性，仅在靠近阵列的区域才明显。

Conclusion: 近场系统在近距离场景下通过波束聚焦能显著提升性能，但增益随距离增加而衰减，需要根据具体应用场景权衡近场优势与系统复杂度。

Abstract: This paper investigates the range ambiguity function of near-field systems
where bandwidth and near-field beamfocusing jointly determine the resolution.
First, the general matched filter ambiguity function is derived and the
near-field array factors of different antenna array geometries are introduced.
Next, the near-field ambiguity function is approximated as a product of the
range-dependent near-field array factor and the ambiguity function due to the
utilized bandwidth and waveform. An approximation criterion based on the
aperture-bandwidth product is formulated, and its accuracy is examined.
Finally, the improvements to the ambiguity function offered by the near-field
beamfocusing, as compared to the far-field case, are presented. The performance
gains are evaluated in terms of resolution improvement offered by beamfocusing,
peak-to-sidelobe and integrated-sidelobe level improvement. The gains offered
by the near-field regime are shown to be range-dependent and substantial only
in close proximity to the array.

</details>
