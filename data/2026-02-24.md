<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 6]
- [eess.SP](#eess.SP) [Total: 20]
- [cs.IT](#cs.IT) [Total: 8]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Triggering hallucinations in model-based MRI reconstruction via adversarial perturbations](https://arxiv.org/abs/2602.18536)
*Suna Buğday,Yvan Saeys,Jonathan Peck*

Main category: eess.IV

TL;DR: 研究发现生成模型在MRI重建中容易产生幻觉，通过对抗性扰动可诱导幻觉，且传统图像质量指标无法可靠检测


<details>
  <summary>Details</summary>
Motivation: 生成模型在医学影像重建中应用日益广泛，但已知存在幻觉问题，可能插入原始图像中不存在的特征，在医疗环境中可能危及患者健康，导致误诊

Method: 通过为未处理的输入图像制作类似随机噪声的对抗性扰动，诱导生成模型在重建时产生幻觉；在fastMRI数据集的脑部和膝盖图像上，使用UNet和端到端VarNet架构进行评估

Result: 这些模型对微小扰动高度敏感，容易被诱导产生幻觉；这种脆弱性可能部分解释了幻觉产生的原因；传统图像质量指标无法可靠检测这些幻觉

Conclusion: 精心设计的对抗性训练可能减少幻觉发生；需要开发新方法来检测幻觉何时发生，以确保医学影像重建的安全性

Abstract: Generative models are increasingly used to improve the quality of medical imaging, such as reconstruction of magnetic resonance images and computed tomography. However, it is well-known that such models are susceptible to hallucinations: they may insert features into the reconstructed image which are not actually present in the original image. In a medical setting, such hallucinations may endanger patient health as they can lead to incorrect diagnoses. In this work, we aim to quantify the extent to which state-of-the-art generative models suffer from hallucinations in the context of magnetic resonance image reconstruction. Specifically, we craft adversarial perturbations resembling random noise for the unprocessed input images which induce hallucinations when reconstructed using a generative model. We perform this evaluation on the brain and knee images from the fastMRI data set using UNet and end-to-end VarNet architectures to reconstruct the images. Our results show that these models are highly susceptible to small perturbations and can be easily coaxed into producing hallucinations. This fragility may partially explain why hallucinations occur in the first place and suggests that a carefully constructed adversarial training routine may reduce their prevalence. Moreover, these hallucinations cannot be reliably detected using traditional image quality metrics. Novel approaches will therefore need to be developed to detect when hallucinations have occurred.

</details>


### [2] [4D-UNet improves clutter rejection in human transcranial contrast enhanced ultrasound](https://arxiv.org/abs/2602.18542)
*Tristan Beruard,Armand Delbos,Arthur Chavignon,Maxence Reberol,Vincent Hingot*

Main category: eess.IV

TL;DR: 提出一种用于经颅3D对比增强超声的4D U-Net方法，利用时空信息增强微泡检测，改善传统杂波滤波器在低信噪比数据中的性能


<details>
  <summary>Details</summary>
Motivation: 经颅超声成像受颅骨高吸收限制，传统杂波滤波器在低信噪比超声数据中难以分离血液和组织信号，即使使用对比剂改善血液回声性也难以解决

Method: 采用4D U-Net方法进行经颅3D对比增强超声的杂波滤波，通过4D-UNet实现利用空间和时间信息增强微泡检测

Result: 4D-UNet改善了时间杂波滤波器性能，通过深度学习集成到CEUS中，提供了改进的杂波抑制和可视化效果

Conclusion: 该研究推进了神经血管成像，展示了AI驱动方法增强超声医学成像的潜力，为更准确诊断和更广泛临床应用铺平道路

Abstract: Transcranial ultrasound imaging is limited by high skull absorption, limiting vascular imaging to only the largest vessels. Traditional clutter filters struggle with low signal-to-noise ratio (SNR) ultrasound datasets, where blood and tissue signals cannot be easily separated, even when the echogenicity of the blood is improved with contrast agents. Here, we present a novel 4D U-Net approach for clutter filtering in transcranial 3D Contrast Enhanced Ultrasound (CEUS) exploiting spatial and temporal information via a 4D-UNet implementation to enhance microbubble detection in transcranial data acquired in human adults. Our results show that the 4D-UNet improves temporal clutter filters. By integrating deep learning into CEUS, this study advances neurovascular imaging, offering improved clutter rejection and visualization. The findings underscore the potential of AI-driven approaches to enhance ultrasound-based medical imaging, paving the way for more accurate diagnostics and broader clinical applications.

</details>


### [3] [DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction](https://arxiv.org/abs/2602.18589)
*Jiayang Shi,Daniel M. Pelt,K. Joost Batenburg*

Main category: eess.IV

TL;DR: DM4CT是一个用于CT重建的综合基准测试，评估扩散模型在医学和工业CT重建中的表现，并与传统方法比较，包含真实实验条件下的高分辨率数据集。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在逆问题中表现出强大潜力，但CT重建面临相关噪声、伪影结构、系统几何依赖和数值范围不对齐等实际挑战，需要系统评估扩散模型在该领域的表现。

Method: 引入DM4CT基准测试，包含医学和工业领域的稀疏视图和噪声配置数据集，并采集高能同步辐射设施的高分辨率CT数据集进行真实实验条件评估。对比10种扩散模型方法和7种基线方法。

Result: 提供了对扩散模型在CT重建中行为、优势和局限性的详细分析，建立了公开可用的真实世界数据集和开源代码库。

Conclusion: DM4CT为系统评估扩散模型在CT重建中的表现提供了全面基准，有助于理解其在实际应用中的挑战和潜力，相关数据和代码已开源。

Abstract: Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark ten recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.

</details>


### [4] [TIACam: Text-Anchored Invariant Feature Learning with Auto-Augmentation for Camera-Robust Zero-Watermarking](https://arxiv.org/abs/2602.18863)
*Abdullah All Tanvir,Agnibh Dasgupta,Xin Zhong*

Main category: eess.IV

TL;DR: TIACam：基于文本锚定不变特征学习和自动增强的相机鲁棒零水印框架，通过可学习的自动增强器、文本锚定不变特征学习器和零水印头实现相机重拍场景下的鲁棒水印提取。


<details>
  <summary>Details</summary>
Motivation: 相机重拍会引入复杂的光学退化（如透视变形、光照变化、莫尔干涉等），这对深度水印系统构成挑战。现有方法难以处理这些复杂的相机重拍退化，需要一种更鲁棒的水印方案。

Method: 提出TIACam框架，包含三个核心创新：1）可学习的自动增强器，通过可微分的几何、光度学和莫尔算子发现相机类失真；2）文本锚定不变特征学习器，通过图像和文本之间的跨模态对抗对齐强制语义一致性；3）零水印头，在不修改图像像素的情况下将二进制消息绑定到不变特征空间中。

Result: 在合成和真实世界相机捕获数据上的大量实验表明，TIACam实现了最先进的特征稳定性和水印提取准确性，在多模态不变性学习和物理鲁棒零水印之间建立了原则性桥梁。

Conclusion: TIACam为相机重拍场景下的鲁棒零水印提供了一种统一框架，通过联合优化不变性、语义对齐和水印可恢复性，在复杂光学退化条件下实现了稳定的水印提取。

Abstract: Camera recapture introduces complex optical degradations, such as perspective warping, illumination shifts, and Moiré interference, that remain challenging for deep watermarking systems. We present TIACam, a text-anchored invariant feature learning framework with auto-augmentation for camera-robust zero-watermarking. The method integrates three key innovations: (1) a learnable auto-augmentor that discovers camera-like distortions through differentiable geometric, photometric, and Moiré operators; (2) a text-anchored invariant feature learner that enforces semantic consistency via cross-modal adversarial alignment between image and text; and (3) a zero-watermarking head that binds binary messages in the invariant feature space without modifying image pixels. This unified formulation jointly optimizes invariance, semantic alignment, and watermark recoverability. Extensive experiments on both synthetic and real-world camera captures demonstrate that TIACam achieves state-of-the-art feature stability and watermark extraction accuracy, establishing a principled bridge between multimodal invariance learning and physically robust zero-watermarking.

</details>


### [5] [Automated Disentangling Analysis of Skin Colour for Lesion Images](https://arxiv.org/abs/2602.19055)
*Wenbo Yang,Eman Rezk,Walaa M. Moursi,Zhou Wang*

Main category: eess.IV

TL;DR: 提出皮肤颜色解耦框架，通过压缩解耦学习可操纵的皮肤颜色表示空间，解决皮肤图像颜色不匹配问题，支持反事实编辑和颜色转移。


<details>
  <summary>Details</summary>
Motivation: 皮肤图像中的颜色差异（由光照、相机设置等环境因素和肤色等内在因素引起）会导致机器学习模型性能下降。现有方法无法准确描述这些复杂因素，需要更好的颜色解耦方法。

Method: 1) 基于压缩解耦学习结构化、可操纵的皮肤颜色潜在空间；2) 引入随机化、大多单调的去色映射防止暗色特征信息泄漏；3) 提出几何对齐后处理步骤抑制局部模式的颜色偏移。

Result: 框架能够：1) 实现忠实的反事实编辑（回答"不同皮肤颜色下皮肤状况会是什么样子"）；2) 支持图像间的直接颜色转移；3) 沿物理意义方向（如血液灌注、相机白平衡）进行可控遍历；4) 基于该框架的数据集增强和颜色归一化在病变分类任务中达到竞争性性能。

Conclusion: 提出的皮肤颜色解耦框架能有效处理皮肤图像颜色不匹配问题，支持多种颜色操作任务，并在实际应用中展现出良好的性能。

Abstract: Machine-learning models working on skin images often have degraded performance when the skin colour captured in images (SCCI) differs between training and deployment. Such differences arise from entangled environmental factors (e.g., illumination, camera settings), and intrinsic factors (e.g., skin tone) that cannot be accurately described by a single "skin tone" scalar. To mitigate such colour mismatch, we propose a skin-colour disentangling framework that adapts disentanglement-by-compression to learn a structured, manipulable latent space for SCCI from unlabelled dermatology images. To prevent information leakage that hinders proper learning of dark colour features, we introduce a randomized, mostly monotonic decolourization mapping. To suppress unintended colour shifts of localized patterns (e.g., ink marks, scars) during colour manipulation, we further propose a geometry-aligned post-processing step. Together, these components enable faithful counterfactual editing and answering an essential question: "What would this skin condition look like under a different SCCI?", as well as direct colour transfer between images and controlled traversal along physically meaningful directions (e.g., blood perfusion, camera white balance), enabling educational visualization of skin conditions under varying SCCI. We demonstrate that dataset-level augmentation and colour normalization based on our framework achieve competitive lesion classification performance.

</details>


### [6] [Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images](https://arxiv.org/abs/2602.19891)
*Wen-Liang Lin,Yun-Chien Cheng*

Main category: eess.IV

TL;DR: 提出基于Transformer和Mean-Teacher的无监督域适应框架，通过原型对齐、全局局部对比学习和注意力辅助局部预测模块提升肺栓塞分割的伪标签可靠性，在跨中心数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在肺栓塞计算机辅助诊断中面临"域偏移"问题和专家标注成本高昂的挑战，需要开发能够在不同临床环境中稳健部署的无监督域适应方法。

Method: 采用Transformer主干网络和Mean-Teacher架构，集成三个模块：1) 原型对齐机制减少类别级分布差异；2) 全局局部对比学习捕获像素级拓扑关系和全局语义表示；3) 注意力辅助局部预测模块自动从Transformer注意力图中提取高信息切片，增强对小病灶的敏感性。

Result: 在跨中心数据集(FUMPE->CAD-PE)上IoU从0.1152提升至0.4153，(CAD-PE->FUMPE)从0.1705提升至0.4302。在CT->MRI跨模态任务中，在MMWHS数据集上达到69.9%的Dice分数，无需使用目标域标签进行模型选择。

Conclusion: 提出的无监督域适应框架通过增强伪标签可靠性和学习深度结构信息，显著提升了肺栓塞分割在不同临床环境中的泛化能力和鲁棒性，为实际医疗部署提供了有效解决方案。

Abstract: While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by "domain shift" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [7] [ZUNA: Flexible EEG Superresolution with Position-Aware Diffusion Autoencoders](https://arxiv.org/abs/2602.18478)
*Christopher Warner,Jonas Mago,JR Huml,Mohamed Osman,Beren Millidge*

Main category: eess.SP

TL;DR: ZUNA是一个380M参数的掩码扩散自编码器，用于处理任意电极数量和位置的EEG信号，通过4D旋转位置编码实现时空结构注入，在208个公开数据集上训练，性能优于传统插值方法且能泛化到新数据集。


<details>
  <summary>Details</summary>
Motivation: 解决EEG信号处理中传统插值方法（如球面样条插值）的局限性，以及现有深度学习方法缺乏泛化能力的问题，开发一个能够处理任意电极配置、泛化到新数据集且计算实用的EEG信号重建模型。

Method: 使用掩码扩散自编码器架构，将多通道EEG信号分块为短时间窗口，通过4D旋转位置编码(x,y,z,t)注入时空结构，采用重建和重度通道丢弃的联合目标函数，在208个公开数据集（约200万通道小时）上进行训练。

Result: ZUNA显著优于传统的球面样条插值方法，特别是在高丢弃率下差距更大；相比其他深度学习方法，ZUNA能够泛化到不同数据集和电极位置，可直接应用于新数据集；模型计算实用，已发布Apache-2.0权重和MNE兼容工具栈。

Conclusion: ZUNA是一个通用、可泛化的EEG信号处理模型，能够处理任意电极配置，性能优于传统方法，具有良好的泛化能力和计算实用性，为EEG分析提供了新的工具。

Abstract: We present \texttt{ZUNA}, a 380M-parameter masked diffusion autoencoder trained to perform masked channel infilling and superresolution for arbitrary electrode numbers and positions in EEG signals. The \texttt{ZUNA} architecture tokenizes multichannel EEG into short temporal windows and injects spatiotemporal structure via a 4D rotary positional encoding over (x,y,z,t), enabling inference on arbitrary channel subsets and positions. We train ZUNA on an aggregated and harmonized corpus spanning 208 public datasets containing approximately 2 million channel-hours using a combined reconstruction and heavy channel-dropout objective. We show that \texttt{ZUNA} substantially improves over ubiquitous spherical-spline interpolation methods, with the gap widening at higher dropout rates. Crucially, compared to other deep learning methods in this space, \texttt{ZUNA}'s performance \emph{generalizes} across datasets and channel positions allowing it to be applied directly to novel datasets and problems. Despite its generative capabilities, \texttt{ZUNA} remains computationally practical for deployment. We release Apache-2.0 weights and an MNE-compatible preprocessing/inference stack to encourage reproducible comparisons and downstream use in EEG analysis pipelines.

</details>


### [8] [Heterogeneity-agnostic AI/ML-assisted beam selection for multi-panel arrays](https://arxiv.org/abs/2602.18678)
*Ibrahim Kilinc,Robert W. Heath*

Main category: eess.SP

TL;DR: 提出一种支持天线异质性的统一AI/ML波束选择算法，通过预测与天线配置无关的无线传播特性，解决异构天线硬件带来的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 异构天线硬件（不同尺寸、方向、码本、单元模式和极化角）限制了现有基于位置信息的AI/ML波束选择方法的可行性和泛化能力。需要开发对异质性不敏感的模型，而不是为每种配置单独训练模型。

Method: 1) 推导解耦传播特性与天线配置的RSRP模型；2) 提出优化框架从波束赋形RSRP测量中提取传播变量（AoA、AoD、包含路径增益和信道去极化的矩阵）；3) 开发三阶段自回归网络从用户位置预测这些变量，从而计算任意天线配置的RSRP并进行波束选择。

Result: 仿真结果表明，该异质性不敏感方法在有/无天线异质性的情况下，频谱效率都接近理想选择（genie-aided selection）。

Conclusion: 提出的方法能够有效支持天线异质性，无需为每种天线配置重新训练或维护单独模型，实现了统一的波束选择解决方案。

Abstract: AI/ML-based beam selection methods coupled with location information effectively reduce beam training overhead. Unfortunately, heterogeneous antenna hardware with varying dimensions, orientations, codebooks, element patterns, and polarization angles limits their feasibility and generalization. This challenge requires either a heterogeneity-agnostic model functional under these variations, or developing many models for each configuration, which is infeasible and expensive in practice. In this paper, we propose a unifying AI/ML-based beam selection algorithm supporting antenna heterogeneity by predicting wireless propagation characteristics independent of antenna configuration. We derive a reference signal received power (RSRP) model that decouples propagation characteristics from antenna configuration. We propose an optimization framework to extract propagation variables consisting of angle-of-arrival (AoA), angle-of-departure (AoD), and a matrix incorporating path gain and channel depolarization from beamformed RSRP measurements. We develop a three-stage autoregressive network to predict these variables from user location, enabling RSRP calculation and beam selection for arbitrary antenna configurations without retraining or having a separate model for each configuration. Simulation results show our heterogeneity-agnostic method provides spectral efficiency close to that of genie-aided selection both with and without antenna heterogeneity.

</details>


### [9] [Channel-Correlation-Based Access Point Selection and Pilot Power Allocation for Cell-Free Massive MIMO](https://arxiv.org/abs/2602.18875)
*Saeed Mohammadzadeh,Rodrigo C. De Lamare,Kanapathippillai Cumanan,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: 提出动态AP选择和导频功率分配框架，通过分层聚类算法将AP按信道相关性分组，结合用户容量约束和导频功率优化，提升无蜂窝大规模MIMO系统上行频谱效率


<details>
  <summary>Details</summary>
Motivation: 解决无蜂窝大规模MIMO系统中用户间干扰问题，提高整体频谱效率，同时减少频繁的信道状态信息估计需求，降低网络更新开销，增强系统可扩展性

Method: 开发分层相关性聚类算法将AP按信道相关性分组；引入用户容量约束防止硬件过载；提出导频功率分配策略，将问题建模为加权和速率最大化问题，使用二次变换迭代求解

Result: 数值结果表明，该方法显著提升频谱效率，在高密度多用户场景下保持性能，且比基准方案收敛更快

Conclusion: 提出的动态AP选择和导频功率分配框架有效缓解了用户间干扰，提高了频谱效率，同时通过灵活的AP聚类和用户容量约束增强了系统的可扩展性和实际部署可行性

Abstract: This paper proposes a dynamic access point (AP) selection and pilot power allocation (DAPPA) framework for uplink cell-free massive multiple-input multiple-output (CFmMIMO) systems, aiming to mitigate inter-user interference and improve overall spectral efficiency (SE). A hierarchical correlation-based clustering algorithm is developed to group APs according to their channel correlation, enabling each user to be associated with APs that simultaneously provide strong channel gains and low mutual correlation. This association ensures reliable connectivity, maximizes coherent combining gains, and reduces inter-user interference, while also allowing the number of AP clusters to be adjusted flexibly, without the need to reorganize the network completely. By maintaining links to low-correlated APs, the proposed scheme reduces the need for frequent channel state information (CSI) estimation and minimizes network-wide update overhead. To enhance scalability, a user-capacity constraint per AP is incorporated, preventing hardware overload and alleviating the effects of pilot reuse. Furthermore, an effective pilot power allocation strategy is introduced to boost the signal-to-interference-plus-noise ratio (SINR) during channel training. This is formulated as a weighted sum-rate maximization (WSRM) problem and solved iteratively using a quadratic transform, which enables efficient optimization while ensuring fairness and high-quality service across all users. Numerical results demonstrate that the proposed method delivers significant SE gains, maintains performance in high-density multi-user scenarios, and converges faster than benchmark schemes.

</details>


### [10] [A Spatial Similarity-Guided Pilot Assignment and Access Point Selection for Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.18901)
*Saeed Mohammadzadeh,Kanapathippillai Cumanan,Pei Liu,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: 提出基于信道相似度的导频分配和接入点选择策略，用于提升无小区大规模MIMO系统的上行频谱效率


<details>
  <summary>Details</summary>
Motivation: 在无小区大规模MIMO系统中，导频污染和接入点间干扰限制了频谱效率，特别是在密集网络部署中需要有效的干扰管理策略

Method: 1. 信道相似度感知导频分配：根据用户间信道相似度动态分配导频序列，高相似度用户分配正交导频以减少导频污染；2. 接入点选择算法：优先选择低相关性接入点，减少干扰并增强空间分集，同时保持稳健的用户-接入点连接

Result: 所提出的组合策略显著提高了频谱效率，特别是在密集网络部署中，仿真结果验证了在动态用户场景下的有效性

Conclusion: 信道相似度感知的导频分配和接入点选择策略能有效管理干扰，提升无小区大规模MIMO系统的上行频谱效率，适用于密集网络部署

Abstract: This paper investigates pilot assignment and access point (AP) selection strategies for uplink cell-free massive multiple-input multiple-output (CF-mMIMO) systems. We propose channel similarity-aware pilot assignment (CAPA) and AP selection schemes to improve interference management and, consequently, spectral efficiency (SE). The pilot assignment strategy dynamically allocates pilot sequences by evaluating inter-user channel similarity, ensuring that users (UEs) with high channel similarity are assigned orthogonal pilots to mitigate pilot contamination. Subsequently, an AP selection algorithm is introduced that prioritizes the selection of low-correlation APs to reduce interference and enhance spatial diversity. This selection process maintains robust UE-AP links while minimizing inter-AP redundancy. The combined approach significantly improves SE, particularly in dense network deployments. Simulation results are provided to demonstrate the effectiveness of the proposed strategies under dynamic UE scenarios.

</details>


### [11] [Event-Triggered Gossip for Distributed Learning](https://arxiv.org/abs/2602.19116)
*Zhiyuan Zhai,Xiaojun Yuan,Wei Ni,Xin Wang,Rui Zhang,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 提出基于事件触发的分布式学习框架，通过自适应通信控制减少节点间通信开销，相比全通信基线减少71.61%的传输量


<details>
  <summary>Details</summary>
Motivation: 分布式学习面临节点间通信瓶颈问题，传统方法需要频繁通信，导致通信开销大。需要设计能够自主决定通信时机的机制来降低通信成本

Method: 开发事件触发通信框架，每个节点基于本地模型偏差自主决定何时与邻居交换模型信息，采用自适应通信控制机制，分析非凸目标下的遍历收敛性

Result: 相比最先进的分布式学习方法，通信开销显著降低，累计点对点传输减少71.61%，性能损失很小

Conclusion: 事件触发通信框架能有效减少分布式学习的通信开销，在保持性能的同时大幅降低传输需求，适用于通信受限的分布式网络

Abstract: While distributed learning offers a new learning paradigm for distributed network with no central coordination, it is constrained by communication bottleneck between nodes.
  We develop a new event-triggered gossip framework for distributed learning to reduce inter-node communication overhead. The framework introduces an adaptive communication control mechanism that enables each node to autonomously decide in a fully decentralized fashion when to exchange model information with its neighbors based on local model deviations. We analyze the ergodic convergence of the proposed framework under noconvex objectives and interpret the convergence guarantees under different triggering conditions. Simulation results show that the proposed framework achieves substantially lower communication overhead than the state-of-the-art distributed learning methods, reducing cumulative point-to-point transmissions by \textbf{71.61\%} with only a marginal performance loss, compared with the conventional full-communication baseline.

</details>


### [12] [Downlink Beamforming Design for NOMA Using Convolutional Neural Networks](https://arxiv.org/abs/2602.19136)
*Chentong Li,Saeed Mohammadzadeh,Kanapathippillai Cumanan,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 提出基于CNN的波束赋形设计方法，用于下行NOMA系统，解决发射功率最小化问题，显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 传统NOMA和波束赋形技术中，最优波束赋形方案依赖复杂迭代算法和优化方法，导致计算负担增加和延迟，不适合延迟敏感应用

Method: 采用基于卷积神经网络的方法，利用两种信道状态信息表示作为输入特征，生成归一化波束赋形向量

Result: 仿真结果显示，CNN方案能接近最优标签性能，同时相比传统高复杂度算法显著减少计算时间

Conclusion: CNN方法增强了实时应用的实用性，为未来无线网络中大规模连接提供高效波束赋形解决方案

Abstract: Non-orthogonal multiple access (NOMA) and beamforming are well-established techniques for enabling massive connectivity in future wireless networks. However, many optimal beamforming solutions rely on highly complex iterative algorithms and optimization methods, resulting in an increase in computational burden and latency, making them less suitable for delay-sensitive applications and services. To address these challenges, we propose an effective convolutional neural network (CNN)-based approach for beamforming design in downlink NOMA systems to solve the transmit power minimization problem. The proposed method utilizes two representations of channel state information as input features to produce normalized beamforming vectors. Simulation results show that the CNN-based solution closely approximates the optimal label performance while significantly reducing computational time compared to conventional high-complexity algorithms, enhancing its practicality for real-time applications.

</details>


### [13] [A data-driven model-free physical-informed deep operator network for solving nonlinear dynamic system](https://arxiv.org/abs/2602.19262)
*Jieming Sun,Lichun Li*

Main category: eess.SP

TL;DR: 提出一种基于少量数据的无模型物理信息深度算子网络框架，用于学习非线性动态系统


<details>
  <summary>Details</summary>
Motivation: 现有物理信息深度算子网络通常需要精确数学模型或大量场景数据，但在某些动态系统中难以获得精确公式和大量数据，只能获取少量实验数据或有限数学信息

Method: 首先探索可用数据的短期依赖性，使用代理机器学习模型提取短期依赖关系；然后将该代理模型作为物理信息部分整合到DeepOnet中；最后训练构建的DeepOnet来模拟给定控制输入和初始条件下系统的动态响应

Result: 在不同系统上的数值实验证实，该DeepOnet框架能够有效学习近似某些非线性动态系统的动态响应

Conclusion: 提出了一种基于少量数据的无模型物理信息深度算子网络框架，为解决难以获得精确数学模型和大量数据的动态系统建模问题提供了有效方法

Abstract: The existing physical-informed Deep Operator Networks are mostly based on either the well-known mathematical formula of the system or huge amounts of data for different scenarios. However, in some cases, it is difficult to get the exact mathematical formula and vast amounts of data in some dynamic systems, we can only get a few experimental data or limited mathematical information. To address the cases, we propose a data-driven model-free physical-informed Deep Operator Network (DeepOnet) framework to learn the nonlinear dynamic systems from few available data. We first explore the short-term dependence of the available data and use a surrogate machine learning model to extract the short-term dependence. Then, the surrogate machine learning model is incorporated into the DeepOnet as the physical information part. Then, the constructed DeepOnet is trained to simulate the system's dynamic response for given control inputs and initial conditions. Numerical experiments on different systems confirm that our DeepOnet framework learns to approximate the dynamic response of some nonlinear dynamic systems effectively.

</details>


### [14] [Elevation-Aware Supplementary Uplink for Direct Satellite-to-Device Communications](https://arxiv.org/abs/2602.19427)
*Rajan Shrestha,Hayder Al-Hraishawi*

Main category: eess.SP

TL;DR: 该论文提出了一种基于辅助上行链路(SUL)技术的卫星直连设备(DS2D)通信系统，通过利用LEO卫星轨道几何的可预测性，开发了仰角感知的SUL框架，根据仰角相关的链路裕度估计自适应选择上行链路频段，以增强上行链路鲁棒性同时保持用户设备功率效率。


<details>
  <summary>Details</summary>
Motivation: DS2D上行链路面临长传播距离、严重路径损耗和严格的用户设备功率限制等挑战，特别是在低仰角和波束边缘区域，上行链路可靠性尤为困难。需要一种解决方案来增强上行链路鲁棒性，同时保持用户设备的功率效率。

Method: 提出仰角感知的SUL框架，利用LEO卫星轨道几何的可预测性，根据仰角相关的链路裕度估计自适应调度用户设备在主要上行链路载波或低频SUL载波上传输。引入具有迟滞特性的仰角感知SUL激活算法来指导上行链路载波选择，防止频繁切换。

Result: 仿真结果表明，所提出的SUL框架能够将有效上行链路覆盖范围扩展到低仰角和波束边缘区域，提高卫星过境期间的上行链路可用性，并在现实的用户设备功率约束下实现稳定操作，上行链路切换次数最少。

Conclusion: SUL技术可以成功集成到DS2D系统中，通过仰角感知的自适应载波选择策略，显著增强上行链路鲁棒性，扩展覆盖范围，同时保持用户设备功率效率，为全球卫星直连通信提供可靠的上行链路解决方案。

Abstract: Direct satellite-to-device (DS2D) communication enables standard mobile devices to connect directly to low Earth orbit (LEO) satellites, providing global coverage without reliance on terrestrial infrastructure. However, the DS2D uplink is fundamentally constrained by long propagation distances, severe path loss, and stringent user equipment (UE) power limits, making uplink reliability particularly challenging at low elevation angles and beam edges. This paper investigates the integration of supplementary uplink (SUL) technology into DS2D systems to enhance uplink robustness while preserving UE power efficiency. Leveraging the predictable geometry of LEO satellite orbits, we develop an elevation-aware SUL framework that adapts uplink operation across frequency bands based on elevation-dependent link margin estimates. The proposed approach schedules the UE to transmit on either a primary uplink carrier or a lower-frequency SUL carrier. An elevation-aware SUL activation algorithm with hysteresis is introduced to guide uplink carrier selection while preventing frequent switching. Simulation results demonstrate that the proposed SUL framework extends effective uplink coverage toward low-elevation and beam-edge regions, improves uplink availability over a satellite pass, and achieves stable operation with a minimal number of uplink transitions under realistic UE power constraints.

</details>


### [15] [Dual Security for MIMO-OFDM ISAC Systems: Artificial Ghosts or Artificial Noise](https://arxiv.org/abs/2602.20045)
*Yinchao Yang,Prabhat Raj Gautam,Yathreb Bouazizi,Michael Breza,Julie McCann*

Main category: eess.SP

TL;DR: 提出一个双层双安全ISAC框架，同时保护感知和通信安全，无需窃听者信道信息，通过人工噪声和人工鬼影技术对抗被动窃听。


<details>
  <summary>Details</summary>
Motivation: ISAC系统虽然能高效共享无线资源，但也带来了新的基于感知的安全漏洞。现有研究尚未探索感知和通信安全的联合保护问题，需要解决被动窃听者利用目标回波推断感知参数而不被用户察觉的安全威胁。

Method: 提出双层双安全ISAC框架：1) 联合设计发射波束赋形器，注入人工噪声干扰通信窃听者；2) 故意扭曲感知窃听者可用的参考信号以损害其感知能力；3) 生成具有虚假角度-距离-速度轮廓的人工鬼影，合法接收器可抑制这些鬼影而感知窃听者无法抑制。

Result: 数值结果表明，所提框架能有效增强通信和感知安全性，同时保持通信用户和合法感知接收器的性能。

Conclusion: 该研究首次探索了ISAC系统中感知和通信安全的联合保护问题，提出的双层双安全框架无需窃听者信道信息即可有效对抗被动窃听威胁，为ISAC系统的安全设计提供了新思路。

Abstract: Integrated sensing and communication (ISAC) enables the efficient sharing of wireless resources to support emerging applications, but it also gives rise to new sensing-based security vulnerabilities. Here, potential communication security threats whereby confidential messages intended for legitimate users are intercepted, but also unauthorized receivers (Eves) can passively exploit target echoes to infer sensing parameters without users being aware. Despite these risks, the joint protection of sensing and communication security in ISAC systems remains unexplored. To address this challenge, this paper proposes a two-layer dual-secure ISAC framework that simultaneously protects sensing and communication against passive sensing Eves and communication Eves, without requiring their channel state information (CSI). Specifically, transmit beamformers are jointly designed to inject artificial noise (AN) to introduce interference to communication Eves, while deliberately distorting the reference signal available to sensing Eves to impair their sensing capability. Furthermore, the proposed design generates artificial ghosts (AGs) with fake angle-range-velocity profiles observable by all receivers. Legitimate receivers can suppress these AGs, whereas sensing Eves cannot, thereby significantly reducing their probability of correctly detecting the true targets. Numerical results demonstrate that the proposed framework effectively enhances both communication and sensing security, while preserving the performance of communication users and legitimate sensing receivers.

</details>


### [16] [An LLM-Enabled Frequency-Aware Flow Diffusion Model for Natural-Language-Guided Power System Scenario Generation](https://arxiv.org/abs/2602.19522)
*Zhenghao Zhou,Yiyan Li,Fei Xie,Lu Wang,Bo Wang,Jiansheng Wang,Zheng Yan,Mo-Yuen Chow*

Main category: eess.SP

TL;DR: 提出LFFD框架，使用自然语言指导电力系统场景生成，结合LLM语义转换和流扩散模型，解决传统方法用户便利性和生成灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI场景生成方法（如条件生成对抗网络）主要依赖固定长度的数值条件向量来控制生成结果，面临用户便利性和生成灵活性挑战。需要一种更自然、更灵活的方式来生成多样可控的电力系统场景。

Method: 提出LLM-enabled Frequency-aware Flow Diffusion (LFFD)框架：1) 使用预训练LLM将自然语言生成请求转换为有序语义空间；2) 采用流扩散模型（基于修正流匹配目标）进行高效高质量场景生成；3) 引入频率感知多目标优化算法缓解频率偏差问题；4) 设计双智能体框架创建文本-场景训练样本对并标准化语义评估。

Result: 基于大规模光伏和负荷数据集的实验证明了该方法的有效性，能够根据自然语言描述生成期望的电力系统场景。

Conclusion: LFFD框架通过自然语言指导的场景生成，提高了用户便利性和生成灵活性，为电力系统规划和运行提供了更强大的工具。

Abstract: Diverse and controllable scenario generation (e.g., wind, solar, load, etc.) is critical for robust power system planning and operation. As AI-based scenario generation methods are becoming the mainstream, existing methods (e.g., Conditional Generative Adversarial Nets) mainly rely on a fixed-length numerical conditioning vector to control the generation results, facing challenges in user conveniency and generation flexibility. In this paper, a natural-language-guided scenario generation framework, named LLM-enabled Frequency-aware Flow Diffusion (LFFD), is proposed to enable users to generate desired scenarios using plain human language. First, a pretrained LLM module is introduced to convert generation requests described by unstructured natural languages into ordered semantic space. Second, instead of using standard diffusion models, a flow diffusion model employing a rectified flow matching objective is introduced to achieve efficient and high-quality scenario generation, taking the LLM output as the model input. During the model training process, a frequency-aware multi-objective optimization algorithm is introduced to mitigate the frequency-bias issue. Meanwhile, a dual-agent framework is designed to create text-scenario training sample pairs as well as to standardize semantic evaluation. Experiments based on large-scale photovoltaic and load datasets demonstrate the effectiveness of the proposed method.

</details>


### [17] [Dynamic Sensor Scheduling Based on Node Partitioning of Graphs](https://arxiv.org/abs/2602.19561)
*Ryouke Ikura,Junya Hara,Hiroshi Higashi,Yuichi Tanaka*

Main category: eess.SP

TL;DR: 提出一种基于图信号采样理论的动态传感器调度方法，通过图节点分区生成多个同等信息量的节点子集，以应对电池消耗和传感器故障，并适应信号子空间的变化。


<details>
  <summary>Details</summary>
Motivation: 传感器网络中需要多个同等信息量的节点子集进行顺序激活，以提高网络对集中电池消耗和传感器故障的鲁棒性，同时这些子集的质量会动态变化，需要适应这些变化。

Method: 基于图信号采样理论的图节点分区方法，将问题表述为基于图信号子空间先验的DC优化问题，使用近端DC算法求解，保证收敛到临界点。在线场景中自适应估计信号子空间并顺序更新分区先验。

Result: 在合成和真实传感器网络数据上的数值实验表明，与替代方法相比，所提方法实现了更低的平均均方误差。

Conclusion: 提出的动态传感器调度方法能够有效生成多个同等信息量的节点子集，适应信号子空间的变化，提高传感器网络的鲁棒性和性能。

Abstract: This paper proposes a dynamic sensor scheduling method for sensor networks. In sensor network applications, we often need multiple equally-informative node subsets that are activated sequentially to make a sensor network robust against concentrated battery consumption and sensor failures. In addition, quality of these subsets changes dynamically and thus we must adapt those changes. To find those node subsets, we propose a graph node partitioning method based on sampling theory for graph signals. We aim to minimize the average reconstruction error for signals obtained at all node subsets, in contrast to conventional single subset selection. The graph node partitioning problem is formulated as a difference-of-convex (DC) optimization based on a subspace prior of graph signals, and is solved by the proximal DC algorithm. It guarantees convergence to a critical point. To accommodate the online scenario where the signal subspace and optimal partitioning may change over time, we adaptively estimate the signal subspace from historical data and sequentially update the prior for our partitioning method. Numerical experiments on synthetic and real-world sensor network data demonstrate that the proposed method achieves lower average mean squared errors compared to alternative methods.

</details>


### [18] [Extracting Patterns of Chemical Information from Differential Mobility Spectrometry Measurements under Varying Conditions of Humidity and Temperature](https://arxiv.org/abs/2602.19572)
*Philipp Müller,Gary A. Eiceman,Anton Rauhameri,Anton Kontunen,Antti Roine,Niku Oksala,Antti Vehkaoja,Maiju Lepomäki*

Main category: eess.SP

TL;DR: 研究提出使用回归模型标准化DMS测量数据，以消除湿度和温度变化对手术烟雾分析的影响


<details>
  <summary>Details</summary>
Motivation: DMS技术用于分析手术烟雾，但测量结果受湿度和温度影响，难以在不同环境条件下进行比较。传统方法需要控制环境条件，但这往往不可行。

Method: 分析1852个猪脂肪和肌肉组织手术烟雾的DMS测量数据，确认湿度和温度的影响。对原始和标准化DMS数据拟合回归模型，用于基于已知湿度和温度估计特定组织类型的DMS测量。

Result: 分析证实DMS测量明显依赖于湿度和温度。通过按分离电压标准化DMS测量并训练多元回归模型，可以在特定环境条件下估计手术烟雾的DMS测量。

Conclusion: 该方法首次实现了无需标准化测量条件即可估计DMS测量，为消除环境条件依赖迈出了第一步。

Abstract: Differential Mobility Spectrometry (DMS), also known as Field Asymmetric Ion Mobility Spectrometry, is a rapid and affordable technology for extracting information from gas phase samples containing complex volatile organic compounds, and can therefore be used for analyzing surgical smoke. One obstacle to its widespread application is the dependence of DMS measurements on humidity and, to a lesser degree, temperature, making comparison of data measured under different environmental conditions arbitrary. The commonly used solution is to regulate these environmental conditions to some predefined humidity and temperature levels. However, this approach is often unfeasible or even impossible. Therefore, in this paper we analyzed a dataset of 1,852 DMS measurements of surgical smoke evaporated from porcine adipose and muscle tissue to get an understanding of the impact of varying humidity and temperature on DMS measurements. Our analysis confirmed clear dependence of the measurements on these two factors. To overcome this challenge, we fitted regression models to raw and normalized DMS measurement data. Subsequently, these models were used for estimating DMS measurements for known tissue types based on recorded humidity and temperatures. Our test suggests that it is possible to estimate DMS measurements of surgical smoke from porcine adipose and muscle tissue under specific environmental conditions by standardizing DMS measurements separation voltage-wise and training multivariate regression models on the normalized data, which is the first step in removing the need for standardized measurement conditions.

</details>


### [19] [Active IoT User Detection in Near-Field with Location Information](https://arxiv.org/abs/2602.19613)
*Gabriel Martins de Jesus,Richard Demo Souza,Onel Luis Alcaraz López*

Main category: eess.SP

TL;DR: 本文提出了一种利用用户位置先验知识来增强近场物联网网络中活跃用户检测性能的方法，通过重构用户的视距信道分量来辅助检测过程。


<details>
  <summary>Details</summary>
Motivation: 在近场物联网网络中，传统的活跃用户检测方法可能性能有限。考虑到用户位置信息通常可用（静态用户可直接告知，移动用户可通过定位算法推断），利用这些位置先验知识来重构视距信道分量，有望显著提升检测性能。

Method: 基站利用用户位置估计重构其视距信道分量，结合用户导频序列，增强接收信号与活跃用户之间的相关性。将位置辅助的活跃用户检测建模为凸优化问题，采用交替方向乘子法（ADMM）求解。

Result: 在完美位置估计和强视距信道条件下，所提方法显著优于基线方法。鲁棒性分析表明，即使在位置估计存在误差的情况下，只要误差保持在系统参数确定的范围内，性能增益仍然存在。

Conclusion: 利用用户位置先验知识可以有效提升近场物联网网络中的活跃用户检测性能，该方法在位置估计误差可控的情况下具有鲁棒性，为实际部署提供了可行性。

Abstract: In this paper, we address active users detection (AUD) in near-field Internet of Things (IoT) networks by exploring prior knowledge of users' locations. We consider a scenario where users are distributed in a semi-circular area within the Rayleigh distance of a multi-antenna base station (BS). We propose the BS to use location estimates of the users to reconstruct their line-of-sight (LoS) channel components, hence assisting the AUD process. For this, the BS combines these reconstructed channels with users' pilot sequences, enhancing the correlation between received signals and active users. We formulate the location-aided AUD as a convex optimization problem, solved via the alternating direction method of multipliers (ADMM). {Our proposal has a higher computational complexity compared to the baseline ADMM approach where location information is not used. Moreover, the proposal requires location information of users, which can be readily informed if users are static, or inferred via established localization algorithms if they are mobile.} Simulation results compare our proposal against the baseline across varying systems parameters, such as number of users, pilot length and LoS component strength. We demonstrate that under perfect location estimation and strong LoS, our proposed method significantly outperforms the baseline. Furthermore, robustness analysis shows that performance gains persist under imperfect location estimation, provided the estimation error remains within bounds determined by the system parameters.

</details>


### [20] [Topological Signal Processing for 3D Point Cloud Data](https://arxiv.org/abs/2602.19636)
*Tiziana Cattai,Stefania Sardellitti,Stefania Colonnese,Sergio Barbarossa*

Main category: eess.SP

TL;DR: 将拓扑信号处理框架应用于3D点云分析，通过高阶拉普拉斯算子处理三角形网格上的信号，同时表征颜色属性和几何信息


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时处理3D点云的几何和属性信息，需要一种能够统一表征点云几何和颜色属性的拓扑信号处理方法

Method: 基于离散外微积分理论，引入高阶拉普拉斯算子处理三角形网格上的信号，将颜色属性建模为节点上的3D向量，几何信息建模为三角形重心上的3D向量

Result: 在合成点云上实现了准确的色彩重建，对稀疏数据具有鲁棒性，在噪声点云坐标情况下能够进行几何细化

Conclusion: 提出的方法提供了基于拓扑的表征方式，能够有效处理点云的几何和属性信息，为点云分析提供了新的工具

Abstract: Our goal in this paper is to apply the topological signal processing (TSP) framework to the analysis of 3D Point Clouds (PCs) represented on simplicial complexes. Building on Discrete Exterior Calculus (DEC) theory for vector fields, we introduce higher-order Laplacian operators that enable the processing of signals over triangular meshes. Unlike traditional approaches, the proposed approach allows us to characterize both color attributes, modeled as 3D vectors on nodes, and geometry, modeled as 3D vectors on the barycenter of each triangle. Then, we show as TSP tools may efficiently be used to sample, recover and filter PCs attributes treating them as edge signals. Numerical results on synthetic PCs demonstrate accurate color reconstruction with robustness to sparse data and geometry refinement in the case of noisy PC coordinates. The proposed approach provides a topology-based representation to characterize the geometry and attributes of PCs.

</details>


### [21] [Hardware-Accelerated Geometrical Simulation of Biological and Engineered In-Air Ultrasonic Systems](https://arxiv.org/abs/2602.19652)
*Wouter Jansen,Jan Steckel*

Main category: eess.SP

TL;DR: SonoTraceUE是一个基于Unreal Engine的高保真声学仿真框架，通过硬件加速的射线追踪和蒙特卡洛衍射模型，实现动态多材料环境中近实时的主动和被动声学传感仿真。


<details>
  <summary>Details</summary>
Motivation: 当前工业监测和自主机器人中空中声学传感器的部署日益增长，但现有仿真框架在模拟高频声波传播时面临计算成本高、动态场景支持不足、缺乏复杂衍射和闭环机器人集成等问题。

Method: 开发了基于Unreal Engine的插件SonoTraceUE，采用硬件加速的射线追踪镜面反射模型和基于曲率的蒙特卡洛衍射模型，支持动态多材料环境中的声学传感仿真。

Result: 通过生物声学和机器人实验验证，SonoTraceUE在频谱和空间数据上与真实世界数据具有高度相关性，能够用于合成数据生成、生物声学假设测试和机器人系统快速原型开发。

Conclusion: SonoTraceUE提供了一个多功能平台，解决了现有声学仿真框架的局限性，为生物声学研究和机器人声学传感系统的快速开发提供了高效工具。

Abstract: The deployment of in-air acoustic sensors for industrial monitoring and autonomous robotics has grown significantly, often drawing inspiration from biological echolocation. However, developing and validating these systems in existing simulation frameworks remains challenging due to the computational cost of simulating high-frequency wave propagation in large, dynamic, and complex environments. While wave-based methods offer high accuracy, they scale poorly with frequency and volume. Conversely, existing geometric acoustic solvers often lack support for dynamic scenes, complex diffraction, or closed-loop robotic integration. In this work, we introduce SonoTraceUE, a high-fidelity acoustic simulation framework built as a plugin for Unreal Engine. By using a hardware-accelerated ray tracing-based specular reflection model, and a curvature-based Monte Carlo diffraction model, the system enables near real-time simulation of active and passive acoustic sensing in dynamic, multi-material environments. We validate the framework through two distinct experimental domains: a bioacoustic study and a robotics experiment. Our results demonstrate that SonoTraceUE achieves high correlation with real-world spectral and spatial data. The framework provides a versatile platform for synthetic data generation, hypothesis testing in bioacoustics, and the rapid prototyping of closed-loop robotic systems that use acoustic sensing.

</details>


### [22] [Breaking the CP Limit: Robust Long-Range OFDM Sensing via Interference Cleaning](https://arxiv.org/abs/2602.19877)
*Umut Utku Erdem,Lucas Giroto,Benedikt Geiger,Taewon Jeong,Silvio Mandelli,Christian Karle,Benjamin Nuss,Laurent Schmalen,Thomas Zwick*

Main category: eess.SP

TL;DR: 该论文提出了两种处理OFDM雷达中超出循环前缀范围的远距离目标检测方法，解决了ISI和ICI干扰问题，在计算成本和检测性能之间提供灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 在OFDM雷达和集成感知通信系统中，传统上感知范围受限于循环前缀对应的往返时间。超出该范围的目标回波会引起ISI和ICI干扰，显著降低检测性能，提高雷达图像中的干扰噪声基底，并由于窗口失配减少有用信号功率。现有方法在恢复有用信号和抑制干扰之间存在权衡，特别是在多目标场景中。

Method: 提出了两种框架：1）联合干扰消除与相干补偿方法，这是连续干扰消除算法的高效演进，利用高精度Chirp Z变换估计和频域相干补偿来恢复弱远距离目标；2）基于全重构的滑动窗口方案，通过移动接收窗口捕获最佳信号能量，同时对所有检测到的目标进行全信号重构。

Result: 数值结果表明，两种方法都优于最先进的基准方法。联合干扰消除方法在计算效率和性能之间取得良好平衡，而全重构滑动窗口方案在需要最大精度时提供最佳性能。

Conclusion: 论文提出的两种框架有效解决了OFDM雷达中远距离目标检测的ISI/ICI干扰问题，提供了在计算成本和检测性能之间的灵活权衡，为高动态范围场景下的雷达感知系统提供了实用解决方案。

Abstract: In orthogonal frequency-division multiplexing-based radar and integrated sensing and communication systems, the sensing range is traditionally limited by the round-trip time corresponding to the cyclic prefix duration. Targets whose echoes arrive after this duration induce intersymbol interference (ISI) and associated intercarrier interference (ICI), which significantly degrade detection performance, elevate the interference-noise floor in the radar image, and reduce the useful signal power due to window mismatch. Existing methods face a trade-off between recovering useful signal and suppressing interference, particularly in multi-target scenarios. This paper proposes two frameworks to resolve this dilemma, offering a flexible trade-off between computational cost and target detection performance. First, a signal model is derived, demonstrating that ISI and ICI-oriented interference often dominates thermal noise in high-dynamic-range scenarios. To combat the ISI and ICI-based interference-noise floor increase, joint-interference cancellation with coherent compensation is proposed. This approach is an efficient evolution of the successive-interference cancellation algorithm, utilizing high-precision chirp Z-transform estimation and frequency-domain coherent compensation to recover weak distant targets. For scenarios requiring maximum precision, the full reconstruction-based sliding window scheme is presented, which shifts the receive window to capture optimal signal energy while performing full-signal reconstruction for all detected targets. Numerical results show that both methods outperform state-of-the-art benchmarks.

</details>


### [23] [Rethinking Chronological Causal Discovery with Signal Processing](https://arxiv.org/abs/2602.19903)
*Kurt Butler,Damian Machlanski,Panagiotis Dimitrakopoulos,Sotirios A. Tsaftaris*

Main category: eess.SP

TL;DR: 论文研究因果发现方法对采样率与时间窗口长度的敏感性，指出传统与最新方法都受这些超参数影响，并提出信号处理视角可帮助理解此现象。


<details>
  <summary>Details</summary>
Motivation: 现实世界因果发现通常基于定期观测数据，但观测时间间隔未必与底层生物或物理事件发生时间匹配。研究者希望了解这种时间不匹配对因果发现方法性能的影响。

Method: 通过经验证据与理论分析相结合的方法，研究采样率和时间窗口长度变化对因果发现性能的影响，并探讨如何利用信号处理思想理解这些现象。

Result: 研究发现，无论是经典还是最新的因果发现方法，都对采样率和时间窗口长度这两个超参数表现出敏感性，性能会随着这些参数的变化而波动。

Conclusion: 因果发现方法对采样率与时间窗口长度敏感，需要谨慎选择这些超参数，信号处理视角可为理解这种敏感性提供有价值的见解。

Abstract: Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.

</details>


### [24] [From High-Level Requirements to KPIs: Conformal Signal Temporal Logic Learning for Wireless Communications](https://arxiv.org/abs/2602.20018)
*Jiechen Chen,Michele Polese,Osvaldo Simeone*

Main category: eess.SP

TL;DR: 提出C-STLL框架，通过信号时序逻辑学习和符合性校准，从RAN的KPI数据中学习可解释的时序模式，以预测QoE并保证可靠性。


<details>
  <summary>Details</summary>
Motivation: 软体化无线接入网络（如O-RAN）产生大量KPI数据，但如何将这些低层测量与高层QoE需求联系起来，需要既能捕捉预测性时序模式又具有可解释性的方法。

Method: 提出符合性信号时序逻辑学习（C-STLL）框架，结合STL学习算法和基于Learn Then Test框架的符合性校准过程，通过多重假设检验验证的接受和停止规则，联合优化可靠性、公式复杂度和多样性。

Result: 在移动游戏场景的ns-3网络模拟实验中，C-STLL能有效将风险控制在目标水平以下，同时返回紧凑、多样的可解释时序规范集，将KPI行为与QoE结果关联起来。

Conclusion: C-STLL为RAN优化提供了一种既能保证可靠性又具有可解释性的方法，能够从KPI数据中提取可操作的智能，帮助运营商验证并采取行动。

Abstract: Softwarized radio access networks (RANs), such as those based on the Open RAN (O-RAN) architecture, generate rich streams of key performance indicators (KPIs) that can be leveraged to extract actionable intelligence for network optimization. However, bridging the gap between low-level KPI measurements and high-level requirements, such as quality of experience (QoE), requires methods that are both relevant, capturing temporal patterns predictive of user-level outcomes, and interpretable, providing human-readable insights that operators can validate and act upon. This paper introduces conformal signal temporal logic learning (C-STLL), a framework that addresses both requirements. C-STLL leverages signal temporal logic (STL), a formal language for specifying temporal properties of time series, to learn interpretable formulas that distinguish KPI traces satisfying high-level requirements from those that do not. To ensure reliability, C-STLL wraps around existing STL learning algorithms with a conformal calibration procedure based on the Learn Then Test (LTT) framework. This procedure produces a set of STL formulas with formal guarantees: with high probability, the set contains at least one formula achieving a user-specified accuracy level. The calibration jointly optimizes for reliability, formula complexity, and diversity through principled acceptance and stopping rules validated via multiple hypothesis testing. Experiments using the ns-3 network simulator on a mobile gaming scenario demonstrate that C-STLL effectively controls risk below target levels while returning compact, diverse sets of interpretable temporal specifications that relate KPI behavior to QoE outcomes.

</details>


### [25] [Digital Twin--Driven Adaptive Wavelet Strategy for Efficient 6G Backbone Network Telemetry](https://arxiv.org/abs/2602.20034)
*Alexandre Barbosa de Lima,Xavier Hesselbach,José Roberto de Almeida Amazonas*

Main category: eess.SP

TL;DR: 该论文提出了一种基于MERA张量网络和paraunitary滤波器组的自适应小波学习方法，在保证完美重构和正交性的同时，显著提升了网络流量压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统正交小波虽然保证完美重构，但基于固定基函数，对具有分形谱特征的信号压缩效果不佳；而现有学习方法通常通过软惩罚强制正交性，牺牲了结构保证。

Method: 建立了MERA张量网络与paraunitary滤波器组的严格等价关系，通过流形约束优化学习自适应小波，同时强制执行精确正交性，保证完美重构和能量守恒。

Result: 在长程依赖网络流量验证中，学习到的滤波器在六个MAWI骨干网迹线(2020-2025年，314Mbps-1.75Gbps)上比经典小波提升0.5-3.8dB PSNR，同时保持Hurst指数在估计不确定性内(|ΔH| ≤ 0.03)。

Conclusion: MERA启发的小波为6G数字孪生同步中的遥测压缩提供了一种原则性方法，在保持信号统计特性的同时实现更好的压缩性能。

Abstract: Classical orthogonal wavelets guarantee perfect reconstruction but rely on fixed bases optimized for polynomial smoothness, achieving suboptimal compression on signals with fractal spectral signatures. Conversely, learned methods offer adaptivity but typically enforce orthogonality via soft penalties, sacrificing structural guarantees.
  This work establishes a rigorous equivalence between Multiscale Entanglement Renormalization Ansatz (MERA) tensor networks and paraunitary filter banks. The resulting framework learns adaptive wavelets while enforcing exact orthogonality through manifold-constrained optimization, guaranteeing perfect reconstruction and energy conservation throughout training.
  Validation on Long-Range Dependent (LRD) network traffic demonstrates that learned filters outperform classical wavelets by 0.5--3.8~dB PSNR on six MAWI backbone traces (2020--2025, 314~Mbps--1.75~Gbps) while preserving the Hurst exponent within estimation uncertainty ($|ΔH| \le 0.03$). These results establish MERA-inspired wavelets as a principled approach for telemetry compression in 6G digital twin synchronization.

</details>


### [26] [On the Spatial Consistency of Sub-Terahertz Channel Characteristics for Beyond-6G Systems](https://arxiv.org/abs/2602.20039)
*Hossein Amininasab,Huda Farooqui,Dmitri Moltchanov,Sergey Andreev,Michele Polese,Mikko Valkama,Josep M. Jornet*

Main category: eess.SP

TL;DR: 实验研究表明，在140-150 GHz频段的室内环境中，信道特性（延迟扩展、角度延迟扩展、K因子）在数十厘米距离内变化很小，这意味着在稳定视距方向可以使用10-50波长的粗网格进行射线追踪建模，从而大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在亚太赫兹频段（100-300 GHz）进行射线追踪信道建模时，由于波长接近毫米级，理论上需要在波长尺度上进行密集采样，导致计算量极大。然而实际信道特性可能在更大距离上保持相似，本研究旨在通过实验验证亚太赫兹信道特性的空间一致性程度，以优化建模效率。

Method: 在140-150 GHz频段的室内大厅环境中进行了大规模测量活动，在2.5毫米到1米的不同分离距离上测量信道特性，分析延迟扩展、角度延迟扩展和K因子等参数的空间变化。

Result: 测量结果显示，在考虑的室内环境中，信道特性在数十厘米距离内变化很小。在稳定的视距方向，网格分辨率可以放宽到10-50个波长（145 GHz），而在非视距主导区域需要更精细的分辨率。对于较粗的网格，需要高级插值来捕捉快速变化的散射分量。

Conclusion: 亚太赫兹信道在室内环境中表现出显著的空间一致性，这为优化射线追踪建模提供了重要依据。通过适当放宽网格分辨率，可以大幅降低计算复杂度，同时保持建模精度，特别是在视距稳定的区域。

Abstract: Ray tracing is a versatile approach for precise sub-terahertz (sub-THz, 100-300 GHz) channel modeling when designing new mechanisms for beyond-6G cellular systems. Theoretically, wireless channels may exhibit variations over wavelength distances. In the sub-THz band, close-to-millimeter wavelengths thus require extremely large computational efforts for ray-tracing modeling. However, in practice, channel characteristics may remain quantitatively similar over much larger distances, which can drastically decrease computational efforts. The aim of this study is to experimentally characterize the degree of spatial consistency in sub-THz channel characteristics. To this end, we performed a large-scale measurement campaign in the 140-150 GHz frequency band in an indoor-hall (InH) environment and characterized the channel at separation distances from 2.5 mm up to 1 m. Our results show that channel characteristics including delay spread, angular delay spread, and K-factor change only slightly over multiple tens of centimeter distances. This implies that, in the considered InH environment, the mesh grid can be in the range of 10-50 wavelengths (at 145 GHz) along stable line-of-sight (LoS) directions, while a finer resolution is needed in regions not dominated by LoS. For coarser grids, advanced interpolation is required to capture rapidly varying scattered components.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [27] [Turbo Coded Single Sideband OFDM-OQAM Signaling through Frequency Selective Rayleigh Fading Channels](https://arxiv.org/abs/2602.18881)
*Kasturi Vasudevan*

Main category: cs.IT

TL;DR: 研究在存在载波频率偏移和高斯噪声的频率选择性瑞利衰落信道中，Turbo编码OFDM-OQAM信号的误码率性能，提出使用根升余弦脉冲及其希尔伯特变换作为复值发射滤波器。


<details>
  <summary>Details</summary>
Motivation: 研究在频率选择性瑞利衰落信道中，存在载波频率偏移(CFO)和加性高斯白噪声(AWGN)的情况下，Turbo编码OFDM-OQAM信号的误码率性能。此前没有类似的研究工作。

Method: 使用根升余弦(RRC)脉冲及其希尔伯特变换作为复值发射滤波器，接收端使用简单的匹配滤波器。系统类似于单边带调制。采用Turbo码和子载波分集来改善误码率性能。提出了帧检测、两步CFO、信道和噪声方差估计的离散时间算法。假设单发单收天线。

Result: 提出了一个完整的系统方案，包括使用RRC脉冲的复值滤波器设计、Turbo编码、子载波分集以及相应的估计算法，用于在存在CFO和AWGN的频率选择性衰落信道中改善OFDM-OQAM系统的误码率性能。

Conclusion: 该研究首次在存在载波频率偏移和高斯噪声的频率选择性瑞利衰落信道中，研究了Turbo编码OFDM-OQAM系统的性能，提出了基于RRC脉冲的复值滤波器设计和相应的估计算法，为改善系统误码率性能提供了有效方案。

Abstract: This work investigates the bit-error-rate (BER) performance of turbo coded orthogonal frequency division multiplexed - offset quadrature amplitude modulated (OFDM- OQAM) signals transmitted through frequency selective Rayleigh fading channels in the presence of carrier frequency offset (CFO) and additive white Gaussian noise (AWGN). The highlight of this work is to use the root raised cosine (RRC) pulse and its Hilbert transform as the complex-valued transmit filter and a simple matched filter at the receiver. The proposed system is similar to single sideband (SSB) modulation, that has roots in analog communications. Turbo code and subcarrier diversity is employed to improve the BER performance over that of an uncoded system. Discrete-time algorithms for frame detection, two-step CFO, channel and noise variance estimation have been proposed. A single transmit and receive antenna is assumed. Similar work has not been done earlier.

</details>


### [28] [Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs](https://arxiv.org/abs/2602.19137)
*Jianfeng Xu*

Main category: cs.IT

TL;DR: 该论文引入推导深度作为衡量基于给定前提回答查询所需推理努力的可计算度量，建立了存储与计算之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对推理过程中计算努力的可量化度量，难以优化知识系统中的存储与计算权衡。需要一种能够衡量回答查询所需推理复杂度的指标，以指导缓存策略和系统优化。

Method: 1. 将信息建模为连接抽象知识与物理载体的双层结构，区分核心事实与操作捷径；2. 为任何有限前提基定义并证明推导深度的可计算性；3. 通过编码推理轨迹和应用信息论不可压缩性论证，建立深度与查询描述复杂度的基本界限；4. 将最优缓存分配表述为具有近似保证的数学优化问题；5. 扩展框架以处理噪声或不完整知识库。

Result: 1. 推导深度是可计算的度量；2. 对于频繁访问的信息丰富查询，最小描述长度与深度乘以知识库大小的对数成正比；3. 确定了临界阈值，超过该阈值的查询缓存比重新计算更便宜；4. 提供了具有近似保证的最优缓存分配解决方案；5. 框架可扩展到噪声或不完整知识库。

Conclusion: 推导深度为量化推理努力提供了理论基础，揭示了存储与计算之间的基本权衡关系。该框架不仅提供了理论界限，还产生了实用的缓存优化策略，可应用于各种知识系统以提高效率。

Abstract: We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases.

</details>


### [29] [Physics-Compliant Modeling and Optimization of MIMO Systems Aided by Microwave Linear Analog Computers](https://arxiv.org/abs/2602.19379)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文提出了考虑天线互耦的微波线性模拟计算机（MiLAC）辅助MIMO系统的物理合规模型，证明了互耦对MiLAC有益，并推导了全局最优的互耦感知优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有MiLAC辅助通信研究依赖理想信道模型并忽略天线互耦效应，但MiLAC在射频处理时互耦变得至关重要，不仅影响信道特性，还会改变MiLAC实现的线性变换操作。

Method: 采用多端口网络理论建立考虑互耦的MiLAC辅助MIMO系统物理合规模型，推导发射端、接收端或两端使用MiLAC时的端到端系统模型，并构建互耦感知的MiLAC优化问题，获得最大化接收信号功率的闭式全局最优解。

Result: 建立了三个关键理论结果：1）互耦在MiLAC辅助系统中平均有益；2）考虑互耦时，MiLAC性能等同于配备匹配网络的数字架构，但使用更少的射频链；3）考虑互耦时，MiLAC始终优于无匹配网络的数字架构。数值仿真验证了理论发现。

Conclusion: 互耦在MiLAC辅助MIMO系统中具有积极作用，通过物理合规建模和优化，MiLAC能以更少射频链实现与配备匹配网络的数字架构相当的性能，为实际系统设计提供了理论基础。

Abstract: Microwave linear analog computer (MiLAC) has emerged as a promising architecture for implementing linear multiple-input multiple-output (MIMO) processing in the analog domain, with radio frequency (RF) signals. Existing studies on MiLAC-aided communications rely on idealized channel models and neglect antenna mutual coupling. However, since MiLAC performs processing at RF, mutual coupling becomes critical and alters the implemented operation, not only the channel characteristics. In this paper, we develop a physics-compliant model for MiLAC-aided MIMO systems accounting for mutual coupling with multiport network theory. We derive end-to-end system models for scenarios with MiLACs at the transmitter, the receiver, or both, showing how mutual coupling impacts the linear transformation implemented by the MiLACs. Furthermore, we formulate and solve a mutual coupling aware MiLAC optimization problem, deriving a closed-form globally optimal solution that maximizes the received signal power. We establish the fundamental performance limits of MiLAC with mutual coupling, and derive three analytical results. First, mutual coupling is beneficial in MiLAC-aided systems, on average. Second, with mutual coupling, MiLAC performs as digital architectures equipped with a matching network, while having fewer RF chains. Third, with mutual coupling, MiLAC always outperforms digital architectures with no matching network. Numerical simulations confirm our theoretical findings.

</details>


### [30] [Toward a Quiet Wireless World: Multi-Cell Pinching-Antenna Transmission](https://arxiv.org/abs/2602.19459)
*Zhiguo Ding*

Main category: cs.IT

TL;DR: 该论文提出使用捏合天线技术替代传统天线进行多小区干扰管理，通过让收发对近距离通信实现低功耗传输，创造"安静"的无线世界。


<details>
  <summary>Details</summary>
Motivation: 传统天线多小区干扰管理需要基站以高功率传输来克服大尺度路径损耗，特别是对小区边缘用户，导致功耗过高。

Method: 采用捏合天线技术进行多小区传输，使每个收发对能够近距离通信，从而降低传输功率需求。

Result: 捏合天线技术能够满足用户服务质量要求的同时显著降低传输功率，实现"低语"式通信而非"高喊"式传输。

Conclusion: 捏合天线技术在多小区干扰管理中具有显著优势，能够创建低功耗的"安静"无线通信环境。

Abstract: Conventional-antenna-based multi-cell interference management can lead to excessive power consumption. For example, in order to serve those users which are close to the cell edge, base stations often must transmit at very high power levels to overcome severe large-scale path-loss, i.e., the base stations have to ``shout" at the users to realize the users' target quality of service (QoS). This letter focuses on the application of pinching antennas to multi-cell interference management and demonstrates that the use of multi-cell pinching-antenna transmission leads to a quiet wireless world. In particular, each transceiver pair can be positioned in close proximity, and hence the users' QoS requirements can be met with only low transmit power, i.e., via ``whispering" rather than high-power transmission.

</details>


### [31] [Physics-Aware, Shannon-Optimal Compression via Arithmetic Coding for Distributional Fidelity](https://arxiv.org/abs/2602.19476)
*Cristiano Fanelli*

Main category: cs.IT

TL;DR: 提出使用算术编码进行无损可逆压缩，通过压缩长度差异评估数据集间的分布一致性，作为生成AI合成数据保真度的度量工具。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI广泛用于合成数据，需要严格验证合成数据与原始数据的保真度。数据量和维度增长使传统验证方法面临挑战，需要一种全局、可解释的保真度度量方法。

Method: 采用算术编码对数据集进行无损可逆压缩，基于物理信息概率表示。比较数据集在相同物理相关模型下的最优压缩长度，差异表现为不可约的额外编码长度，以此作为保真度度量。

Result: 该方法定义的保真度度量是全局的、可解释的、可加分的，且在香农意义下渐近最优。压缩比优于传统通用算法如gzip，并能通过预期负对数似然差异解释编码长度差异。

Conclusion: 基于算术编码的无损物理感知压缩不仅是压缩工具，更是测量数据集间保真度的仪器，为验证生成AI合成数据提供有效方法。

Abstract: Assessing whether two datasets are distributionally consistent has become a central theme in modern scientific analysis, particularly as generative artificial intelligence is increasingly used to produce synthetic datasets whose fidelity must be rigorously validated against the original data on which they are trained, a task made more challenging by the continued growth in data volume and problem dimensionality. In this work, we propose the use of arithmetic coding to provide a lossless and invertible compression of datasets under a physics-informed probabilistic representation. Datasets that share the same underlying physical correlations admit comparable optimal descriptions, while discrepancies in those correlations-arising from miscalibration, mismodeling, or bias-manifest as an irreducible excess in code length. This excess codelength defines an operational fidelity metric, quantified directly in bits through differences in achievable compression length relative to a physics-inspired reference distribution. We demonstrate that this metric is global, interpretable, additive across components, and asymptotically optimal in the Shannon sense. Moreover, we show that differences in codelength correspond to differences in expected negative log-likelihood evaluated under the same physics-informed reference model. As a byproduct, we also demonstrate that our compression approach achieves a higher compression ratio than traditional general-purpose algorithms such as gzip. Our results establish lossless, physics-aware compression based on arithmetic coding not as an end in itself, but as a measurement instrument for testing the fidelity between datasets.

</details>


### [32] [Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding](https://arxiv.org/abs/2602.19626)
*Roberto Tacconelli*

Main category: cs.IT

TL;DR: Nacrith是一个无损压缩系统，结合了135M参数Transformer语言模型与轻量级在线预测器，通过多项技术创新在文本压缩上超越了传统方法和现有神经压缩方法。


<details>
  <summary>Details</summary>
Motivation: 传统无损压缩方法如gzip、bzip2等存在压缩率限制，而现有基于LLM的压缩方法通常需要大型模型和大量计算资源。Nacrith旨在开发一个高效、轻量级的神经压缩系统，能够在消费级GPU上运行，同时实现超越传统方法和现有神经压缩方法的压缩性能。

Method: Nacrith结合了135M参数Transformer语言模型(SmolLM2-135M)与轻量级在线预测器集合和32位算术编码器。主要创新包括：1) CDF精度从2^16提升到2^24，减少量化开销；2) 令牌级N-gram模型用于快速局部预测；3) 自适应对数空间偏置头通过在线梯度下降校正每文档LLM错误；4) 基于置信度的LLM跳过机制加速高可预测令牌；5) 混合二进制格式(NC06)扩展神经压缩到任意二进制文件；6) llama.cpp推理后端实现比PyTorch快7倍的单令牌解码；7) 支持最多8个worker的并行多GPU压缩；8) 原生KV缓存滑动窗口减少每次滑动成本37倍。

Result: 在alice29.txt(152KB)上，Nacrith达到0.918 bits per byte(bpb)，比gzip好3.1倍，比bzip2好2.5倍，比CMIX v21好44%，比ts_zip好20%，并且压缩率低于0阶、1阶和2阶字节级香农熵界。在enwik8(100MB)上，达到0.9389 bpb(11.74%)，比ts_zip(~1.11 bpb)好15%，比FineZip(1.024 bpb)好8%，尽管使用了60倍小的模型且无需微调。在模型训练截止后发布的文档上的分布外评估确认这些收益不是记忆伪影，在未见文本上达到0.723 bpb。

Conclusion: Nacrith展示了通过精心设计的系统优化，即使是相对较小的语言模型也能在无损压缩任务上超越传统方法和现有神经压缩方法。该系统的高效性使其能够在消费级GPU上运行，为实际应用中的神经压缩提供了可行的解决方案。

Abstract: We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.
  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.

</details>


### [33] [Secure Communications, Sensing, and Computing Towards Next-Generation Networks](https://arxiv.org/abs/2602.19942)
*Ruiqi Liu,Beixiong Zheng,Jemin Lee,Si-Hyeon Lee,Georges Kaddoum,Onur Günlü,Deniz Gündüz*

Main category: cs.IT

TL;DR: 该论文全面调查了集成无线通信-感知-计算系统中的安全与隐私威胁及防护措施，涵盖物理层安全、语义通信、感知安全、分布式计算安全，并提出统一安全框架。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络正从单纯连接向集成感知与计算能力演进，这种融合带来了新的安全挑战。系统复杂度增加、攻击面扩大、数据密集型AI应用带来的隐私问题，都使得集成系统的安全防护变得至关重要。

Method: 采用系统性文献综述方法，首先回顾通信网络的物理层安全技术，然后分析语义和语用通信的安全隐私影响及其跨层设计方法。针对感知功能，从信号源、传播信道和感知目标三个层面识别风险并总结防护策略。针对分布式计算需求，讨论安全编码计算方法。最后提出面向集成通信-感知-计算架构的统一安全框架。

Result: 论文系统梳理了集成无线系统中的各类安全隐私威胁，包括通信安全、感知安全、计算安全等方面，并总结了相应的防护技术和策略。提出了端到端的统一安全框架，为未来无线系统的保护提供了全面视角。

Conclusion: 集成通信-感知-计算系统虽然带来了新的应用可能性，但也引入了复杂的安全挑战。需要跨层、跨功能的安全设计方法，统一的端到端安全框架对于保护未来无线系统至关重要。安全编码计算等技术为解决分布式计算风险提供了有效途径。

Abstract: Next-generation wireless networks are progressing beyond conventional connectivity to incorporate emerging sensing and computing capabilities. This convergence gives rise to integrated systems that enable not only uninterrupted communication, but also environmental awareness, intelligent decision-making, and novel applications that take advantage of these combined features. At the same time, this integration brings substantial security challenges. As computing, sensing, and communication become more tightly intertwined, the overall complexity of the system increases, creating new vulnerabilities and expanding the attack surface. The widespread deployment of data-heavy artificial intelligence applications further amplifies concerns regarding data security and privacy. This paper presents a comprehensive survey of security and privacy threats, along with potential countermeasures, in integrated wireless systems. We first review physical-layer security techniques for communication networks, and then investigate the security and privacy implications of semantic and pragmatic communications and their associated cross-layer design methodologies. For sensing functionalities, we pinpoint security and privacy risks at the levels of signal sources, propagation channels, and sensing targets, and summarize state-of-the-art defense strategies for each. The growing computational requirements of these applications drive the need for distributed computing over the network, which introduces additional risks such as data leakage, weak authentication, and multiple points of failure. We subsequently discuss secure coded computing approaches that can help overcome several of these challenges. Finally, we introduce unified security frameworks tailored to integrated communication-sensing-computing architectures, offering an end-to-end perspective on protecting future wireless systems.

</details>


### [34] [Enormous Fluid Antenna Systems (E-FAS)--Part II: Channel Estimation](https://arxiv.org/abs/2602.20127)
*Farshad Rostami Ghadi,Kai-Kit Wong,Masoud Kaveh,Hao Xu,Baiyang Liu,Kin-Fai Tong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: E-FAS系统在非完美信道估计下的性能分析，揭示了SNR饱和和干扰限制现象，但E-FAS仍保持显著性能优势


<details>
  <summary>Details</summary>
Motivation: 现有研究在完美CSI下展示了E-FAS的巨大功率增益，但实际信道获取对E-FAS性能的影响尚未充分探索

Method: 开发了基于导频的信道估计框架，推导了MMSE信道估计及其误差的闭式表达式，分析了单用户和多用户场景下的性能

Result: 单用户场景存在SNR饱和现象，多用户场景在高SNR下变为干扰限制，但E-FAS仍保持显著性能优势并提供鲁棒性

Conclusion: 尽管存在CSI不完美和训练开销，E-FAS仍保持显著性能优势，其放大的大规模信道增益提供了鲁棒性

Abstract: Enormous fluid antenna systems (E-FAS) have recently emerged as a new wireless architecture in which intelligent metasurfaces act as guided electromagnetic interfaces, enabling surface-wave (SW) propagation with much lower attenuation and more control than conventional space-wave transmission. While prior work has reported substantial power gains under perfect channel state information (CSI), the impact of practical channel acquisition on E-FAS performance remains largely unexplored. This paper presents the first comprehensive analysis of E-FAS-assisted downlink transmission under pilot-based channel estimation. We develop an estimation framework for the equivalent end-to-end channel and derive closed-form expressions for the statistics of the minimum mean-square-error (MMSE) channel estimate and its estimation error. Building on these results, we analyze both single-user and multiuser operation while explicitly accounting for the training overhead. For the single-user case, we characterize the outage probability and achievable rate with imperfect CSI, and reveal an inherent signal-to-noise ratio (SNR) saturation phenomenon caused by residual self-interference. For the multiuser case, we study zero-forcing (ZF) precoding based on imperfect channel estimates and show that the system becomes interference-limited in the high SNR regime because of residual inter-user interference. Furthermore, we quantify the trade-off between spatial multiplexing gains and pilot overhead when the number of users increases. Analytical findings are validated via Monte Carlo simulations and benchmarked against least-squares (LS) estimation and conventional non-E-FAS transmission. The results reveal that despite CSI imperfections and training costs, E-FAS retains substantial performance advantages and provides robustness enabled by its amplified large-scale channel gain.

</details>
