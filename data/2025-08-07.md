<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [cs.IT](#cs.IT) [Total: 7]
- [eess.IV](#eess.IV) [Total: 22]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 论文介绍了ETRI Lifelog Dataset 2024，通过智能设备被动收集数据，结合主观报告，用于研究人类日常生活模式。


<details>
  <summary>Details</summary>
Motivation: 提升对人类身心健康状态的准确理解，支持健康研究。

Method: 使用智能手机、智能手表和睡眠传感器被动收集24小时数据，并结合主观调查。

Result: 建立了综合的生命日志数据集，部分数据已公开供研究使用。

Conclusion: 数据集为研究人类日常生活模式提供了基础资源，可用于机器学习预测睡眠质量和压力。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [2] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 该研究提出了一种非侵入性、可解释的机器学习框架，利用多模态可穿戴传感器检测自主神经反射异常（AD），显著优于基线模型，为脊髓损伤患者的实时监测提供了重要进展。


<details>
  <summary>Details</summary>
Motivation: 自主神经反射异常（AD）是脊髓损伤患者中一种潜在危及生命的状况，目前监测方法多为侵入性或依赖主观报告，限制了日常应用。

Method: 研究使用多模态传感器（ECG、PPG、BioZ等）收集数据，通过BorutaSHAP进行特征选择，并采用堆叠集成模型进行训练和验证。

Result: HR和ECG特征最具信息量，集成模型性能最高（Macro F1 = 0.77），HR的AUC达到0.93。

Conclusion: 该框架为脊髓损伤患者的个性化实时监测提供了可行方案，具有临床潜力。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


### [3] [Zak-OTFS over CP-OFDM](https://arxiv.org/abs/2508.03906)
*Saif Khan Mohammed,Saurabh Prakash,Muhammad Ubadah,Imran Ali Khan,Ronny Hadani,Shlomo Rakib,Shachar Kons,Yoav Hebron,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS调制在下一代通信系统中高延迟/多普勒扩展场景下表现优于CP-OFDM，提出了一种低复杂度的Zak-OTFS over CP-OFDM架构，兼容现有网络基础设施。


<details>
  <summary>Details</summary>
Motivation: 解决Zak-OTFS调制在现有CP-OFDM调制解调器中的实际应用挑战，同时保持其在高延迟/多普勒扩展场景下的性能优势。

Method: 通过限制脉冲成形为sinc滤波和时间矩形窗，将Zak-OTFS调制实现为CP-OFDM的低复杂度预编码器，解调器则通过匹配滤波和矩形窗实现。

Result: 提出的Zak-OTFS over CP-OFDM架构能以低复杂度实现，且CP-OFDM是其特例（延迟周期取最小值时）。

Conclusion: 该架构为现有网络基础设施提供了Zak-OTFS的性能优势，同时保持了实现的低复杂度。

Abstract: Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to
achieve significantly better performance compared to the standardized
Cyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high
delay/Doppler spread scenarios envisaged in next generation communication
systems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)
domain, characterized by two parameters, (i) the pulse period along the delay
axis (``delay period") (Doppler period is related to the delay period), and
(ii) the pulse shaping filter. An important practical challenge is enabling
support for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper
we show that Zak-OTFS modulation with pulse shaping constrained to sinc
filtering (filter bandwidth equal to the communication bandwidth $B$) followed
by time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is
the symbol duration and $T_{cp}$ is the CP duration), can be implemented as a
low-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS
de-modulator with matched filtering constrained to sinc filtering (filter
bandwidth $B$) followed by rectangular time windowing over duration $T$ can be
implemented as a low-complexity post-processing of the CP-OFDM de-modulator
output. This proposed ``Zak-OTFS over CP-OFDM" architecture enables us to
harness the benefits of Zak-OTFS in existing network infrastructure. We also
show that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with
CP-OFDM being a special case when the delay period takes its minimum possible
value equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum
delay period.

</details>


### [4] [Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints](https://arxiv.org/abs/2508.04046)
*Xiao Tong,Lei Lei,Ang Li,A. Lee Swindlehurst,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 论文研究了基于构造性干扰的波形设计，针对多用户MIMO系统中的PSK和QAM符号，提出了一种非线性优化框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性CI预编码方法因严格的符号级功率限制或自由度不足而性能受限，需改进。

Method: 提出非线性波形优化框架，引入额外变量，最大化CI指标，并通过QP问题和改进的ADMM算法求解。

Result: 仿真显示，新算法在高阶调制和大块长度下显著优于传统CI-SLP和CI-BLP方法。

Conclusion: 提出的非线性框架和高效算法有效解决了现有方法的局限性，提升了系统性能。

Abstract: This paper investigates constructive interference (CI)-based waveform design
for phase shift keying and quadrature amplitude modulation symbols under
relaxed block-level power constraints in multi-user multiple-input
single-output (MU-MIMO) communication systems. Existing linear CI-based
precoding methods, including symbol-level precoding (SLP) and block-level
precoding (BLP), suffer from performance limitations due to strict symbol-level
power budgets or insufficient degrees of freedom over the block. To overcome
these challenges, we propose a nonlinear waveform optimization framework that
introduces additional optimization variables and maximizes the minimum CI
metric across the transmission block. The optimal waveform is derived in closed
form using the function and Karush Kuhn Tucker conditions, and the solution is
explicitly expressed with respect to the dual variables. Moreover, the original
problems are equivalently reformulated as tractable quadratic programming (QP)
problems. To efficiently solve the derived QP problems, we develop an improved
alternating direction method of multipliers (ADMM) algorithm by integrating a
linear-time projection technique, which significantly enhances the
computational efficiency. Simulation results demonstrate that the proposed
algorithms substantially outperform the conventional CI-SLP and CI-BLP
approaches, particularly under high-order modulations and large block lengths.

</details>


### [5] [WiFo-CF: Wireless Foundation Model for CSI Feedback](https://arxiv.org/abs/2508.04068)
*Liu Xuanyu,Gao Shijian,Liu Boxun,Cheng Xiang,Yang Liuqing*

Main category: eess.SP

TL;DR: WiFo-CF提出了一种基于深度学习的无线基础模型，通过多用户、多速率自监督预训练和S-R MoE架构，解决了传统CSI反馈方案在异构配置下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统CSI反馈方案在异构配置（如不同信道维度、反馈速率和数据分布）下表现不佳，缺乏灵活性和泛化能力。

Method: 采用多用户、多速率自监督预训练策略和S-R MoE架构，支持异构配置的统一处理。

Result: WiFo-CF在模拟和实际场景中均表现出色，支持下游任务（如CSI室内定位）。

Conclusion: WiFo-CF具有优异的泛化能力和部署潜力，适用于多种无线通信场景。

Abstract: Deep learning-based channel state information (CSI) feedback schemes
demonstrate strong compression capabilities but are typically constrained to
fixed system configurations, limiting their generalization and flexibility. To
address this challenge, WiFo-CF, a novel wireless foundation model tailored for
CSI feedback, is proposed, uniquely accommodating heterogeneous configurations
such as varying channel dimensions, feedback rates, and data distributions
within a unified framework through its key innovations: (1) a multi-user,
multi-rate self-supervised pre-training strategy; and (2) a Mixture of Shared
and Routed Expert (S-R MoE) architecture. Supporting the large-scale
pre-training of WiFo-CF is the first heterogeneous channel feedback dataset,
whose diverse patterns enable the model to achieve superior performance on both
in-distribution and out-of-distribution data across simulated and real-world
scenarios. Furthermore, the learned representations effectively facilitate
adaptation to downstream tasks such as CSI-based indoor localization,
validating WiFo-CF's scalability and deployment potential.

</details>


### [6] [DFT-s-OFDM with Chirp Modulation](https://arxiv.org/abs/2508.04075)
*Yujie Liu,Yong Liang Guan,David González G.,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出了一种新的波形DFT-s-OFDM-CM，结合了啁啾调制，提高了频谱效率并保持了低PAPR和频率多样性。


<details>
  <summary>Details</summary>
Motivation: 为下一代无线通信设计一种既能保持低PAPR和频率多样性，又能提高频谱效率的波形。

Method: 通过Q进制星座符号和啁啾信号的起始频率传递信息比特，结合啁啾调制与DFT-s-OFDM。

Result: 仿真显示DFT-s-OFDM-CM在保持类似BER的同时提高了频谱效率，且在相同频谱效率下，通过分信息流降低了BER。

Conclusion: DFT-s-OFDM-CM是一种高效的波形设计，适用于未来无线通信系统。

Abstract: In this paper, a new waveform called discrete Fourier transform spread
orthogonal frequency division multiplexing with chirp modulation
(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.
The information bits are conveyed by not only Q-ary constellation symbols but
also the starting frequency of chirp signal. It could maintain the benefits
provided by the chirped discrete Fourier transform spread orthogonal frequency
division multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio
(PAPR), full frequency diversity exploitation, etc. Simulation results confirm
that the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while
keeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In
addition, when maintaining the same spectral efficiency, the proposed
DFT-s-OFDM-CM with the splitting of information bits into two streams enables
the use of lower-order constellation modulation and offers greater resilience
to noise, resulting in a lower BER than the chirped DFT-s-OFDM.

</details>


### [7] [Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving](https://arxiv.org/abs/2508.04128)
*Di Wu,Yifei Jia,Siyuan Li,Shiqi Zhao,Jie Yang,Mohamad Sawan*

Main category: eess.SP

TL;DR: Neuro-MoBRE是一种新型的神经生理解码框架，通过区域专家混合和预训练策略解决数据异质性，在多任务和跨被试解码中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经生理解码方法局限于单任务和单个被试，缺乏通用性和泛化能力，亟需解决数据异质性带来的挑战。

Method: 提出Neuro-MoBRE框架，结合脑区-时间嵌入机制和专家混合方法，采用区域掩码自编码预训练和任务解耦信息聚合策略。

Result: 在11名被试的五个任务中表现优于现有方法，并展示了零样本解码的强泛化能力。

Conclusion: Neuro-MoBRE通过显式处理数据异质性，为神经生理解码提供了通用且高效的解决方案。

Abstract: Neurophysiological decoding, fundamental to advancing brain-computer
interface (BCI) technologies, has significantly benefited from recent advances
in deep learning. However, existing decoding approaches largely remain
constrained to single-task scenarios and individual subjects, limiting their
broader applicability and generalizability. Efforts towards creating
large-scale neurophysiological foundation models have shown promise, but
continue to struggle with significant challenges due to pervasive data
heterogeneity across subjects and decoding tasks. Simply increasing model
parameters and dataset size without explicitly addressing this heterogeneity
fails to replicate the scaling successes seen in natural language processing.
Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),
a general-purpose decoding framework explicitly designed to manage the
ubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE
incorporates a brain-regional-temporal embedding mechanism combined with a
mixture-of-experts approach, assigning neural signals from distinct brain
regions to specialized regional experts on a unified embedding basis, thus
explicitly resolving both structural and functional heterogeneity.
Additionally, our region-masked autoencoding pre-training strategy further
enhances representational consistency among subjects, complemented by a
task-disentangled information aggregation method tailored to effectively handle
task-specific neural variations. Evaluations conducted on intracranial
recordings from 11 subjects across five diverse tasks, including complex
language decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE
surpasses prior art and exhibits robust generalization for zero-shot decoding
on unseen subjects.

</details>


### [8] [Dual-Function Radar-Communication Beamforming with Outage Probability Metric](https://arxiv.org/abs/2508.04144)
*Hossein Maleki,Carles Diaz-Vilor,Ali Pezeshki,Vahid Tarokh,Hamid Jafarkhani*

Main category: eess.SP

TL;DR: 提出了一种用于双功能雷达通信系统的波束成形方法，优化雷达和通信性能，解决频谱拥塞问题。


<details>
  <summary>Details</summary>
Motivation: 频谱拥塞问题需要通信与感知的集成设计，以同时满足雷达监测和多用户通信的需求。

Method: 针对雷达中心和通信中心两种场景，分别优化雷达性能或通信性能，通过随机优化问题转化为确定性非凸问题，再松弛为半定规划问题。

Result: 数值实验验证了所提设计的有效性。

Conclusion: 该方法在频谱资源有限的情况下，能够平衡雷达和通信的性能需求。

Abstract: The integrated design of communication and sensing may offer a potential
solution to address spectrum congestion. In this work, we develop a beamforming
method for a dual-function radar-communication system, where the transmit
signal is used for both radar surveillance and communication with multiple
downlink users, despite imperfect channel state information (CSI). We focus on
two scenarios of interest: radar-centric and communication-centric. In the
radar-centric scenario, the primary goal is to optimize radar performance while
attaining acceptable communication performance. To this end, we minimize a
weighted sum of the mean-squared error in achieving a desired beampattern and a
mean-squared cross correlation of the radar returns from directions of interest
(DOI). We also seek to ensure that the probability of outage for the
communication users remains below a desired threshold. In the
communication-centric scenario, our main objective is to minimize the maximum
probability of outage among the communication users while keeping the
aforementioned radar metrics below a desired threshold. Both optimization
problems are stochastic and untractable. We first take advantage of central
limit theorem to obtain deterministic non-convex problems and then consider
relaxations of these problems in the form of semidefinite programs with rank-1
constraints. We provide numerical experiments demonstrating the effectiveness
of the proposed designs.

</details>


### [9] [Subspace Fitting Approach for Wideband Near-Field Localization](https://arxiv.org/abs/2508.04169)
*Ruiyun Zhang,Zhaolin Wang,Zhiqing Wei,Yuanwei Liu,Zehui Xiong,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出了两种宽频近场定位的子空间拟合方法，分别基于精确模型和简化模型。


<details>
  <summary>Details</summary>
Motivation: 近场系统中球面波传播导致距离和角度参数耦合，传统远场方法无法直接应用。

Method: 1. 推导了多目标宽频近场信号模型，提出基于子空间拟合的MUSIC方法联合估计距离和角度；2. 引入Fresnel近似MUSIC算法降低复杂度并解耦参数。

Result: 数值实验验证了两种方法的有效性。

Conclusion: 提出的方法在近场定位中表现良好，简化模型在降低复杂度的同时保持性能。

Abstract: Two subspace fitting approaches are proposed for wideband near-field
localization. Unlike in conventional far-field systems, where distance and
angle can be estimated separately, spherical wave propagation in near-field
systems couples these parameters. We therefore derive a frequency-domain
near-field signal model for multi-target wideband systems and develop a
subspace fitting-based MUSIC method that jointly estimates distance and angle.
To reduce complexity, a Fresnel approximation MUSIC algorithm is further
introduced to decouple the distance and angle parameters. Numerical results
verify the effectiveness of both proposed approaches.

</details>


### [10] [Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems](https://arxiv.org/abs/2508.04185)
*Evangelos Koutsonas,Xiaonan Mu,Nan Qi,Stylianos Trevlakis,Theodoros A. Tsiftsis,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 论文提出了一种同时信息和控制信号（SICS）协议，以解决RIS（可重构智能表面）在无线接入网络中的信号延迟问题。


<details>
  <summary>Details</summary>
Motivation: 在RIS与边缘单元之间的信号延迟高于信道相干时间时，会导致信号过时，影响性能。

Method: 提出SICS协议，利用非正交多址（NOMA）技术叠加信息和控制信号，优化RIS的反射和传输系数。

Result: SICS方法表现出鲁棒性，能够最大化用户数据速率并确保控制信号的解码。

Conclusion: SICS协议有效解决了信号延迟问题，提升了RIS在无线网络中的性能。

Abstract: Integration of RIS in radio access networks requires signaling between edge
units and the RIS microcontroller (MC). Unfortunately, in several practical
scenarios, the signaling latency is higher than the communication channel
coherence time, which causes outdated signaling at the RIS. To counterbalance
this, we introduce a simultaneous information and control signaling (SICS)
protocol that enables operation adaptation through wireless control signal
transmission. SICS assumes that the MC is equipped with a single antenna that
operates at the same frequency as the RIS. RIS operates in simultaneous
transmission and reflection (STAR) mode, and the source employs non-orthogonal
multiple access (NOMA) to superposition the information signal to the control
signal. To maximize the achievable user data rate while ensuring the MC's
ability to decode the control signal, we formulate and solve the corresponding
optimization problem that returns RIS's reflection and transmission
coefficients as well as the superposition coefficients of the NOMA scheme. Our
results reveal the robustness of the SICS approach.

</details>


### [11] [Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems](https://arxiv.org/abs/2508.04214)
*Yasaman Khorsandmanesh,Emil Björnson,Joakim Jaldén,Bengt Lindoff*

Main category: eess.SP

TL;DR: 提出了一种针对毫米波宽带MIMO系统的两阶段数字合并方案，以降低计算和硬件复杂度，并在性能上优于混合波束成形。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户设备（UE）中处理大量基带样本的计算和硬件复杂度问题。

Method: 采用两阶段数字合并方案：第一阶段利用信道几何特性减少信号流数量；第二阶段针对每个衰落实现更新合并。同时提出了基于最大似然估计的信道估计框架。

Result: 数值结果表明，该方法在频谱效率上优于混合波束成形。

Conclusion: 两阶段全数字收发器在未来系统中具有吸引力。

Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system
with fully digital transceivers at the base station and the user equipment
(UE), focusing on mobile UE scenarios. A main challenge when building a digital
UE combining is the large volume of baseband samples to handle. To mitigate
computational and hardware complexity, we propose a novel two-stage digital
combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received
signals to $N_{\text{c}}$ streams before baseband processing, leveraging
channel geometry for dimension reduction and updating at the beam coherence
time, which is longer than the channel coherence time of the small-scale
fading. By contrast, the second-stage combining is updated per fading
realization. We develop a pilot-based channel estimation framework for this
hardware setup based on maximum likelihoodestimation in both uplink and
downlink. Digital precoding and combining designs are proposed, and a spectral
efficiency expression that incorporates imperfect channel knowledge is derived.
The numerical results demonstrate that the proposed approach outperforms hybrid
beamforming, showcasing the attractiveness of using two-stage fully digital
transceivers in future systems.

</details>


### [12] [Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP](https://arxiv.org/abs/2508.04222)
*Thibaut Ceulemans,Cel Thys,Robbert Beerten,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出了一种针对超大规模天线阵列（ELAA）系统的信道估计算法VR-HMM-P-SOMP，结合物理模型和隐马尔可夫模型（HMM），显著提升了估计精度，尤其在低信噪比和稀疏场景下。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法在ELAA系统中因近场传播和空间非平稳性而失效，需开发新算法以应对这些挑战。

Method: 开发了基于物理的混合信道模型，引入非二进制可见区域（VR）掩码，并提出VR-HMM-P-SOMP算法，结合HMM和贪婪稀疏恢复框架。

Result: 仿真显示该算法在低信噪比和稀疏场景下优于现有方法，且计算复杂度低，适应性强。

Conclusion: VR-HMM-P-SOMP为ELAA系统提供了一种高效且鲁棒的信道估计解决方案。

Abstract: This work focuses on channel estimation in extremely large aperture array
(ELAA) systems, where near-field propagation and spatial non-stationarity
introduce complexities that hinder the effectiveness of traditional estimation
techniques. A physics-based hybrid channel model is developed, incorporating
non-binary visibility region (VR) masks to simulate diffraction-induced power
variations across the antenna array. To address the estimation challenges posed
by these channel conditions, a novel algorithm is proposed:
Visibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching
Pursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework
by integrating VR estimation through a hidden Markov model (HMM), using a novel
emission formulation and Viterbi decoding. This allows the algorithm to
adaptively mask steering vectors and account for spatial non-stationarity at
the antenna level. Simulation results demonstrate that the proposed method
enhances estimation accuracy compared to existing techniques, particularly in
low-SNR and sparse scenarios, while maintaining a low computational complexity.
The algorithm presents robustness across a range of design parameters and
channel conditions, offering a practical solution for ELAA systems.

</details>


### [13] [Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications](https://arxiv.org/abs/2508.04223)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Shuai Liu,Hongyang Du,Geyong Min*

Main category: eess.SP

TL;DR: 论文提出了一种基于Wasserstein距离的自适应混合分布方案（WS-DC），用于设计高效的任务导向语义通信（ToSC）码本，以提高频谱效率和信道容量利用率。


<details>
  <summary>Details</summary>
Motivation: 现有ToSC方法依赖稀疏激活的码本，导致频谱效率低和信道容量未充分利用。需要设计一种既能支持任务推理又能接近信道容量理论极限的码本。

Method: 构建了一个频谱效率感知的码本设计框架，将码本激活概率纳入优化过程，并引入Wasserstein距离作为正则化指标，最小化学习分布与最优信道输入分布之间的差距。

Result: 实验表明，WS-DC在推理准确性和码本效率上均优于现有方法。

Conclusion: WS-DC为接近信道容量的语义通信系统提供了有前景的方向。

Abstract: Digital task-oriented semantic communication (ToSC) aims to transmit only
task-relevant information, significantly reducing communication overhead.
Existing ToSC methods typically rely on learned codebooks to encode semantic
features and map them to constellation symbols. However, these codebooks are
often sparsely activated, resulting in low spectral efficiency and
underutilization of channel capacity. This highlights a key challenge: how to
design a codebook that not only supports task-specific inference but also
approaches the theoretical limits of channel capacity. To address this
challenge, we construct a spectral efficiency-aware codebook design framework
that explicitly incorporates the codebook activation probability into the
optimization process. Beyond maximizing task performance, we introduce the
Wasserstein (WS) distance as a regularization metric to minimize the gap
between the learned activation distribution and the optimal channel input
distribution. Furthermore, we reinterpret WS theory from a generative
perspective to align with the semantic nature of ToSC. Combining the above two
aspects, we propose a WS-based adaptive hybrid distribution scheme, termed
WS-DC, which learns compact, task-driven and channel-aware latent
representations. Experimental results demonstrate that WS-DC not only
outperforms existing approaches in inference accuracy but also significantly
improves codebook efficiency, offering a promising direction toward
capacity-approaching semantic communication systems.

</details>


### [14] [ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening](https://arxiv.org/abs/2508.04240)
*Sitong Chen,Beiqianyi Li,Cuilin He,Dongyang Li,Mingyang Wu,Xinke Shen,Song Wang,Xuetao Wei,Xindi Wang,Haiyan Wu,Quanying Liu*

Main category: eess.SP

TL;DR: ChineseEEG-2是一个高密度EEG数据集，用于多模态语言任务下的神经解码模型基准测试，支持中文的脑-语言模型对齐。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏非英语语言的脑-语言配对数据，尤其是支持多模态（说、听、读）的基准数据集。

Method: 扩展了之前的ChineseEEG数据集，新增了朗读和被动听两种模态，记录了四名参与者的EEG和音频数据，并用于其他八名参与者的听任务。

Result: 提供了EEG信号、精确音频、语义嵌入和任务标签，支持跨模态的语义对齐学习。

Conclusion: ChineseEEG-2为神经语义解码提供了基准数据集，推动了脑-语言模型对齐的研究。

Abstract: EEG-based neural decoding requires large-scale benchmark datasets. Paired
brain-language data across speaking, listening, and reading modalities are
essential for aligning neural activity with the semantic representation of
large language models (LLMs). However, such datasets are rare, especially for
non-English languages. Here, we present ChineseEEG-2, a high-density EEG
dataset designed for benchmarking neural decoding models under real-world
language tasks. Building on our previous ChineseEEG dataset, which focused on
silent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and
Passive Listening (PL), using the same Chinese corpus. EEG and audio were
simultaneously recorded from four participants during ~10.7 hours of reading
aloud. These recordings were then played to eight other participants,
collecting ~21.6 hours of EEG during listening. This setup enables speech
temporal and semantic alignment across the RA and PL modalities. ChineseEEG-2
includes EEG signals, precise audio, aligned semantic embeddings from
pre-trained language models, and task labels. Together with ChineseEEG, this
dataset supports joint semantic alignment learning across speaking, listening,
and reading. It enables benchmarking of neural decoding algorithms and promotes
brain-LLM alignment under multimodal language tasks, especially in Chinese.
ChineseEEG-2 provides a benchmark dataset for next-generation neural semantic
decoding.

</details>


### [15] [Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond](https://arxiv.org/abs/2508.04253)
*Yiyan Ma,Bo Ai,Jinhong Yuan,Shuangyang Li,Qingqing Cheng,Zhenguo Shi,Weijie Yuan,Zhiqiang Wei,Akram Shafie,Guoyu Ma,Yunlong Lu,Mi Yang,Zhangdui Zhong*

Main category: eess.SP

TL;DR: 论文提出了一种基于延迟-多普勒（DD）域信号处理的OFDM增强方案（DD-a-OFDM），通过结合DDMC研究的见解，提升OFDM在高移动性场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 6G系统中高移动性场景至关重要，但传统OFDM在多普勒扩展下存在子载波正交性损失问题，而OTFS虽能利用时频域信道多样性，却面临接收复杂度高和资源分配不灵活的挑战。因此，需要一种既能保留OFDM优势又能提升性能的方案。

Method: 设计了DD-a-OFDM系统结构，保留经典OFDM收发机，同时引入DD域信道估计和时频域均衡；详细描述了基于离散时频导频的DD域信道估计方法，并证明时频域载波间干扰可转化为DD域高斯干扰；推导了DD域信道估计的CRLB；开发了ML和峰值检测的信道估计器及对应的时频域均衡器。

Result: 数值结果表明，DD-a-OFDM相比经典OFDM降低了误码率，且在信道估计精度上优于OTFS，同时导频开销更低。

Conclusion: DD-a-OFDM是一种在高移动性场景下提升OFDM性能的有效方案，兼具低复杂度和灵活性。

Abstract: High-mobility scenarios will be a critical part of 6G systems. Since the
widely deployed orthogonal frequency division multiplexing (OFDM) waveform
suffers from subcarrier orthogonality loss under severe Doppler spread,
delay-Doppler domain multi-carrier (DDMC) modulation systems, such as
orthogonal time frequency space (OTFS), have been extensively studied. While
OTFS can exploit time-frequency (TF) domain channel diversity, it faces
challenges including high receiver complexity and inflexible TF resource
allocation, making OFDM still the most promising waveform for 6G. In this
article, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme
to enhance OFDM performance based on DDMC research insights. First, we design a
DD-a-OFDM system structure, retaining the classical OFDM transceiver while
incorporating DD domain channel estimation and TF domain equalization. Second,
we detail DD domain channel estimation using discrete TF pilots and prove that
TF domain inter-carrier interference (ICI) could be transformed into DD domain
Gaussian interference. Third, we derive closed-form Cram\'{e}r-Rao lower bounds
(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood
(ML) and peak detection-based channel estimators, along with a corresponding TF
domain equalizer. Numerical results verify the proposed design, showing that
DD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and
outperforms OTFS in channel estimation accuracy with lower pilot overhead.

</details>


### [16] [Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication](https://arxiv.org/abs/2508.04291)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Hongyang Du,Haojin Li,Chen Sun,Haijun Zhang*

Main category: eess.SP

TL;DR: 本文提出了一种针对低功耗边缘网络设计的信道感知离散语义编码框架，通过Wasserstein正则化目标优化语义保真度和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向语义通信（ToSC）框架在离散映射中常忽略信道特性和任务需求，导致性能不佳、任务效用降低及泛化能力有限。

Method: 采用Wasserstein正则化目标，将离散码激活与最优输入分布对齐，提升语义保真度和鲁棒性。

Result: 实验表明，该方法在不同信噪比（SNR）下显著提高了任务准确性和通信效率。

Conclusion: 该研究为离散语义与信道优化的结合提供了新思路，推动了语义通信在未来数字基础设施中的广泛应用。

Abstract: Discrete representation has emerged as a powerful tool in task-oriented
semantic communication (ToSC), offering compact, interpretable, and efficient
representations well-suited for low-power edge intelligence scenarios. Its
inherent digital nature aligns seamlessly with hardware-friendly deployment and
robust storage/transmission protocols. However, despite its strengths, current
ToSC frameworks often decouple semantic-aware discrete mapping from the
underlying channel characteristics and task demands. This mismatch leads to
suboptimal communication performance, degraded task utility, and limited
generalization under variable wireless conditions. Moreover, conventional
designs frequently overlook channel-awareness in codebook construction,
restricting the effectiveness of semantic symbol selection under constrained
resources. To address these limitations, this paper proposes a channel-aware
discrete semantic coding framework tailored for low-power edge networks.
Leveraging a Wasserstein-regularized objective, our approach aligns discrete
code activations with optimal input distributions, thereby improving semantic
fidelity, robustness, and task accuracy. Extensive experiments on the inference
tasks across diverse signal-to-noise ratio (SNR) regimes show that our method
achieves notable gains in accuracy and communication efficiency. This work
provides new insights into integrating discrete semantics and channel
optimization, paving the way for the widespread adoption of semantic
communication in future digital infrastructures.

</details>


### [17] [Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications](https://arxiv.org/abs/2508.04322)
*Ruopeng Xu,Zhaohui Yang,Zhaoyang Zhang,Mohammad Shikh-Bahaei,Kaibin Huang,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于流体天线中继（FAR）的能效无线通信系统，解决了由障碍物引起的非视距（NLoS）链路问题，并通过优化天线位置和协议设计提升能效。


<details>
  <summary>Details</summary>
Motivation: 6G通信需求推动了流体天线系统（FAS）的发展，但现有研究主要关注视距（LoS）场景，忽视了非视距（NLoS）链路的问题。

Method: 设计了结合放大转发（AF）协议的FAR辅助通信系统，通过优化流体天线位置、功率控制和波束成形，最大化能效。

Result: 仿真结果表明，所提算法在能效上优于传统方案，比可重构智能表面（RIS）和传统AF中继方案分别高出23.39%和39.94%。

Conclusion: FAR系统通过优化设计和算法，显著提升了非视距通信场景下的能效，为6G通信提供了有效解决方案。

Abstract: In this paper, we propose an energy efficient wireless communication system
based on fluid antenna relay (FAR) to solve the problem of non-line-of-sight
(NLoS) links caused by blockages with considering the physical properties.
Driven by the demand for the sixth generation (6G) communication, fluid antenna
systems (FASs) have become a key technology due to their flexibility in
dynamically adjusting antenna positions. Existing research on FAS primarily
focuses on line-of-sight (LoS) communication scenarios, and neglects the
situations where only NLoS links exist. To address the issues posted by NLoS
communication, we design an FAR-assisted communication system combined with
amplify-and-forward (AF) protocol. In order to alleviate the high energy
consumption introduced by AF protocol while ensuring communication quality, we
formulate an energy efficiency (EE) maximization problem. By optimizing the
positions of the fluid antennas (FAs) on both sides of the FAR, we achieve
controllable phase shifts of the signals transmitting through the blockage
which causes the NLoS link. Besides, we establish a channel model that jointly
considers the blockage-through matrix, large-scale fading, and small-scale
fading. To maximize the EE of the system, we jointly optimize the FAR position,
FA positions, power control, and beamforming design under given constraints,
and propose an iterative algorithm to solve this formulated optimization
problem. Simulation results show that the proposed algorithm outperforms the
traditional schemes in terms of EE, achieving up to $23.39\%$ and $39.94\%$
higher EE than the conventional reconfigurable intelligent surface (RIS) scheme
and traditional AF relay scheme, respectively.

</details>


### [18] [Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination](https://arxiv.org/abs/2508.04331)
*Mohamadreza Delbari,Qikai Zhou,Robin Neuder,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: 论文提出了一种基于液晶技术的可重构智能表面（RIS）设计，解决了其相位响应频率依赖性问题，提升了宽带OFDM系统的保密通信性能。


<details>
  <summary>Details</summary>
Motivation: 液晶RIS的相位响应具有频率依赖性，可能导致性能下降和信息泄露，尤其在保密通信系统中更为关键。

Method: 设计了一种RIS算法，用于宽带OFDM系统，无需获取完整信道状态信息或频繁重配置RIS，即可在合法用户区域实现信号覆盖，同时避免泄露给潜在窃听者区域。

Result: 仿真结果表明，该方法显著提升了保密速率，在60 GHz中心频率和8 GHz带宽下，保密速率达到约2 bits/symbol。

Conclusion: 提出的方法有效解决了液晶RIS的频率依赖性问题，提升了保密通信性能，适用于宽带系统。

Abstract: Liquid crystal (LC) technology provides a low-power and scalable approach to
implement a reconfigurable intelligent surface (RIS). However, the LC-based
RIS's phase-shift response is inherently frequency-dependent, which can lead to
performance degradation if not properly addressed. This issue becomes
especially critical in secure communication systems, where such variations may
result in considerable information leakage. To avoid the need for full channel
state information (CSI) acquisition and frequent RIS reconfiguration, we design
RIS for a wideband orthogonal frequency division multiplexing (OFDM) system to
illuminate a desired area containing legitimate users while avoiding leakage to
regions where potential eavesdroppers may be located. Our simulation results
demonstrate that the proposed algorithm improves the secrecy rate compared to
methods that neglect frequency-dependent effects. In the considered setup, the
proposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz
bandwidth when the center frequency is 60 GHz.

</details>


### [19] [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](https://arxiv.org/abs/2508.04570)
*A. Tarik Leblebici,Sumeyra Hassan,Erdal Panayirci,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出了一种基于可见光通信（VLC）的联合通信与室内定位系统（JCP），支持高精度2D和3D定位，并通过空间调制（SM）和M进制脉冲幅度调制（PAM）实现高效通信。


<details>
  <summary>Details</summary>
Motivation: 为未来6G室内网络提供一种在复杂信道条件下集成定位与通信的高效解决方案。

Method: 使用接收信号强度（RSS）和激进轴定理提高定位精度，空间调制和PAM实现通信，最小二乘估计器进行信道估计。

Result: 仿真显示亚厘米级定位精度和高信噪比下低于10^{-6}的误码率。

Conclusion: 系统在LED布局几何中心附近表现最佳，验证了其在6G室内网络中的有效性。

Abstract: This paper proposes a joint communication and indoor positioning (JCP) system
based on visible light communication (VLC) designed for high-precision indoor
environments. The framework supports 2D and 3D positioning using received
signal strength (RSS) from pilot transmissions, enhanced by the radical axis
theorem to improve accuracy under measurement uncertainties. Communication is
achieved using spatial modulation (SM) with M-ary pulse amplitude modulation
(PAM), where data is conveyed through the modulation symbol and the active
light-emitting diode (LED) index, improving spectral efficiency while
maintaining low complexity. A pilot-aided least squares (LS) estimator is
employed for joint channel and dimming coefficient estimation, enabling robust
symbol detection in multipath environments characterized by both line-of-sight
(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician
fading. The proposed system incorporates a dimming control mechanism to meet
lighting requirements while maintaining reliable communication and positioning
performance. Simulation results demonstrate sub-centimeter localization
accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below
10^{-6} for low-order PAM schemes. Additionally, comparative analysis across
user locations reveals that positioning and communication performance improve
significantly near the geometric center of the LED layout. These findings
validate the effectiveness of the proposed system for future 6G indoor networks
requiring integrated localization and communication under practical channel
conditions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Single Fragment Forensic Coding from Discrepancy Theory](https://arxiv.org/abs/2508.03938)
*Junsheng Liu,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文提出了一种在3D打印中嵌入多维数据的方法，确保即使物体部分损坏或改变，仍能从中解码信息。


<details>
  <summary>Details</summary>
Motivation: 3D打印的普及带来了安全问题，如非法制造无法追踪的物品。为确保可追溯性，需在打印物体中嵌入唯一标识符。

Method: 利用纠错码原理，提出针对一维、二维和三维数据的编码方案，支持从部分片段解码，并引入纠正位翻转错误的代码。

Result: 展示了从单一足够大的片段解码多维信息的能力，并实现了非零速率的编码。

Conclusion: 该方法为3D打印物体的追踪和法医调查提供了有效工具。

Abstract: Three-dimensional (3D) printing's accessibility enables rapid manufacturing
but also poses security risks, such as the unauthorized production of
untraceable firearms and prohibited items. To ensure traceability and
accountability, embedding unique identifiers within printed objects is
essential, in order to assist forensic investigation of illicit use. This paper
models data embedding in 3D printing using principles from error-correcting
codes, aiming to recover embedded information from partial or altered fragments
of the object. Previous works embedded one-dimensional data (i.e., a vector)
inside the object, and required almost all fragments of the object for
successful decoding. In this work, we study a problem setting in which only one
sufficiently large fragment of the object is available for decoding. We first
show that for one-dimensional embedded information the problem can be easily
solved using existing tools. Then, we introduce novel encoding schemes for
two-dimensional information (i.e., a matrix), and three-dimensional information
(i.e., a cube) which enable the information to be decoded from any sufficiently
large rectangle-shaped or cuboid-shaped fragment. Lastly, we introduce a code
that is also capable of correcting bit-flip errors, using techniques from
recently proposed codes for DNA storage. Our codes operate at non-vanishing
rates, and involve concepts from discrepancy theory called Van der Corput sets
and Halton-Hammersely sets in novel ways.

</details>


### [21] [One-weight codes in the sum-rank metric](https://arxiv.org/abs/2508.04262)
*Usman Mushrraf,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文研究了在求和秩度量下的一重量码，分类了常数秩列表码，探索了常数秩轮廓码，并分析了与MSRD码的关系。


<details>
  <summary>Details</summary>
Motivation: 求和秩度量下的一重量码分类复杂，研究其几何特性有助于理解其结构与有限几何的联系。

Method: 分类常数秩列表码，探索常数秩轮廓码的结构，分析维度二和三的MSRD码构造。

Result: 提出了常数秩列表码的完整分类，给出了常数秩轮廓码的部分结果，并针对维度二和三的MSRD码提出了新构造和不存在性结果。

Conclusion: 求和秩度量下的一重量码研究揭示了新的几何结构和分类挑战，为未来研究提供了方向。

Abstract: One-weight codes, in which all nonzero codewords share the same weight, form
a highly structured class of linear codes with deep connections to finite
geometry. While their classification is well understood in the Hamming and rank
metrics - being equivalent to (direct sums of) simplex codes - the sum-rank
metric presents a far more intricate landscape. In this work, we explore the
geometry of one-weight sum-rank metric codes, focusing on three distinct
classes. First, we introduce and classify \emph{constant rank-list} sum-rank
codes, where each nonzero codeword has the same tuple of ranks, extending
results from the rank-metric setting. Next, we investigate the more general
\emph{constant rank-profile} codes, where, up to reordering, each nonzero
codeword has the same tuple of ranks. Although a complete classification
remains elusive, we present the first examples and partial structural results
for this class. Finally, we consider one-weight codes that are also MSRD
(Maximum Sum-Rank Distance) codes. For dimension two, constructions arise from
partitions of scattered linear sets on projective lines. For dimension three,
we connect their existence to that of special $2$-fold blocking sets in the
projective plane, leading to new bounds and nonexistence results over certain
fields.

</details>


### [22] [Is Lattice Reduction Necessary for Vector Perturbation Precoding?](https://arxiv.org/abs/2508.04313)
*Dominik Semmler,Wolfgang Utschick,Michael Joham*

Main category: cs.IT

TL;DR: VP预编码与LR结合在未编码SER/BER下表现优异，但以互信息为指标时不如THP。优化速率分配矩阵后，LR辅助算法仍无法超越THP。


<details>
  <summary>Details</summary>
Motivation: 探讨VP预编码与LR结合在互信息指标下的性能表现，揭示THP的优越性。

Method: 分析互信息表达式中的速率分配矩阵，推导其最优选择，并比较不同算法性能。

Result: 优化速率分配矩阵后，LR辅助算法性能仍不及THP，尤其在病态信道中。

Conclusion: THP在互信息指标下更具优势，LR辅助算法无法超越其性能。

Abstract: Vector perturbation (VP) precoding is an effective nonlinear precoding
technique in the downlink (DL) with modulo channels. Especially, when combined
with Lattice reduction (LR), low-complexity algorithms achieve very promising
performances, outperforming other popular nonlinear precoding techniques like
Tomlinson-Harashima precoding (THP). However, these results are based on the
uncoded symbol error rate (SER) or uncoded bit error rate (BER). We show that
when using the mutual information as the figure of merit, the observation is
fundamentally different and that these algorithms generally do not outperform
THP. Within the expression of the mutual information, a rate allocation matrix
can be incorporated, which has not received much attention so far. In this
article, we derive the optimal choice of this matrix for different algorithms,
and we show that this matrix is indeed crucial for the performance, especially
for ill-conditioned channels. Furthermore, when using an optimized choice of
this matrix, we show that the classical LR-aided algorithms cannot exceed the
rate of THP, highlighting the effectiveness of the THP method. This concept can
be generalized to a whole class of algorithms for which LR yields no
improvement. We derive the corresponding properties and categorize various
algorithms accordingly.

</details>


### [23] [Bases of Riemann-Roch spaces associated with arbitrary elliptic curve divisors and their application in constructing various elliptic Codes families](https://arxiv.org/abs/2508.04340)
*Artyom Kuninets,Ekaterina Malygina*

Main category: cs.IT

TL;DR: 论文确定了椭圆码Riemann-Roch空间的显式基，提供了构造任意除子对应基的算法，并应用于准循环椭圆码和Goppa类椭圆码。


<details>
  <summary>Details</summary>
Motivation: 为代数几何码应用提供Riemann-Roch空间的显式基描述，以支持高效编码和密码分析。

Method: 建立显式基构造的可行性，并提供精确算法。

Result: 成功构造了任意除子对应的Riemann-Roch空间基，并应用于准循环椭圆码和Goppa类椭圆码。

Conclusion: 显式基描述对高效编码和密码分析具有重要意义。

Abstract: In this paper, we determine explicit bases for Riemann--Roch spaces
associated with various families of elliptic codes. We establish the
feasibility and provide exact algorithms for constructing bases of
Riemann--Roch spaces corresponding to arbitrary divisors on elliptic curves.
These results are subsequently applied to derive bases for quasi-cyclic
elliptic codes and their subfield subcodes as well as for the class of
Goppa-like elliptic codes. For algebraic geometry code applications, having an
explicit description of Riemann--Roch space bases for arbitrary divisors is
particularly valuable as it simultaneously enables efficient code construction
and reveals structural properties of the codes leading to the new cryptanalysis
methods when these codes are employed in cryptographic schemes

</details>


### [24] [Grid-like Error-Correcting Codes for Matrix Multiplication with Better Correcting Capability](https://arxiv.org/abs/2508.04355)
*Hao Shi,Zhengyi Jiang,Zhongyi Huang,Bo Bai,Gong Zhang,Hanxu Hou*

Main category: cs.IT

TL;DR: 提出了一种针对矩阵乘法的新型纠错编码框架，用于检测和纠正分布式训练中的静默数据损坏（SDC），显著提升计算容错能力。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式训练中，静默数据损坏（SDC）在矩阵乘法过程中难以检测，导致模型性能下降。

Method: 采用基于网格的结构编码方案，增强错误定位和纠正能力。

Result: 实验表明，该方法能100%可靠地纠正三个矩阵中最多两个错误符号，计算时间仅增加24%。

Conclusion: 提出的编码框架在理论分析和实验验证中均表现出高效性和鲁棒性。

Abstract: Matrix multiplication over the real field constitutes a foundational
operation in the training of deep learning models, serving as a computational
cornerstone for both forward and backward propagation processes. However, the
presence of silent data corruption (SDC) in large-scale distributed training
environments poses a significant threat to model convergence and predictive
accuracy, particularly when such errors manifest during matrix multiplication.
Due to their transient and non-intrusive nature, these errors often evade
detection, allowing them to propagate and accumulate over time, ultimately
leading to substantial degradation in model performance. In this paper, we
introduce a novel error-correcting coding framework specifically tailored for
matrix multiplication operations. Our proposed framework is designed to detect
and correct multiple computational errors that may arise during the execution
of matrix products. By leveraging a grid-based structural encoding scheme, our
approach enhances error localization and correction capabilities across all
participating matrices, thereby significantly improving the fault tolerance of
the computation. Experimental results demonstrate that our method achieves
deterministic correction of up to two erroneous symbols distributed across
three matrices with 100\% reliability, while incurring only a 24\% overhead in
computational time on GPU architectures. Furthermore, we provide a rigorous
theoretical analysis of the error-correction properties inherent to our coding
scheme, establishing its correctness and robustness under well-defined fault
models.

</details>


### [25] [Tradeoff Between the Number of Transmitted Molecules and the BER Performance in Molecular Communication between Bionanosensors](https://arxiv.org/abs/2508.04466)
*Dongliang Jing,Linjuan Li,Lin Lin,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究了分子通信中传输分子数量与误码率（BER）的平衡问题，提出了一种平衡函数和梯度下降算法以优化分子数量，实现最佳通信性能。


<details>
  <summary>Details</summary>
Motivation: 分子通信中，发射器的存储容量限制了可用分子数量，影响了通信可靠性，因此需要找到传输分子数量与BER之间的最佳平衡。

Method: 分析了传输分子数量与BER的关系，提出平衡函数并归一化参数，使用梯度下降算法确定最优分子数量。

Result: 理论和仿真结果表明，优化后的分子数量与BER达到了理想平衡。

Conclusion: 通过提出的方法，成功实现了分子通信系统中传输分子数量与BER的最佳平衡。

Abstract: In the domain of molecular communication (MC), information is conveyed
through the characteristics of molecules transmitted between the transmitter
and the receiver bionanosensors via propagation. The constrained size of the
transmitter imposes limitations on its storage capacity, constraining the
number of available molecules for transmission, with a resulting effect on
communication reliability. This paper primarily focuses on achieving an
equilibrium between the number of transmitted molecules and the bit error rate
(BER) performance. To this end, we first analyze the relationship between the
number of transmitted molecules and the BER performance. Subsequently, a
balancing function that considers both the number of transmitted molecules and
the BER performance is introduced, taking into account the molecules'
respective weights. Given the difference in magnitude between the number of
transmitted molecules and the BER, these parameters are normalized to
facilitate analysis. Subsequently, a Gradient Descent Algorithm is employed to
determine the optimal number of transmitted molecules, aiming to achieve the
optimal equilibrium in the analyzed MC system. Theoretical and simulation
results are provided, substantiating that the optimal outcome indeed
establishes an ideal balance between the number of transmitted molecules and
the BER.

</details>


### [26] [Energy-Efficient Hybrid Beamfocusing for Near-Field Integrated Sensing and Communication](https://arxiv.org/abs/2508.04627)
*Wenhao Hu,Zhenyao He,Wei Xu,Yongming Huang,Derrick Wing Kwan Ng,Naofal Al-Dhahir*

Main category: cs.IT

TL;DR: 论文研究了6G网络中集成感知与通信（ISAC）的能量高效混合波束聚焦设计，针对近场效应和硬件成本问题，提出了优化算法，揭示了系统能效与目标估计精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的ISAC技术面临近场效应、高硬件成本和功耗问题，混合架构设计成为硬件和能源高效的解决方案。

Method: 推导了点目标和扩展目标的CRB/BCRB，优化了发射波束聚焦，提出了基于惩罚的凸近似技术和交替优化算法。

Result: 仿真结果表明近场区域可实现联合距离和角度估计，但混合架构会降低距离估计精度，系统能效提升会牺牲目标估计精度。

Conclusion: 混合架构在ISAC中具有潜力，但需权衡能效与估计精度。

Abstract: Integrated sensing and communication (ISAC) is a pivotal component of
sixth-generation (6G) wireless networks, leveraging high-frequency bands and
massive multiple-input multiple-output (M-MIMO) to deliver both high-capacity
communication and high-precision sensing. However, these technological
advancements lead to significant near-field effects, while the implementation
of M-MIMO \mbox{is associated with considerable} hardware costs and escalated
power consumption. In this context, hybrid architecture designs emerge as both
hardware-efficient and energy-efficient solutions. Motivated by these
considerations, we investigate the design of energy-efficient hybrid
beamfocusing for near-field ISAC under two distinct target scenarios, i.e., a
point target and an extended target. Specifically, we first derive the
closed-form Cram\'{e}r-Rao bound (CRB) of joint angle-and-distance estimation
for the point target and the Bayesian CRB (BCRB) of the target response matrix
for the extended target. Building on these derived results, we minimize the
CRB/BCRB by optimizing the transmit beamfocusing, while ensuring the energy
efficiency (EE) of the system and the quality-of-service (QoS) for
communication users. To address the resulting \mbox{nonconvex problems}, we
first utilize a penalty-based successive convex approximation technique with a
fully-digital beamformer to obtain a suboptimal solution. Then, we propose an
efficient alternating \mbox{optimization} algorithm to design the
analog-and-digital beamformer. \mbox{Simulation} results indicate that joint
distance-and-angle estimation is feasible in the near-field region. However,
the adopted hybrid architectures inevitably degrade the accuracy of distance
estimation, compared with their fully-digital counterparts. Furthermore,
enhancements in system EE would compromise the accuracy of target estimation,
unveiling a nontrivial tradeoff.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Technical specification of a framework for the collection of clinical images and data](https://arxiv.org/abs/2508.03723)
*Alistair Mackenzie,Mark Halling-Brown,Ruben van Engen,Carlijn Roozemond,Lucy Warren,Dominic Ward,Nadia Smith*

Main category: eess.IV

TL;DR: 报告描述了一个用于收集临床图像和数据的框架，用于训练和验证AI工具，包括伦理、信息治理、基础设施和数据共享协议。


<details>
  <summary>Details</summary>
Motivation: 确保数据收集的安全性和代表性，以支持AI工具的准确训练和验证。

Method: 提出自动化和半自动化的数据收集框架，强调长期和当前数据的结合。

Result: 框架支持持续更新的数据集，确保AI工具验证的时效性和准确性。

Conclusion: 自动化方法适合大规模长期数据收集，但半自动化方法也可作为起点。

Abstract: In this report a framework for the collection of clinical images and data for
use when training and validating artificial intelligence (AI) tools is
described. The report contains not only information about the collection of the
images and clinical data, but the ethics and information governance processes
to consider ensuring the data is collected safely, and the infrastructure and
agreements required to allow for the sharing of data with other groups.
  A key characteristic of the main collection framework described here is that
it can enable automated and ongoing collection of datasets to ensure that the
data is up-to-date and representative of current practice. This is important in
the context of training and validating AI tools as it is vital that datasets
have a mix of older cases with long term follow-up such that the clinical
outcome is as accurate as possible, and current data. Validations run on old
data will provide findings and conclusions relative to the status of the
imaging units when that data was generated. It is important that a validation
dataset can assess the AI tools with data that it would see if deployed and
active now.
  Other types of collection frameworks, which do not follow a fully automated
approach, are also described. Whilst the fully automated method is recommended
for large scale, long-term image collection, there may be reasons to start data
collection using semi-automated methods and indications of how to do that are
provided.

</details>


### [28] [A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models](https://arxiv.org/abs/2508.03734)
*Xiaoling Luo,Ruli Zheng,Qiaojian Zheng,Zibo Du,Shuo Yang,Meidan Ding,Qihao Xu,Chengliang Liu,Linlin Shen*

Main category: eess.IV

TL;DR: 综述了截至2025年眼科多模态深度学习方法的最新进展，包括任务特定方法和基础模型，探讨了数据集、评估指标、方法创新及未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍是全球重大健康挑战，多模态成像为眼科诊断提供互补信息，需系统总结深度学习方法的最新进展。

Method: 分为任务特定方法和基础模型两类，前者针对特定临床应用，后者结合视觉-语言架构和大语言模型。

Result: 总结了重要数据集、评估指标和方法创新（如自监督学习、注意力融合），并指出数据变异性、注释不足等挑战。

Conclusion: 未来方向包括超广域成像和强化学习框架，以开发智能、可解释且临床适用的眼科AI系统。

Abstract: Visual impairment represents a major global health challenge, with multimodal
imaging providing complementary information that is essential for accurate
ophthalmic diagnosis. This comprehensive survey systematically reviews the
latest advances in multimodal deep learning methods in ophthalmology up to the
year 2025. The review focuses on two main categories: task-specific multimodal
approaches and large-scale multimodal foundation models. Task-specific
approaches are designed for particular clinical applications such as lesion
detection, disease diagnosis, and image synthesis. These methods utilize a
variety of imaging modalities including color fundus photography, optical
coherence tomography, and angiography. On the other hand, foundation models
combine sophisticated vision-language architectures and large language models
pretrained on diverse ophthalmic datasets. These models enable robust
cross-modal understanding, automated clinical report generation, and decision
support. The survey critically examines important datasets, evaluation metrics,
and methodological innovations including self-supervised learning,
attention-based fusion, and contrastive alignment. It also discusses ongoing
challenges such as variability in data, limited annotations, lack of
interpretability, and issues with generalizability across different patient
populations. Finally, the survey outlines promising future directions that
emphasize the use of ultra-widefield imaging and reinforcement learning-based
reasoning frameworks to create intelligent, interpretable, and clinically
applicable AI systems for ophthalmology.

</details>


### [29] [Improve Retinal Artery/Vein Classification via Channel Couplin](https://arxiv.org/abs/2508.03738)
*Shuang Zeng,Chee Hong Lee,Kaiwen Li,Boxu Xie,Ourui Fu,Hangzhou He,Lei Zhu,Yanye Lu,Fangxiao Cheng*

Main category: eess.IV

TL;DR: 该论文提出了一种新的损失函数和正则化方法，用于视网膜血管分割和动静脉分类，解决了现有方法忽略血管结构耦合关系的问题，并在多个数据集上取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割和动静脉分类对诊断系统性和眼部疾病至关重要，但现有方法将任务视为三个独立的二元分割问题，忽略了血管结构的耦合关系。

Method: 设计了Channel-Coupled Vessel Consistency Loss以增强血管、动脉和静脉预测的一致性，并引入intra-image pixel-level contrastive loss提取更精细的特征表示。

Result: 在RITE、LES-AV和HRF三个公开数据集上取得了最佳分类结果。

Conclusion: 提出的方法通过一致性损失和对比损失，显著提升了视网膜动静脉分类的准确性。

Abstract: Retinal vessel segmentation plays a vital role in analyzing fundus images for
the diagnosis of systemic and ocular diseases. Building on this, classifying
segmented vessels into arteries and veins (A/V) further enables the extraction
of clinically relevant features such as vessel width, diameter and tortuosity,
which are essential for detecting conditions like diabetic and hypertensive
retinopathy. However, manual segmentation and classification are
time-consuming, costly and inconsistent. With the advancement of Convolutional
Neural Networks, several automated methods have been proposed to address this
challenge, but there are still some issues. For example, the existing methods
all treat artery, vein and overall vessel segmentation as three separate binary
tasks, neglecting the intrinsic coupling relationships between these anatomical
structures. Considering artery and vein structures are subsets of the overall
retinal vessel map and should naturally exhibit prediction consistency with it,
we design a novel loss named Channel-Coupled Vessel Consistency Loss to enforce
the coherence and consistency between vessel, artery and vein predictions,
avoiding biasing the network toward three simple binary segmentation tasks.
Moreover, we also introduce a regularization term named intra-image pixel-level
contrastive loss to extract more discriminative feature-level fine-grained
representations for accurate retinal A/V classification. SOTA results have been
achieved across three public A/V classification datasets including RITE, LES-AV
and HRF. Our code will be available upon acceptance.

</details>


### [30] [A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection](https://arxiv.org/abs/2508.03739)
*Md. Ehsanul Haque,Abrar Fahim,Shamik Dey,Syoda Anamika Jahan,S. M. Jahidul Islam,Sakib Rokoni,Md Sakib Morshed*

Main category: eess.IV

TL;DR: 提出了一种基于改进VGG-19模型的自动化骨折检测框架，结合CLAHE等预处理技术和Grad-CAM解释性方法，实现高精度和实时诊断。


<details>
  <summary>Details</summary>
Motivation: 骨折早期准确检测对治疗至关重要，但传统X光片解读耗时且易出错，现有深度学习方法缺乏可解释性。

Method: 使用改进的VGG-19模型，结合CLAHE、Otsu阈值和Canny边缘检测等预处理技术，并采用Grad-CAM提供模型决策的可视化解释。

Result: 模型分类准确率达99.78%，AUC为1.00，实时诊断反馈仅需0.5秒。

Conclusion: 该框架提供高效、可靠且可解释的骨折检测方案，提升诊断效率和患者护理质量。

Abstract: Early and accurate detection of the bone fracture is paramount to initiating
treatment as early as possible and avoiding any delay in patient treatment and
outcomes. Interpretation of X-ray image is a time consuming and error prone
task, especially when resources for such interpretation are limited by lack of
radiology expertise. Additionally, deep learning approaches used currently,
typically suffer from misclassifications and lack interpretable explanations to
clinical use. In order to overcome these challenges, we propose an automated
framework of bone fracture detection using a VGG-19 model modified to our
needs. It incorporates sophisticated preprocessing techniques that include
Contrast Limited Adaptive Histogram Equalization (CLAHE), Otsu's thresholding,
and Canny edge detection, among others, to enhance image clarity as well as to
facilitate the feature extraction. Therefore, we use Grad-CAM, an Explainable
AI method that can generate visual heatmaps of the model's decision making
process, as a type of model interpretability, for clinicians to understand the
model's decision making process. It encourages trust and helps in further
clinical validation. It is deployed in a real time web application, where
healthcare professionals can upload X-ray images and get the diagnostic
feedback within 0.5 seconds. The performance of our modified VGG-19 model
attains 99.78\% classification accuracy and AUC score of 1.00, making it
exceptionally good. The framework provides a reliable, fast, and interpretable
solution for bone fracture detection that reasons more efficiently for
diagnoses and better patient care.

</details>


### [31] [Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training](https://arxiv.org/abs/2508.03742)
*Weiwei Cao,Jianpeng Zhang,Zhongyi Shui,Sinuo Wang,Zeli Chen,Xi Li,Le Lu,Xianghua Ye,Tingbo Liang,Qi Zhang,Ling Zhang*

Main category: eess.IV

TL;DR: 该论文提出了一种提升视觉语义密度的方法，以解决医学图像与报告之间的语义密度差距问题，通过疾病级视觉对比学习和解剖学正态建模，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像与报告之间存在语义密度差距，导致视觉对齐偏差，影响了多功能和通用医学诊断能力的发展。

Method: 采用疾病级视觉对比学习增强视觉语义，并通过解剖学正态建模方法（利用VQ-VAE）重建正常视觉嵌入，以放大异常信号。

Result: 在两个胸部CT数据集和一个腹部CT数据集上进行了实验，平均AUC达到84.9%，显著优于现有方法，并展示了优越的迁移学习能力。

Conclusion: 提出的方法有效提升了视觉语义密度，改善了医学图像与诊断报告的对齐效果，实现了先进的零样本性能。

Abstract: Vision-language pre-training (VLP) has great potential for developing
multifunctional and general medical diagnostic capabilities. However, aligning
medical images with a low signal-to-noise ratio (SNR) to reports with a high
SNR presents a semantic density gap, leading to visual alignment bias. In this
paper, we propose boosting vision semantic density to improve alignment
effectiveness. On one hand, we enhance visual semantics through disease-level
vision contrastive learning, which strengthens the model's ability to
differentiate between normal and abnormal samples for each anatomical
structure. On the other hand, we introduce an anatomical normality modeling
method to model the distribution of normal samples for each anatomy, leveraging
VQ-VAE for reconstructing normal vision embeddings in the latent space. This
process amplifies abnormal signals by leveraging distribution shifts in
abnormal samples, enhancing the model's perception and discrimination of
abnormal attributes. The enhanced visual representation effectively captures
the diagnostic-relevant semantics, facilitating more efficient and accurate
alignment with the diagnostic report. We conduct extensive experiments on two
chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,
MedVL-CT69K, and comprehensively evaluate the diagnosis performance across
multiple tasks in the chest and abdominal CT scenarios, achieving
state-of-the-art zero-shot performance. Notably, our method achieved an average
AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing
methods. Additionally, we demonstrate the superior transfer learning
capabilities of our pre-trained model. Code is available at
https://github.com/alibaba-damo-academy/ViSD-Boost.

</details>


### [32] [Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?](https://arxiv.org/abs/2508.03744)
*Sarah Grube,Sören Grünhagen,Sarah Latus,Michael Meyling,Alexander Schlaefer*

Main category: eess.IV

TL;DR: 研究探讨深度学习在超声剪切波弹性成像中是否需要预处理步骤，结果表明即使使用原始射频数据，深度学习也能可靠区分不同弹性组。


<details>
  <summary>Details</summary>
Motivation: 超声剪切波弹性成像的通用性和标准化在不同系统和处理流程中存在限制，研究旨在减少传统预处理步骤的需求和偏差。

Method: 使用3D卷积神经网络从时空超声图像预测剪切波速度，评估不同预处理程度（从完全波束形成到原始射频数据）的影响。

Result: 深度学习方法在所有弹性组中均能显著区分剪切波速度，预处理仅略微提升性能。

Conclusion: 深度学习可减少传统预处理步骤的需求，实现更快、更可靠的临床弹性评估。

Abstract: Estimating the elasticity of soft tissue can provide useful information for
various diagnostic applications. Ultrasound shear wave elastography offers a
non-invasive approach. However, its generalizability and standardization across
different systems and processing pipelines remain limited. Considering the
influence of image processing on ultrasound based diagnostics, recent
literature has discussed the impact of different image processing steps on
reliable and reproducible elasticity analysis. In this work, we investigate the
need of ultrasound pre-processing steps for deep learning-based ultrasound
shear wave elastography. We evaluate the performance of a 3D convolutional
neural network in predicting shear wave velocities from spatio-temporal
ultrasound images, studying different degrees of pre-processing on the input
images, ranging from fully beamformed and filtered ultrasound images to raw
radiofrequency data. We compare the predictions from our deep learning approach
to a conventional time-of-flight method across four gelatin phantoms with
different elasticity levels. Our results demonstrate statistically significant
differences in the predicted shear wave velocity among all elasticity groups,
regardless of the degree of pre-processing. Although pre-processing slightly
improves performance metrics, our results show that the deep learning approach
can reliably differentiate between elasticity groups using raw, unprocessed
radiofrequency data. These results show that deep learning-based approaches
could reduce the need for and the bias of traditional ultrasound pre-processing
steps in ultrasound shear wave elastography, enabling faster and more reliable
clinical elasticity assessments.

</details>


### [33] [M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2508.03752)
*Yajun Liu,Zenghui Zhang,Jiang Yue,Weiwei Guo,Dongying Li*

Main category: eess.IV

TL;DR: 提出了一种名为M$^3$HL的新方法，结合动态可调掩码和高低层次特征一致性，改进了半监督医学图像分割任务中的CutMix数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有CutMix方法在医学图像分割中应用时过于僵化，且缺乏对特征一致性的关注。

Method: M$^3$HL包含两部分：1）M$^3$：基于MIM的动态掩码策略生成互补图像对；2）HL：高低层次特征一致性正则化框架。

Result: 在ACDC和LA数据集上实现了最先进的性能。

Conclusion: M$^3$HL通过动态掩码和特征一致性约束，显著提升了半监督医学图像分割的效果。

Abstract: Data augmentation methods inspired by CutMix have demonstrated significant
potential in recent semi-supervised medical image segmentation tasks. However,
these approaches often apply CutMix operations in a rigid and inflexible
manner, while paying insufficient attention to feature-level consistency
constraints. In this paper, we propose a novel method called Mutual Mask Mix
with High-Low level feature consistency (M$^3$HL) to address the aforementioned
challenges, which consists of two key components: 1) M$^3$: An enhanced data
augmentation operation inspired by the masking strategy from Masked Image
Modeling (MIM), which advances conventional CutMix through dynamically
adjustable masks to generate spatially complementary image pairs for
collaborative training, thereby enabling effective information fusion between
labeled and unlabeled images. 2) HL: A hierarchical consistency regularization
framework that enforces high-level and low-level feature consistency between
unlabeled and mixed images, enabling the model to better capture discriminative
feature representations.Our method achieves state-of-the-art performance on
widely adopted medical image segmentation benchmarks including the ACDC and LA
datasets. Source code is available at https://github.com/PHPJava666/M3HL

</details>


### [34] [Classification non supervis{é}es d'acquisitions hyperspectrales cod{é}es : quelles v{é}rit{é}s terrain ?](https://arxiv.org/abs/2508.03753)
*Trung-tin Dinh,Hervé Carfantan,Antoine Monmayrant,Simon Lacroix*

Main category: eess.IV

TL;DR: 提出了一种基于DD-CASSI高光谱成像器的无监督分类方法，能够在数据压缩十倍的情况下识别类别并估计参考光谱。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估方法中类别定义不明确、类内变异性高及分类错误的问题。

Method: 基于类内光谱变异性的简单模型，利用有限编码采集数据进行无监督分类。

Result: 在Pavia University场景中，通过简单假设检测到光谱更一致的区域。

Conclusion: 需要重新思考无监督分类方法的评估标准。

Abstract: We propose an unsupervised classification method using a limited number of
coded acquisitions from a DD-CASSI hyperspectral imager. Based on a simple
model of intra-class spectral variability, this approach allow to identify
classes and estimate reference spectra, despite data compression by a factor of
ten. Here, we highlight the limitations of the ground truths commonly used to
evaluate this type of method: lack of a clear definition of the notion of
class, high intra-class variability, and even classification errors. Using the
Pavia University scene, we show that with simple assumptions, it is possible to
detect regions that are spectrally more coherent, highlighting the need to
rethink the evaluation of classification methods, particularly in unsupervised
scenarios.

</details>


### [35] [FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation](https://arxiv.org/abs/2508.03758)
*Akwasi Asare,Mary Sagoe,Justice Williams Asare*

Main category: eess.IV

TL;DR: 提出了一种结合Vision Transformers和U-Net的混合架构FUTransUNet，用于糖尿病足溃疡（DFU）的自动分割，解决了传统CNN在长距离空间依赖建模上的不足，并在公开数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病足溃疡（DFU）的自动分割在临床诊断和治疗规划中至关重要，但由于溃疡区域的异质性外观、不规则形态和复杂背景，传统CNN（如U-Net）难以有效建模长距离空间依赖关系。

Method: 提出FUTransUNet，将Vision Transformers的全局注意力机制与U-Net框架结合，通过跳跃连接和解码路径保持细粒度空间分辨率，同时提取全局上下文特征。

Result: 在FUSeg数据集上，训练集Dice系数为0.8679，IoU为0.7672；验证集Dice系数为0.8751，IoU为0.7780。Grad-CAM可视化验证了模型的透明性。

Conclusion: FUTransUNet成功整合了全局和局部特征提取范式，提供了高鲁棒性、准确性、可解释性的解决方案，对临床足溃疡分析具有重要价值。

Abstract: Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role
in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring.
However, this task remains challenging due to the heterogeneous appearance,
irregular morphology, and complex backgrounds associated with ulcer regions in
clinical photographs. Traditional convolutional neural networks (CNNs), such as
U-Net, provide strong localization capabilities but struggle to model
long-range spatial dependencies due to their inherently limited receptive
fields. To address this, we propose FUTransUNet, a hybrid architecture that
integrates the global attention mechanism of Vision Transformers (ViTs) into
the U-Net framework. This combination allows the model to extract global
contextual features while maintaining fine-grained spatial resolution through
skip connections and an effective decoding pathway. We trained and validated
FUTransUNet on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset.
FUTransUNet achieved a training Dice Coefficient of 0.8679, an IoU of 0.7672,
and a training loss of 0.0053. On the validation set, the model achieved a Dice
Coefficient of 0.8751, an IoU of 0.7780, and a validation loss of 0.009045. To
ensure clinical transparency, we employed Grad-CAM visualizations, which
highlighted model focus areas during prediction. These quantitative outcomes
clearly demonstrate that our hybrid approach successfully integrates global and
local feature extraction paradigms, thereby offering a highly robust, accurate,
explainable, and interpretable solution and clinically translatable solution
for automated foot ulcer analysis. The approach offers a reliable,
high-fidelity solution for DFU segmentation, with implications for improving
real-world wound assessment and patient care.

</details>


### [36] [Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy](https://arxiv.org/abs/2508.03759)
*Tatwadarshi P. Nagarhalli,Shruti S. Pawar,Soham A. Dahanukar,Uday Aswalekar,Ashwini M. Save,Sanket D. Patil*

Main category: eess.IV

TL;DR: 研究探讨了通过图像超分辨率技术提升白细胞显微镜图像分辨率对深度学习分类性能的影响，结合增强图像和原始图像训练模型以提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 低分辨率的白细胞显微镜图像影响分类准确性，研究旨在通过图像增强技术提升分辨率，改善分类性能。

Method: 使用图像超分辨率技术提升图像分辨率，并将增强图像与原始图像结合训练深度学习模型。

Result: 通过实验验证了增强图像对模型性能的积极影响，提高了分类准确性。

Conclusion: 图像分辨率提升有助于模型捕捉更细微的形态变化，从而提高白细胞分类的准确性。

Abstract: Accurately classifying white blood cells from microscopic images is essential
to identify several illnesses and conditions in medical diagnostics. Many deep
learning technologies are being employed to quickly and automatically classify
images. However, most of the time, the resolution of these microscopic pictures
is quite low, which might make it difficult to classify them correctly. Some
picture improvement techniques, such as image super-resolution, are being
utilized to improve the resolution of the photos to get around this issue. The
suggested study uses large image dimension upscaling to investigate how
picture-enhancing approaches affect classification performance. The study
specifically looks at how deep learning models may be able to understand more
complex visual information by capturing subtler morphological changes when
image resolution is increased using cutting-edge techniques. The model may
learn from standard and augmented data since the improved images are
incorporated into the training process. This dual method seeks to comprehend
the impact of image resolution on model performance and enhance classification
accuracy. A well-known model for picture categorization is used to conduct
extensive testing and thoroughly evaluate the effectiveness of this approach.
This research intends to create more efficient image identification algorithms
customized to a particular dataset of white blood cells by understanding the
trade-offs between ordinary and enhanced images.

</details>


### [37] [Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Population-Based Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)](https://arxiv.org/abs/2508.03762)
*Anindo Saha,Joeran S. Bosma,Jasper J. Twilt,Alexander B. C. D. Ng,Aqua Asif,Kirti Magudia,Peder Larson,Qinglin Xie,Xiaodong Zhang,Chi Pham Minh,Samuel N. Gitau,Ivo G. Schoots,Martijn F. Boomsma,Renato Cuocolo,Nikolaos Papanikolaou,Daniele Regge,Derya Yakar,Mattijs Elschot,Jeroen Veltman,Baris Turkbey,Nancy A. Obuchowski,Jurgen J. Fütterer,Anwar R. Padhani,Hashim U. Ahmed,Tobias Nordström,Martin Eklund,Veeru Kasivisvanathan,Maarten de Rooij,Henkjan Huisman*

Main category: eess.IV

TL;DR: 本研究通过多中心、多国数据训练并验证了PI-CAI-2B模型，用于检测Gleason分级≥2的前列腺癌，目标是证明其与标准诊断方法的等效性。


<details>
  <summary>Details</summary>
Motivation: 开发新一代AI系统（PI-CAI-2B），以提高前列腺癌MRI检测的准确性和效率，并验证其与标准诊断方法的等效性。

Method: 使用22,481例MRI检查数据（来自46个城市的22个国家），分为训练集（20,471例）和外部测试集（2,010例），通过统计预设分析验证AI模型的诊断性能。

Result: 主要终点是AI与标准诊断方法的一致性，假设在PI-RADS≥3（诊断）或≥4（筛查）截断值下具有等效性。次要终点包括AUROC分析，以评估模型在不同条件下的表现。

Conclusion: PI-CAI-2B模型在多中心数据中表现出与标准诊断方法的一致性，有望成为前列腺癌检测的高效工具。

Abstract: In this intercontinental, confirmatory study, we include a retrospective
cohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries)
to train and externally validate the PI-CAI-2B model, i.e., an efficient,
next-generation iteration of the state-of-the-art AI system that was developed
for detecting Gleason grade group $\geq$2 prostate cancer on MRI during the
PI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities
in 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12
independent centers based in Europe, North America, Asia and Africa, are used
for training and internal testing. Additionally, 2010 cases (2010 patients; 20
external cities in 12 countries) from population-based screening (STHLM3-MRI,
IP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in
Europe, North and South Americas, Asia and Australia, are used for external
testing. Primary endpoint is the proportion of AI-based assessments in
agreement with the standard of care diagnoses (i.e., clinical assessments made
by expert uropathologists on histopathology, if available, or at least two
expert urogenital radiologists in consensus; with access to patient history and
peer consultation) in the detection of Gleason grade group $\geq$2 prostate
cancer within the external testing cohorts. Our statistical analysis plan is
prespecified with a hypothesis of diagnostic interchangeability to the standard
of care at the PI-RADS $\geq$3 (primary diagnosis) or $\geq$4 (screening)
cut-off, considering an absolute margin of 0.05 and reader estimates derived
from the PI-CAI observer study (62 radiologists reading 400 cases). Secondary
measures comprise the area under the receiver operating characteristic curve
(AUROC) of the AI system stratified by imaging quality, patient age and patient
ethnicity to identify underlying biases (if any).

</details>


### [38] [When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection](https://arxiv.org/abs/2508.03773)
*Emanuele Nardone,Tiziana D'Alessandro,Francesco Fontanella,Claudio De Stefano*

Main category: eess.IV

TL;DR: 研究探索了深度学习是否可以通过手写分析实现非侵入性阿尔茨海默病检测，发现传统机器学习方法优于循环神经网络。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病检测通常依赖昂贵或侵入性方法，限制了可及性，因此研究希望通过手写分析提供非侵入性解决方案。

Method: 使用34种手写任务数据集，比较了三种循环神经网络（LSTM、GRU、RNN）与传统机器学习模型，重点分析了从离散笔画中提取的特征。

Result: 循环神经网络表现不佳，特异性低且方差高；传统集成方法显著优于深度学习模型。

Conclusion: 循环神经网络不适合处理离散笔画特征，研究为未来数据表示和模型兼容性研究提供了方向。

Abstract: Alzheimer's disease detection requires expensive neuroimaging or invasive
procedures, limiting accessibility. This study explores whether deep learning
can enable non-invasive Alzheimer's disease detection through handwriting
analysis. Using a dataset of 34 distinct handwriting tasks collected from
healthy controls and Alzheimer's disease patients, we evaluate and compare
three recurrent neural architectures (LSTM, GRU, RNN) against traditional
machine learning models. A crucial distinction of our approach is that the
recurrent models process pre-extracted features from discrete strokes, not raw
temporal signals. This violates the assumption of a continuous temporal flow
that recurrent networks are designed to capture. Results reveal that they
exhibit poor specificity and high variance. Traditional ensemble methods
significantly outperform all deep architectures, achieving higher accuracy with
balanced metrics. This demonstrates that recurrent architectures, designed for
continuous temporal sequences, fail when applied to feature vectors extracted
from ambiguously segmented strokes. Despite their complexity, deep learning
models cannot overcome the fundamental disconnect between their architectural
assumptions and the discrete, feature-based nature of stroke-level handwriting
data. Although performance is limited, the study highlights several critical
issues in data representation and model compatibility, pointing to valuable
directions for future research.

</details>


### [39] [UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation](https://arxiv.org/abs/2508.03982)
*Jinwei Zhang,Lianrui Zuo,Blake E. Dewey,Samuel W. Remedios,Yihao Liu,Savannah P. Hays,Dzung L. Pham,Ellen M. Mowry,Scott D. Newsome,Peter A. Calabresi,Aaron Carass,Jerry L. Prince*

Main category: eess.IV

TL;DR: UNISELF是一种深度学习方法，用于多发性硬化（MS）病变的自动分割，通过测试时自集成和特征归一化技术，在单源训练数据下实现了高精度和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在单源有限数据训练时难以同时优化域内精度和域外泛化能力，UNISELF旨在填补这一空白。

Method: UNISELF采用测试时自集成病变融合和测试时实例归一化（TTIN）技术，处理域偏移和缺失输入对比度问题。

Result: 在ISBI 2015测试数据集上表现优异，并在多个域外测试数据集（如MICCAI 2016和UMCL）上超越基准方法。

Conclusion: UNISELF在单源训练下实现了高精度和强泛化能力，适用于多变的临床数据场景。

Abstract: Automated segmentation of multiple sclerosis (MS) lesions using multicontrast
magnetic resonance (MR) images improves efficiency and reproducibility compared
to manual delineation, with deep learning (DL) methods achieving
state-of-the-art performance. However, these DL-based methods have yet to
simultaneously optimize in-domain accuracy and out-of-domain generalization
when trained on a single source with limited data, or their performance has
been unsatisfactory. To fill this gap, we propose a method called UNISELF,
which achieves high accuracy within a single training domain while
demonstrating strong generalizability across multiple out-of-domain test
datasets. UNISELF employs a novel test-time self-ensembled lesion fusion to
improve segmentation accuracy, and leverages test-time instance normalization
(TTIN) of latent features to address domain shifts and missing input contrasts.
Trained on the ISBI 2015 longitudinal MS segmentation challenge training
dataset, UNISELF ranks among the best-performing methods on the challenge test
dataset. Additionally, UNISELF outperforms all benchmark methods trained on the
same ISBI training data across diverse out-of-domain test datasets with domain
shifts and missing contrasts, including the public MICCAI 2016 and UMCL
datasets, as well as a private multisite dataset. These test datasets exhibit
domain shifts and/or missing contrasts caused by variations in acquisition
protocols, scanner types, and imaging artifacts arising from imperfect
acquisition. Our code is available at https://github.com/uponacceptance.

</details>


### [40] [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
*Yichi Zhang,Wenbo Zhang,Zehui Ling,Gang Feng,Sisi Peng,Deshu Chen,Yuchen Liu,Hongwei Zhang,Shuqi Wang,Lanlan Li,Limei Han,Yuan Cheng,Zixin Hu,Yuan Qi,Le Xue*

Main category: eess.IV

TL;DR: 论文介绍了PET2Rep，首个专注于PET影像报告生成的基准数据集，填补了现有基准的空白，并评估了30种前沿视觉语言模型（VLMs）的表现，发现当前最先进模型在PET报告生成任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: PET影像在临床决策中至关重要，但手动生成报告耗时耗力。现有视觉语言模型（VLMs）主要关注结构影像，忽略了PET影像的代谢特性，因此需要填补这一空白。

Method: 提出PET2Rep数据集，包含全身影像-报告对，并引入临床效率指标评估报告质量。对30种前沿VLMs进行对比分析。

Result: 当前最先进的VLMs在PET报告生成任务上表现不佳，未能满足实际需求。

Conclusion: 研究揭示了VLMs在PET报告生成中的不足，提出了未来改进的关键方向。

Abstract: Positron emission tomography (PET) is a cornerstone of modern oncologic and
neurologic imaging, distinguished by its unique ability to illuminate dynamic
metabolic processes that transcend the anatomical focus of traditional imaging
technologies. Radiology reports are essential for clinical decision making, yet
their manual creation is labor-intensive and time-consuming. Recent
advancements of vision-language models (VLMs) have shown strong potential in
medical applications, presenting a promising avenue for automating report
generation. However, existing applications of VLMs in the medical domain have
predominantly focused on structural imaging modalities, while the unique
characteristics of molecular PET imaging have largely been overlooked. To
bridge the gap, we introduce PET2Rep, a large-scale comprehensive benchmark for
evaluation of general and medical VLMs for radiology report generation for PET
images. PET2Rep stands out as the first dedicated dataset for PET report
generation with metabolic information, uniquely capturing whole-body
image-report pairs that cover dozens of organs to fill the critical gap in
existing benchmarks and mirror real-world clinical comprehensiveness. In
addition to widely recognized natural language generation metrics, we introduce
a series of clinical efficiency metrics to evaluate the quality of radiotracer
uptake pattern description in key organs in generated reports. We conduct a
head-to-head comparison of 30 cutting-edge general-purpose and
medical-specialized VLMs. The results show that the current state-of-the-art
VLMs perform poorly on PET report generation task, falling considerably short
of fulfilling practical needs. Moreover, we identify several key insufficiency
that need to be addressed to advance the development in medical applications.

</details>


### [41] [Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation](https://arxiv.org/abs/2508.04305)
*Nathan Hollet,Oumeymah Cherkaoui,Philippe C. Cattin,Sidaty El hadramy*

Main category: eess.IV

TL;DR: Edge2Prompt是一种新型多模态肝脏分割方法，结合边缘检测和基础模型，适用于数据稀缺和分布外任务。


<details>
  <summary>Details</summary>
Motivation: 解决临床工作流中因模态特异性工具和数据稀缺导致的肝脏分割挑战。

Method: 通过边缘检测生成模态无关的边缘图，利用U-Net生成提示，再通过SAM-2模型完成分割。

Result: 在CHAOS数据集上表现优异，OOD任务中Dice分数达86.4%，优于基线方法。

Conclusion: Edge2Prompt结合经典与基础模型，实现了临床适应性强且数据高效的分割。

Abstract: Liver segmentation is essential for preoperative planning in interventions
like tumor resection or transplantation, but implementation in clinical
workflows faces challenges due to modality-specific tools and data scarcity. We
propose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentation
that generalizes to out-of-distribution (OOD) data. Our method integrates
classical edge detection with foundation models. Modality-agnostic edge maps
are first extracted from input images, then processed by a U-Net to generate
logit-based prompts. These prompts condition the Segment Anything Model 2
(SAM-2) to generate 2D liver segmentations, which can then be reconstructed
into 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Prompt
achieves competitive results compared to classical segmentation methods when
trained and tested in-distribution (ID), and outperforms them in data-scarce
scenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Score
of 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and other
self-prompting methods by 9.1%, demonstrating its effectiveness. This work
bridges classical and foundation models for clinically adaptable,
data-efficient segmentation.

</details>


### [42] [Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics Using Dynamic Susceptibility Contrast MRI](https://arxiv.org/abs/2508.04404)
*Marijn Borghouts,Richard McKinley,Josien Pluim,Manuel Köstner,Roland Wiest,Ruisheng Su*

Main category: eess.IV

TL;DR: 该研究探讨了磁共振灌注成像（MRP）在区分远端急性缺血性卒中（AIS）与癫痫发作（一种常见的卒中模拟）中的潜力，通过分析灌注图描述符（PMDs）取得了较高的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 区分急性缺血性卒中（AIS）与卒中模拟（SMs），尤其是涉及中小血管闭塞的情况，是一个重要的诊断挑战。CT在急诊中的敏感性有限，因此需要更有效的工具。

Method: 研究使用162名患者（129例AIS，33例癫痫发作）的回顾性数据集，从动态磁敏感对比（DSC）图像中提取区域灌注图描述符（PMDs），并通过统计分析和逻辑回归模型评估其诊断性能。

Result: 逻辑回归模型在PMDs上的AUROC为0.90，AUPRC为0.74，特异性92%，敏感性73%，表明其在区分远端AIS与癫痫发作方面表现优异。

Conclusion: MRP-based PMDs可作为区分真实卒中与模拟的可靠特征，值得进一步研究。代码已公开。

Abstract: Distinguishing acute ischemic strokes (AIS) from stroke mimics (SMs),
particularly in cases involving medium and small vessel occlusions, remains a
significant diagnostic challenge. While computed tomography (CT) based
protocols are commonly used in emergency settings, their sensitivity for
detecting distal occlusions is limited. This study explores the potential of
magnetic resonance perfusion (MRP) imaging as a tool for differentiating distal
AIS from epileptic seizures, a prevalent SM. Using a retrospective dataset of
162 patients (129 AIS, 33 seizures), we extracted region-wise perfusion map
descriptors (PMDs) from dynamic susceptibility contrast (DSC) images.
Statistical analyses identified several brain regions, located mainly in the
temporal and occipital lobe, exhibiting significant group differences in
certain PMDs. Hemispheric asymmetry analyses further highlighted these regions
as discriminative. A logistic regression model trained on PMDs achieved an area
under the receiver operating characteristic (AUROC) curve of 0.90, and an area
under the precision recall curve (AUPRC) of 0.74, with a specificity of 92% and
a sensitivity of 73%, suggesting strong performance in distinguishing distal
AIS from seizures. These findings support further exploration of MRP-based PMDs
as interpretable features for distinguishing true strokes from various mimics.
The code is openly available at our GitHub
https://github.com/Marijn311/PMD_extraction_and_analysis{github.com/Marijn311/PMD\_extraction\_and\_analysis

</details>


### [43] [Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis](https://arxiv.org/abs/2508.04429)
*Ethan Dack,Lorenzo Brigato,Vasilis Dedousis,Janine Gote-Schniering,Cheryl,Hanno Hoppe,Aristomenis Exadaktylos,Manuela Funke-Chambour,Thomas Geiser,Andreas Christe,Lukas Ebner,Stavroula Mougiakakou*

Main category: eess.IV

TL;DR: MAEs用于无标签数据预训练，在稀缺标注数据的弥漫性肺病研究中表现优异，通过5000多张CT扫描训练，显著提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决弥漫性肺病研究中标注数据稀缺的问题，利用MAEs从无标签数据中学习有效特征。

Method: 使用5000多张胸部CT扫描（包括内部和公开数据）预训练MAE，并在下游分类任务中微调。

Result: MAE能提取临床相关特征，显著提升诊断性能，即使在小规模标注数据集下。

Conclusion: MAEs是解决标注数据稀缺问题的有效工具，可显著提升弥漫性肺病诊断性能。

Abstract: Masked autoencoders (MAEs) have emerged as a powerful approach for
pre-training on unlabelled data, capable of learning robust and informative
feature representations. This is particularly advantageous in diffused lung
disease research, where annotated imaging datasets are scarce. To leverage
this, we train an MAE on a curated collection of over 5,000 chest computed
tomography (CT) scans, combining in-house data with publicly available scans
from related conditions that exhibit similar radiological patterns, such as
COVID-19 and bacterial pneumonia. The pretrained MAE is then fine-tuned on a
downstream classification task for diffused lung disease diagnosis. Our
findings demonstrate that MAEs can effectively extract clinically meaningful
features and improve diagnostic performance, even in the absence of large-scale
labelled datasets. The code and the models are available here:
https://github.com/eedack01/lung_masked_autoencoder.

</details>


### [44] [TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration](https://arxiv.org/abs/2508.04450)
*Xuan Loc Pham,Gwendolyn Vuurberg,Marjan Doppen,Joey Roosen,Tip Stille,Thi Quynh Ha,Thuy Duong Quach,Quoc Vu Dang,Manh Ha Luu,Ewoud J. Smit,Hong Son Mai,Mattias Heinrich,Bram van Ginneken,Mathias Prokop,Alessa Hering*

Main category: eess.IV

TL;DR: TotalRegistrator是一个基于UNet架构和新型场分解策略的图像配准框架，能够同时对齐多个解剖区域，具有轻量化和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像配准方法多针对单器官设计，限制了在其他解剖区域的通用性。

Method: 采用标准UNet架构和新型场分解策略，构建轻量化模型（训练仅需11GB GPU内存）。

Result: 在内部数据集上优于基线方法，在外部数据集上表现竞争力强，展示了良好的泛化能力。

Conclusion: TotalRegistrator在多器官配准任务中表现出色，具有广泛的应用潜力。

Abstract: Image registration is a fundamental technique in the analysis of longitudinal
and multi-phase CT images within clinical practice. However, most existing
methods are tailored for single-organ applications, limiting their
generalizability to other anatomical regions. This work presents
TotalRegistrator, an image registration framework capable of aligning multiple
anatomical regions simultaneously using a standard UNet architecture and a
novel field decomposition strategy. The model is lightweight, requiring only
11GB of GPU memory for training. To train and evaluate our method, we
constructed a large-scale longitudinal dataset comprising 695 whole-body
(thorax-abdomen-pelvic) paired CT scans from individual patients acquired at
different time points. We benchmarked TotalRegistrator against a generic
classical iterative algorithm and a recent foundation model for image
registration. To further assess robustness and generalizability, we evaluated
our model on three external datasets: the public thoracic and abdominal
datasets from the Learn2Reg challenge, and a private multiphase abdominal
dataset from a collaborating hospital. Experimental results on the in-house
dataset show that the proposed approach generally surpasses baseline methods in
multi-organ abdominal registration, with a slight drop in lung alignment
performance. On out-of-distribution datasets, it achieved competitive results
compared to leading single-organ models, despite not being fine-tuned for those
tasks, demonstrating strong generalizability. The source code will be publicly
available at: https://github.com/DIAGNijmegen/oncology_image_registration.git.

</details>


### [45] [OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs](https://arxiv.org/abs/2508.04491)
*Yichi Zhang,Fengqing Zhu*

Main category: eess.IV

TL;DR: OpenDCVCs是一个开源的PyTorch实现，旨在推动学习视频压缩的可重复研究，提供四种代表性DCVC模型的统一实现。


<details>
  <summary>Details</summary>
Motivation: 解决现有公开代码仅限于评估代码的问题，促进可重复性、基准测试和进一步开发。

Method: 提供包含详细文档、评估协议和广泛基准测试结果的综合框架，支持端到端训练和评估。

Result: OpenDCVCs为社区提供了透明一致的比较和扩展基础。

Conclusion: 该开源工具加速了研究并促进了协作。

Abstract: We present OpenDCVCs, an open-source PyTorch implementation designed to
advance reproducible research in learned video compression. OpenDCVCs provides
unified and training-ready implementations of four representative Deep
Contextual Video Compression (DCVC) models--DCVC, DCVC with Temporal Context
Modeling (DCVC-TCM), DCVC with Hybrid Entropy Modeling (DCVC-HEM), and DCVC
with Diverse Contexts (DCVC-DC). While the DCVC series achieves substantial
bitrate reductions over both classical codecs and advanced learned models,
previous public code releases have been limited to evaluation codes, presenting
significant barriers to reproducibility, benchmarking, and further development.
OpenDCVCs bridges this gap by offering a comprehensive, self-contained
framework that supports both end-to-end training and evaluation for all
included algorithms. The implementation includes detailed documentation,
evaluation protocols, and extensive benchmarking results across diverse
datasets, providing a transparent and consistent foundation for comparison and
extension. All code and experimental tools are publicly available at
https://gitlab.com/viper-purdue/opendcvcs, empowering the community to
accelerate research and foster collaboration.

</details>


### [46] [Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation](https://arxiv.org/abs/2508.04522)
*Johannes Tischer,Patric Kienast,Marlene Stümpflen,Gregor Kasprian,Georg Langs,Roxane Licandro*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的连续年龄特异性胎儿脑图谱生成框架，用于实时脑组织分割，具有高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI评估因脑成熟度、成像协议和孕龄估计的变异性而具有挑战性，需要标准化参考框架。

Method: 结合直接配准模型和条件判别器，训练219例21至37孕周的胎儿MRI数据。

Result: 实现了高配准精度和86.3%的平均DSC分割性能，揭示了详细的神经典型生长轨迹。

Conclusion: 该方法支持个体化发育评估，适用于研究和临床，代码已开源。

Abstract: Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for
studying brain development in vivo. Yet, its assessment remains challenging due
to variability in brain maturation, imaging protocols, and uncertain estimates
of Gestational Age (GA). To overcome these, brain atlases provide a
standardized reference framework that facilitates objective evaluation and
comparison across subjects by aligning the atlas and subjects in a common
coordinate system. In this work, we introduce a novel deep-learning framework
for generating continuous, age-specific fetal brain atlases for real-time fetal
brain tissue segmentation. The framework combines a direct registration model
with a conditional discriminator. Trained on a curated dataset of 219
neurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method
achieves high registration accuracy, captures dynamic anatomical changes with
sharp structural detail, and robust segmentation performance with an average
Dice Similarity Coefficient (DSC) of 86.3% across six brain tissues.
Furthermore, volumetric analysis of the generated atlases reveals detailed
neurotypical growth trajectories, providing valuable insights into the
maturation of the fetal brain. This approach enables individualized
developmental assessment with minimal pre-processing and real-time performance,
supporting both research and clinical applications. The model code is available
at https://github.com/cirmuw/fetal-brain-atlas

</details>


### [47] [LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation](https://arxiv.org/abs/2508.04553)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: eess.IV

TL;DR: 提出了一种名为LA-CaRe-CNN的两阶段CNN模型，用于从LGE MR扫描中准确分割左心房和左心房瘢痕组织，以支持个性化消融治疗。


<details>
  <summary>Details</summary>
Motivation: 心房颤动（AF）是最常见的心律失常，消融治疗需要准确分割健康与瘢痕组织。患者特异性心脏数字孪生模型对此有需求，但需要精确的语义分割。

Method: LA-CaRe-CNN是一个两阶段的3D CNN级联模型，第一阶段预测左心房，第二阶段结合原始图像信息细化瘢痕组织预测。采用强化的强度和空间数据增强以应对训练中未知的域偏移。

Result: 模型在5折集成下表现优异：左心房分割DSC为89.21%，ASSD为1.6969 mm；左心房瘢痕组织DSC为64.59%，G-DSC为91.80%。

Conclusion: LA-CaRe-CNN的分割结果对生成患者特异性心脏数字孪生模型及个性化消融治疗具有重要潜力。

Abstract: Atrial fibrillation (AF) represents the most prevalent type of cardiac
arrhythmia for which treatment may require patients to undergo ablation
therapy. In this surgery cardiac tissues are locally scarred on purpose to
prevent electrical signals from causing arrhythmia. Patient-specific cardiac
digital twin models show great potential for personalized ablation therapy,
however, they demand accurate semantic segmentation of healthy and scarred
tissue typically obtained from late gadolinium enhanced (LGE) magnetic
resonance (MR) scans. In this work we propose the Left Atrial Cascading
Refinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium
as well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage
CNN cascade that is trained end-to-end in 3D, where Stage 1 generates a
prediction for the left atrium, which is then refined in Stage 2 in conjunction
with the original image information to obtain a prediction for the left atrial
scar tissue. To account for domain shift towards domains unknown during
training, we employ strong intensity and spatial augmentation to increase the
diversity of the training dataset. Our proposed method based on a 5-fold
ensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm
ASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more
challenging left atrial scar tissue. Thus, segmentations obtained through
LA-CaRe-CNN show great potential for the generation of patient-specific cardiac
digital twin models and downstream tasks like personalized targeted ablation
therapy to treat AF.

</details>


### [48] [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](https://arxiv.org/abs/2508.04588)
*Nicola Casali,Alessandro Brusaferri,Giuseppe Baselli,Stefano Fumagalli,Edoardo Micotti,Gianluigi Forloni,Riaz Hussein,Giovanna Rizzo,Alfonso Mastropietro*

Main category: eess.IV

TL;DR: 提出了一种基于深度集成和混合密度网络的概率深度学习框架，用于估计IVIM参数及其不确定性，并在合成和真实数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: IVIM参数估计因逆问题的不适定性和噪声敏感性而具有挑战性，需要一种能量化不确定性的方法。

Method: 使用深度集成（DE）和混合密度网络（MDN）构建概率深度学习框架，分解不确定性为偶然性和认知性。

Result: MDN在D和f参数上表现更优，但D*略有过度自信；认知性不确定性显示真实数据与训练数据存在不匹配。

Conclusion: 该框架为IVIM拟合提供了全面的不确定性量化方法，并可推广至其他物理模型。

Abstract: Accurate estimation of intravoxel incoherent motion (IVIM) parameters from
diffusion-weighted MRI remains challenging due to the ill-posed nature of the
inverse problem and high sensitivity to noise, particularly in the perfusion
compartment. In this work, we propose a probabilistic deep learning framework
based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling
estimation of total predictive uncertainty and decomposition into aleatoric
(AU) and epistemic (EU) components. The method was benchmarked against non
probabilistic neural networks, a Bayesian fitting approach and a probabilistic
network with single Gaussian parametrization. Supervised training was performed
on synthetic data, and evaluation was conducted on both simulated and two in
vivo datasets. The reliability of the quantified uncertainties was assessed
using calibration curves, output distribution sharpness, and the Continuous
Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper
predictive distributions for the D and f parameters, although slight
overconfidence was observed in D*. The Robust Coefficient of Variation (RCV)
indicated smoother in vivo estimates for D* with MDNs compared to Gaussian
model. Despite the training data covering the expected physiological range,
elevated EU in vivo suggests a mismatch with real acquisition conditions,
highlighting the importance of incorporating EU, which was allowed by DE.
Overall, we present a comprehensive framework for IVIM fitting with uncertainty
quantification, which enables the identification and interpretation of
unreliable estimates. The proposed approach can also be adopted for fitting
other physical models through appropriate architectural and simulation
adjustments.

</details>
