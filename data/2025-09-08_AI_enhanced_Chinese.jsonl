{"id": "2509.04576", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04576", "abs": "https://arxiv.org/abs/2509.04576", "authors": ["Ce Zheng", "Tingting Yang"], "title": "Communication-Efficient Collaborative LLM Inference via Distributed Speculative Decoding", "comment": "Accepted in the Seventeenth International Conference on Wireless\n  Communications and Signal Processing Oct. 23-25, 2025", "summary": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method.", "AI": {"tldr": "\u901a\u8fc7\u4f20\u8f93\u53ea\u6700\u5927\u7684K\u4e2a\u4ee4\u724c\u7684\u539f\u59cb\u6982\u7387\u548c\u7d22\u5f15\uff0c\u5927\u5e45\u51cf\u5c11\u5206\u5e03\u5f0f\u731c\u6d4b\u89e3\u7801\u7684\u4e0a\u884c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u731c\u6d4b\u89e3\u7801\u9700\u8981\u4ece\u8bbe\u5907\u4f20\u8f93\u5168\u90e8\u8bcd\u6c47\u8868\u6982\u7387\u5206\u5e03\u5230\u57fa\u7ad9\uff0c\u5bfc\u81f4\u4e0a\u884c\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\uff0c\u7279\u522b\u5728AI-RAN\u7f51\u7edc\u4e2d\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\"Top-K\u7a00\u758f\u5bf9\u6570\u6982\u7387\u4f20\u8f93(TK-SLT)\"\u65b9\u6848\uff0c\u53ea\u4f20\u8f93top-K\u4ee4\u724c\u7684\u539f\u59cb\u6982\u7387\u548c\u5bf9\u5e94\u7d22\u5f15\uff0c\u800c\u4e0d\u662f\u6574\u4e2a\u5206\u5e03\u3002\u8fd8\u63a8\u5bfc\u4e86\u6700\u4f18\u8349\u7a3f\u957f\u5ea6\u7684\u5206\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TK-SLT\u65b9\u6848\u80fd\u591f\u5728AI-RAN\u7f51\u7edc\u4e2d\u9ad8\u6548\u5730\u51cf\u5c11\u5206\u5e03\u5f0f\u731c\u6d4b\u89e3\u7801\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u52a0\u901f\u6548\u679c\u3002"}}
{"id": "2509.04692", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04692", "abs": "https://arxiv.org/abs/2509.04692", "authors": ["Michael Shifrin", "Joseph Tabrikian", "Igal Bilik"], "title": "Tangential Velocity Estimation Using Near-Field Automotive Radar Model", "comment": null, "summary": "This work investigates the problem of tangential velocity estimation in\nautomotive radar systems, addressing the limitations of conventionally\nconsidered models. Conventional automotive radars are usually based on\nfar-field models and estimate the target's range, radial velocity, and\ndirection-of-arrival (DOA) but are not able to estimate the tangential\ncomponent of the target 2-D velocity, which is a critical parameter for\nreliable perception of dynamic environments. To address this challenge, we\nintroduce the near-field radar model, which considers various migration\nelements in range, radial velocity, and Doppler along time and space.\nConventionally, these migration effects result in smearing of the likelihood\nfunction for estimating the target parameters. However, if the model is\ncorrectly specified, these migration effects are informative for tangential\nvelocity estimation. We conduct an identifiability analysis for tangential\nvelocity estimation using the Cram\\'er-Rao bound and ambiguity function. The\ninsights from this study motivate the use of a separated array configuration\nand the development of a computationally efficient maximum likelihood based\nalgorithm designed to utilize target migrations for tangential velocity\nestimation, while maintaining practical computational complexity. In addition\nto tangential velocity estimation, the proposed algorithm mitigates likelihood\nsmearing in range, radial velocity, and Doppler. Simulations validate the\ntheoretical feasibility study, and evaluate the algorithms' performance in both\nsingle- and multi-target scenarios. The proposed approach improves the accuracy\nand reliability of automotive radars, enhancing situational awareness for\nadvanced driver assistance systems and autonomous vehicles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\u7684\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8f66\u8f7d\u96f7\u8fbe\u65e0\u6cd5\u4f30\u8ba1\u76ee\u6807\u5207\u5411\u901f\u5ea6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u5728\u8ddd\u79bb\u3001\u5f84\u5411\u901f\u5ea6\u548c\u591a\u666e\u52d2\u9891\u79fb\u4e0a\u7684\u8fc1\u79fb\u6548\u5e94\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f7d\u96f7\u8fbe\u57fa\u4e8e\u8fdc\u573a\u6a21\u578b\uff0c\u53ea\u80fd\u4f30\u8ba1\u76ee\u6807\u7684\u8ddd\u79bb\u3001\u5f84\u5411\u901f\u5ea6\u548c\u5230\u8fbe\u65b9\u5411\uff0c\u4f46\u65e0\u6cd5\u4f30\u8ba1\u5207\u5411\u901f\u5ea6\u8fd9\u4e00\u5bf9\u52a8\u6001\u73af\u5883\u611f\u77e5\u81f3\u5173\u91cd\u8981\u7684\u53c2\u6570\u3002\u8fd1\u573a\u6a21\u578b\u4e2d\u7684\u8fc1\u79fb\u6548\u5e94\u867d\u7136\u901a\u5e38\u4f1a\u5bfc\u81f4\u4f3c\u7136\u51fd\u6570\u6a21\u7cca\uff0c\u4f46\u5982\u679c\u6b63\u786e\u5efa\u6a21\uff0c\u8fd9\u4e9b\u6548\u5e94\u53ef\u4ee5\u4e3a\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u6709\u7528\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u79bb\u9635\u5217\u914d\u7f6e\u548c\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u5728\u8ddd\u79bb\u3001\u5f84\u5411\u901f\u5ea6\u548c\u591a\u666e\u52d2\u4e0a\u7684\u8fc1\u79fb\u6548\u5e94\u8fdb\u884c\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u5ea6\u5728\u5b9e\u7528\u8303\u56f4\u5185\u3002\u901a\u8fc7\u514b\u62c9\u7f8e-\u7f57\u754c\u548c\u6a21\u7cca\u51fd\u6570\u8fdb\u884c\u53ef\u8fa8\u8bc6\u6027\u5206\u6790\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u53ef\u884c\u6027\u7814\u7a76\uff0c\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u7b97\u6cd5\u6027\u80fd\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8f66\u8f7d\u96f7\u8fbe\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u589e\u5f3a\u4e86\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\u5728\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0d\u4ec5\u80fd\u591f\u4f30\u8ba1\u5207\u5411\u901f\u5ea6\uff0c\u8fd8\u80fd\u51cf\u8f7b\u8ddd\u79bb\u3001\u5f84\u5411\u901f\u5ea6\u548c\u591a\u666e\u52d2\u4e0a\u7684\u4f3c\u7136\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.04768", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04768", "abs": "https://arxiv.org/abs/2509.04768", "authors": ["Yilong Chen", "Zixiang Ren", "Jie Xu", "Rui Zhang"], "title": "Environment-Aware IRS Deployment via Channel Knowledge Map: Joint Sensing-Communications Coverage Optimization", "comment": "13 pages, 11 figures", "summary": "This paper studies the intelligent reflecting surface (IRS) deployment\noptimization problem for IRS-enabled integrated sensing and communications\n(ISAC) systems, in which multiple IRSs are strategically deployed at candidate\nlocations to assist a base station (BS) to enhance the coverage of both sensing\nand communications. We present an environment-aware IRS deployment design via\nexploiting the channel knowledge map (CKM), which provides the channel state\ninformation (CSI) between each candidate IRS location and BS or targeted\nsensing/communication points. Based on the obtained CSI from CKM, we optimize\nthe deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'\nreflective beamforming during operation, with the objective of minimizing the\nsystem cost, while guaranteeing the minimum illumination power requirements at\nsensing areas and the minimum signal-to-noise ratio (SNR) requirements at\ncommunication areas. In particular, we consider two cases when the IRSs'\nreflective beamforming optimization can be implemented dynamically in real time\nand quasi-stationarily over the whole operation period, respectively. For both\ncases, the joint IRS deployment and transmit/reflective beamforming designs are\nformulated as mixed-integer non-convex optimization problems, which are solved\nvia the successive convex approximation (SCA)-based relax-and-bound method.\nSpecifically, we first relax the binary IRS deployment indicators into\ncontinuous variables, then find converged solutions via SCA, and finally round\nrelaxed indicators back to binary values. Numerical results demonstrate the\neffectiveness of our proposed algorithms in reducing the system cost while\nmeeting the sensing and communication requirements.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u667a\u80fd\u53cd\u5c04\u9762(IRS)\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u4fe1\u9053\u77e5\u8bc6\u5730\u56fe(CKM)\u6765\u4f18\u5316IRS\u90e8\u7f72\u4f4d\u7f6e\u3001\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cIRS\u53cd\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u6210\u672c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u548c\u901a\u4fe1\u9700\u6c42\u3002", "motivation": "\u968f\u7740ISAC\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u6709\u6548\u90e8\u7f72\u591a\u4e2aIRS\u6765\u540c\u65f6\u589e\u5f3a\u611f\u77e5\u548c\u901a\u4fe1\u8986\u76d6\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u73af\u5883\u4fe1\u606f\u7684\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u4e00\u79cd\u73af\u5883\u611f\u77e5\u7684\u90e8\u7f72\u65b9\u6848\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCKM\u7684\u73af\u5883\u611f\u77e5IRS\u90e8\u7f72\u8bbe\u8ba1\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002\u91c7\u7528\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c(SCA)\u7684\u677e\u5f1b-\u5b9a\u754c\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u4e8c\u8fdb\u5236\u90e8\u7f72\u6307\u6807\u677e\u5f1b\u4e3a\u8fde\u7eed\u53d8\u91cf\uff0c\u901a\u8fc7SCA\u627e\u5230\u6536\u655b\u89e3\uff0c\u6700\u540e\u5c06\u677e\u5f1b\u6307\u6807\u56db\u820d\u4e94\u5165\u4e3a\u4e8c\u8fdb\u5236\u503c\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6ee1\u8db3\u611f\u77e5\u548c\u901a\u4fe1\u8981\u6c42\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u3002\u7b97\u6cd5\u9002\u7528\u4e8eIRS\u53cd\u5c04\u6ce2\u675f\u6210\u5f62\u7684\u5b9e\u65f6\u52a8\u6001\u4f18\u5316\u548c\u51c6\u9759\u6001\u4f18\u5316\u4e24\u79cd\u60c5\u51b5\u3002", "conclusion": "\u57fa\u4e8eCKM\u7684IRS\u90e8\u7f72\u4f18\u5316\u65b9\u6cd5\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u73af\u5883\u611f\u77e5\u90e8\u7f72\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\u7684\u540c\u65f6\u4f18\u5316\u7cfb\u7edf\u8d44\u6e90\u914d\u7f6e\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.04787", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04787", "abs": "https://arxiv.org/abs/2509.04787", "authors": ["Zhidi Zhang", "Rui Meng", "Song Gao", "Haixiao Gao", "Xiaodong Xu"], "title": "SREC: Encrypted Semantic Super-Resolution Enhanced Communication", "comment": "7 pages, 6 figures, conference", "summary": "Semantic communication (SemCom), as a typical paradigm of deep integration\nbetween artificial intelligence (AI) and communication technology,\nsignificantly improves communication efficiency and resource utilization\nefficiency. However, the security issues of SemCom are becoming increasingly\nprominent. Semantic features transmitted in plaintext over physical channels\nare easily intercepted by eavesdroppers. To address this issue, this paper\nproposes Encrypted Semantic Super-Resolution Enhanced Communication (SREC) to\nsecure SemCom. SREC uses the modulo-256 encryption method to encrypt semantic\nfeatures, and employs super-resolution reconstruction method to improve the\nreconstruction quality of images. The simulation results show that in the\nadditive Gaussian white noise (AWGN) channel, when different modulation methods\nare used, SREC can not only stably guarantee security, but also achieve better\ntransmission performance under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "\u63d0\u51fa\u52a0\u5bc6\u8bed\u4e49\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u901a\u4fe1(SREC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21256\u52a0\u5bc6\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6280\u672f\u4fdd\u62a4\u8bed\u4e49\u901a\u4fe1\u5b89\u5168\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u56fe\u50cf\u4f20\u8f93\u3002", "motivation": "\u8bed\u4e49\u901a\u4fe1(SemCom)\u4f5c\u4e3aAI\u4e0e\u901a\u4fe1\u6280\u672f\u6df1\u5ea6\u878d\u5408\u7684\u5178\u578b\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u660e\u6587\u4f20\u8f93\u7684\u8bed\u4e49\u7279\u5f81\u5bb9\u6613\u88ab\u7a83\u542c\u8005\u622a\u83b7\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6a21256\u52a0\u5bc6\u65b9\u6cd5\u5bf9\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u52a0\u5bc6\uff0c\u5e76\u91c7\u7528\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\u6765\u63d0\u9ad8\u56fe\u50cf\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u5728\u4e0d\u540c\u8c03\u5236\u65b9\u6cd5\u4e0b\u6d4b\u8bd5\u52a0\u5bc6\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "result": "\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0(AWGN)\u4fe1\u9053\u4e2d\uff0c\u5f53\u4f7f\u7528\u4e0d\u540c\u8c03\u5236\u65b9\u6cd5\u65f6\uff0cSREC\u4e0d\u4ec5\u80fd\u7a33\u5b9a\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u8fd8\u80fd\u5728\u4f4e\u4fe1\u566a\u6bd4(SNR)\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u4f20\u8f93\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684SREC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u52a0\u5bc6\u548c\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u7ed3\u5408\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.04865", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.04865", "abs": "https://arxiv.org/abs/2509.04865", "authors": ["Yunpu Zhang", "Changsheng You", "Hing Cheung So", "Dusit Niyato"], "title": "Rotatable Antenna Aided Mixed Near-Field and Far-Field Communications in the Upper Mid-Band: Interference Analysis and Joint Optimization", "comment": "13 pages, 12 figures", "summary": "In this paper, we propose to leverage rotatable antennas (RAs) for improving\nthe communication performance in mixed near-field and far-field communication\nsystems by exploiting a new spatial degree-of-freedom (DoF) offered by antenna\nrotation to mitigate complex near-field interference and mixed-field\ninterference. Specifically, we investigate a modular RA-enabled mixed-field\ndownlink communication system, where a base station (BS) consisting of multiple\nRA subarrays communicates with multiple near-field users in the presence of\nseveral legacy far-field users. We formulate an optimization problem to\nmaximize the sum-rate of the near-field users by jointly optimizing the power\nallocation and rotation angles of all subarrays at the BS. To gain useful\ninsights into the effect of RAs on mixed-field communications, we first analyze\na special case where all subarrays share the same rotation angle and obtain\nclosed-form expressions for the rotation-aware normalized near-field\ninterference and the rotation-aware normalized mixed-field interference using\nthe Fresnel integrals. We then analytically reveal that array rotation\neffectively suppresses both interference types, thereby significantly enhancing\nmixed-field communication performance. For the general case involving\nsubarray-wise rotation, we propose an efficient double-layer algorithm to\nobtain a high-quality solution, where the inner layer optimizes power\nallocation using the successive convex approximation (SCA) technique, while the\nouter layer determines the rotation angles of all subarrays via particle swarm\noptimization (PSO). Finally, numerical results highlight the significant\nperformance gains achieved by RAs over conventional fixed-antenna systems and\ndemonstrate the effectiveness of our developed joint design compared to\nbenchmark schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u53ef\u65cb\u8f6c\u5929\u7ebf(RAs)\u6765\u6539\u5584\u6df7\u5408\u8fd1\u573a\u8fdc\u573a\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\uff0c\u901a\u8fc7\u5929\u7ebf\u65cb\u8f6c\u63d0\u4f9b\u65b0\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\u6765\u6291\u5236\u590d\u6742\u7684\u8fd1\u573a\u5e72\u6270\u548c\u6df7\u5408\u573a\u5e72\u6270\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\u5728\u6df7\u5408\u8fd1\u573a\u8fdc\u573a\u901a\u4fe1\u573a\u666f\u4e2d\u9762\u4e34\u590d\u6742\u7684\u5e72\u6270\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u624b\u6bb5\u6765\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316RA\u4f7f\u80fd\u7684\u6df7\u5408\u573a\u4e0b\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u5b50\u9635\u5217\u65cb\u8f6c\u89d2\u5ea6\u6765\u6700\u5927\u5316\u8fd1\u573a\u7528\u6237\u7684\u548c\u901f\u7387\u3002\u91c7\u7528\u53cc\u5c42\u7b97\u6cd5\uff1a\u5185\u5c42\u4f7f\u7528SCA\u6280\u672f\u4f18\u5316\u529f\u7387\u5206\u914d\uff0c\u5916\u5c42\u4f7f\u7528PSO\u786e\u5b9a\u65cb\u8f6c\u89d2\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5929\u7ebf\u65cb\u8f6c\u80fd\u6709\u6548\u6291\u5236\u8fd1\u573a\u5e72\u6270\u548c\u6df7\u5408\u573a\u5e72\u6270\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\u83b7\u5f97\u663e\u8457\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u53ef\u65cb\u8f6c\u5929\u7ebf\u4e3a\u6df7\u5408\u8fd1\u573a\u8fdc\u573a\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5e72\u6270\u6291\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7a7a\u95f4\u81ea\u7531\u5ea6\u4f18\u5316\u663e\u8457\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.04677", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04677", "abs": "https://arxiv.org/abs/2509.04677", "authors": ["Mayur S Gowda", "John Shi", "Augusto Santos", "Jos\u00e9 M. F. Moura"], "title": "Inferring the Graph Structure of Images for Graph Neural Networks", "comment": null, "summary": "Image datasets such as MNIST are a key benchmark for testing Graph Neural\nNetwork (GNN) architectures. The images are traditionally represented as a grid\ngraph with each node representing a pixel and edges connecting neighboring\npixels (vertically and horizontally). The graph signal is the values\n(intensities) of each pixel in the image. The graphs are commonly used as input\nto graph neural networks (e.g., Graph Convolutional Neural Networks (Graph\nCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the\nimages. In this work, we improve the accuracy of downstream graph neural\nnetwork tasks by finding alternative graphs to the grid graph and superpixel\nmethods to represent the dataset images, following the approach in [5, 6]. We\nfind row correlation, column correlation, and product graphs for each image in\nMNIST and Fashion-MNIST using correlations between the pixel values building on\nthe method in [5, 6]. Experiments show that using these different graph\nrepresentations and features as input into downstream GNN models improves the\naccuracy over using the traditional grid graph and superpixel methods in the\nliterature.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790MNIST\u548cFashion-MNIST\u56fe\u50cf\u50cf\u7d20\u503c\u7684\u76f8\u5173\u6027\uff0c\u6784\u5efa\u884c\u76f8\u5173\u3001\u5217\u76f8\u5173\u548c\u79ef\u56fe\u7b49\u66ff\u4ee3\u56fe\u8868\u793a\uff0c\u63d0\u5347\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u7684\u683c\u5b50\u56fe\u548c\u8d85\u50cf\u7d20\u65b9\u6cd5\u5728\u56fe\u50cf\u8868\u793a\u4e2d\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u627e\u5230\u66f4\u4f18\u7684\u56fe\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u9ad8\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "method": "\u4f7f\u7528\u50cf\u7d20\u503c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6784\u5efa\u884c\u76f8\u5173\u56fe\u3001\u5217\u76f8\u5173\u56fe\u548c\u79ef\u56fe\uff0c\u5e76\u5c06\u8fd9\u4e9b\u66ff\u4ee3\u56fe\u8868\u793a\u4f5c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u8fd9\u4e9b\u65b0\u7684\u56fe\u8868\u793a\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u683c\u5b50\u56fe\u548c\u8d85\u50cf\u7d20\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u7387", "conclusion": "\u901a\u8fc7\u5206\u6790\u50cf\u7d20\u76f8\u5173\u6027\u6765\u6784\u5efa\u66f4\u4f18\u7684\u56fe\u8868\u793a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347GNN\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd"}}
{"id": "2509.04801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04801", "abs": "https://arxiv.org/abs/2509.04801", "authors": ["Dayu Fan", "Rui Meng", "Song Gao", "Xiaodong Xu"], "title": "KGRAG-SC: Knowledge Graph RAG-Assisted Semantic Communication", "comment": "7 pages,4 figures,conference", "summary": "The state-of-the-art semantic communication (SC) schemes typically rely on\nend-to-end deep learning frameworks that lack interpretability and struggle\nwith robust semantic selection and reconstruction under noisy conditions. To\naddress this issue, this paper presents KGRAG-SC, a knowledge graph-assisted SC\nframework that leverages retrieval-augmented generation principles. KGRAG-SC\nemploys a multi-dimensional knowledge graph, enabling efficient semantic\nextraction through community-guided entity linking and GraphRAG-assisted\nprocessing. The transmitter constructs minimal connected subgraphs that capture\nessential semantic relationships and transmits only compact entity indices\nrather than full text or semantic triples. An importance-aware adaptive\ntransmission strategy provides unequal error protection based on structural\ncentrality metrics, prioritizing critical semantic elements under adverse\nchannel conditions. At the receiver, large language models perform\nknowledge-driven text reconstruction using the shared knowledge graph as\nstructured context, ensuring robust semantic recovery even with partial\ninformation loss. Experimental results demonstrate that KGRAG-SC achieves\nsuperior semantic fidelity in low Signal-to-Noise Ratio (SNR) conditions while\nsignificantly reducing transmission overhead compared to traditional\ncommunication methods, highlighting the effectiveness of integrating structured\nknowledge representation with generative language models for SC systems.", "AI": {"tldr": "KGRAG-SC\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u8bed\u4e49\u901a\u4fe1\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u95ee\u9898\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u66f4\u4f4e\u7684\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u96be\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u8bed\u4e49\u9009\u62e9\u548c\u91cd\u5efa\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u7684\u591a\u7ef4\u6846\u67b6\uff0c\u901a\u8fc7\u793e\u533a\u5f15\u5bfc\u7684\u5b9e\u4f53\u94fe\u63a5\u548cGraphRAG\u5904\u7406\u5b9e\u73b0\u9ad8\u6548\u8bed\u4e49\u63d0\u53d6\u3002\u53d1\u5c04\u7aef\u6784\u5efa\u6700\u5c0f\u8fde\u901a\u5b50\u56fe\u4f20\u8f93\u7d27\u51d1\u5b9e\u4f53\u7d22\u5f15\uff0c\u63a5\u6536\u7aef\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u9a71\u52a8\u7684\u6587\u672c\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eKGRAG-SC\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u5f00\u9500\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u4e0e\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u63d0\u4f9b\u9c81\u68d2\u7684\u8bed\u4e49\u6062\u590d\u548c\u9ad8\u6548\u7684\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.04819", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04819", "abs": "https://arxiv.org/abs/2509.04819", "authors": ["Shuhan Ding", "Jingjing Fu", "Yu Gu", "Naiteek Sangani", "Mu Wei", "Paul Vozila", "Nan Liu", "Jiang Bian", "Hoifung Poon"], "title": "AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations", "comment": null, "summary": "Medical image synthesis has become an essential strategy for augmenting\ndatasets and improving model generalization in data-scarce clinical settings.\nHowever, fine-grained and controllable synthesis remains difficult due to\nlimited high-quality annotations and domain shifts across datasets. Existing\nmethods, often designed for natural images or well-defined tumors, struggle to\ngeneralize to chest radiographs, where disease patterns are morphologically\ndiverse and tightly intertwined with anatomical structures. To address these\nchallenges, we propose AURAD, a controllable radiology synthesis framework that\njointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike\nprior approaches that rely on randomly sampled masks-limiting diversity,\ncontrollability, and clinical relevance-our method learns to generate masks\nthat capture multi-pathology coexistence and anatomical-pathological\nconsistency. It follows a progressive pipeline: pseudo masks are first\ngenerated from clinical prompts conditioned on anatomical structures, and then\nused to guide image synthesis. We also leverage pretrained expert medical\nmodels to filter outputs and ensure clinical plausibility. Beyond visual\nrealism, the synthesized masks also serve as labels for downstream tasks such\nas detection and segmentation, bridging the gap between generative modeling and\nreal-world clinical applications. Extensive experiments and blinded radiologist\nevaluations demonstrate the effectiveness and generalizability of our method\nacross tasks and datasets. In particular, 78% of our synthesized images are\nclassified as authentic by board-certified radiologists, and over 40% of\npredicted segmentation overlays are rated as clinically useful. All code,\npre-trained models, and the synthesized dataset will be released upon\npublication.", "AI": {"tldr": "AURAD\u662f\u4e00\u4e2a\u53ef\u63a7\u7684\u653e\u5c04\u5b66\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u751f\u6210\u9ad8\u4fdd\u771f\u80f8\u90e8X\u5149\u7247\u548c\u4f2a\u8bed\u4e49\u63a9\u7801\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7ba1\u9053\u548c\u9884\u8bad\u7ec3\u533b\u5b66\u6a21\u578b\u786e\u4fdd\u4e34\u5e8a\u5408\u7406\u6027\uff0c\u5728\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4f30\u4e2d78%\u7684\u5408\u6210\u56fe\u50cf\u88ab\u5206\u7c7b\u4e3a\u771f\u5b9e\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u7ec6\u7c92\u5ea6\u548c\u53ef\u63a7\u6027\u5408\u6210\u7684\u96be\u9898\uff0c\u7279\u522b\u662f\u5728\u80f8\u90e8X\u5149\u7247\u9886\u57df\uff0c\u75be\u75c5\u6a21\u5f0f\u5f62\u6001\u591a\u6837\u4e14\u4e0e\u89e3\u5256\u7ed3\u6784\u7d27\u5bc6\u4ea4\u7ec7\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u8fd9\u79cd\u590d\u6742\u573a\u666f\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u7ba1\u9053\uff1a\u9996\u5148\u4ece\u57fa\u4e8e\u89e3\u5256\u7ed3\u6784\u7684\u4e34\u5e8a\u63d0\u793a\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u63a9\u7801\u6307\u5bfc\u56fe\u50cf\u5408\u6210\uff1b\u5229\u7528\u9884\u8bad\u7ec3\u4e13\u5bb6\u533b\u5b66\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\u786e\u4fdd\u4e34\u5e8a\u5408\u7406\u6027\uff1b\u751f\u6210\u7684\u63a9\u7801\u8fd8\u53ef\u7528\u4e8e\u4e0b\u6e38\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u3002", "result": "78%\u7684\u5408\u6210\u56fe\u50cf\u88ab\u8ba4\u8bc1\u653e\u5c04\u79d1\u533b\u751f\u5206\u7c7b\u4e3a\u771f\u5b9e\u56fe\u50cf\uff1b\u8d85\u8fc740%\u7684\u9884\u6d4b\u5206\u5272\u8986\u76d6\u88ab\u8bc4\u5b9a\u4e3a\u4e34\u5e8a\u6709\u7528\uff1b\u5728\u8de8\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AURAD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u80f8\u90e8X\u5149\u7247\u5408\u6210\u7684\u53ef\u63a7\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u6311\u6218\uff0c\u751f\u6210\u7684\u56fe\u50cf\u548c\u63a9\u7801\u4e0d\u4ec5\u89c6\u89c9\u903c\u771f\uff0c\u8fd8\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u751f\u6210\u5efa\u6a21\u4e0e\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u5e94\u7528\u4e4b\u95f4\u642d\u5efa\u4e86\u6865\u6881\u3002"}}
{"id": "2509.04803", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04803", "abs": "https://arxiv.org/abs/2509.04803", "authors": ["Song Gao", "Rui Meng", "Xiaodong Xu", "Haixiao Gao", "Yiming Liu", "Chenyuan Feng", "Ping Zhang", "Tony Q. S. Quek", "Dusit Niyato"], "title": "SemSteDiff: Generative Diffusion Model-based Coverless Semantic Steganography Communication", "comment": "13 pages, 11 figures", "summary": "Semantic communication (SemCom), as a novel paradigm for future communication\nsystems, has recently attracted much attention due to its superiority in\ncommunication efficiency. However, similar to traditional communication, it\nalso suffers from eavesdropping threats. Intelligent eavesdroppers could launch\nadvanced semantic analysis techniques to infer secret semantic information.\nTherefore, some researchers have designed Semantic Steganography Communication\n(SemSteCom) scheme to confuse semantic eavesdroppers. However, the\nstate-of-the-art SemSteCom schemes for image transmission rely on the\npre-selected cover image, which limits the universality. To address this issue,\nwe propose a Generative Diffusion Model-based Coverless Semantic Steganography\nCommunication (SemSteDiff) scheme to hide secret images into generated stego\nimages. The semantic related private and public keys enable legitimate receiver\nto decode secret images correctly while the eavesdropper without completely\ntrue key-pairs fail to obtain them. Simulation results demonstrate the\neffectiveness of the plug-and-play design in different Joint Source-Channel\nCoding (JSCC) frameworks. The comparison results under different eavesdroppers'\nthreats show that, when Signal-to-Noise Ratio (SNR) = 0 dB, the peak\nsignal-to-noise ratio (PSNR) of the legitimate receiver is 4.14 dB higher than\nthat of the eavesdropper.", "AI": {"tldr": "\u57fa\u4e8e\u751f\u6210\u5f0f\u6ef4\u6e29\u6a21\u578b\u7684\u65e0\u5c01\u88c5\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u65b9\u6848SemSteDiff\uff0c\u901a\u8fc7\u751f\u6210\u9690\u85cf\u56fe\u50cf\u6765\u4fdd\u62a4\u79d8\u5bc6\u56fe\u50cf\u4f20\u8f93\uff0c\u514d\u53bb\u5bf9\u9884\u9009\u5c01\u88c5\u56fe\u50cf\u7684\u4f9d\u8d56\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u65b9\u6848\u4f9d\u8d56\u4e8e\u9884\u9009\u5c01\u88c5\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u666e\u9002\u6027\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u9ad8\u901a\u4fe1\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u6ef4\u6e29\u6a21\u578b\u751f\u6210\u542b\u6709\u79d8\u5bc6\u56fe\u50cf\u4fe1\u606f\u7684\u9690\u85cf\u56fe\u50cf\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u5173\u7684\u79c1\u94a5\u548c\u516c\u94a5\u5b9e\u73b0\u5408\u6cd5\u63a5\u6536\u65b9\u6b63\u786e\u89e3\u7801\u3002", "result": "\u5728SNR=0dB\u65f6\uff0c\u5408\u6cd5\u63a5\u6536\u65b9\u7684PSNR\u6bd4\u5077\u542c\u8005\u9ad84.14dB\uff0c\u8bc1\u660e\u65b9\u6848\u5728\u4e0d\u540cJSCC\u6846\u67b6\u4e2d\u90fd\u6709\u6548\u3002", "conclusion": "SemSteDiff\u65b9\u6848\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u5c01\u88c5\u8bed\u4e49\u9690\u5199\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.04870", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04870", "abs": "https://arxiv.org/abs/2509.04870", "authors": ["Yuanyuan Gui", "Wei Li", "Yinjian Wang", "Xiang-Gen Xia", "Mauro Marty", "Christian Ginzler", "Zuyuan Wang"], "title": "Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images", "comment": null, "summary": "Recent advances in semantic segmentation of multi-modal remote sensing images\nhave significantly improved the accuracy of tree cover mapping, supporting\napplications in urban planning, forest monitoring, and ecological assessment.\nIntegrating data from multiple modalities-such as optical imagery, light\ndetection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown\nsuperior performance over single-modality methods. However, these data are\noften acquired days or even months apart, during which various changes may\noccur, such as vegetation disturbances (e.g., logging, and wildfires) and\nvariations in imaging quality. Such temporal misalignments introduce\ncross-modal uncertainty, especially in high-resolution imagery, which can\nseverely degrade segmentation accuracy. To address this challenge, we propose\nMURTreeFormer, a novel multi-modal segmentation framework that mitigates and\nleverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer\ntreats one modality as primary and others as auxiliary, explicitly modeling\npatch-level uncertainty in the auxiliary modalities via a probabilistic latent\nrepresentation. Uncertain patches are identified and reconstructed from the\nprimary modality's distribution through a VAE-based resampling mechanism,\nproducing enhanced auxiliary features for fusion. In the decoder, a gradient\nmagnitude attention (GMA) module and a lightweight refinement head (RH) are\nfurther integrated to guide attention toward tree-like structures and to\npreserve fine-grained spatial details. Extensive experiments on multi-modal\ndatasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly\nimproves segmentation performance and effectively reduces the impact of\ntemporally induced aleatoric uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86MURTreeFormer\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6f5c\u5728\u8868\u793a\u548cVAE\u91cd\u91c7\u6837\u673a\u5236\u5904\u7406\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u65f6\u5e8f\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u6811\u51a0\u8986\u76d6\u5206\u5272\u7cbe\u5ea6", "motivation": "\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\uff08\u5149\u5b66\u3001LiDAR\u3001SAR\uff09\u901a\u5e38\u5728\u4e0d\u540c\u65f6\u95f4\u91c7\u96c6\uff0c\u5b58\u5728\u65f6\u5e8f\u9519\u4f4d\u5bfc\u81f4\u7684\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4e25\u91cd\u5f71\u54cd\u5206\u5272\u51c6\u786e\u6027", "method": "\u5c06\u4e00\u79cd\u6a21\u6001\u4f5c\u4e3a\u4e3b\u6a21\u6001\uff0c\u5176\u4ed6\u4f5c\u4e3a\u8f85\u52a9\u6a21\u6001\uff0c\u901a\u8fc7\u6982\u7387\u6f5c\u5728\u8868\u793a\u5efa\u6a21\u8f85\u52a9\u6a21\u6001\u7684\u8865\u4e01\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528VAE\u91cd\u91c7\u6837\u673a\u5236\u91cd\u5efa\u4e0d\u786e\u5b9a\u8865\u4e01\uff0c\u5e76\u96c6\u6210\u68af\u5ea6\u5e45\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u7ec6\u5316\u5934", "result": "\u5728\u4e0a\u6d77\u548c\u82cf\u9ece\u4e16\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMURTreeFormer\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u65f6\u5e8f\u5f15\u8d77\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u5904\u7406\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u7684\u6811\u51a0\u8986\u76d6\u6620\u5c04\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.04805", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04805", "abs": "https://arxiv.org/abs/2509.04805", "authors": ["Keqin Zhang"], "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design", "comment": null, "summary": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.", "AI": {"tldr": "AI\u9a71\u52a8\u7684\u65e0\u7ebf\u524d\u4f20\u538b\u7f29\u6280\u672f\u7efc\u8ff0\uff0c\u91cd\u70b9\u5206\u6790CSI\u53cd\u9988\u548cRB\u7c92\u5ea6\u9884\u7f16\u7801\u538b\u7f29\uff0c\u63d0\u51fa\u9762\u5411cell-free\u67b6\u6784\u7684\u9ad8\u538b\u7f29\u6bd4\u4f4e\u5ef6\u8fdf\u538b\u7f29\u7b56\u7565", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u524d\u4f20\u94fe\u8def\u9700\u8981\u5728\u9ad8\u5e26\u5bbd\u548c\u4f4e\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4f20\u8f93\u9ad8\u7ef4\u4fe1\u53f7\uff0c\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u4e14\u96be\u4ee5\u8c03\u4f18\uff0c\u9700\u8981AI\u6280\u672f\u6765\u66f4\u597d\u5730\u5229\u7528\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u7b49\u4fe1\u53f7\u7ed3\u6784", "method": "1) \u7efc\u8ff0AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\uff1b2) \u91cd\u70b9\u5206\u6790\u4e24\u79cd\u9ad8\u538b\u7f29\u8def\u5f84\uff1a\u7aef\u5230\u7aef\u5b66\u4e60\u7684CSI\u53cd\u9988\u538b\u7f29\u3001RB\u7c92\u5ea6\u9884\u7f16\u7801\u4f18\u5316\u4e0e\u538b\u7f29\u7ed3\u5408\uff1b3) \u63d0\u51fa\u9762\u5411cell-free\u67b6\u6784\u7684\u524d\u4f20\u538b\u7f29\u7b56\u7565\uff0c\u652f\u6301RB\u7ea7\u901f\u7387\u81ea\u9002\u5e94\u548c\u4f4e\u5ef6\u8fdf\u63a8\u7406", "result": "\u63d0\u51fa\u7684\u538b\u7f29\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u4e0b\u7684\u53ef\u63a7\u6027\u80fd\u635f\u5931\uff0c\u652f\u6301RB\u7ea7\u901f\u7387\u81ea\u9002\u5e94\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u7f51\u7edc\u96c6\u4e2d\u5f0f\u534f\u4f5c\u4f20\u8f93\u7684\u4f4e\u5ef6\u8fdf\u9700\u6c42", "conclusion": "AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e0\u7ebf\u524d\u4f20\u94fe\u8def\u7684\u9ad8\u7ef4\u4fe1\u53f7\u4f20\u8f93\u6311\u6218\uff0c\u63d0\u51fa\u7684cell-free\u67b6\u6784\u538b\u7f29\u7b56\u7565\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.04888", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.04888", "abs": "https://arxiv.org/abs/2509.04888", "authors": ["Natascha Niessen", "Carolin M. Pirkl", "Ana Beatriz Solana", "Hannah Eichhorn", "Veronika Spieker", "Wenqi Huang", "Tim Sprenger", "Marion I. Menzel", "Julia A. Schnabel"], "title": "INR meets Multi-Contrast MRI Reconstruction", "comment": null, "summary": "Multi-contrast MRI sequences allow for the acquisition of images with varying\ntissue contrast within a single scan. The resulting multi-contrast images can\nbe used to extract quantitative information on tissue microstructure. To make\nsuch multi-contrast sequences feasible for clinical routine, the usually very\nlong scan times need to be shortened e.g. through undersampling in k-space.\nHowever, this comes with challenges for the reconstruction. In general,\nadvanced reconstruction techniques such as compressed sensing or deep\nlearning-based approaches can enable the acquisition of high-quality images\ndespite the acceleration. In this work, we leverage redundant anatomical\ninformation of multi-contrast sequences to achieve even higher acceleration\nrates. We use undersampling patterns that capture the contrast information\nlocated at the k-space center, while performing complementary undersampling\nacross contrasts for high frequencies. To reconstruct this highly sparse\nk-space data, we propose an implicit neural representation (INR) network that\nis ideal for using the complementary information acquired across contrasts as\nit jointly reconstructs all contrast images. We demonstrate the benefits of our\nproposed INR method by applying it to multi-contrast MRI using the MPnRAGE\nsequence, where it outperforms the state-of-the-art parallel imaging compressed\nsensing (PICS) reconstruction method, even at higher acceleration factors.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u7684\u591a\u5bf9\u6bd4\u5ea6MRI\u91cd\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u6bd4\u5ea6\u95f4\u7684\u4e92\u8865\u4fe1\u606f\u5b9e\u73b0\u66f4\u9ad8\u52a0\u901f\u56e0\u5b50\uff0c\u5728MPnRAGE\u5e8f\u5217\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5e76\u884c\u6210\u50cf\u538b\u7f29\u611f\u77e5\u65b9\u6cd5", "motivation": "\u591a\u5bf9\u6bd4\u5ea6MRI\u626b\u63cf\u65f6\u95f4\u8fc7\u957f\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\uff0c\u9700\u8981\u901a\u8fc7k\u7a7a\u95f4\u6b20\u91c7\u6837\u52a0\u901f\uff0c\u4f46\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u5728\u9ad8\u52a0\u901f\u56e0\u5b50\u4e0b\u6548\u679c\u6709\u9650", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5ea6\u7279\u5b9a\u7684\u6b20\u91c7\u6837\u6a21\u5f0f\uff08\u4e2d\u5fc3\u4fdd\u7559\u5bf9\u6bd4\u4fe1\u606f\uff0c\u9ad8\u9891\u4e92\u8865\u91c7\u6837\uff09\uff0c\u63d0\u51faINR\u7f51\u7edc\u8054\u5408\u91cd\u5efa\u6240\u6709\u5bf9\u6bd4\u5ea6\u56fe\u50cf\uff0c\u5229\u7528\u5bf9\u6bd4\u5ea6\u95f4\u7684\u5197\u4f59\u89e3\u5256\u4fe1\u606f", "result": "\u5728MPnRAGE\u5e8f\u5217\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u66f4\u9ad8\u52a0\u901f\u56e0\u5b50\u4e0b\u4ecd\u4f18\u4e8estate-of-the-art\u7684\u5e76\u884c\u6210\u50cf\u538b\u7f29\u611f\u77e5(PICS)\u91cd\u5efa\u65b9\u6cd5", "conclusion": "INR\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u591a\u5bf9\u6bd4\u5ea6MRI\u4e2d\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u52a0\u901f\u56e0\u5b50\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u6709\u671b\u63a8\u52a8\u591a\u5bf9\u6bd4\u5ea6MRI\u5728\u4e34\u5e8a\u7684\u5e38\u89c4\u5e94\u7528"}}
{"id": "2509.04860", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04860", "abs": "https://arxiv.org/abs/2509.04860", "authors": ["Rui Guo", "Yi Zhang", "Yhonatan Kvich", "Tianyao Huang", "Maokun Li", "Yonina C. Eldar"], "title": "Plug-and-Play Latent Diffusion for Electromagnetic Inverse Scattering with Application to Brain Imaging", "comment": null, "summary": "Electromagnetic (EM) imaging is an important tool for non-invasive sensing\nwith low-cost and portable devices. One emerging application is EM stroke\nimaging, which enables early diagnosis and continuous monitoring of brain\nstrokes. Quantitative imaging is achieved by solving an inverse scattering\nproblem (ISP) that reconstructs permittivity and conductivity maps from\nmeasurements. In general, the reconstruction accuracy is limited by its\ninherent nonlinearity and ill-posedness. Existing methods, including\nlearning-free and learning-based approaches, fail to either incorporate\ncomplicated prior distributions or provide theoretical guarantees, posing\ndifficulties in balancing interpretability, distortion error, and reliability.\nTo overcome these limitations, we propose a posterior sampling method based on\nlatent diffusion for quantitative EM brain imaging, adapted from a generative\nplug-and-play (PnP) posterior sampling framework. Our approach allows to\nflexibly integrate prior knowledge into physics-based inversion without\nrequiring paired measurement-label datasets. We first learn the prior\ndistribution of targets from an unlabeled dataset, and then incorporate the\nlearned prior into posterior sampling. In particular, we train a latent\ndiffusion model on permittivity and conductivity maps to capture their prior\ndistribution. Then, given measurements and the forward model describing EM wave\nphysics, we perform posterior sampling by alternating between two samplers that\nrespectively enforce the likelihood and prior distributions. Finally, reliable\nreconstruction is obtained through minimum mean squared error (MMSE) estimation\nbased on the samples. Experimental results on brain imaging demonstrate that\nour approach achieves state-of-the-art performance in reconstruction accuracy\nand structural similarity while maintaining high measurement fidelity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u7528\u4e8e\u7535\u78c1\u8111\u6210\u50cf\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u524d\u5411\u6a21\u578b\u548c\u5b66\u4e60\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u5e76\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u7535\u78c1\u6210\u50cf\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u517c\u987e\u590d\u6742\u5148\u9a8c\u5206\u5e03\u6574\u5408\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u5728\u89e3\u91ca\u6027\u3001\u5931\u771f\u8bef\u5dee\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u56f0\u96be", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b66\u4e60\u76ee\u6807\u5148\u9a8c\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u4ea4\u66ff\u91c7\u6837\u5668\u6267\u884c\u540e\u9a8c\u91c7\u6837\uff0c\u5206\u522b\u5f3a\u5236\u6267\u884c\u4f3c\u7136\u548c\u5148\u9a8c\u5206\u5e03\uff0c\u6700\u540e\u57fa\u4e8e\u6837\u672c\u8fdb\u884c\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1", "result": "\u5728\u8111\u6210\u50cf\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6d4b\u91cf\u4fdd\u771f\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\u5230\u57fa\u4e8e\u7269\u7406\u7684\u53cd\u6f14\u4e2d\uff0c\u65e0\u9700\u914d\u5bf9\u6d4b\u91cf-\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u5b9a\u91cf\u7535\u78c1\u8111\u6210\u50cf\u7684\u53ef\u9760\u91cd\u5efa"}}
{"id": "2509.05154", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05154", "abs": "https://arxiv.org/abs/2509.05154", "authors": ["Julia Dietlmeier", "Oluwabukola Grace Adegboro", "Vayangi Ganepola", "Claudia Mazo", "Noel E. O'Connor"], "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation", "comment": "Medical Imaging with Deep Learning (MIDL 2025) short paper", "summary": "Vision-language models and their adaptations to image segmentation tasks\npresent enormous potential for producing highly accurate and interpretable\nresults. However, implementations based on CLIP and BiomedCLIP are still\nlagging behind more sophisticated architectures such as CRIS. In this work,\ninstead of focusing on text prompt engineering as is the norm, we attempt to\nnarrow this gap by showing how to ensemble vision-language segmentation models\n(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice\nscore improvement of 6.3% on the BKAI polyp dataset using the ensembled\nBiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.\nFurthermore, we provide initial results on additional four radiology and\nnon-radiology datasets. We conclude that ensembling works differently across\nthese datasets (from outperforming to underperforming the CRIS model),\nindicating a topic for future investigation by the community. The code is\navailable at https://github.com/juliadietlmeier/VLSM-Ensemble.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u4e0e\u4f4e\u590d\u6742\u5ea6CNN\u96c6\u6210\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728BKAI\u606f\u8089\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u9ad8\u4e866.3%\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u4e5f\u67091-6%\u7684\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCLIP\u548cBiomedCLIP\u7684\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u6027\u80fd\u4ecd\u843d\u540e\u4e8e\u66f4\u590d\u6742\u7684\u67b6\u6784\u5982CRIS\uff0c\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u800c\u975e\u6587\u672c\u63d0\u793a\u5de5\u7a0b\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b(VLSMs)\u4e0e\u4f4e\u590d\u6742\u5ea6CNN\u8fdb\u884c\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u6587\u672c\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u3002", "result": "\u5728BKAI\u606f\u8089\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u5347\u4e866.3%\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u4e5f\u67091-6%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u56db\u4e2a\u653e\u5c04\u5b66\u548c\u975e\u653e\u5c04\u5b66\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u521d\u6b65\u7ed3\u679c\u3002", "conclusion": "\u96c6\u6210\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff08\u4ece\u4f18\u4e8e\u5230\u52a3\u4e8eCRIS\u6a21\u578b\uff09\uff0c\u8fd9\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.05169", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05169", "abs": "https://arxiv.org/abs/2509.05169", "authors": ["Huu-Tai Phung", "Yu-Hsiang Lin", "Yen-Kuan Ho", "Wen-Hsiao Peng"], "title": "Exploring Autoregressive Vision Foundation Models for Image Compression", "comment": null, "summary": "This work presents the first attempt to repurpose vision foundation models\n(VFMs) as image codecs, aiming to explore their generation capability for\nlow-rate image compression. VFMs are widely employed in both conditional and\nunconditional generation scenarios across diverse downstream tasks, e.g.,\nphysical AI applications. Many VFMs employ an encoder-decoder architecture\nsimilar to that of end-to-end learned image codecs and learn an autoregressive\n(AR) model to perform next-token prediction. To enable compression, we\nrepurpose the AR model in VFM for entropy coding the next token based on\npreviously coded tokens. This approach deviates from early semantic compression\nefforts that rely solely on conditional generation for reconstructing input\nimages. Extensive experiments and analysis are conducted to compare VFM-based\ncodec to current SOTA codecs optimized for distortion or perceptual quality.\nNotably, certain pre-trained, general-purpose VFMs demonstrate superior\nperceptual quality at extremely low bitrates compared to specialized learned\nimage codecs. This finding paves the way for a promising research direction\nthat leverages VFMs for low-rate, semantically rich image compression.", "AI": {"tldr": "\u9996\u6b21\u5c1d\u8bd5\u91cd\u65b0\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u4f5c\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u4f4e\u7801\u7387\u56fe\u50cf\u538b\u7f29\uff0c\u5728\u6781\u4f4e\u7801\u7387\u4e0b\u5c55\u73b0\u4f18\u79c0\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4f4e\u7801\u7387\u56fe\u50cf\u538b\u7f29\u4e2d\u7684\u6f14\u7ece\u80fd\u529b\uff0c\u5229\u7528\u5176\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7279\u6027\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u7684\u56fe\u50cf\u538b\u7f29\u3002", "method": "\u91cd\u65b0\u5229\u7528VFMs\u4e2d\u7684\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\uff0c\u57fa\u4e8e\u5df2\u7f16\u7801\u7684token\u5bf9\u4e0b\u4e00\u4e2atoken\u8fdb\u884c\u4f4d\u7f6e\u9884\u6d4b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u540c\u4e8e\u4ee5\u5f80\u4ec5\u4f9d\u8d56\u6761\u4ef6\u751f\u6210\u7684\u8bed\u4e49\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5b8c\u6574\u7684\u5b9e\u9a8c\u5206\u6790\u663e\u793a\uff0c\u67d0\u4e9b\u9884\u8bad\u7ec3\u7684\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u6781\u4f4e\u7801\u7387\u4e0b\u6bd4\u4e13\u95e8\u7684\u5b66\u4e60\u578b\u56fe\u50cf\u7f16\u7801\u5668\u5177\u6709\u66f4\u4f18\u7684\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4f4e\u7801\u7387\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u56fe\u50cf\u538b\u7f29\u5f00\u542f\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.04873", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04873", "abs": "https://arxiv.org/abs/2509.04873", "authors": ["Yue Geng", "Tee Hiang Cheng", "Kai Zhong", "Kah Chan Teh", "Qingqing Wu"], "title": "Movable IRS-Aided ISAC Systems: Joint Beamforming and Position Optimization", "comment": "13 pages, 8 figures", "summary": "Driven by intelligent reflecting surface (IRS) and movable antenna (MA)\ntechnologies, movable IRS (MIRS) has been proposed to improve the adaptability\nand performance of conventional IRS, enabling flexible adjustment of the IRS\nreflecting element positions. This paper investigates MIRS-aided integrated\nsensing and communication (ISAC) systems. The objective is to minimize the\npower required for satisfying the quality-of-service (QoS) of sensing and\ncommunication by jointly optimizing the MIRS element positions, IRS reflection\ncoefficients, transmit beamforming, and receive filters. To balance the\nperformance-cost trade-off, we proposed two MIRS schemes: element-wise control\nand array-wise control, where the positions of individual reflecting elements\nand arrays consisting of multiple elements are controllable, respectively. To\naddress the joint beamforming and position optimization, a product Riemannian\nmanifold optimization (PRMO) method is proposed, where the variables are\nupdated over a constructed product Riemannian manifold space (PRMS) in parallel\nvia penalty-based transformation and Riemannian\nBroyden-Fletcher-Goldfarb-Shanno (RBFGS) algorithm. Simulation results\ndemonstrate that the proposed MIRS outperforms conventional IRS in power\nminimization with both element-wise control and array-wise control.\nSpecifically, with different system parameters, the minimum power is achieved\nby the MIRS with the element-wise control scheme, while suboptimal solution and\nhigher computational efficiency are achieved by the MIRS with array-wise\ncontrol scheme.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u79fb\u52a8\u667a\u80fd\u53cd\u5c04\u8868\u9762(MIRS)\u5728\u96c6\u6210\u611f\u77e5\u901a\u4fe1(ISAC)\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5173\u952e\u53c2\u6570\u4f18\u5316\u6765\u6700\u5c0f\u5316\u7cfb\u7edf\u529f\u8017\uff0c\u5e76\u63d0\u51fa\u4e86\u5143\u7d20\u7ea7\u548c\u6570\u7ec4\u7ea7\u4e24\u79cd\u63a7\u5236\u65b9\u6848\u4ee5\u53ca\u57fa\u4e8e\u9c81\u66fc\u6d41\u5f62\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u7ed3\u5408\u667a\u80fd\u53cd\u5c04\u8868\u9762(IRS)\u548c\u53ef\u79fb\u52a8\u5929\u7ebf(MA)\u6280\u672f\uff0c\u53ef\u79fb\u52a8IRS(MIRS)\u80fd\u591f\u63d0\u9ad8\u4f20\u7edfIRS\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u7075\u6d3b\u8c03\u6574\u53cd\u5c04\u5143\u7d20\u4f4d\u7f6e\u6765\u6539\u5584\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u5143\u7d20\u7ea7\u63a7\u5236\u548c\u6570\u7ec4\u7ea7\u63a7\u5236\u4e24\u79cdMIRS\u65b9\u6848\uff0c\u5e76\u4f7f\u7528\u4ea7\u54c1\u9c81\u66fc\u6d41\u5f62\u4f18\u5316(PRMO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f5a\u51fd\u6570\u8f6c\u6362\u548c\u9c81\u66fc\u6d41\u5f62BFGS\u7b97\u6cd5\u5728\u6784\u5efa\u7684\u4ea7\u54c1\u9c81\u66fc\u6d41\u5f62\u7a7a\u95f4(PRMS)\u4e0a\u5e76\u884c\u66f4\u65b0\u53d8\u91cf\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684MIRS\u65b9\u6848\u5728\u529f\u8017\u6700\u5c0f\u5316\u65b9\u9762\u8d85\u8fc7\u4f20\u7edfIRS\u3002\u5143\u7d20\u7ea7\u63a7\u5236\u65b9\u6848\u80fd\u5b9e\u73b0\u6700\u4f18\u529f\u8017\uff0c\u800c\u6570\u7ec4\u7ea7\u63a7\u5236\u65b9\u6848\u867d\u7136\u662f\u6b21\u4f18\u89e3\u4f46\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MIRS\u6280\u672f\u5728ISAC\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5143\u7d20\u7ea7\u63a7\u5236\u80fd\u63d0\u4f9b\u6700\u4f18\u6027\u80fd\uff0c\u800c\u6570\u7ec4\u7ea7\u63a7\u5236\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002\u4ea7\u54c1\u9c81\u66fc\u6d41\u5f62\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5173\u952e\u53c2\u6570\u7684\u805a\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.05261", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05261", "abs": "https://arxiv.org/abs/2509.05261", "authors": ["Thierry Judge", "Nicolas Duchateau", "Khuram Faraz", "Pierre-Marc Jodoin", "Olivier Bernard"], "title": "Generation of realistic cardiac ultrasound sequences with ground truth motion and speckle decorrelation", "comment": "4 pages. IUS 2025", "summary": "Simulated ultrasound image sequences are key for training and validating\nmachine learning algorithms for left ventricular strain estimation. Several\nsimulation pipelines have been proposed to generate sequences with\ncorresponding ground truth motion, but they suffer from limited realism as they\ndo not consider speckle decorrelation. In this work, we address this limitation\nby proposing an improved simulation framework that explicitly accounts for\nspeckle decorrelation. Our method builds on an existing ultrasound simulation\npipeline by incorporating a dynamic model of speckle variation. Starting from\nreal ultrasound sequences and myocardial segmentations, we generate meshes that\nguide image formation. Instead of applying a fixed ratio of myocardial and\nbackground scatterers, we introduce a coherence map that adapts locally over\ntime. This map is derived from correlation values measured directly from the\nreal ultrasound data, ensuring that simulated sequences capture the\ncharacteristic temporal changes observed in practice. We evaluated the realism\nof our approach using ultrasound data from 98 patients in the CAMUS database.\nPerformance was assessed by comparing correlation curves from real and\nsimulated images. The proposed method achieved lower mean absolute error\ncompared to the baseline pipeline, indicating that it more faithfully\nreproduces the decorrelation behavior seen in clinical data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u8d85\u58f0\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u6591\u70b9\u53bb\u76f8\u5173\u6765\u63d0\u9ad8\u5de6\u5ba4\u5e94\u53d8\u4f30\u8ba1\u7b97\u6cd5\u8bad\u7ec3\u7684\u56fe\u50cf\u5b9e\u4f53\u6027", "motivation": "\u73b0\u6709\u7684\u8d85\u58f0\u6a21\u62df\u6d41\u6c34\u7ebf\u5b58\u5728\u9650\u5236\uff0c\u4e0d\u8003\u8651\u6591\u70b9\u53bb\u76f8\u5173\uff0c\u5bfc\u81f4\u6a21\u62df\u56fe\u50cf\u5b9e\u4f53\u6027\u4e0d\u8db3", "method": "\u5728\u73b0\u6709\u6a21\u62df\u6d41\u7a0b\u57fa\u7840\u4e0a\u6dfb\u52a0\u52a8\u6001\u6591\u70b9\u53d8\u5316\u6a21\u578b\uff0c\u4f7f\u7528\u771f\u5b9e\u8d85\u58f0\u5e8f\u5217\u548c\u5fc3\u808c\u5206\u5272\u751f\u6210\u7f51\u683c\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u5730\u56fe\u672c\u5730\u9002\u914d\u6591\u70b9\u53bb\u76f8\u5173", "result": "\u5728CAMUS\u6570\u636e\u5e9398\u540d\u60a3\u8005\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u548c\u6a21\u62df\u56fe\u50cf\u7684\u76f8\u5173\u66f2\u7ebf\uff0c\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u91cd\u73b0\u4e34\u5e8a\u6570\u636e\u4e2d\u89c2\u5bdf\u5230\u7684\u53bb\u76f8\u5173\u884c\u4e3a\uff0c\u63d0\u9ad8\u4e86\u8d85\u58f0\u6a21\u62df\u5e8f\u5217\u7684\u5b9e\u4f53\u6027"}}
{"id": "2509.04930", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04930", "abs": "https://arxiv.org/abs/2509.04930", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part I, Principles and algorithms", "comment": null, "summary": "In this article, a Probability Mass Function (PMF) estimation method which\ntames the curse of dimensionality is proposed. This method, called Partial\nCoupled Tensor Factorization of 3D marginals or PCTF3D, has for principle to\npartially couple order-3 data projections -- seen as order-3 tensors -- to\nobtain a tensor decomposition of the probability mass tensor. The novelty of\nPCTF3D relies on partial coupling which consists in choosing a subset of 3D\nmarginals. The choice of marginals is then formulated with hypergraphs. After\npresenting possible coupling strategies, some numerical experiments and an\napplication of the method are proposed. This article is the first of a two-part\narticle. While this first article focuses on a new algorithmic framework for\nPMF estimation, the second studies uniqueness properties of the model\nintroduced in this article.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCTF3D\u7684\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u8026\u54083D\u8fb9\u9645\u5f20\u91cf\u6765\u514b\u670d\u7ef4\u5ea6\u707e\u96be\u95ee\u9898", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9ad8\u7ef4\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u9700\u6c42\u7684\u6311\u6218", "method": "PCTF3D\u65b9\u6cd5\u901a\u8fc7\u9009\u62e93D\u8fb9\u9645\u5f20\u91cf\u7684\u5b50\u96c6\u8fdb\u884c\u90e8\u5206\u8026\u5408\uff0c\u5229\u7528\u8d85\u56fe\u7406\u8bba\u6765\u8868\u8ff0\u8fb9\u9645\u9009\u62e9\u7b56\u7565\uff0c\u6784\u5efa\u5f20\u91cf\u5206\u89e3\u6a21\u578b", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u80fd\u7684\u8026\u5408\u7b56\u7565\u3001\u6570\u503c\u5b9e\u9a8c\u548c\u65b9\u6cd5\u5e94\u7528\uff0c\u4e3a\u6982\u7387\u8d28\u91cf\u5f20\u91cf\u5206\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84", "conclusion": "PCTF3D\u662f\u5904\u7406\u9ad8\u7ef4PMF\u4f30\u8ba1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u672c\u6587\u662f\u7b2c\u4e00\u90e8\u5206\u4e13\u6ce8\u4e8e\u7b97\u6cd5\u6846\u67b6\uff0c\u7b2c\u4e8c\u90e8\u5206\u5c06\u7814\u7a76\u6a21\u578b\u552f\u4e00\u6027\u6027\u8d28"}}
{"id": "2509.04931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04931", "abs": "https://arxiv.org/abs/2509.04931", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part II, Uniqueness of the model", "comment": null, "summary": "In this paper, uniqueness properties of a coupled tensor model are studied.\nThis new coupled tensor model is used in a new method called Partial Coupled\nTensor Factorization of 3D marginals or PCTF3D. This method performs estimation\nof probability mass functions by coupling 3D marginals, seen as order-3\ntensors. The core novelty of PCTF3D's approach (detailed in the part I article)\nrelies on the partial coupling which consists on the choice of 3D marginals to\nbe coupled. Tensor methods are ubiquitous in many applications of statistical\nlearning, with their biggest advantage of having strong uniqueness properties.\nIn this paper, the uniqueness properties of PCTF3D's constrained coupled\nlow-rank model is assessed. While probabilistic constraints of the coupled\nmodel are handled properly, it is shown that uniqueness highly depends on the\ncoupling used in PCTF3D. After proposing a Jacobian algorithm providing maximum\nrecoverable rank, different coupling strategies presented in the Part I article\nare examined with respect to their uniqueness properties. Finally, an\nidentifiability bound is given for a so-called Cartesian coupling which permits\nenhancing sufficient bounds of the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86PCTF3D\u65b9\u6cd5\u4e2d\u8026\u5408\u5f20\u91cf\u6a21\u578b\u7684\u552f\u4e00\u6027\u7279\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8026\u5408\u7b56\u7565\u5bf9\u552f\u4e00\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u7ed9\u51fa\u4e86\u7b1b\u5361\u5c14\u8026\u5408\u7684\u53ef\u8fa8\u8bc6\u6027\u8fb9\u754c\u3002", "motivation": "\u5f20\u91cf\u65b9\u6cd5\u5728\u7edf\u8ba1\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6700\u5927\u4f18\u52bf\u662f\u5177\u6709\u5f3a\u552f\u4e00\u6027\u7279\u6027\u3002PCTF3D\u65b9\u6cd5\u901a\u8fc7\u8026\u54083D\u8fb9\u7f18\u5206\u5e03\u6765\u4f30\u8ba1\u6982\u7387\u8d28\u91cf\u51fd\u6570\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u7ea6\u675f\u8026\u5408\u4f4e\u79e9\u6a21\u578b\u7684\u552f\u4e00\u6027\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u96c5\u53ef\u6bd4\u7b97\u6cd5\u6765\u63d0\u4f9b\u6700\u5927\u53ef\u6062\u590d\u79e9\uff0c\u5e76\u68c0\u67e5\u4e86Part I\u6587\u7ae0\u4e2d\u63d0\u51fa\u7684\u4e0d\u540c\u8026\u5408\u7b56\u7565\u7684\u552f\u4e00\u6027\u7279\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u552f\u4e00\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8ePCTF3D\u4e2d\u4f7f\u7528\u7684\u8026\u5408\u65b9\u5f0f\u3002\u5bf9\u4e8e\u7b1b\u5361\u5c14\u8026\u5408\u7ed9\u51fa\u4e86\u53ef\u8fa8\u8bc6\u6027\u8fb9\u754c\uff0c\u8fd9\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u7684\u5145\u5206\u8fb9\u754c\u3002", "conclusion": "PCTF3D\u65b9\u6cd5\u7684\u552f\u4e00\u6027\u7279\u6027\u4e0e\u8026\u5408\u7b56\u7565\u5bc6\u5207\u76f8\u5173\uff0c\u7b1b\u5361\u5c14\u8026\u5408\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u8fa8\u8bc6\u6027\u4fdd\u8bc1\uff0c\u4e3a\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2509.04962", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04962", "abs": "https://arxiv.org/abs/2509.04962", "authors": ["Antonio Spallone", "Marco Coraggio", "Francesco De Lellis", "Mario di Bernardo"], "title": "ROPE: A Novel Method for Real-Time Phase Estimation of Complex Biological Rhythms", "comment": null, "summary": "Accurate phase estimation -- the process of assigning phase values between\n$0$ and $2\\pi$ to repetitive or periodic signals -- is a cornerstone in the\nanalysis of oscillatory signals across diverse fields, from neuroscience to\nrobotics, where it is fundamental, e.g., to understanding coordination in\nneural networks, cardiorespiratory coupling, and human-robot interaction.\nHowever, existing methods are often limited to offline processing and/or\nconstrained to one-dimensional signals. In this paper, we introduce ROPE,\nwhich, to the best of our knowledge, is the first phase-estimation algorithm\ncapable of (i) handling signals of arbitrary dimension and (ii) operating in\nreal-time, with minimal error. ROPE identifies repetitions within the signal to\nsegment it into (pseudo-)periods and assigns phase values by performing\nefficient, tractable searches over previous signal segments. We extensively\nvalidate the algorithm on a variety of signal types, including trajectories\nfrom chaotic dynamical systems, human motion-capture data, and\nelectrocardiographic recordings. Our results demonstrate that ROPE is robust\nagainst noise and signal drift, and achieves significantly superior performance\ncompared to state-of-the-art phase estimation methods. This advancement enables\nreal-time analysis of complex biological rhythms, opening new pathways, for\nexample, for early diagnosis of pathological rhythm disruptions and developing\nrhythm-based therapeutic interventions in neurological and cardiovascular\ndisorders.", "AI": {"tldr": "ROPE\u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u610f\u7ef4\u5ea6\u4fe1\u53f7\u5e76\u5b9e\u65f6\u8fd0\u884c\u4e14\u8bef\u5dee\u6700\u5c0f\u7684\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u4fe1\u53f7\u91cd\u590d\u6027\u8fdb\u884c\u5206\u6bb5\u5e76\u6267\u884c\u9ad8\u6548\u641c\u7d22\u6765\u5206\u914d\u76f8\u4f4d\u503c\u3002", "motivation": "\u73b0\u6709\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u79bb\u7ebf\u5904\u7406\u548c/\u6216\u4e00\u7ef4\u4fe1\u53f7\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5206\u6790\u590d\u6742\u751f\u7269\u8282\u5f8b\u7684\u9700\u6c42\u3002", "method": "ROPE\u901a\u8fc7\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u91cd\u590d\u6a21\u5f0f\u5c06\u5176\u5206\u5272\u4e3a\uff08\u4f2a\uff09\u5468\u671f\uff0c\u5e76\u5728\u5148\u524d\u4fe1\u53f7\u6bb5\u4e0a\u6267\u884c\u9ad8\u6548\u3001\u53ef\u5904\u7406\u7684\u641c\u7d22\u6765\u5206\u914d\u76f8\u4f4d\u503c\u3002", "result": "\u5728\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u8f68\u8ff9\u3001\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u5fc3\u7535\u56fe\u8bb0\u5f55\u7b49\u591a\u79cd\u4fe1\u53f7\u7c7b\u578b\u4e0a\u9a8c\u8bc1\uff0cROPE\u5bf9\u566a\u58f0\u548c\u4fe1\u53f7\u6f02\u79fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "ROPE\u5b9e\u73b0\u4e86\u590d\u6742\u751f\u7269\u8282\u5f8b\u7684\u5b9e\u65f6\u5206\u6790\uff0c\u4e3a\u75c5\u7406\u8282\u5f8b\u7d0a\u4e71\u7684\u65e9\u671f\u8bca\u65ad\u548c\u57fa\u4e8e\u8282\u5f8b\u7684\u795e\u7ecf\u4e0e\u5fc3\u8840\u7ba1\u75be\u75c5\u6cbb\u7597\u5e72\u9884\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
