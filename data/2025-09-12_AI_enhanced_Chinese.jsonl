{"id": "2509.08869", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.08869", "abs": "https://arxiv.org/abs/2509.08869", "authors": ["Michael Greenwood", "Robert Hunter"], "title": "Improved Receiver Chain Performance via Error Location Inference", "comment": null, "summary": "Modern spacecraft communication systems rely on concatenated error correction\nschemes, typically combining convolutional and Reed-Solomon (RS) codes. This\npaper presents a decoder-side method that uses a machine learning model to\nestimate the likelihood of byte-level corruption in received data frames. These\nestimates are used to mark erasures prior to RS decoding, enhancing its\ncorrection capacity without requiring changes to spacecraft hardware or\nencoding standards. The approach enables improved data recovery under degraded\nsignal conditions at a gain of 0.3 decibels.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f30\u8ba1\u5b57\u8282\u7ea7\u5220\u9664\u6982\u7387\uff0c\u901a\u8fc7\u6807\u8bb0\u5220\u9664\u6765\u63d0\u5347\u91cd\u5e94\u58f9\u68af\u5c0f\u5927\u7801\u7684\u7f16\u7801\u80fd\u529b\uff0c\u5728\u4e0d\u6539\u53d8\u8bbe\u5907\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b00.3dB\u7684\u4fe1\u9053\u589e\u76ca", "motivation": "\u73b0\u4ee3\u5b87\u822a\u5668\u901a\u4fe1\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5377\u79ef\u7801\u548cReed-Solomon\u7801\u7684\u7f16\u7801\u7ec4\u5408\uff0c\u9700\u8981\u5728\u4e0d\u6539\u53d8\u786c\u4ef6\u548c\u7f16\u7801\u6807\u51c6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6548\u80fd", "method": "\u5728\u89e3\u7801\u7aef\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u4f30\u8ba1\u63a5\u6536\u6570\u636e\u5e27\u4e2d\u5b57\u8282\u7ea7\u5220\u9664\u7684\u6982\u7387\uff0c\u5e76\u7528\u8fd9\u4e9b\u4f30\u8ba1\u503c\u5728RS\u89e3\u7801\u524d\u6807\u8bb0\u5220\u9664", "result": "\u65b9\u6cd5\u5728\u4fe1\u53f7\u52a3\u5316\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u6570\u636e\u6062\u590d\u80fd\u529b\uff0c\u5b9e\u73b0\u4e860.3\u5206\u8d1d\u7684\u4fe1\u9053\u589e\u76ca", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b87\u822a\u5668\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u786c\u4ef6\u6539\u9020\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6848\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u5220\u9664\u6765\u589e\u5f3a\u9519\u8bef\u7f16\u7801\u80fd\u529b"}}
{"id": "2509.09411", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.09411", "abs": "https://arxiv.org/abs/2509.09411", "authors": ["Rui Xu", "Yinghui Ye", "Xiaoli Chu", "Guangyue Lu", "Farshad Rostami Ghadi", "Kai-Kit Wong"], "title": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?", "comment": null, "summary": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf(FAS)\u4e2d\u4f7f\u7528\u989c\u8272\u76f8\u5173\u77e9\u9635\u66ff\u4ee3\u7cfb\u6570\u76f8\u5173\u77e9\u9635\u6765\u8bc4\u4f30\u65ad\u65ad\u6027\u80fd\u7684\u4f18\u52bf\uff0c\u5e76\u5728Nakagami-m\u8870\u843d\u4e0b\u9a8c\u8bc1\u4e86\u5176\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u4f7f\u7528Jake\u6a21\u578b\u7684\u901a\u9053\u7cfb\u6570\u76f8\u5173\u77e9\u9635\u6765\u8fd1\u4f3cGaussian copula\u4e2d\u7684\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u4f46\u56e0\u4e3a\u591a\u5143\u6b63\u6001\u968f\u673a\u53d8\u91cf\u662f\u901a\u8fc7\u53d8\u6362\u76f8\u5173\u989c\u8272\u751f\u6210\u7684\uff0c\u6240\u4ee5\u9700\u8981\u63a2\u7d22\u4f7f\u7528\u989c\u8272\u76f8\u5173\u77e9\u9635\u662f\u5426\u66f4\u51c6\u786e\u3002", "method": "\u5728\u5b8c\u5168\u76f8\u5173\u7684Nakagami-m\u8870\u843d\u73af\u5883\u4e0b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u751f\u6210\u8fd9\u79cd\u8870\u843d\u901a\u9053\u7684\u65b9\u6cd5\uff0c\u7528\u4e8eMonte Carlo\u6a21\u62df\u4f5c\u4e3a\u7406\u8bba\u7ed3\u679c\u7684\u9a8c\u8bc1\u6807\u51c6\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8bc1\u5b9e\u4e86\u63d0\u51fa\u7684\u901a\u9053\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u793a\u4f7f\u7528\u989c\u8272\u76f8\u5173\u77e9\u9635\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u7aef\u53e3\u90e8\u7f72\u548c\u4f4e\u65ad\u65ad\u533a\u57df\u3002", "conclusion": "\u4f7f\u7528\u989c\u8272\u76f8\u5173\u77e9\u9635\u5728\u8bc4\u4f30FAS\u65ad\u65ad\u6027\u80fd\u65f6\u6bd4\u4f7f\u7528\u7cfb\u6570\u76f8\u5173\u77e9\u9635\u66f4\u51c6\u786e\uff0c\u7279\u522b\u5728\u5177\u4f53\u5e94\u7528\u573a\u666f\u4e2d\u663e\u793a\u51fa\u4f18\u52bf\u3002"}}
{"id": "2509.09499", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.09499", "abs": "https://arxiv.org/abs/2509.09499", "authors": ["Junjie Ni", "Tong Wu", "Zhiyong Chen", "Yin Xu", "Meixia Tao", "Wenjun Zhang"], "title": "Mixture of Semantics Transmission for Generative AI-Enabled Semantic Communication Systems", "comment": "accepted by IEEE Communications Letters", "summary": "In this paper, we propose a mixture of semantics (MoS) transmission strategy\nfor wireless semantic communication systems based on generative artificial\nintelligence (AI). At the transmitter, we divide an image into regions of\ninterest (ROI) and reigons of non-interest (RONI) to extract their semantic\ninformation respectively. Semantic information of ROI can be allocated more\nbandwidth, while RONI can be represented in a compact form for transmission. At\nthe receiver, a diffusion model reconstructs the full image using the received\nsemantic information of ROI and RONI. Compared to existing generative AI-based\nmethods, MoS enables more efficient use of channel resources by balancing\nvisual fidelity and semantic relevance. Experimental results demonstrate that\nappropriate ROI-RONI allocation is critical. The MoS achieves notable\nperformance gains in peak signal-to-noise ratio (PSNR) of ROI and CLIP score of\nRONI.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u8bed\u4e49\u6df7\u5408\u4f20\u8f93\u7b56\u7565(MoS)\uff0c\u901a\u8fc7ROI\u548cRONI\u5206\u533a\u5904\u7406\u5b9e\u73b0\u65e0\u7ebf\u8bed\u4e49\u901a\u4fe1\u4e2d\u5e26\u5bbd\u8d44\u6e90\u7684\u4f18\u5316\u5206\u914d\u548c\u56fe\u50cf\u91cd\u5efa", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u5728\u4fe1\u9053\u8d44\u6e90\u5229\u7528\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u76f8\u5173\u6027\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861", "method": "\u5728\u53d1\u9001\u7aef\u5c06\u56fe\u50cf\u5206\u4e3a\u611f\u5174\u8da3\u533a\u57df(ROI)\u548c\u975e\u611f\u5174\u8da3\u533a\u57df(RONI)\uff0c\u5206\u522b\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u5e76\u5206\u914d\u4e0d\u540c\u5e26\u5bbd\uff1b\u5728\u63a5\u6536\u7aef\u4f7f\u7528\u6269\u6563\u6a21\u578b\u6839\u636e\u63a5\u6536\u5230\u7684\u8bed\u4e49\u4fe1\u606f\u91cd\u5efa\u5b8c\u6574\u56fe\u50cf", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u9002\u5f53\u7684ROI-RONI\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0cMoS\u5728ROI\u7684PSNR\u548cRONI\u7684CLIP\u5206\u6570\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "MoS\u7b56\u7565\u901a\u8fc7\u8bed\u4e49\u6df7\u5408\u4f20\u8f93\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4fe1\u9053\u8d44\u6e90\u5229\u7528\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf"}}
{"id": "2509.09554", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.09554", "abs": "https://arxiv.org/abs/2509.09554", "authors": ["Joseph Jabbour", "Ali Chamas Al-Ghouwayel", "Emmanuel Boutillon"], "title": "Fast Polarisation-Aware Decoder for Non-Binary Polar Codes", "comment": "8 pages and 8 figures. Paper submitted to Annals of\n  Telecommunications (August 2025)", "summary": "The paper investigates the emerging field of low-complexity non-binary polar\ncode (NB-PC) decoders. It shows that customizing each kernel of an NB-PC\ndecoder through offline analysis can significantly reduce the overall decoding\ncomplexity. The proposed decoder, referred to as the Fast Successive\nCancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing\nthe computational load of parity-check nodes that share the same level of input\npolarization. The NB polar decoder is developed for both BPSK and CCSK\nmodulations. Compared to the state-of-the-art extended min-sum algorithm, the\nFSC-PA algorithm achieves an overall reduction of 60 percents in field\nadditions and 30 percents in real additions, while incurring only a negligible\nperformance loss (less than 0.2 dB degradation).", "AI": {"tldr": "\u63d0\u51faFSC-PA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u5236\u5316NB-PC\u89e3\u7801\u5668\u5185\u6838\uff0c\u663e\u8457\u964d\u4f4e\u975e\u4e8c\u8fdb\u5236\u6781\u5316\u7801\u89e3\u7801\u590d\u6742\u5ea6\uff0c\u5728BPSK\u548cCCSK\u8c03\u5236\u4e0b\u5b9e\u73b060%\u57df\u52a0\u6cd5\u548c30%\u5b9e\u6570\u52a0\u6cd5\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931\u4ec50.2dB", "motivation": "\u7814\u7a76\u4f4e\u590d\u6742\u5ea6\u975e\u4e8c\u8fdb\u5236\u6781\u5316\u7801\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5206\u6790\u8f93\u5165\u6781\u5316\u6c34\u5e73\u76f8\u540c\u7684\u5947\u5076\u6821\u9a8c\u8282\u70b9\u6765\u964d\u4f4e\u6574\u4f53\u89e3\u7801\u8ba1\u7b97\u590d\u6742\u5ea6", "method": "\u91c7\u7528\u5feb\u901f\u8fde\u7eed\u6d88\u9664-\u6781\u5316\u611f\u77e5(FSC-PA)\u65b9\u6848\uff0c\u901a\u8fc7\u79bb\u7ebf\u5206\u6790\u5b9a\u5236\u6bcf\u4e2a\u89e3\u7801\u5668\u5185\u6838\uff0c\u6700\u5c0f\u5316\u5171\u4eab\u76f8\u540c\u8f93\u5165\u6781\u5316\u6c34\u5e73\u7684\u5947\u5076\u6821\u9a8c\u8282\u70b9\u7684\u8ba1\u7b97\u8d1f\u8f7d", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u5c55\u6700\u5c0f\u548c\u7b97\u6cd5\u76f8\u6bd4\uff0cFSC-PA\u7b97\u6cd5\u5b9e\u73b0\u4e8660%\u7684\u57df\u52a0\u6cd5\u548c30%\u7684\u5b9e\u6570\u52a0\u6cd5\u603b\u4f53\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931\u4ec5\u4e3a0.2dB\u7684\u8f7b\u5fae\u9000\u5316", "conclusion": "FSC-PA\u65b9\u6848\u80fd\u6709\u6548\u964d\u4f4e\u975e\u4e8c\u8fdb\u5236\u6781\u5316\u7801\u89e3\u7801\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8fd0\u7b97\u91cf\uff0c\u9002\u7528\u4e8eBPSK\u548cCCSK\u8c03\u5236"}}
{"id": "2509.08860", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.08860", "abs": "https://arxiv.org/abs/2509.08860", "authors": ["Jingyi Gao", "Di Wu", "Baha lhnaini"], "title": "USEANet: Ultrasound-Specific Edge-Aware Multi-Branch Network for Lightweight Medical Image Segmentation", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Ultrasound image segmentation faces unique challenges including speckle\nnoise, low contrast, and ambiguous boundaries, while clinical deployment\ndemands computationally efficient models. We propose USEANet, an\nultrasound-specific edge-aware multi-branch network that achieves optimal\nperformance-efficiency balance through four key innovations: (1)\nultrasound-specific multi-branch processing with specialized modules for noise\nreduction, edge enhancement, and contrast improvement; (2) edge-aware attention\nmechanisms that focus on boundary information with minimal computational\noverhead; (3) hierarchical feature aggregation with adaptive weight learning;\nand (4) ultrasound-aware decoder enhancement for optimal segmentation\nrefinement. Built on an ultra-lightweight PVT-B0 backbone, USEANet\nsignificantly outperforms existing methods across five ultrasound datasets\nwhile using only 3.64M parameters and 0.79G FLOPs. Experimental results\ndemonstrate superior segmentation accuracy with 67.01 IoU on BUSI dataset,\nrepresenting substantial improvements over traditional approaches while\nmaintaining exceptional computational efficiency suitable for real-time\nclinical applications. Code is available at\nhttps://github.com/chouheiwa/USEANet.", "AI": {"tldr": "\u63d0\u51fa\u4e86USEANet\u7f51\u7edc\uff0c\u9488\u5bf9\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u7684\u72ec\u7279\u6311\u6218\uff0c\u901a\u8fc7\u591a\u5206\u652f\u5904\u7406\u548c\u8fb9\u7f18\u611f\u77e5\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8d85\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u89e3\u51b3\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u9762\u4e34\u7684\u6591\u70b9\u566a\u58f0\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u8fb9\u754c\u7b49\u72ec\u7279\u6311\u6218\uff0c\u540c\u65f6\u6ee1\u8db3\u4e34\u5e8a\u90e8\u7f72\u5bf9\u8ba1\u7b97\u6548\u7387\u7684\u9ad8\u8981\u6c42", "method": "\u57fa\u4e8e\u8d85\u8f7b\u91cf\u7ea7PVT-B0\u4e3b\u5e72\u7f51\u7edc\uff0c\u91c7\u7528\u8d85\u58f0\u7279\u5f02\u6027\u591a\u5206\u652f\u5904\u7406\u3001\u8fb9\u7f18\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u3001\u5206\u5c42\u7279\u5f81\u805a\u5408\u548c\u8d85\u58f0\u611f\u77e5\u89e3\u7801\u5668\u589e\u5f3a\u56db\u9879\u5173\u952e\u6280\u672f", "result": "\u5728\u4e94\u4e2a\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u75283.64M\u53c2\u6570\u548c0.79G FLOPs\uff0c\u5728BUSI\u6570\u636e\u96c6\u4e0a\u8fbe\u523067.01 IoU", "conclusion": "USEANet\u5728\u4fdd\u6301\u4f18\u5f02\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u9002\u5408\u5b9e\u65f6\u4e34\u5e8a\u5e94\u7528"}}
{"id": "2509.08830", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08830", "abs": "https://arxiv.org/abs/2509.08830", "authors": ["Seong-A Park", "Jong-Eui Chae", "Sungdong Kim", "Hyung-Chul Lee", "Hyun-Lim Yang"], "title": "A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals", "comment": "16 pages, 5 figures", "summary": "In clinical settings, monitoring hemodynamics is crucial for managing patient\nprognosis, necessitating the integrated analysis of multiple physiological\nsignals. While recent research has analyzed single signals such as\nelectrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a\nproposal for an approach that encompasses the complex signal analysis required\nin actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul\nNational University hospital PHYsiological signal Masked representation\nlearning) model extracts physiological features reflecting the electrical,\npressure, and fluid characteristics of the cardiac cycle in the process of\nrestoring three masked physiological signals based on self-supervised learning\n(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing\nmultiple physical characteristics, the model can extract more enriched features\nonly using non-invasive signals. We evaluated the model's performance in\nclinical downstream tasks such as hypotension, stroke volume, systolic blood\npressure, diastolic blood pressure, and age prediction. Our results showed that\nthe SNUPHY-M significantly outperformed supervised or SSL models, especially in\nprediction tasks using non-invasive signals. To the best of our knowledge,\nSNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis\ninvolving ECG, PPG, and ABP signals. This approach effectively supports\nclinical decision-making and enables precise diagnostics, contributing\nsignificantly to the early diagnosis and management of hemodynamics without\ninvasiveness.", "AI": {"tldr": "SNUPHY-M\u6a21\u578b\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u540c\u65f6\u5206\u6790ECG\u3001PPG\u548cABP\u4e09\u79cd\u751f\u7406\u4fe1\u53f7\uff0c\u63d0\u53d6\u5fc3\u810f\u5468\u671f\u7684\u7535\u5b66\u3001\u538b\u529b\u548c\u6d41\u4f53\u7279\u5f81\uff0c\u5728\u8840\u6d41\u52a8\u529b\u5b66\u76d1\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u5176\u4ed6\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u9700\u8981\u76d1\u6d4b\u591a\u79cd\u751f\u7406\u4fe1\u53f7\u6765\u7ba1\u7406\u60a3\u8005\u9884\u540e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u4fe1\u53f7\u5206\u6790\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u5b9e\u9645\u4e34\u5e8a\u590d\u6742\u573a\u666f\u7684\u7efc\u5408\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u63a9\u7801\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6062\u590d\u4e09\u79cd\u88ab\u63a9\u7801\u7684\u751f\u7406\u4fe1\u53f7\uff08ECG\u3001PPG\u3001ABP\uff09\u6765\u63d0\u53d6\u53cd\u6620\u5fc3\u810f\u5468\u671f\u591a\u79cd\u7269\u7406\u7279\u5f81\u7684\u751f\u7406\u7279\u5f81\u3002", "result": "\u5728\u4f4e\u8840\u538b\u3001\u6bcf\u640f\u8f93\u51fa\u91cf\u3001\u6536\u7f29\u538b\u3001\u8212\u5f20\u538b\u548c\u5e74\u9f84\u9884\u6d4b\u7b49\u4e34\u5e8a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cSNUPHY-M\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u6216\u5176\u4ed6\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u65e0\u521b\u4fe1\u53f7\u7684\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SNUPHY-M\u662f\u9996\u4e2a\u5c06\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u4e8e\u5fc3\u8840\u7ba1\u5206\u6790\uff08\u6d89\u53caECG\u3001PPG\u548cABP\u4fe1\u53f7\uff09\u7684\u6a21\u578b\uff0c\u80fd\u6709\u6548\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u5b9e\u73b0\u7cbe\u786e\u8bca\u65ad\uff0c\u5bf9\u65e0\u521b\u65e9\u671f\u8bca\u65ad\u548c\u8840\u6d41\u52a8\u529b\u5b66\u7ba1\u7406\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.09644", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.09644", "abs": "https://arxiv.org/abs/2509.09644", "authors": ["Chunjie Wang", "Xuhui Zhang", "Wenchao Liu", "Jinke Ren", "Shuqiang Wang", "Yanyan Shen", "Kejiang Ye", "Kim Fung Tsang"], "title": "RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems", "comment": "This manuscript has been submitted to IEEE", "summary": "This paper investigates the data collection enhancement problem in a\nreconfigurable intelligent surface (RIS)-empowered intelligent consumer\ntransportation system (ICTS). We propose a novel framework where a data center\n(DC) provides energy to pre-configured roadside unit (RSU) pairs during the\ndownlink stage. While in the uplink stage, these RSU pairs utilize a hybrid\nrate-splitting multiple access (RSMA) and time-division multiple access (TDMA)\nprotocol to transmit the processed data to the DC, while simultaneously\nperforming local data processing using the harvested energy. Our objective is\nto maximize the minimal processed data volume of the RSU pairs by jointly\noptimizing the RIS downlink and uplink phase shifts, the transmit power of the\nDC and RSUs, the RSU computation resource allocation, and the time slot\nallocation. To address the formulated non-convex problem, we develop an\nefficient iterative algorithm integrating alternating optimization and\nsequential rank-one constraint relaxation methods. Extensive simulations\ndemonstrate that the proposed algorithm significantly outperforms baseline\nschemes under diverse scenarios, validating its effectiveness in enhancing the\ndata processing performance for intelligent transportation applications.", "AI": {"tldr": "\u63d0\u51faRIS\u8d4b\u80fd\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u6570\u636e\u6536\u96c6\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316RIS\u76f8\u79fb\u3001\u529f\u7387\u5206\u914d\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u9699\u5206\u914d\uff0c\u6700\u5927\u5316\u6700\u5c0f\u5904\u7406\u6570\u636e\u91cf", "motivation": "\u89e3\u51b3\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6570\u636e\u6536\u96c6\u548c\u5904\u7406\u6548\u7387\u95ee\u9898\uff0c\u5229\u7528RIS\u6280\u672f\u589e\u5f3a\u901a\u4fe1\u6027\u80fd\uff0c\u63d0\u9ad8\u4ea4\u901a\u6570\u636e\u5904\u7406\u80fd\u529b", "method": "\u91c7\u7528\u6df7\u5408RSMA\u548cTDMA\u534f\u8bae\uff0c\u7ed3\u5408\u4ea4\u66ff\u4f18\u5316\u548c\u987a\u5e8f\u79e9\u4e00\u7ea6\u675f\u677e\u5f1b\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316RIS\u76f8\u79fb\u3001\u53d1\u5c04\u529f\u7387\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u9699\u5206\u914d", "result": "\u6240\u63d0\u7b97\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u7684\u6570\u636e\u5904\u7406\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3aRIS\u8d4b\u80fd\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.08872", "categories": ["eess.IV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.08872", "abs": "https://arxiv.org/abs/2509.08872", "authors": ["Felipe \u00c1lvarez Barrientos", "Tom\u00e1s Banduc", "Isabeau Sirven", "Francisco Sahli Costabal"], "title": "WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks", "comment": null, "summary": "The contractile motion of the heart is strongly determined by the\ndistribution of the fibers that constitute cardiac tissue. Strain analysis\ninformed with the orientation of fibers allows to describe several pathologies\nthat are typically associated with impaired mechanics of the myocardium, such\nas cardiovascular disease. Several methods have been developed to estimate\nstrain-derived metrics from traditional imaging techniques. However, the\nphysical models underlying these methods do not include fiber mechanics,\nrestricting their capacity to accurately explain cardiac function. In this\nwork, we introduce WarpPINN-fibers, a physics-informed neural network framework\nto accurately obtain cardiac motion and strains enhanced by fiber information.\nWe train our neural network to satisfy a hyper-elastic model and promote fiber\ncontraction with the goal to predict the deformation field of the heart from\ncine magnetic resonance images. For this purpose, we build a loss function\ncomposed of three terms: a data-similarity loss between the reference and the\nwarped template images, a regularizer enforcing near-incompressibility of\ncardiac tissue and a fiber-stretch penalization that controls strain in the\ndirection of synthetically produced fibers. We show that our neural network\nimproves the former WarpPINN model and effectively controls fiber stretch in a\nsynthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers\noutperforms alternative methodologies in landmark-tracking and strain curve\nprediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We\nexpect that our method will enable a more precise quantification of cardiac\nstrains through accurate deformation fields that are consistent with fiber\nphysiology, without requiring imaging techniques more sophisticated than MRI.", "AI": {"tldr": "WarpPINN-fibers\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ea4\u7ef4\u4fe1\u606f\u6765\u51c6\u786e\u4f30\u8ba1\u5fc3\u810f\u8fd0\u52a8\u548c\u5e94\u53d8\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7ea4\u7ef4\u529b\u5b66\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5fc3\u810f\u6536\u7f29\u8fd0\u52a8\u53d7\u7ea4\u7ef4\u5206\u5e03\u5f3a\u70c8\u5f71\u54cd\uff0c\u4f20\u7edf\u5e94\u53d8\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u7ea4\u7ef4\u529b\u5b66\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u51c6\u786e\u89e3\u91ca\u5fc3\u810f\u529f\u80fd\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u635f\u5931\u51fd\u6570\u5305\u542b\u4e09\u90e8\u5206\uff1a\u6570\u636e\u76f8\u4f3c\u6027\u635f\u5931\u3001\u8fd1\u4e0d\u53ef\u538b\u7f29\u6027\u6b63\u5219\u5316\u548c\u7ea4\u7ef4\u62c9\u4f38\u60e9\u7f5a\u9879\uff0c\u901a\u8fc7\u5408\u6210\u7ea4\u7ef4\u4fe1\u606f\u63a7\u5236\u5e94\u53d8\u65b9\u5411\u3002", "result": "\u5728\u5408\u6210\u5e7b\u5f71\u5b9e\u9a8c\u4e2d\u6539\u8fdb\u539f\u6709\u6a21\u578b\u5e76\u6709\u6548\u63a7\u5236\u7ea4\u7ef4\u62c9\u4f38\uff0c\u572815\u540d\u5065\u5eb7\u5fd7\u613f\u8005\u7684cine-MRI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u6807\u5fd7\u70b9\u8ffd\u8e2a\u548c\u5e94\u53d8\u66f2\u7ebf\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u51c6\u786e\u7684\u53d8\u5f62\u573a\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u5fc3\u810f\u5e94\u53d8\u91cf\u5316\uff0c\u4e14\u4ec5\u9700MRI\u6210\u50cf\u6280\u672f\uff0c\u65e0\u9700\u66f4\u590d\u6742\u7684\u6210\u50cf\u65b9\u6cd5\u3002"}}
{"id": "2509.08950", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08950", "abs": "https://arxiv.org/abs/2509.08950", "authors": ["Jarvis Haupt", "Qin Lu", "Yanning Shen", "Jia Chen", "Yue Dong", "Dan McCreary", "Mehmet Ak\u00e7akaya", "Georgios B. Giannakis"], "title": "Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities", "comment": "Accepted to the IEEE Signal Processing Magazine Special Issue on\n  Artificial Intelligence for Education: A Signal Processing Perspective", "summary": "Powerful artificial intelligence (AI) tools that have emerged in recent years\n-- including large language models, automated coding assistants, and advanced\nimage and speech generation technologies -- are the result of monumental human\nachievements. These breakthroughs reflect mastery across multiple technical\ndisciplines and the resolution of significant technological challenges.\nHowever, some of the most profound challenges may still lie ahead. These\nchallenges are not purely technical but pertain to the fair and responsible use\nof AI in ways that genuinely improve the global human condition. This article\nexplores one promising application aligned with that vision: the use of AI\ntools to facilitate and enhance education, with a specific focus on signal\nprocessing (SP). It presents two interrelated perspectives: identifying and\naddressing technical limitations, and applying AI tools in practice to improve\neducational experiences. Primers are provided on several core technical issues\nthat arise when using AI in educational settings, including how to ensure\nfairness and inclusivity, handle hallucinated outputs, and achieve efficient\nuse of resources. These and other considerations -- such as transparency,\nexplainability, and trustworthiness -- are illustrated through the development\nof an immersive, structured, and reliable \"smart textbook.\" The article serves\nas a resource for researchers and educators seeking to advance AI's role in\nengineering education.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8AI\u5728\u4fe1\u53f7\u5904\u7406\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6280\u672f\u9650\u5236\u89e3\u51b3\u548c\u5b9e\u8df5\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u667a\u80fd\u6559\u79d1\u4e66\u7684\u5f00\u53d1\u6982\u5ff5\u3002", "motivation": "\u5c3d\u7ba1AI\u6280\u672f\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u8fd8\u9762\u4e34\u7740\u5982\u4f55\u516c\u5e73\u8d1f\u8d23\u4f7f\u7528\u4ee5\u6539\u5584\u5168\u7403\u4eba\u7c7b\u751f\u6d3b\u7684\u6311\u6218\u3002\u6559\u80b2\u9886\u57df\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u5e94\u7528\u65b9\u5411\u3002", "method": "\u4ece\u4e24\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u89d2\u5ea6\u51fa\u53d1\uff1a\u8bc6\u522b\u548c\u89e3\u51b3\u6280\u672f\u9650\u5236\uff0c\u4ee5\u53ca\u5728\u5b9e\u8df5\u4e2d\u5e94\u7528AI\u5de5\u5177\u6539\u5584\u6559\u80b2\u4f53\u9a8c\u3002\u63d0\u4f9b\u4e86\u6838\u5fc3\u6280\u672f\u95ee\u9898\u7684\u5165\u95e8\u6307\u5357\u3002", "result": "\u901a\u8fc7\u5f00\u53d1\u6c89\u6d78\u5f0f\u3001\u7ed3\u6784\u5316\u548c\u53ef\u9760\u7684\"\u667a\u80fd\u6559\u79d1\u4e66\"\u6765\u5c55\u793a\u516c\u5e73\u6027\u3001\u5305\u5bb9\u6027\u3001\u900f\u660e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u7b49\u8003\u8651\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6587\u7ae0\u4e3a\u5e0c\u671b\u63a8\u8fdbAI\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u4f5c\u7528\u7684\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u8d44\u6e90\u548c\u6307\u5357\u3002"}}
{"id": "2509.08913", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.08913", "abs": "https://arxiv.org/abs/2509.08913", "authors": ["Sin-Yu Huang", "Vincent W. S. Wong"], "title": "Generalized User-Oriented Image Semantic Coding Empowered by Large Vision-Language Model", "comment": "Accepted by IEEE Global Communications Conference (GLOBECOM), Taipei,\n  Taiwan, Dec. 2025", "summary": "Semantic communication has shown outstanding performance in preserving the\noverall source information in wireless transmission. For semantically rich\ncontent such as images, human users are often interested in specific regions\ndepending on their intent. Moreover, recent semantic coding models are mostly\ntrained on specific datasets. However, real-world applications may involve\nimages out of the distribution of training dataset, which makes generalization\na crucial but largely unexplored problem. To incorporate user's intent into\nsemantic coding, in this paper, we propose a generalized user-oriented image\nsemantic coding (UO-ISC) framework, where the user provides a text query\nindicating its intent. The transmitter extracts features from the source image\nwhich are relevant to the user's query. The receiver reconstructs an image\nbased on those features. To enhance the generalization ability, we integrate\ncontrastive language image pre-training (CLIP) model, which is a pretrained\nlarge vision-language model (VLM), into our proposed UO-ISC framework. To\nevaluate the relevance between the reconstructed image and the user's query, we\nintroduce the user-intent relevance loss, which is computed by using a\npretrained large VLM, large language-and-vision assistant (LLaVA) model. When\nperforming zero-shot inference on unseen objects, simulation results show that\nthe proposed UO-ISC framework outperforms the state-of-the-art query-aware\nimage semantic coding in terms of the answer match rate.", "AI": {"tldr": "\u7528\u6237\u5bfc\u5411\u7684\u56fe\u50cf\u8bed\u4e49\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u8bc6\u522b\u7528\u6237\u610f\u56fe\uff0c\u5229\u7528CLIP\u6a21\u578b\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u4f7f\u7528LLaVA\u6a21\u578b\u8bc4\u4f30\u91cd\u5efa\u56fe\u50cf\u4e0e\u7528\u6237\u67e5\u8be2\u7684\u76f8\u5173\u6027\uff0c\u5728\u672a\u89c1\u5bf9\u8c61\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u4e2d\u7528\u6237\u5173\u6ce8\u7279\u5b9a\u533a\u57df\u7684\u9700\u6c42\uff0c\u5e76\u6d89\u53ca\u5230\u771f\u5b9e\u5e94\u7528\u4e2d\u53ef\u80fd\u9047\u5230\u8bad\u7ec3\u96c6\u5916\u56fe\u50cf\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faUO-ISC\u6846\u67b6\uff0c\u7528\u6237\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u6307\u660e\u610f\u56fe\uff0c\u53d1\u9001\u7aef\u4f7f\u7528CLIP\u6a21\u578b\u63d0\u53d6\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u56fe\u50cf\u7279\u5f81\uff0c\u63a5\u6536\u7aef\u91cd\u5efa\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528LLaVA\u6a21\u578b\u8ba1\u7b97\u7528\u6237\u610f\u56fe\u76f8\u5173\u6027\u635f\u5931\u3002", "result": "\u5728\u672a\u89c1\u5bf9\u8c61\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\u65f6\uff0c\u8be5\u6846\u67b6\u5728\u7b54\u6848\u5339\u914d\u7387\u65b9\u9762\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u67e5\u8be2\u654f\u611f\u56fe\u50cf\u8bed\u4e49\u7f16\u7801\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06\u7528\u6237\u610f\u56fe\u6574\u5408\u5230\u8bed\u4e49\u7f16\u7801\u4e2d\uff0c\u901a\u8fc7\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9762\u5411\u7528\u6237\u9700\u6c42\u7684\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08973", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08973", "abs": "https://arxiv.org/abs/2509.08973", "authors": ["Harshit Agrawal", "Ari Hietanen", "Simo S\u00e4rkk\u00e4"], "title": "Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography", "comment": null, "summary": "Purpose: Scatter artifacts drastically degrade the image quality of cone-beam\ncomputed tomography (CBCT) scans. Although deep learning-based methods show\npromise in estimating scatter from CBCT measurements, their deployment in\nmobile CBCT systems or edge devices is still limited due to the large memory\nfootprint of the networks. This study addresses the issue by applying networks\nat varying resolutions and suggesting an optimal one, based on speed and\naccuracy.\n  Methods: First, the reconstruction error in down-up sampling of CBCT scatter\nsignal was examined at six resolutions by comparing four interpolation methods.\nNext, a recent state-of-the-art method was trained across five image\nresolutions and evaluated for the reductions in floating-point operations\n(FLOPs), inference times, and GPU memory requirements.\n  Results: Reducing the input size and network parameters achieved a 78-fold\nreduction in FLOPs compared to the baseline method, while maintaining comarable\nperformance in terms of mean-absolute-percentage-error (MAPE) and\nmean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to\n4.42%, and the MSE decreased to 1.34 \\times 10^{-2} compared to 2.01 \\times\n10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and\n12, respectively. Further experiments comparing scatter-corrected\nreconstructions on a large, simulated dataset and real CBCT scans from water\nand Sedentex CT phantoms clearly demonstrated the robustness of our method.\n  Conclusion: This study highlights the underappreciated role of downsampling\nin deep learning-based scatter estimation. The substantial reduction in FLOPs\nand GPU memory requirements achieved by our method enables scatter correction\nin resource-constrained environments, such as mobile CBCT and edge devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u5e94\u7528\u7f51\u7edc\u5e76\u57fa\u4e8e\u901f\u5ea6\u548c\u51c6\u786e\u6027\u9009\u62e9\u6700\u4f18\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86CBCT\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u4e2d\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5185\u5b58\u5360\u7528\u5927\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8678\u500d\u7684FLOPs\u51cf\u5c11\u548c\u663e\u8457\u7684\u63a8\u7406\u65f6\u95f4\u4e0eGPU\u5185\u5b58\u4f18\u5316\u3002", "motivation": "\u9525\u675fCT\u626b\u63cf\u4e2d\u7684\u6563\u5c04\u4f2a\u5f71\u4e25\u91cd\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u867d\u7136\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4f30\u8ba1\u6563\u5c04\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7f51\u7edc\u5185\u5b58\u5360\u7528\u5927\uff0c\u5728\u79fb\u52a8CBCT\u7cfb\u7edf\u6216\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u4ecd\u7136\u53d7\u9650\u3002", "method": "\u9996\u5148\u5728\u516d\u4e2a\u5206\u8fa8\u7387\u4e0b\u6bd4\u8f83\u56db\u79cd\u63d2\u503c\u65b9\u6cd5\u68c0\u67e5CBCT\u6563\u5c04\u4fe1\u53f7\u7684\u4e0b\u91c7\u6837-\u4e0a\u91c7\u6837\u91cd\u5efa\u8bef\u5dee\uff0c\u7136\u540e\u5728\u4e94\u4e2a\u56fe\u50cf\u5206\u8fa8\u7387\u4e0a\u8bad\u7ec3\u6700\u65b0\u7684SOTA\u65b9\u6cd5\uff0c\u8bc4\u4f30FLOPs\u3001\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u9700\u6c42\u7684\u51cf\u5c11\u3002", "result": "\u51cf\u5c11\u8f93\u5165\u5927\u5c0f\u548c\u7f51\u7edc\u53c2\u6570\u5b9e\u73b0\u4e8678\u500d\u7684FLOPs\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684MAPE\uff083.85% vs 4.42%\uff09\u548cMSE\uff081.34\u00d710\u207b\u00b2 vs 2.01\u00d710\u207b\u00b2\uff09\u6027\u80fd\uff0c\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u5206\u522b\u51cf\u5c1116\u500d\u548c12\u500d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u964d\u91c7\u6837\u5728\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6563\u5c04\u4f30\u8ba1\u4e2d\u88ab\u4f4e\u4f30\u7684\u4f5c\u7528\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7684FLOPs\u548cGPU\u5185\u5b58\u9700\u6c42\u5927\u5e45\u51cf\u5c11\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u79fb\u52a8CBCT\u548c\u8fb9\u7f18\u8bbe\u5907\uff09\u4e2d\u8fdb\u884c\u6563\u5c04\u6821\u6b63\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2509.09227", "categories": ["eess.IV", "cs.CV", "I.4.6"], "pdf": "https://arxiv.org/pdf/2509.09227", "abs": "https://arxiv.org/abs/2509.09227", "authors": ["Yinzheng Zhao", "Zhihao Zhao", "Rundong Jiang", "Louisa Sackewitz", "Quanmin Liang", "Mathias Maier", "Daniel Zapp", "Peter Charbel Issa", "Mohammad Ali Nasseri"], "title": "Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery", "comment": "TVST", "summary": "Purpose: To introduce novel dynamic structural parameters and evaluate their\nintegration within a multimodal deep learning (DL) framework for predicting\npostoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)\npatients. Methods: We utilized a publicly available longitudinal OCT dataset at\nfive stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage\nspecific segmentation model delineated related structures, and an automated\npipeline extracted quantitative, composite, qualitative, and dynamic features.\nBinary logistic regression models, constructed with and without dynamic\nparameters, assessed their incremental predictive value for best-corrected\nvisual acuity (BCVA). A multimodal DL model combining clinical variables,\nOCT-derived features, and raw OCT images was developed and benchmarked against\nregression models. Results: The segmentation model achieved high accuracy\nacross all timepoints (mean Dice > 0.89). Univariate and multivariate analyses\nidentified base diameter, ellipsoid zone integrity, and macular hole area as\nsignificant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates\nconsistently improved logistic regression AUC, especially at the 3-month\nfollow-up. The multimodal DL model outperformed logistic regression, yielding\nhigher AUCs and overall accuracy at each stage. The difference is as high as\n0.12, demonstrating the complementary value of raw image volume and dynamic\nparameters. Conclusions: Integrating dynamic parameters into the multimodal DL\nmodel significantly enhances the accuracy of predictions. This fully automated\nprocess therefore represents a promising clinical decision support tool for\npersonalized postoperative management in macular hole surgery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u7ed3\u6784\u53c2\u6570\u5e76\u5c06\u5176\u96c6\u6210\u5230\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u9884\u6d4bid\u6027\u5168\u5c42\u9ec4\u6591\u6f6e\u7834\u88c2\u75c5\u4eba\u7684\u624b\u672f\u540e\u89c6\u529b\u6062\u590d\u60c5\u51b5\uff0c\u5f97\u5230\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5bf9id\u6027\u5168\u5c42\u9ec4\u6591\u6f6e\u7834\u88c2\u75c5\u4eba\u624b\u672f\u540e\u89c6\u529b\u6062\u590d\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u63d0\u51fa\u65b0\u7684\u52a8\u6001\u7ed3\u6784\u53c2\u6570\u5e76\u5c06\u5176\u96c6\u6210\u5230\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684\u7eb5\u5411OCT\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9636\u6bb5\u7279\u5f02\u5206\u5272\u6a21\u578b\u548c\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d6\u5404\u79cd\u7279\u5f81\uff0c\u6784\u5efa\u5305\u542b\u52a8\u6001\u53c2\u6570\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u548c\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5206\u5272\u6a21\u578b\u51c6\u786e\u6027\u9ad8(\u5747\u503cDice>0.89)\uff0c\u52a8\u6001\u6062\u590d\u7387\u63d0\u5347\u4e86\u903b\u8f91\u56de\u5f52AUC\uff0c\u591a\u6a21\u6001DL\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0cAUC\u6700\u9ad8\u63d0\u53470.12\u3002", "conclusion": "\u96c6\u6210\u52a8\u6001\u53c2\u6570\u7684\u591a\u6a21\u6001DL\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2509.09005", "categories": ["eess.SP", "cs.ET", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.09005", "abs": "https://arxiv.org/abs/2509.09005", "authors": ["Hirley Alves", "Nurul H. Mahmood", "Onel L. A. L\u00f3pez", "Sumudu Samarakoon", "Seppo Yrj\u00f6l\u00e4", "Matti Latva-Aho", "Markku Juntti", "Ari Pouttu", "Armin Dekorsy", "Arthur Sousa de Sena", "Aydin Sezgin", "Bho Matthiesen", "Chafika Benzaid", "Chathuranga Weeraddana", "David Hutchison", "Dileepa Marasinghe", "Doganalp Ergenc", "Eduard Jorswieck", "Erkki Harjula", "Falko Dressler", "Harri Saarnisaari", "Italo Atzeni", "Jaap Van De Beek", "Jacek Rak", "Konstantin Mikhaylov", "Lauri Loven", "Madhusanka Liyanage", "Marcos Katz", "Marja Matinmikko-Blue", "Mehdi Rasti", "Mika Ylianttila Nhan Nguyen", "Pawani Porambage", "Petar Popovski", "Petri Ahokangas", "Premanandana Rajatheva", "Robert-Jeron Reifert", "Tharaka Hewa", "Tommy Svensson"], "title": "6G Resilience -- White Paper", "comment": null, "summary": "6G must be designed to withstand, adapt to, and evolve amid prolonged,\ncomplex disruptions. Mobile networks' shift from efficiency-first to\nsustainability-aware has motivated this white paper to assert that resilience\nis a primary design goal, alongside sustainability and efficiency, encompassing\ntechnology, architecture, and economics. We promote resilience by analysing\ndependencies between mobile networks and other critical systems, such as\nenergy, transport, and emergency services, and illustrate how cascading\nfailures spread through infrastructures. We formalise resilience using the 3R\nframework: reliability, robustness, resilience. Subsequently, we translate this\ninto measurable capabilities: graceful degradation, situational awareness,\nrapid reconfiguration, and learning-driven improvement and recovery.\n  Architecturally, we promote edge-native and locality-aware designs, open\ninterfaces, and programmability to enable islanded operations, fallback modes,\nand multi-layer diversity (radio, compute, energy, timing). Key enablers\ninclude AI-native control loops with verifiable behaviour, zero-trust security\nrooted in hardware and supply-chain integrity, and networking techniques that\nprioritise critical traffic, time-sensitive flows, and inter-domain\ncoordination.\n  Resilience also has a techno-economic aspect: open platforms and high-quality\ncomplementors generate ecosystem externalities that enhance resilience while\nopening new markets. We identify nine business-model groups and several\npatterns aligned with the 3R objectives, and we outline governance and\nstandardisation. This white paper serves as an initial step and catalyst for 6G\nresilience. It aims to inspire researchers, professionals, government\nofficials, and the public, providing them with the essential components to\nunderstand and shape the development of 6G resilience.", "AI": {"tldr": "6G\u5e94\u5c06\u7a33\u5065\u6027\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u76ee\u6807\uff0c\u901a\u8fc73R\u6846\u67b6\uff08\u53ef\u9760\u6027\u3001\u7a33\u5065\u6027\u3001\u6062\u590d\u529b\uff09\u548c\u5173\u952e\u6280\u672f\u63d0\u5347\u7f51\u7edc\u5728\u4e2d\u65ad\u60c5\u51b5\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "6G\u9700\u8981\u5728\u957f\u671f\u3001\u590d\u6742\u4e2d\u65ad\u73af\u5883\u4e2d\u4fdd\u6301\u7a33\u5b9a\u8fd0\u884c\uff0c\u5e76\u4ece\u6548\u679c\u4f18\u5148\u8f6c\u5411\u53ef\u6301\u7eed\u6027\u8ba4\u77e5\u3002", "method": "\u63d0\u51fa3R\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fb9\u7f18\u539f\u751f\u8bbe\u8ba1\u3001\u5f00\u653e\u63a5\u53e3\u3001\u7f16\u7a0b\u80fd\u529b\uff0c\u4ee5\u53caAI\u63a7\u5236\u5faa\u73af\u3001\u96f6\u4fe1\u4efb\u5b89\u5168\u548c\u7f51\u7edc\u4f18\u5148\u7ea7\u6280\u672f\u3002", "result": "\u5f62\u6210\u4e86\u5305\u542b\u4e5d\u4e2a\u5546\u4e1a\u6a21\u5f0f\u7ec4\u7684\u6280\u672f\u7ecf\u6d4e\u6846\u67b6\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u7ba1\u7406\u548c\u6807\u51c6\u5316\u5efa\u8bae\u3002", "conclusion": "\u672c\u767d\u76ae\u4e66\u4e3a6G\u7a33\u5065\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5e76\u5c06\u6fc0\u52b1\u5404\u754c\u53c2\u4e0e\u8005\u5171\u540c\u63a8\u52a8\u76f8\u5173\u53d1\u5c55\u3002"}}
{"id": "2509.09235", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.comp-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.09235", "abs": "https://arxiv.org/abs/2509.09235", "authors": ["Sarah C. Irvine", "Christian Lucas", "Diana Kr\u00fcger", "Bianca Guedert", "Julian Moosmann", "Berit Zeller-Plumhoff"], "title": "Virtual staining for 3D X-ray histology of bone implants", "comment": null, "summary": "Three-dimensional X-ray histology techniques offer a non-invasive alternative\nto conventional 2D histology, enabling volumetric imaging of biological tissues\nwithout the need for physical sectioning or chemical staining. However, the\ninherent greyscale image contrast of X-ray tomography limits its biochemical\nspecificity compared to traditional histological stains. Within digital\npathology, deep learning-based virtual staining has demonstrated utility in\nsimulating stained appearances from label-free optical images. In this study,\nwe extend virtual staining to the X-ray domain by applying cross-modality image\ntranslation to generate artificially stained slices from\nsynchrotron-radiation-based micro-CT scans. Using over 50 co-registered image\npairs of micro-CT and toluidine blue-stained histology from bone-implant\nsamples, we trained a modified CycleGAN network tailored for limited paired\ndata. Whole slide histology images were downsampled to match the voxel size of\nthe CT data, with on-the-fly data augmentation for patch-based training. The\nmodel incorporates pixelwise supervision and greyscale consistency terms,\nproducing histologically realistic colour outputs while preserving\nhigh-resolution structural detail. Our method outperformed Pix2Pix and standard\nCycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the\nmodel can be applied to full CT volumes to generate virtually stained 3D\ndatasets, enhancing interpretability without additional sample preparation.\nWhile features such as new bone formation were able to be reproduced, some\nvariability in the depiction of implant degradation layers highlights the need\nfor further training data and refinement. This work introduces virtual staining\nto 3D X-ray imaging and offers a scalable route for chemically informative,\nlabel-free tissue characterisation in biomedical research.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u865a\u62df\u67d3\u8272\u6280\u672f\u6269\u5c55\u5230X\u5c04\u7ebf\u9886\u57df\uff0c\u4f7f\u7528\u6539\u8fdb\u7684CycleGAN\u7f51\u7edc\u4ece\u540c\u6b65\u8f90\u5c04\u5faeCT\u626b\u63cf\u751f\u6210\u4eba\u5de5\u67d3\u8272\u5207\u7247\uff0c\u5b9e\u73b0\u4e863D X\u5c04\u7ebf\u7ec4\u7ec7\u5b66\u7684\u865a\u62df\u67d3\u8272\u3002", "motivation": "\u4f20\u7edf2D\u7ec4\u7ec7\u5b66\u9700\u8981\u7269\u7406\u5207\u7247\u548c\u5316\u5b66\u67d3\u8272\uff0c\u800c3D X\u5c04\u7ebf\u7ec4\u7ec7\u5b66\u867d\u7136\u65e0\u521b\u4f46\u7f3a\u4e4f\u751f\u5316\u7279\u5f02\u6027\u3002\u865a\u62df\u67d3\u8272\u6280\u672f\u53ef\u4ee5\u4ece\u65e0\u6807\u8bb0\u56fe\u50cf\u6a21\u62df\u67d3\u8272\u5916\u89c2\uff0c\u4f46\u6b64\u524d\u4e3b\u8981\u5e94\u7528\u4e8e\u5149\u5b66\u56fe\u50cf\u9886\u57df\u3002", "method": "\u4f7f\u752850\u591a\u5bf9\u5171\u914d\u51c6\u7684\u5faeCT\u548c\u7532\u82ef\u80fa\u84dd\u67d3\u8272\u7ec4\u7ec7\u5b66\u56fe\u50cf\u5bf9\uff0c\u8bad\u7ec3\u9488\u5bf9\u6709\u9650\u914d\u5bf9\u6570\u636e\u4f18\u5316\u7684\u6539\u8fdbCycleGAN\u7f51\u7edc\u3002\u91c7\u7528\u50cf\u7d20\u7ea7\u76d1\u7763\u548c\u7070\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u901a\u8fc7\u8865\u4e01\u8bad\u7ec3\u548c\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728SSIM\u3001PSNR\u548cLPIPS\u6307\u6807\u4e0a\u5747\u4f18\u4e8ePix2Pix\u548c\u6807\u51c6CycleGAN\u57fa\u7ebf\uff0c\u80fd\u591f\u751f\u6210\u7ec4\u7ec7\u5b66\u771f\u5b9e\u7684\u5f69\u8272\u8f93\u51fa\u5e76\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u7ec6\u8282\u3002\u6a21\u578b\u53ef\u5e94\u7528\u4e8e\u5b8c\u6574CT\u4f53\u79ef\u751f\u6210\u865a\u62df\u67d3\u8272\u76843D\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5c06\u865a\u62df\u67d3\u8272\u5f15\u51653D X\u5c04\u7ebf\u6210\u50cf\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u3001\u5316\u5b66\u4fe1\u606f\u4e30\u5bcc\u7684\u65e0\u6807\u8bb0\u7ec4\u7ec7\u8868\u5f81\u9014\u5f84\uff0c\u4f46\u9700\u8981\u66f4\u591a\u8bad\u7ec3\u6570\u636e\u548c\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6539\u5584\u690d\u5165\u7269\u964d\u89e3\u5c42\u7b49\u7279\u5f81\u7684\u63cf\u7ed8\u3002"}}
{"id": "2509.09018", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09018", "abs": "https://arxiv.org/abs/2509.09018", "authors": ["Xueyi Wang", "C. J. C.", "Lamoth", "Elisabeth Wilhelm"], "title": "Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data", "comment": "The paper has been acceptted and presented in the 47th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society", "summary": "A sleep forecast allows individuals and healthcare providers to anticipate\nand proactively address factors influencing restful rest, ultimately improving\nmental and physical well-being. This work presents an adaptive spatial and\ntemporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model\ncombines convolutional layers to capture spatial feature interactions between\nmultiple features and recurrent neural network layers to handle longer-term\ntemporal health-related data. A domain classifier is further integrated to\ngeneralize across different subjects. We conducted several experiments using\nfive input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes\n(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline\nmodels, achieving its lowest RMSE (0.282) with a seven-day input window and a\none-day predicting window. Moreover, the method maintained strong performance\neven when forecasting multiple days into the future, demonstrating its\nversatility for real-world applications. Visual comparisons reveal that the\nmodel accurately tracks both the overall sleep score level and daily\nfluctuations. These findings prove that the proposed framework provides a\nrobust and adaptable solution for personalized sleep forecasting using sparse\ndata from commercial wearable devices and domain adaptation techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdaST-Sleep\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cRNN\u8fdb\u884c\u7761\u7720\u8bc4\u5206\u9884\u6d4b\uff0c\u901a\u8fc7\u9886\u57df\u5206\u7c7b\u5668\u5b9e\u73b0\u8de8\u7528\u6237\u6cdb\u5316\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u7a97\u53e3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7761\u7720\u9884\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u4e2a\u4eba\u548c\u533b\u7597\u63d0\u4f9b\u8005\u63d0\u524d\u9884\u77e5\u5e76\u4e3b\u52a8\u5904\u7406\u5f71\u54cd\u826f\u597d\u4f11\u606f\u7684\u56e0\u7d20\uff0c\u4ece\u800c\u6539\u5584\u8eab\u5fc3\u5065\u5eb7\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u5c42\u6355\u6349\u591a\u7279\u5f81\u95f4\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u5904\u7406\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u5065\u5eb7\u6570\u636e\uff0c\u96c6\u6210\u9886\u57df\u5206\u7c7b\u5668\u5b9e\u73b0\u8de8\u4e0d\u540c\u7528\u6237\u7684\u6cdb\u5316\u3002", "result": "\u57285\u79cd\u8f93\u5165\u7a97\u53e3\u548c5\u79cd\u9884\u6d4b\u7a97\u53e3\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e4\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u4f4eRMSE\u4e3a0.282\uff087\u5929\u8f93\u5165\u7a97\u53e3+1\u5929\u9884\u6d4b\u7a97\u53e3\uff09\uff0c\u5373\u4f7f\u5728\u591a\u65e5\u9884\u6d4b\u4e2d\u4e5f\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f7f\u7528\u5546\u4e1a\u53ef\u7a7f\u6234\u8bbe\u5907\u7a00\u758f\u6570\u636e\u548c\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u4e2a\u6027\u5316\u7761\u7720\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09241", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09241", "abs": "https://arxiv.org/abs/2509.09241", "authors": ["Antonio Montanaro", "Diego Valsesia"], "title": "A novel method and dataset for depth-guided image deblurring from smartphone Lidar", "comment": null, "summary": "Modern smartphones are equipped with Lidar sensors providing depth-sensing\ncapabilities. Recent works have shown that this complementary sensor allows to\nimprove various tasks in image processing, including deblurring. However, there\nis a current lack of datasets with realistic blurred images and paired mobile\nLidar depth maps to further study the topic. At the same time, there is also a\nlack of blind zero-shot methods that can deblur a real image using the depth\nguidance without requiring extensive training sets of paired data. In this\npaper, we propose an image deblurring method based on denoising diffusion\nmodels that can leverage the Lidar depth guidance and does not require training\ndata with paired Lidar depth maps. We also present the first dataset with real\nblurred images with corresponding Lidar depth maps and sharp ground truth\nimages, acquired with an Apple iPhone 15 Pro, for the purpose of studying\nLidar-guided deblurring. Experimental results on this novel dataset show that\nLidar guidance is effective and the proposed method outperforms\nstate-of-the-art deblurring methods in terms of perceptual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u5229\u7528LiDAR\u6df1\u5ea6\u5f15\u5bfc\u4e14\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u540c\u65f6\u521b\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u771f\u5b9e\u6a21\u7cca\u56fe\u50cf\u3001LiDAR\u6df1\u5ea6\u56fe\u548c\u6e05\u6670\u771f\u503c\u56fe\u50cf\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5305\u542b\u771f\u5b9e\u6a21\u7cca\u56fe\u50cf\u548c\u914d\u5bf9\u79fb\u52a8LiDAR\u6df1\u5ea6\u56fe\u7684\u6570\u636e\u96c6\uff0c\u540c\u65f6\u4e5f\u7f3a\u4e4f\u65e0\u9700\u5927\u91cf\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u5c31\u80fd\u5229\u7528\u6df1\u5ea6\u5f15\u5bfc\u5bf9\u771f\u5b9e\u56fe\u50cf\u8fdb\u884c\u53bb\u6a21\u7cca\u7684\u76f2\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528LiDAR\u6df1\u5ea6\u5f15\u5bfc\uff0c\u4e14\u4e0d\u9700\u8981\u8bad\u7ec3\u5177\u6709\u914d\u5bf9LiDAR\u6df1\u5ea6\u56fe\u7684\u6570\u636e\u3002", "result": "\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLiDAR\u5f15\u5bfc\u662f\u6709\u6548\u7684\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53bb\u6a21\u7cca\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86LiDAR\u5f15\u5bfc\u53bb\u6a21\u7cca\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u4fe1\u606f\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u3002"}}
{"id": "2509.09056", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09056", "abs": "https://arxiv.org/abs/2509.09056", "authors": ["Michael Caulfield", "Randy Palamar", "Darren Dahunsi", "Mohammad Rahim Sobhani", "Negar Majidi", "Roger Zemp"], "title": "Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)", "comment": "6 pages, 8 figures", "summary": "Recent developments in Row Column Arrays (RCAs) have presented promising\noptions for volumetric imaging without the need for the excessive channel\ncounts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top\nOrthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that\nimaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning\n(FORCES) allow for full transmit and receive focusing everywhere in the image\nplane. However, due to its fixed elevational focus and large transmit aperture,\nFORCES experiences poor elevational focusing away from the focal point. In this\nstudy we present a modification to the FORCES imaging scheme by applying\nRetrospective Transmit Beamforming (RTB) in the elevational direction to allow\nfor elevational transmit focusing everywhere in the imaging plane. We evaluate\nFORCES and uFORCES methods, with and without RTB applied, when imaging both a\ncyst and wire phantom. With experiment we show improved elevational focusing\ncapabilities away from the focal point when RTB is applied to both FORCES and\nuFORCES. At the focal point, performance with RTB remains comparable or\nimproved relative to standard FORCES. This is quantified by the measurement of\nFull Width Half Max when imaging the wire phantom, and by the generalized\nContrast to Noise Ratio when imaging the tubular cyst phantom. We also\ndemonstrate the volumetric imaging capabilities of FORCES RTB with the wire\nphantom.", "AI": {"tldr": "\u57fa\u4e8e\u884c\u5217\u6570\u7ec4(RCA)\u7684FORCES\u6210\u50cf\u65b9\u6848\u5b58\u5728\u8f7d\u6ce2\u65b9\u5411\u7126\u70b9\u5916\u7126\u805a\u6027\u80fd\u5dee\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u8f7d\u6ce2\u65b9\u5411\u56de\u987e\u6027\u53d1\u5c04\u6ce2\u675f\u6210\u578b(RTB)\u6280\u672f\u6765\u6539\u5584\u8f7d\u6ce2\u65b9\u5411\u7684\u5168\u5c40\u7126\u805a\u6027\u80fd", "motivation": "\u89e3\u51b3FORCES\u6210\u50cf\u65b9\u6848\u56e0\u56fa\u5b9a\u8f7d\u6ce2\u7126\u70b9\u548c\u5927\u53d1\u5c04\u5b54\u5f84\u5bfc\u81f4\u7684\u8f7d\u6ce2\u65b9\u5411\u7126\u70b9\u5916\u7126\u805a\u6027\u80fd\u5dee\u7684\u95ee\u9898", "method": "\u5728FORCES\u548cuFORCES\u65b9\u6848\u4e2d\u5e94\u7528\u8f7d\u6ce2\u65b9\u5411\u56de\u987e\u6027\u53d1\u5c04\u6ce2\u675f\u6210\u578b(RTB)\u6280\u672f\uff0c\u901a\u8fc7\u7eb3\u7c73\u7ebf\u5e7b\u5b50\u548c\u56ca\u6027\u5e7b\u5b50\u5b9e\u9a8c\u8bc4\u4f30\u6027\u80fd", "result": "RTB\u6280\u672f\u663e\u8457\u6539\u5584\u4e86FORCES\u548cuFORCES\u5728\u7126\u70b9\u5916\u533a\u57df\u7684\u8f7d\u6ce2\u65b9\u5411\u7126\u805a\u6027\u80fd\uff0c\u5728\u7126\u70b9\u5904\u6027\u80fd\u4fdd\u6301\u76f8\u5f53\u6216\u6709\u6240\u63d0\u5347\uff0c\u901a\u8fc7\u534a\u9ad8\u5bbd(FWHM)\u548c\u5bf9\u6bd4\u5ea6\u566a\u58f0\u6bd4(CNR)\u6307\u6807\u8fdb\u884c\u4e86\u91cf\u5316\u9a8c\u8bc1", "conclusion": "\u8f7d\u6ce2\u65b9\u5411RTB\u6280\u672f\u6709\u6548\u5730\u63d0\u5347\u4e86FORCES\u6210\u50cf\u65b9\u6848\u7684\u8f7d\u6ce2\u7126\u805a\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u8f7d\u6ce2\u7126\u805a\uff0c\u4e3a\u884c\u5217\u6570\u7ec4\u4f53\u79ef\u6210\u50cf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09494", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09494", "abs": "https://arxiv.org/abs/2509.09494", "authors": ["Zhuoyuan Li", "Jiacheng Li", "Yao Li", "Jialin Li", "Li Li", "Dong Liu", "Feng Wu"], "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding", "comment": "25 pages", "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u6df1\u5ea6\u7f51\u7edc\u8fc7\u6ee4\u7ed3\u679c\u9884\u5148\u8ba1\u7b97\u5e76\u5b58\u50a8\u5728\u67e5\u627e\u8868\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5faa\u73af\u8fc7\u6ee4\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7f16\u7801\u6548\u679c\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u57fa\u4e8e\u5faa\u73af\u8fc7\u6ee4\u65b9\u6848\u867d\u7136\u7f16\u7801\u6548\u679c\u4f18\u5f02\uff0c\u4f46\u5e26\u6765\u8f83\u9ad8\u7684\u8ba1\u7b97\u548c\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u6216\u9700\u8981\u4e13\u7528\u786c\u4ef6\u652f\u6301\uff0c\u9650\u5236\u4e86\u5176\u666e\u904d\u5e94\u7528\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u53c2\u8003\u8303\u56f4\u9650\u5236\u7684DNN\u8fc7\u6ee4\u5668\uff0c\u5c06\u6240\u6709\u53ef\u80fd\u8f93\u5165\u7684\u8fc7\u6ee4\u7ed3\u679c\u9884\u5148\u8ba1\u7b97\u5e76\u5b58\u50a8\u5230LUT\u4e2d\u3002\u8fc7\u6ee4\u8fc7\u7a0b\u901a\u8fc7\u67e5\u627e\u8868\u548c\u63d2\u503c\u5b8c\u6210\uff0c\u907f\u514d\u91cd\u8f7d\u7684DNN\u63a8\u7406\u8ba1\u7b97\u3002\u63d0\u51fa\u591a\u79cd\u8fc7\u6ee4LUT\u534f\u4f5c\u673a\u5236\u3001\u81ea\u5b9a\u4e49\u7d22\u5f15\u673a\u5236\u3001\u8de8\u5206\u91cf\u7d22\u5f15\u673a\u5236\u4ee5\u53caLUT\u538b\u7f29\u65b9\u6848\u3002", "result": "\u5728VVC\u53c2\u8003\u8f6f\u4ef6\u4e2d\u5b9e\u73b0\uff0c\u5728AI\u548cRA\u914d\u7f6e\u4e0b\u5e73\u5747\u5b9e\u73b0\u4e86\u7801\u73870.82%/2.97%/1.63%\u548c0.85%/4.11%/2.06%\u7684\u51cf\u5c11\u3002\u76f8\u6bd4DNN\u57fa\u4e8e\u65b9\u6848\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "LUT-ILF++\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u5faa\u73af\u8fc7\u6ee4\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u826f\u597d\u7f16\u7801\u6548\u679c\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u9891\u7f16\u7801\u6807\u51c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8fc7\u6ee4\u6280\u672f\u9009\u62e9\u3002"}}
{"id": "2509.09120", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09120", "abs": "https://arxiv.org/abs/2509.09120", "authors": ["Rong Ye", "Xue-Qin Jiang", "Hui Feng", "Jian Wang", "Runhe Qiu"], "title": "Signed Graph Learning with Hidden Nodes", "comment": "25 pages, 7 figures, published to Signal Processing", "summary": "Signed graphs, which are characterized by both positive and negative edge\nweights, have recently attracted significant attention in the field of graph\nsignal processing (GSP). Existing works on signed graph learning typically\nassume that all graph nodes are available. However, in some specific\napplications, only a subset of nodes can be observed while the remaining nodes\nstay hidden. To address this challenge, we propose a novel method for\nidentifying signed graph that accounts for hidden nodes, termed \\textit{signed\ngraph learning with hidden nodes under column-sparsity regularization}\n(SGL-HNCS). Our method is based on the assumption that graph signals are smooth\nover signed graphs, i.e., signal values of two nodes connected by positive\n(negative) edges are similar (dissimilar). Rooted in this prior assumption, the\ntopology inference of a signed graph is formulated as a constrained\noptimization problem with column-sparsity regularization, where the goal is to\nreconstruct the signed graph Laplacian matrix without disregarding the\ninfluence of hidden nodes. We solve the constrained optimization problem using\na tailored block coordinate descent (BCD) approach. Experimental results using\nsynthetic data and real-world data demonstrate the efficiency of the proposed\nSGL-HNCS method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u9690\u85cf\u8282\u70b9\u7684\u7b26\u53f7\u56fe\u5b66\u4e60\u65b9\u6cd5SGL-HNCS\uff0c\u901a\u8fc7\u5217\u7a00\u758f\u6b63\u5219\u5316\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6765\u91cd\u6784\u7b26\u53f7\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635", "motivation": "\u73b0\u6709\u7b26\u53f7\u56fe\u5b66\u4e60\u901a\u5e38\u5047\u8bbe\u6240\u6709\u8282\u70b9\u90fd\u53ef\u89c1\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u53ea\u6709\u90e8\u5206\u8282\u70b9\u53ef\u89c2\u6d4b\uff0c\u5176\u4f59\u8282\u70b9\u4fdd\u6301\u9690\u85cf\u72b6\u6001\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218", "method": "\u57fa\u4e8e\u56fe\u4fe1\u53f7\u5728\u7b26\u53f7\u56fe\u4e0a\u7684\u5e73\u6ed1\u6027\u5047\u8bbe\uff0c\u5c06\u62d3\u6251\u63a8\u65ad\u6784\u5efa\u4e3a\u5e26\u5217\u7a00\u758f\u6b63\u5219\u5316\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u5b9a\u5236\u7684\u5757\u5750\u6807\u4e0b\u964d(BCD)\u65b9\u6cd5\u6c42\u89e3", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0SGL-HNCS\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5b58\u5728\u9690\u85cf\u8282\u70b9\u7684\u7b26\u53f7\u56fe\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09144", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09144", "abs": "https://arxiv.org/abs/2509.09144", "authors": ["G Dhinesh Chandran", "Kota Srinivas Reddy", "Srikrishna Bhashyam"], "title": "Sequential Spectral Clustering of Data Sequences", "comment": null, "summary": "We study the problem of nonparametric clustering of data sequences, where\neach data sequence comprises i.i.d. samples generated from an unknown\ndistribution. The true clusters are the clusters obtained using the Spectral\nclustering algorithm (SPEC) on the pairwise distance between the true\ndistributions corresponding to the data sequences. Since the true distributions\nare unknown, the objective is to estimate the clusters by observing the minimum\nnumber of samples from the data sequences for a given error probability. To\nsolve this problem, we propose the Sequential Spectral clustering algorithm\n(SEQ-SPEC), and show that it stops in finite time almost surely and is\nexponentially consistent. We also propose a computationally more efficient\nalgorithm called the Incremental Approximate Sequential Spectral clustering\nalgorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed\nalgorithms perform better than the fixed sample size SPEC, the Sequential\n$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage\nclustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally\nefficient, performs close to SEQ-SPEC on both synthetic and real-world\ndatasets. To the best of our knowledge, this is the first work on spectral\nclustering of data sequences under a sequential framework.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6570\u636e\u5e8f\u5217\u7684\u975e\u53c2\u6570\u5206\u7ec4\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u5e8f\u5217\u8c31\u805a\u7c7b\u7b97\u6cd5\uff08SEQ-SPEC\u548cIA-SEQ-SPEC\uff09\uff0c\u5728\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u505c\u6b62\u548c\u6307\u6570\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6837\u672c\u91cf\u7ba1\u7406\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u771f\u5b9e\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u89c2\u5bdf\u6700\u5c0f\u6837\u672c\u91cf\u6765\u4f30\u8ba1\u6570\u636e\u5e8f\u5217\u7684\u805a\u7c7b\uff0c\u4ee5\u83b7\u5f97\u7ed9\u5b9a\u9519\u8bef\u6982\u7387\u4e0b\u7684\u6700\u4f18\u805a\u7c7b\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u5e8f\u5217\u8c31\u805a\u7c7b\u7b97\u6cd5\uff08SEQ-SPEC\uff09\u548c\u589e\u91cf\u8fd1\u4f3c\u5e8f\u5217\u8c31\u805a\u7c7b\u7b97\u6cd5\uff08IA-SEQ-SPEC\uff09\uff0c\u901a\u8fc7\u5e8f\u5217\u91c7\u6837\u65b9\u5f0f\u52a8\u6001\u5730\u6536\u96c6\u6837\u672c\uff0c\u76f4\u5230\u8fbe\u5230\u9884\u8bbe\u7684\u805a\u7c7b\u7cbe\u5ea6\u8981\u6c42\u3002", "result": "\u7b97\u6cd5\u51e0\u4e4e\u5fc5\u7136\u5728\u6709\u9650\u65f6\u95f4\u5185\u505c\u6b62\uff0c\u5e76\u5177\u6709\u6307\u6570\u4e00\u81f4\u6027\u3002\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u7b97\u6cd5\u5728\u5408\u6210\u548c\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u66f4\u4f18\u4e8e\u56fa\u5b9a\u6837\u672c\u91cf\u8c31\u805a\u7c7b\u548c\u5176\u4ed6\u5e8f\u5217\u805a\u7c7b\u7b97\u6cd5\u3002IA-SEQ-SPEC\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u6027\u80fd\u63a5\u8fd1SEQ-SPEC\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5e8f\u5217\u6846\u67b6\u4e0b\u8fdb\u884c\u6570\u636e\u5e8f\u5217\u8c31\u805a\u7c7b\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u7b97\u6cd5\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6837\u672c\u91cf\u5229\u7528\uff0c\u4e3a\u5e8f\u5217\u805a\u7c7b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09147", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09147", "abs": "https://arxiv.org/abs/2509.09147", "authors": ["Ziqi Yan", "Zhichao Zhang"], "title": "JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information", "comment": null, "summary": "Wiener filtering in the joint time-vertex fractional Fourier transform\n(JFRFT) domain has shown high effectiveness in denoising time-varying graph\nsignals. Traditional filtering models use grid search to determine the\ntransform-order pair and compute filter coefficients, while learnable ones\nemploy gradient-descent strategies to optimize them; both require complete\nprior information of graph signals. To overcome this shortcoming, this letter\nproposes a data-model co-driven denoising approach, termed neural-network-aided\njoint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the\nJFRFT-domain Wiener filter model into a neural network and updates the\ntransform-order pair and filter coefficients through a data-driven approach.\nThis design enables effective denoising using only partial prior information.\nExperiments demonstrate that JFRFFNet achieves significant improvements in\noutput signal-to-noise ratio compared with some state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faJFRFFNet\u65b9\u6cd5\uff0c\u5c06\u8054\u5408\u65f6-\u9876\u70b9\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362\u57df\u7684\u7ef4\u7eb3\u6ee4\u6ce2\u6a21\u578b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u66f4\u65b0\u53d8\u6362\u9636\u6570\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\uff0c\u4ec5\u9700\u90e8\u5206\u5148\u9a8c\u4fe1\u606f\u5373\u53ef\u6709\u6548\u53bb\u566a\u3002", "motivation": "\u4f20\u7edf\u6ee4\u6ce2\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u7684\u56fe\u4fe1\u53f7\u5148\u9a8c\u4fe1\u606f\uff0c\u8981\u4e48\u901a\u8fc7\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u53d8\u6362\u9636\u6570\u5bf9\u548c\u8ba1\u7b97\u6ee4\u6ce2\u5668\u7cfb\u6570\uff0c\u8981\u4e48\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b56\u7565\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6570\u636e-\u6a21\u578b\u534f\u540c\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5c06JFRFT\u57df\u7684\u7ef4\u7eb3\u6ee4\u6ce2\u6a21\u578b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u5b66\u4e60\u6700\u4f18\u7684\u53d8\u6362\u9636\u6570\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJFRFFNet\u5728\u8f93\u51fa\u4fe1\u566a\u6bd4\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "JFRFFNet\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u5148\u9a8c\u4fe1\u606f\u7684\u9650\u5236\uff0c\u4ec5\u9700\u90e8\u5206\u5148\u9a8c\u4fe1\u606f\u5c31\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u65f6\u53d8\u56fe\u4fe1\u53f7\u53bb\u566a\u3002"}}
{"id": "2509.09225", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09225", "abs": "https://arxiv.org/abs/2509.09225", "authors": ["Lin Jin", "Hang Sheng", "Hui Feng", "Bo Hu"], "title": "On Sampling of Multiple Correlated Stochastic Signals", "comment": null, "summary": "Multiple stochastic signals possess inherent statistical correlations, yet\nconventional sampling methods that process each channel independently result in\ndata redundancy. To leverage this correlation for efficient sampling, we model\ncorrelated channels as a linear combination of a smaller set of uncorrelated,\nwide-sense stationary latent sources. We establish a theoretical lower bound on\nthe total sampling density for zero mean-square error reconstruction, proving\nit equals the ratio of the joint spectral bandwidth of latent sources to the\nnumber of correlated signal channels. We then develop a constructive multi-band\nsampling scheme that attains this bound. The proposed method operates via\nspectral partitioning of the latent sources, followed by spatio-temporal\nsampling and interpolation. Experiments on synthetic and real datasets confirm\nthat our scheme achieves near-lossless reconstruction precisely at the\ntheoretical sampling density, validating its efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u901a\u9053\u4fe1\u53f7\u7edf\u8ba1\u76f8\u5173\u6027\u7684\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e3a\u5c11\u91cf\u4e0d\u76f8\u5173\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u8fbe\u5230\u4e86\u7406\u8bba\u6700\u5c0f\u91c7\u6837\u5bc6\u5ea6\u3002", "motivation": "\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u901a\u9053\u5bfc\u81f4\u6570\u636e\u5197\u4f59\uff0c\u591a\u901a\u9053\u968f\u673a\u4fe1\u53f7\u5b58\u5728\u56fa\u6709\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u9700\u8981\u5229\u7528\u8fd9\u79cd\u76f8\u5173\u6027\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002", "method": "\u5c06\u76f8\u5173\u901a\u9053\u5efa\u6a21\u4e3a\u5c11\u91cf\u4e0d\u76f8\u5173\u5bbd\u5e73\u7a33\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u901a\u8fc7\u6f5c\u5728\u6e90\u7684\u9891\u8c31\u5206\u5272\uff0c\u7136\u540e\u8fdb\u884c\u65f6\u7a7a\u91c7\u6837\u548c\u63d2\u503c\uff0c\u6784\u5efa\u591a\u6ce2\u6bb5\u91c7\u6837\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u91c7\u6837\u5bc6\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u91cd\u5efa\uff0c\u9a8c\u8bc1\u4e86\u5176\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6ce2\u6bb5\u91c7\u6837\u65b9\u6848\u8fbe\u5230\u4e86\u7406\u8bba\u6700\u5c0f\u91c7\u6837\u5bc6\u5ea6\uff0c\u6709\u6548\u5229\u7528\u4e86\u591a\u901a\u9053\u4fe1\u53f7\u7684\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u91c7\u6837\u3002"}}
{"id": "2509.09264", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09264", "abs": "https://arxiv.org/abs/2509.09264", "authors": ["Davoud Hajhassani", "Quentin Barth\u00e9lemy", "J\u00e9r\u00e9mie Mattout", "Marco Congedo"], "title": "Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG", "comment": null, "summary": "Electroencephalography (EEG) signal cleaning has long been a critical\nchallenge in the research community. The presence of artifacts can\nsignificantly degrade EEG data quality, complicating analysis and potentially\nleading to erroneous interpretations. While various artifact rejection methods\nhave been proposed, the gold standard remains manual visual inspection by human\nexperts-a process that is time-consuming, subjective, and impractical for\nlarge-scale EEG studies. Existing techniques are often hindered by a strong\nreliance on manual hyperparameter tuning, sensitivity to outliers, and high\ncomputational costs. In this paper, we introduce the improved Riemannian Potato\nField (iRPF), a fast and fully automated method for EEG artifact rejection that\naddresses key limitations of current approaches. We evaluate iRPF against\nseveral state-of-the-art artifact rejection methods, using two publicly\navailable EEG databases, labeled for various artifact types, comprising 226 EEG\nrecordings. Our results demonstrate that iRPF outperforms all competitors\nacross multiple metrics, with gains of up to 22% in recall, 102% in\nspecificity, 54% in precision, and 24% in F1-score, compared to Isolation\nForest, Autoreject, Riemannian Potato, and Riemannian Potato Field,\nrespectively. Statistical analysis confirmed the significance of these\nimprovements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most\ncomparisons. Additionally, on a typical EEG recording iRPF performs artifact\ncleaning in under 8 milliseconds per epoch using a standard laptop,\nhighlighting its efficiency for large-scale EEG data processing and real-time\napplications. iRPF offers a robust and data-driven artifact rejection solution\nfor high-quality EEG pre-processing in brain-computer interfaces and clinical\nneuroimaging applications.", "AI": {"tldr": "iRPF\u662f\u4e00\u79cd\u5feb\u901f\u5168\u81ea\u52a8\u7684EEG\u4fe1\u53f7\u4f2a\u5f71\u53bb\u9664\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5904\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u5927\u89c4\u6a21EEG\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "EEG\u4fe1\u53f7\u4e2d\u7684\u4f2a\u5f71\u4f1a\u4e25\u91cd\u5f71\u54cd\u6570\u636e\u8d28\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u5bf9\u5f02\u5e38\u503c\u654f\u611f\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6539\u8fdb\u7684\u9ece\u66fc\u571f\u8c46\u573a(iRPF)\u65b9\u6cd5\uff0c\u662f\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u5feb\u901f\u5168\u81ea\u52a8\u4f2a\u5f71\u53bb\u9664\u6280\u672f\u3002", "result": "\u5728226\u4e2aEEG\u8bb0\u5f55\u4e0a\u6d4b\u8bd5\uff0ciRPF\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u63d0\u534722%\uff0c\u7279\u5f02\u6027\u63d0\u5347102%\uff0c\u7cbe\u786e\u5ea6\u63d0\u534754%\uff0cF1\u5206\u6570\u63d0\u534724%\uff0c\u5904\u7406\u901f\u5ea6\u8fbe\u5230\u6bcfepoch 8\u6beb\u79d2\u3002", "conclusion": "iRPF\u4e3a\u8111\u673a\u63a5\u53e3\u548c\u4e34\u5e8a\u795e\u7ecf\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u6570\u636e\u9a71\u52a8\u7684\u4f2a\u5f71\u53bb\u9664\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u6027\u80fd\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2509.09282", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09282", "abs": "https://arxiv.org/abs/2509.09282", "authors": ["Leonardo M\u00f6rlein", "Dirk Manteuffel"], "title": "On the Relation of Characteristic Modes of Different Conducting Structures", "comment": null, "summary": "A formalism is derived to analyze the scattering of a conducting structure\nbased on the characteristic modes of another structure whose surface is a\nsuperset of the first structure. This enables the analysis and comparison of\ndifferent structures using a common basis of characteristic modes.\nAdditionally, it is shown that the scattering matrices and perturbation\nmatrices are no longer diagonal in these cases. Based on this, a modal\ntransformation matrix is defined to describe the mapping between the\ncharacteristic fields and the weighting coefficients of the two structures.\nThis matrix enables the conversion of the perturbation matrices in different\nbases. Finally, two examples are provided along with a discussion of some\naspects of the theory. The first example aims to validate and illustrate the\nformalism. The second example shows how the formalism can be applied in the\ndesign process of an antenna element that is gradually modified, starting from\na base structure.", "AI": {"tldr": "\u57fa\u4e8e\u7279\u5f81\u6a21\u6001\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e00\u4e2a\u7ed3\u6784\u7684\u7279\u5f81\u6a21\u6001\u5206\u6790\u5176\u5b50\u7ed3\u6784\u7684\u6563\u5c04\u7279\u6027\uff0c\u5e76\u5b9a\u4e49\u6a21\u6001\u53d8\u6362\u77e9\u9635\u6765\u8fdb\u884c\u57fa\u51c6\u8f6c\u6362\u3002", "motivation": "\u4e3a\u4e86\u5728\u5171\u540c\u7684\u7279\u5f81\u6a21\u6001\u57fa\u7840\u4e0a\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u5bfc\u4f53\u7ed3\u6784\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u548c\u5206\u6790\u51c6\u786e\u6027\u3002", "method": "\u63a8\u5bfc\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5b9a\u4e49\u6a21\u6001\u53d8\u6362\u77e9\u9635\u6765\u63cf\u8ff0\u4e24\u4e2a\u7ed3\u6784\u4e4b\u95f4\u7279\u5f81\u573a\u548c\u6743\u91cd\u7cfb\u6570\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u8fdb\u884c\u57fa\u51c6\u8f6c\u6362\u3002", "result": "\u8bc1\u660e\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6563\u5c04\u77e9\u9635\u548c\u5fae\u6c27\u77e9\u9635\u4e0d\u518d\u662f\u5bf9\u89d2\u77e9\u9635\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3a\u5929\u7ebf\u5143\u4ef6\u7684\u6e10\u8fdb\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u5728\u5171\u540c\u57fa\u7840\u4e0a\u8fdb\u884c\u7ed3\u6784\u6bd4\u8f83\u548c\u4f18\u5316\u3002"}}
{"id": "2509.09373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09373", "abs": "https://arxiv.org/abs/2509.09373", "authors": ["Huayan Guo", "Jichen Zhang", "Junhui Rao", "Ross Murch", "Vincent K. N. Lau"], "title": "Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems", "comment": "13 pages, 12 figures", "summary": "Pixel-based fluid antennas provide enhanced multiplexing gains and quicker\nradiation pattern switching than traditional designs. However, this innovation\nintroduces challenges for channel estimation and analog precoding due to the\nstate-non-separable channel response problem. This paper explores a multiuser\nMIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements\nfrom a real-world prototype. We present a sparse channel recovery framework for\nuplink channel sounding, employing an approximate separable channel response\nmodel with DNN-based antenna radiation functions. We then propose two\nlow-complexity channel estimation algorithms that leverage orthogonal matching\npursuit and variational Bayesian inference to accurately recover channel\nresponses across various scattering cluster angles. These estimations enable\nthe prediction of composite channels for all fluid antenna states, leading to\nan analog precoding scheme that optimally selects switching states for\ndifferent antennas. Our simulation results indicate that the proposed approach\nsignificantly outperforms several baseline methods, especially in high\nsignal-to-noise ratio environments with numerous users.", "AI": {"tldr": "\u57fa\u4e8e\u5b9e\u9645\u539f\u578b\u6e2f\u5f84\u6d4b\u91cf\uff0c\u63d0\u51fa\u4e86\u7528\u4e8e\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u7684\u7a00\u758f\u6e2f\u9053\u6062\u590d\u6846\u67b6\u548c\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u6e2f\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u6e2f\u9053\u9884\u6d4b\u548c\u6a21\u62df\u9884\u7f16\u7801\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd", "motivation": "\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u867d\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u591a\u5de5\u6536\u76ca\u548c\u66f4\u5feb\u7684\u8f90\u5c04\u56fe\u5207\u6362\uff0c\u4f46\u5f15\u5165\u4e86\u72b6\u6001\u975e\u53ef\u5206\u79bb\u6e2f\u9053\u54cd\u5e94\u95ee\u9898\uff0c\u7ed9\u6e2f\u9053\u4f30\u8ba1\u548c\u6a21\u62df\u9884\u7f16\u7801\u5e26\u6765\u6311\u6218", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a00\u758f\u6e2f\u9053\u6062\u590d\u6846\u67b6\uff0c\u4f7f\u7528\u8fd1\u4f3c\u53ef\u5206\u79bb\u6e2f\u9053\u54cd\u5e94\u6a21\u578b\u548cDNN\u57fa\u4e8e\u5929\u7ebf\u8f90\u5c04\u51fd\u6570\uff1b\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u6e2f\u9053\u4f30\u8ba1\u7b97\u6cd5\uff08\u6b63\u4ea4\u5339\u914d\u8ffd\u8e76\u548c\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u65ad\uff09", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u9ad8\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u6570\u91cf\u8f83\u591a\u65f6", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6e2f\u9053\u4f30\u8ba1\u548c\u9884\u7f16\u7801\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u72b6\u6001\u975e\u53ef\u5206\u79bb\u6e2f\u9053\u54cd\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7528\u6237MIMO-OFDM\u7cfb\u7edf\u7684\u6027\u80fd"}}
{"id": "2509.09606", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09606", "abs": "https://arxiv.org/abs/2509.09606", "authors": ["Sajjad Hussain"], "title": "A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Accurate pathloss prediction is essential for the design and optimization of\nUAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches\nhave shown strong potential, their generalization across diverse environments,\nrobustness to noisy inputs, and sensitivity to UAV altitude remain\nunderexplored. To address these challenges, we propose a UNet-based deep\nlearning architecture that combines multi-scale feature extraction,\nconvolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)\nbottleneck for efficient context aggregation. The model predicts pathloss maps\nfrom log-distance, line-of-sight (LOS) mask, and building mask inputs. In\naddition, we develop a fully vectorized LOS mask computation algorithm that\nsignificantly accelerates pre-processing and enables large-scale dataset\ngeneration. Extensive evaluations on both in-house ray-tracing data and the\nRadioMapSeer benchmark demonstrate that the proposed model outperforms several\nstate-of-the-art baselines in accuracy and efficiency. All source code is\npublicly released to support reproducibility and future research.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eUNet\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u6beb\u7c73\u6ce2\u7f51\u7edc\u8def\u5f84\u635f\u8017\u9884\u6d4b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548cASPP\u74f6\u9888\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8de8\u73af\u5883\u6cdb\u5316\u6027\u3001\u566a\u58f0\u8f93\u5165\u9c81\u68d2\u6027\u548c\u65e0\u4eba\u673a\u9ad8\u5ea6\u654f\u611f\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8def\u5f84\u635f\u8017\u9884\u6d4b\u65b9\u6848", "method": "UNet\u67b6\u6784\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u5377\u79ef\u7279\u5f81\u878d\u5408\u548cASPP\u74f6\u9888\uff0c\u4ece\u5bf9\u6570\u8ddd\u79bb\u3001LOS\u63a9\u7801\u548c\u5efa\u7b51\u63a9\u7801\u8f93\u5165\u9884\u6d4b\u8def\u5f84\u635f\u8017\u56fe\uff0c\u5e76\u5f00\u53d1\u4e86\u5411\u91cf\u5316LOS\u63a9\u7801\u8ba1\u7b97\u7b97\u6cd5", "result": "\u5728\u5185\u90e8\u5c04\u7ebf\u8ffd\u8e2a\u6570\u636e\u548cRadioMapSeer\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8def\u5f84\u635f\u8017\u9884\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u672a\u6765\u7814\u7a76"}}
