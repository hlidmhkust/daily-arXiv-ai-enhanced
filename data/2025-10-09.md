<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 7]
- [cs.IT](#cs.IT) [Total: 9]
- [eess.SP](#eess.SP) [Total: 12]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)](https://arxiv.org/abs/2510.06235)
*Robert Scholz,Kunal Bagga,Christine Ahrends,Carlo Alberto Barbano*

Main category: eess.IV

TL;DR: 团队Seinfeld在Algonauts 2025挑战赛中排名第10，通过整合多模态模型预测fMRI脑响应，使用堆叠回归融合预测结果。


<details>
  <summary>Details</summary>
Motivation: 开发能够准确预测大脑对电影刺激响应的多模态编码模型，推进神经科学和人工智能交叉领域研究。

Method: 整合大型语言模型、视频编码器、音频模型和视觉语言模型的多模态表示，增强文本输入（详细转录和摘要），探索刺激调优和微调策略，使用堆叠回归组合预测。

Result: 在Algonauts 2025挑战赛中排名第10位，取得了稳健的预测性能。

Conclusion: 多模态方法能有效预测大脑活动，团队公开了所有代码和资源以促进该领域发展。

Abstract: We present our submission to the Algonauts 2025 Challenge, where the goal is
to predict fMRI brain responses to movie stimuli. Our approach integrates
multimodal representations from large language models, video encoders, audio
models, and vision-language models, combining both off-the-shelf and fine-tuned
variants. To improve performance, we enhanced textual inputs with detailed
transcripts and summaries, and we explored stimulus-tuning and fine-tuning
strategies for language and vision models. Predictions from individual models
were combined using stacked regression, yielding solid results. Our submission,
under the team name Seinfeld, ranked 10th. We make all code and resources
publicly available, contributing to ongoing efforts in developing multimodal
encoding models for brain activity.

</details>


### [2] [A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation](https://arxiv.org/abs/2510.06276)
*Mehdi Rabiee,Sergio Greco,Reza Shahbazian,Irina Trubitsyna*

Main category: eess.IV

TL;DR: 提出了一种结合Dice损失和各向异性总变差(TV)损失的新框架，用于3D脑MRI中局灶性皮质发育不良(FCD)的精确分割，显著提升了分割精度并减少了假阳性。


<details>
  <summary>Details</summary>
Motivation: FCD是药物难治性癫痫的主要原因，但由于病灶细微、尺度小、对比度弱，在脑MRI中难以检测。现有方法面临标注数据有限、3D多模态输入复杂、缺乏空间平滑性和解剖一致性等问题。

Method: 采用先进的transformer增强编码器-解码器架构，并引入结合Dice损失和各向异性TV项的新型损失函数，无需后处理即可促进空间平滑性并减少假阳性簇。

Result: 在85名癫痫患者的公共FCD数据集上评估，与标准损失函数相比，提出的TV损失使Dice系数提高11.9%，精度提高13.3%，假阳性簇数量减少61.6%。

Conclusion: 所提出的结合TV损失的框架能够有效分割FCD区域，显著提升分割精度和一致性，为癫痫手术规划提供了可靠的工具。

Abstract: Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy
and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to
the subtle and small-scale nature of its lesions. Accurate segmentation of FCD
regions in 3D multimodal brain MRI images is essential for effective surgical
planning and treatment. However, this task remains highly challenging due to
the limited availability of annotated FCD datasets, the extremely small size
and weak contrast of FCD lesions, the complexity of handling 3D multimodal
inputs, and the need for output smoothness and anatomical consistency, which is
often not addressed by standard voxel-wise loss functions. This paper presents
a new framework for segmenting FCD regions in 3D brain MRI images. We adopt
state-of-the-art transformer-enhanced encoder-decoder architecture and
introduce a novel loss function combining Dice loss with an anisotropic {Total
Variation} (TV) term. This integration encourages spatial smoothness and
reduces false positive clusters without relying on post-processing. The
framework is evaluated on a public FCD dataset with 85 epilepsy patients and
demonstrates superior segmentation accuracy and consistency compared to
standard loss formulations. The model with the proposed TV loss shows an 11.9\%
improvement on the Dice coefficient and 13.3\% higher precision over the
baseline model. Moreover, the number of false positive clusters is reduced by
61.6%

</details>


### [3] [SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation](https://arxiv.org/abs/2510.06283)
*Sashank Makanaboyina*

Main category: eess.IV

TL;DR: SER-Diff是一个将扩散模型与增量学习相结合的新框架，通过生成合成错误图来缓解灾难性遗忘问题，在脑肿瘤分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分割模型在增量学习中的灾难性遗忘问题，传统方法依赖生成重放或辅助存储，而扩散模型在分割细化中有效但尚未用于增量学习。

Method: 提出SER-Diff框架，利用冻结的教师扩散模型生成过去任务的合成错误图，在新任务训练时重放这些错误图，采用结合Dice损失和知识蒸馏损失的双损失函数。

Result: 在BraTS2020、2021和2023数据集上，SER-Diff获得最高Dice分数（95.8%、94.9%、94.6%）和最低HD95值（4.4mm、4.7mm、4.9mm），优于现有方法。

Conclusion: SER-Diff不仅能有效缓解灾难性遗忘，还能在演化数据集上提供更准确和解剖学上更一致的分割结果。

Abstract: Incremental brain tumor segmentation is critical for models that must adapt
to evolving clinical datasets without retraining on all prior data. However,
catastrophic forgetting, where models lose previously acquired knowledge,
remains a major obstacle. Recent incremental learning frameworks with knowledge
distillation partially mitigate forgetting but rely heavily on generative
replay or auxiliary storage. Meanwhile, diffusion models have proven effective
for refining tumor segmentations, but have not been explored in incremental
learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the
first framework that unifies diffusion-based refinement with incremental
learning. SER-Diff leverages a frozen teacher diffusion model to generate
synthetic error maps from past tasks, which are replayed during training on new
tasks. A dual-loss formulation combining Dice loss for new data and knowledge
distillation loss for replayed errors ensures both adaptability and retention.
Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff
consistently outperforms prior methods. It achieves the highest Dice scores of
95.8\%, 94.9\%, and 94.6\%, along with the lowest HD95 values of 4.4 mm, 4.7
mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only
mitigates catastrophic forgetting but also delivers more accurate and
anatomically coherent segmentations across evolving datasets.

</details>


### [4] [Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data](https://arxiv.org/abs/2510.06335)
*Mohammed Alsubaie,Wenxi Liu,Linxia Gu,Ovidiu C. Andronesi,Sirani M. Perera,Xianqi Li*

Main category: eess.IV

TL;DR: 提出了一种条件去噪扩散框架，通过在每个反向扩散步骤中嵌入数据一致性校正，结合配对监督训练，显著提升了加速MRI重建的图像质量和感知真实性。


<details>
  <summary>Details</summary>
Motivation: 解决MRI采集时间过长的问题，现有方法要么依赖无监督分数函数，要么仅将数据一致性作为后处理步骤，缺乏对MRI物理模型的直接嵌入。

Method: 条件去噪扩散框架，在每个反向扩散步骤中嵌入测量模型，使用配对的欠采样-真实数据进行训练，实现生成灵活性与MRI物理约束的融合。

Result: 在fastMRI数据集上，SSIM、PSNR和LPIPS指标均优于现有深度学习和扩散方法，LPIPS更好地捕捉了感知质量的提升。

Conclusion: 将条件监督与迭代一致性更新相结合，在像素级保真度和感知真实性方面都取得了显著改进，为稳健的加速MRI重建提供了原则性和实用的进展。

Abstract: Magnetic Resonance Imaging (MRI) is a critical tool in modern medical
diagnostics, yet its prolonged acquisition time remains a critical limitation,
especially in time-sensitive clinical scenarios. While undersampling strategies
can accelerate image acquisition, they often result in image artifacts and
degraded quality. Recent diffusion models have shown promise for reconstructing
high-fidelity images from undersampled data by learning powerful image priors;
however, most existing approaches either (i) rely on unsupervised score
functions without paired supervision or (ii) apply data consistency only as a
post-processing step. In this work, we introduce a conditional denoising
diffusion framework with iterative data-consistency correction, which differs
from prior methods by embedding the measurement model directly into every
reverse diffusion step and training the model on paired undersampled-ground
truth data. This hybrid design bridges generative flexibility with explicit
enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that
our framework consistently outperforms recent state-of-the-art deep learning
and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing
perceptual improvements more faithfully. These results demonstrate that
integrating conditional supervision with iterative consistency updates yields
substantial improvements in both pixel-level fidelity and perceptual realism,
establishing a principled and practical advance toward robust, accelerated MRI
reconstruction.

</details>


### [5] [FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images](https://arxiv.org/abs/2510.06621)
*Jiasong Chen,Linchen Qian,Ruonan Gong,Christina Sun,Tongran Qin,Thuy Pham,Caitlin Martin,Mohammad Zafar,John Elefteriades,Wei Sun,Liang Liang*

Main category: eess.IV

TL;DR: 开发了一个端到端的深度神经网络，能够直接从3D CT图像生成患者特定的主动脉有限元网格，以解决胸主动脉瘤破裂风险评估中的3D重建障碍。


<details>
  <summary>Details</summary>
Motivation: 胸主动脉瘤是成人死亡的主要原因之一，当前基于有限元分析的破裂风险评估方法面临两个主要障碍：劳动密集型的3D重建和计算负担。团队已通过PyTorch FEA库解决了计算负担问题，现在需要解决手动分割带来的3D重建障碍。

Method: 开发了一个端到端的深度神经网络，能够直接从3D CT图像自动生成患者特定的主动脉有限元网格，避免了传统方法中需要手动分割的步骤。

Result: 通过PyTorch FEA库和静态确定性原理，将基于FEA的应力计算时间减少到约3分钟/例；通过集成DNN和FEA，进一步将计算时间减少到仅几秒/例。

Conclusion: 该研究成功开发了能够直接从医学图像生成有限元网格的深度学习方法，有望克服胸主动脉瘤破裂风险评估临床应用中的人工分割障碍，提高评估效率并扩大应用规模。

Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in
the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal
bulging of thoracic aortic wall and it is a leading cause of death in adults.
From the perspective of biomechanics, rupture occurs when the stress acting on
the aortic wall exceeds the wall strength. Wall stress distribution can be
obtained by computational biomechanical analyses, especially structural Finite
Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be
calculated by comparing stress with material strength using a material failure
model. Although these engineering tools are currently available for TAA rupture
risk assessment on patient specific level, clinical adoption has been limited
due to two major barriers: labor intensive 3D reconstruction current patient
specific anatomical modeling still relies on manual segmentation, making it
time consuming and difficult to scale to a large patient population, and
computational burden traditional FEA simulations are resource intensive and
incompatible with time sensitive clinical workflows. The second barrier was
successfully overcome by our team through the development of the PyTorch FEA
library and the FEA DNN integration framework. By incorporating the FEA
functionalities within PyTorch FEA and applying the principle of static
determinacy, we reduced the FEA based stress computation time to approximately
three minutes per case. Moreover, by integrating DNN and FEA through the
PyTorch FEA library, our approach further decreases the computation time to
only a few seconds per case. This work focuses on overcoming the first barrier
through the development of an end to end deep neural network capable of
generating patient specific finite element meshes of the aorta directly from 3D
CT images.

</details>


### [6] [Fitzpatrick Thresholding for Skin Image Segmentation](https://arxiv.org/abs/2510.06655)
*Duncan Stothers,Sophia Xu,Carlie Reeves,Lia Gracey*

Main category: eess.IV

TL;DR: 该论文提出了一种基于Fitzpatrick皮肤类型调整决策阈值的方法，显著提升了深色皮肤（Fitz VI）上银屑病皮疹分割的性能，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有炎症性皮肤病（如银屑病）分割模型在深色皮肤上表现明显较差，可能影响公平医疗。需要解决皮肤类型对分割性能的不平等影响。

Method: 收集来自六个公共图集的银屑病数据集，标注Fitzpatrick皮肤类型和详细分割掩码。训练U-Net、ResU-Net和SETR-small模型，不使用皮肤类型信息。在调优集上搜索决策阈值，选择全局最优和按Fitzpatrick皮肤类型的最优阈值。

Result: 使用Fitzpatrick特定阈值将最暗子组（Fitz VI）的分割性能提升：U-Net +31% bIoU和+24% Dice，ResU-Net +25% bIoU和+18% Dice，SETR-small +17% bIoU和+11% Dice。

Conclusion: Fitzpatrick阈值调整方法简单、模型无关、无需架构更改或重新训练，成本几乎为零，可作为未来公平性基准。

Abstract: Accurate estimation of the body surface area (BSA) involved by a rash, such
as psoriasis, is critical for assessing rash severity, selecting an initial
treatment regimen, and following clinical treatment response. Attempts at
segmentation of inflammatory skin disease such as psoriasis perform markedly
worse on darker skin tones, potentially impeding equitable care. We assembled a
psoriasis dataset sourced from six public atlases, annotated for Fitzpatrick
skin type, and added detailed segmentation masks for every image. Reference
models based on U-Net, ResU-Net, and SETR-small are trained without tone
information. On the tuning split we sweep decision thresholds and select (i)
global optima and (ii) per Fitzpatrick skin tone optima for Dice and binary
IoU. Adapting Fitzpatrick specific thresholds lifted segmentation performance
for the darkest subgroup (Fitz VI) by up to +31 % bIoU and +24 % Dice on UNet,
with consistent, though smaller, gains in the same direction for ResU-Net (+25
% bIoU, +18 % Dice) and SETR-small (+17 % bIoU, +11 % Dice). Because
Fitzpatrick skin tone classifiers trained on Fitzpatrick-17k now exceed 95 %
accuracy, the cost of skin tone labeling required for this technique has fallen
dramatically. Fitzpatrick thresholding is simple, model-agnostic, requires no
architectural changes, no re-training, and is virtually cost free. We
demonstrate the inclusion of Fitzpatrick thresholding as a potential future
fairness baseline.

</details>


### [7] [Content-Adaptive Inference for State-of-the-art Learned Video Compression](https://arxiv.org/abs/2510.07283)
*Ahmet Bilican,M. Akın Yılmaz,A. Murat Tekalp*

Main category: eess.IV

TL;DR: 提出了一种通用框架，通过自适应下采样帧来控制运动矢量尺度，使测试视频和训练视频的运动矢量范围匹配，从而提升学习视频编码器在复杂/大运动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习视频编码器在复杂/大运动场景下的性能提升有限，主要原因是编码器模型无法泛化到训练集未见的运动矢量范围，导致流场编码和帧预测性能下降。

Method: 提出模型无关框架，在推理时根据内容自适应下采样帧，使运动矢量尺度与训练集匹配，从而改善流估计和流压缩效率。

Result: 该框架将最先进的低延迟视频编码器DCVC-FM在单个视频上的BD-rate性能提升高达41%，且无需模型微调。

Conclusion: 内容自适应推理框架能显著提升学习视频编码器在复杂运动场景下的性能，运动和场景复杂度可作为预测框架有效性的指标。

Abstract: While the BD-rate performance of recent learned video codec models in both
low-delay and random-access modes exceed that of respective modes of
traditional codecs on average over common benchmarks, the performance
improvements for individual videos with complex/large motions is much smaller
compared to scenes with simple motion. This is related to the inability of a
learned encoder model to generalize to motion vector ranges that have not been
seen in the training set, which causes loss of performance in both coding of
flow fields as well as frame prediction and coding. As a remedy, we propose a
generic (model-agnostic) framework to control the scale of motion vectors in a
scene during inference (encoding) to approximately match the range of motion
vectors in the test and training videos by adaptively downsampling frames. This
results in down-scaled motion vectors enabling: i) better flow estimation;
hence, frame prediction and ii) more efficient flow compression. We show that
the proposed framework for content-adaptive inference improves the BD-rate
performance of already state-of-the-art low-delay video codec DCVC-FM by up to
41\% on individual videos without any model fine tuning. We present ablation
studies to show measures of motion and scene complexity can be used to predict
the effectiveness of the proposed framework.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [8] [A doubly composite Chernoff-Stein lemma and its applications](https://arxiv.org/abs/2510.06342)
*Ludovico Lami*

Main category: cs.IT

TL;DR: 本文建立了适用于复合假设和真正相关分布的通用Chernoff-Stein引理，严格包含了大多数先前工作，并应用于零假设与复合i.i.d.或任意变化备择假设的区分。


<details>
  <summary>Details</summary>
Motivation: 现有Chernoff-Stein引理主要适用于简单假设或非真正相关的分布，需要扩展到复合假设和真正相关分布的情况。

Method: 使用在广义量子Stein引理背景下开发的模糊技术的改进版本，逐符号应用模糊，使其更强且在没有置换对称性时也适用。

Result: 建立了适用于复合假设和真正相关分布的通用Chernoff-Stein引理，提供了Stein指数的单字母公式，并建立了约束de Finetti约简陈述。

Conclusion: 该结果严格包含了大多数先前工作，为复合假设和真正相关分布的假设检验提供了理论基础，并在量子假设检验中有相关应用。

Abstract: Given a sequence of random variables $X^n=X_1,\ldots, X_n$, discriminating
between two hypotheses on the underlying probability distribution is a key task
in statistics and information theory. Of interest here is the Stein exponent,
i.e. the largest rate of decay (in $n$) of the type II error probability for a
vanishingly small type I error probability. When the hypotheses are simple and
i.i.d., the Chernoff-Stein lemma states that this is given by the relative
entropy between the single-copy probability distributions. Generalisations of
this result exist in the case of composite hypotheses, but mostly to settings
where the probability distribution of $X^n$ is not genuinely correlated, but
rather, e.g., a convex combination of product distributions with components
taken from a base set. Here, we establish a general Chernoff-Stein lemma that
applies to the setting where both hypotheses are composite and genuinely
correlated, satisfying only generic assumptions such as convexity (on both
hypotheses) and some weak form of permutational symmetry (on either
hypothesis). Our result, which strictly subsumes most prior work, is proved
using a refinement of the blurring technique developed in the context of the
generalised quantum Stein's lemma [Lami, IEEE Trans. Inf. Theory 2025]. In this
refined form, blurring is applied symbol by symbol, which makes it both
stronger and applicable also in the absence of permutational symmetry. The
second part of the work is devoted to applications: we provide a single-letter
formula for the Stein exponent characterising the discrimination of broad
families of null hypotheses vs a composite i.i.d. or an arbitrarily varying
alternative hypothesis, and establish a 'constrained de Finetti reduction'
statement that covers a wide family of convex constraints. Applications to
quantum hypothesis testing are explored in a related paper [Lami, arXiv:today].

</details>


### [9] [$α$-leakage Interpretation of Rényi Capacity](https://arxiv.org/abs/2510.06622)
*Ni Ding,Farhad Farokhi,Tao Guo,Yinfei Xu,Xiang Zhang*

Main category: cs.IT

TL;DR: 本文证明了Sibson互信息是α-泄漏在对手的f-均值相对信息增益上的平均，提出了Y-基本α-泄漏概念，将点态最大泄漏扩展到Rényi阶数范围α∈[0,∞)，并解释了Rényi容量作为最大f-均值信息泄漏。


<details>
  <summary>Details</summary>
Motivation: 研究α-泄漏的统计解释，扩展点态最大泄漏概念到更广泛的Rényi阶数范围，为信息泄漏分析提供更全面的理论框架。

Method: 通过f-均值相对信息增益分析Sibson互信息，提出Y-基本α-泄漏概念，使用交替最大-最大方法实现广义Blahut-Arimoto算法。

Result: 建立了α-泄漏与Sibson互信息的关系，扩展了泄漏度量的适用范围，提供了δ-近似ε-上界α-泄漏的充分条件。

Conclusion: Sibson互信息可解释为α-泄漏的平均值，Rényi容量可视为最大f-均值信息泄漏，为信息泄漏分析提供了新的理论视角和计算方法。

Abstract: For $\tilde{f}(t) = \exp(\frac{\alpha-1}{\alpha}t)$, this paper shows that
the Sibson mutual information is an $\alpha$-leakage averaged over the
adversary's $\tilde{f}$-mean relative information gain (on the secret) at
elementary event of channel output $Y$ as well as the joint occurrence of
elementary channel input $X$ and output $Y$. This interpretation is used to
derive a sufficient condition that achieves a $\delta$-approximation of
$\epsilon$-upper bounded $\alpha$-leakage. A $Y$-elementary $\alpha$-leakage is
proposed, extending the existing pointwise maximal leakage to the overall
R\'{e}nyi order range $\alpha \in [0,\infty)$. Maximizing this $Y$-elementary
leakage over all attributes $U$ of channel input $X$ gives the R\'{e}nyi
divergence. Further, the R\'{e}nyi capacity is interpreted as the maximal
$\tilde{f}$-mean information leakage over both the adversary's malicious
inference decision and the channel input $X$ (represents the adversary's prior
belief). This suggests an alternating max-max implementation of the existing
generalized Blahut-Arimoto method.

</details>


### [10] [Optimizing Fronthaul Quantization for Flexible User Load in Cell-Free Massive MIMO](https://arxiv.org/abs/2510.06734)
*Fabian Göttsch,Max Franke,Arash Pourdamghani,Giuseppe Caire,Stefan Schmid*

Main category: cs.IT

TL;DR: 研究可扩展的用户中心无蜂窝大规模MIMO系统的物理层频谱效率和前传网络负载，通过优化量化率和前传路由来平衡系统性能与网络负载。


<details>
  <summary>Details</summary>
Motivation: 在用户中心无蜂窝大规模MIMO系统中，前传链路容量有限，需要量化数据以降低前传负载，同时保持物理层性能。

Method: 使用混合整数线性规划联合优化簇处理器放置和前传流量路由，并基于率失真理论计算量化率。

Result: 优化量化率后，前传负载在广泛的用户负载范围内保持稳定，且物理层性能损失很小。

Conclusion: 无蜂窝大规模MIMO系统和前传网络对变化的用户密度具有弹性，通过优化量化率可以有效平衡性能与负载。

Abstract: We investigate the physical layer (PHY) spectral efficiency and fronthaul
network load of a scalable user-centric cell-free massive MIMO system. Each
user-centric cluster processor responsible for cluster-level signal processing
is located at one of multiple decentralized units (DUs). Thus, the radio units
in the cluster must exchange data with the corresponding DU over the fronthaul.
Because the fronthaul links have limited capacity, this data must be quantized
before it is sent over the fronthaul. We consider a routed fronthaul network,
where the cluster processor placement and fronthaul traffic routing are jointly
optimized with a mixed-integer linear program. For different numbers of users
in the network, we investigate the effect of fronthaul quantization rates, a
system parameter computed based on rate-distortion theory. Our results show
that with optimized quantization rates, the fronthaul load is quite stable for
a wide range of user loads without significant PHY performance loss. This
demonstrates that the cell-free massive MIMO PHY and fronthaul network are
resilient to varying user densities.

</details>


### [11] [Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval](https://arxiv.org/abs/2510.06868)
*Didrik Bergström,Deniz Gündüz,Onur Günlü*

Main category: cs.IT

TL;DR: 该论文提出了一种结合深度哈希蒸馏的深度联合源信道编码方法，用于多跳AWGN信道中的图像传输，通过语义聚类提高语义一致性和感知重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统DeepJSCC在多跳设置中可能遭受噪声累积问题，需要提高语义一致性和感知重建质量，特别是在安全导向的应用中。

Method: 训练DeepJSCC编码器-解码器对，结合预训练的深度哈希蒸馏模块，同时最小化MSE和源图像与重建图像DHD哈希之间的余弦距离。

Result: 在不同多跳设置下显著提高了感知质量，通过LPIPS指标验证了语义对齐带来的改进效果。

Conclusion: 结合深度哈希蒸馏的DeepJSCC方法能够有效缓解多跳信道中的噪声累积问题，提升语义一致性和图像重建的感知质量。

Abstract: We consider image transmission via deep joint source-channel coding
(DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by
training a DeepJSCC encoder-decoder pair with a pre-trained deep hash
distillation (DHD) module to semantically cluster images, facilitating
security-oriented applications through enhanced semantic consistency and
improving the perceptual reconstruction quality. We train the DeepJSCC module
to both reduce mean square error (MSE) and minimize cosine distance between DHD
hashes of source and reconstructed images. Significantly improved perceptual
quality as a result of semantic alignment is illustrated for different
multi-hop settings, for which classical DeepJSCC may suffer from noise
accumulation, measured by the learned perceptual image patch similarity (LPIPS)
metric.

</details>


### [12] [A Stochastic Geometric Analysis on Multi-cell Pinching-antenna Systems under Blockage Effect](https://arxiv.org/abs/2510.06972)
*Yanshi Sun,Zhiguo Ding,George K. Karagiannidis*

Main category: cs.IT

TL;DR: 该论文提出了一个多细胞夹持天线系统的分析框架，考虑了空间分布的波导和基站干扰，使用随机几何工具建模并推导了中断概率表达式。


<details>
  <summary>Details</summary>
Motivation: 现有夹持天线技术研究主要关注单细胞场景，忽略了来自空间分布基站的干扰波导的影响，需要填补这一知识空白。

Method: 应用随机几何工具进行系统建模，推导多细胞夹持天线系统的中断概率表达式。

Result: 仿真结果验证了分析的准确性，并证明夹持天线系统相比固定天线系统具有优越性能。

Conclusion: 该研究为多细胞夹持天线系统提供了有效的性能评估框架，证明了其在干扰环境下的优势。

Abstract: Recently, the study on pinching-antenna technique has attracted significant
attention. However, most relevant literature focuses on a single-cell scenario,
where the effect from the interfering pinching-antennas on waveguides connected
to spatially distributed base stations (BSs) was ignored. To fulfill this
knowledge gap, this letter aims to provide an analytical framework on
performance evaluation for multi-cell pinching-antenna systems where spatially
distributed waveguides which are connected to different BSs are considered. In
particular, tools from stochastic geometry is applied for system modeling. The
expression for the outage probability is obtained. Simulation results are
provided to verify the accuracy of the analysis and demonstrate the superior
performance of pinching-antenna system compared to fixed-antenna systems.

</details>


### [13] [Lossless Compression of Time Series Data: A Comparative Study](https://arxiv.org/abs/2510.07015)
*Jonas G. Matt,Pengcheng Huang,Balz Maag*

Main category: cs.IT

TL;DR: 本文对时间序列数据的无损压缩方法进行了大规模比较研究，提出了包含数据变换和熵编码两阶段的统一框架，通过消融实验评估了不同算法在各种数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 数字时代产生了前所未有的海量数据，需要高效管理、传输和存储。数据压缩是关键技术，但现有技术众多，需要系统评估时间序列数据的压缩方法。

Method: 提出两阶段压缩框架：数据变换和熵编码。使用合成和真实数据集，通过消融实验分析各组件对整体压缩性能的影响。

Result: 揭示了不同算法面对不同时间序列特性时的优缺点，强调了完整压缩管道配置的重要性。

Conclusion: 研究为针对特定数据集选择和组合最合适的压缩算法提供了全面指南，突出完整压缩管道配置的重要性。

Abstract: Our increasingly digital and connected world has led to the generation of
unprecedented amounts of data. This data must be efficiently managed,
transmitted, and stored to preserve resources and allow scalability. Data
compression has therein been a key technology for a long time, resulting in a
vast landscape of available techniques. This largest-to-date study analyzes and
compares various lossless data compression methods for time series data. We
present a unified framework encompassing two stages: data transformation and
entropy encoding. We evaluate compression algorithms across both synthetic and
real-world datasets with varying characteristics. Through ablation studies at
each compression stage, we isolate the impact of individual components on
overall compression performance -- revealing the strengths and weaknesses of
different algorithms when facing diverse time series properties. Our study
underscores the importance of well-configured and complete compression
pipelines beyond individual components or algorithms; it offers a comprehensive
guide for selecting and composing the most appropriate compression algorithms
tailored to specific datasets.

</details>


### [14] [Robustness of Covariance Estimators with Application in Activity Detection](https://arxiv.org/abs/2510.07044)
*Hendrik Bernd Zarucha,Peter Jung,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文提出了一类通用的协方差估计器，通过实值函数g和模型协方差矩阵集H构建。证明了在温和条件下这类估计器具有鲁棒性。在第二部分应用于多接收天线随机接入中的活动检测，将大尺度衰落系数恢复问题转化为结构化协方差估计问题，提出了基于有符号核条件的码本设计。


<details>
  <summary>Details</summary>
Motivation: 研究协方差估计的鲁棒性，并将其应用于无线通信中的活动检测问题，解决多用户随机接入场景下的大尺度衰落系数恢复问题。

Method: 第一部分：定义由函数g和模型协方差集H生成的协方差估计器类，证明其鲁棒性。第二部分：将活动检测问题转化为结构化协方差估计，使用非负最小二乘估计器或松弛最大似然估计器，提出基于有符号核条件的码本设计。

Result: 证明了在温和条件下协方差估计器具有鲁棒性；在活动检测中，当接收天线数足够多且S≤⌈1/2M²⌉-1时，两种估计器都能恢复大尺度衰落系数。

Conclusion: 提出的通用协方差估计器类具有良好的鲁棒性，在无线通信活动检测中能有效恢复大尺度衰落系数，为多用户随机接入系统提供了理论保证。

Abstract: The first part of this work considers a general class of covariance
estimators. Each estimator of that class is generated by a real-valued function
$g$ and a set of model covariance matrices $H$. If $\bf{W}$ is a potentially
perturbed observation of a searched covariance matrix, then the estimator is
the minimizer of the sum of $g$ applied to each eigenvalue of
$\bf{W}^\frac{1}{2}\bf{Z}^{-1}\bf{W}^\frac{1}{2}$ under the constraint that
$\bf{Z}$ is from $H$. It is shown that under mild conditions on $g$ and $H$
such estimators are robust, meaning the estimation error can be made
arbitrarily small if the perturbation of $\bf{W}$ gets small enough. \par In
the second part of this work the previous results are applied to activity
detection in random access with multiple receive antennas. In activity
detection recovering the large scale fading coefficients is a sparse recovery
problem which can be reduced to a structured covariance estimation problem. The
recovery can be done with a non-negative least squares estimator or with a
relaxed maximum likelihood estimator. It is shown that under suitable
assumptions on the distributions of the noise and the channel coefficients, the
relaxed maximum likelihood estimator is from the general class of covariance
estimators considered in the first part of this work. Then, codebooks based
upon a signed kernel condition are proposed. It is shown that with the proposed
codebooks both estimators can recover the large-scale fading coefficients if
the number of receive antennas is high enough and
$S\leq\left\lceil\frac{1}{2}M^2\right\rceil-1$ where $S$ is the number of
active users and $M$ is number of pilot symbols per user.

</details>


### [15] [A Theoretically-Grounded Codebook for Digital Semantic Communications](https://arxiv.org/abs/2510.07108)
*Lingyi Wang,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.IT

TL;DR: 提出了一种基于信息理论的语义通信码本设计方法，通过优化量化效率、传输效率和鲁棒性，在图像重建任务中相比现有方法显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 为了解决语义通信中将高维语义特征映射到离散符号表示的问题，从信息理论角度研究码本启用的量化映射，旨在优化量化效率、传输效率和鲁棒性能。

Method: 建立了语义信息理论中的一对多同义映射与基于码本Voronoi分区的多对一量化映射的形式等价性；推导了语义特征与其量化索引之间的互信息；提出了基于经验估计的熵正则化量化损失进行端到端码本训练；提出了通道感知的语义失真损失来减轻物理通道噪声引起的语义失真。

Result: 在图像重建任务中，当信噪比为10dB时，相比现有码本设计，提出的理论驱动码本在峰值信噪比(PSNR)上提升了24.1%，在学习感知图像块相似度(LPIPS)上提升了46.5%。

Conclusion: 提出的理论驱动码本设计方法能够有效优化语义通信中的量化映射，显著提升系统性能，特别是在存在通道噪声的情况下表现出优越的鲁棒性。

Abstract: The use of a learnable codebook provides an efficient way for semantic
communications to map vector-based high-dimensional semantic features onto
discrete symbol representations required in digital communication systems. In
this paper, the problem of codebook-enabled quantization mapping for digital
semantic communications is studied from the perspective of information theory.
Particularly, a novel theoretically-grounded codebook design is proposed for
jointly optimizing quantization efficiency, transmission efficiency, and robust
performance. First, a formal equivalence is established between the one-to-many
synonymous mapping defined in semantic information theory and the many-to-one
quantization mapping based on the codebook's Voronoi partitions. Then, the
mutual information between semantic features and their quantized indices is
derived in order to maximize semantic information carried by discrete indices.
To realize the semantic maximum in practice, an entropy-regularized
quantization loss based on empirical estimation is introduced for end-to-end
codebook training. Next, the physical channel-induced semantic distortion and
the optimal codebook size for semantic communications are characterized under
bit-flip errors and semantic distortion. To mitigate the semantic distortion
caused by physical channel noise, a novel channel-aware semantic distortion
loss is proposed. Simulation results on image reconstruction tasks demonstrate
the superior performance of the proposed theoretically-grounded codebook that
achieves a 24.1% improvement in peak signal-to-noise ratio (PSNR) and a 46.5%
improvement in learned perceptual image patch similarity (LPIPS) compared to
the existing codebook designs when the signal-to-noise ratio (SNR) is 10 dB.

</details>


### [16] [Spectral Graph Clustering under Differential Privacy: Balancing Privacy, Accuracy, and Efficiency](https://arxiv.org/abs/2510.07136)
*Mohamed Seif,Antti Koskela,H. Vincent Poor,Andrea J. Goldsmith*

Main category: cs.IT

TL;DR: 该论文提出了三种在边差分隐私保护下的谱图聚类机制：图扰动、私有图投影和噪声幂迭代方法，通过理论分析和实验验证了隐私保护与聚类性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究在边差分隐私约束下进行谱图聚类的问题，需要在保护图结构隐私的同时保持聚类的准确性。

Method: 开发了三种机制：(1) 通过随机边翻转和邻接矩阵重排进行图扰动；(2) 在低维空间中使用加性高斯噪声进行私有图投影；(3) 在迭代过程中分布高斯噪声的噪声幂迭代方法。

Result: 理论分析提供了严格的隐私保证和对误分类错误率的精确刻画，在合成和真实网络上的实验验证了理论分析并展示了实际的隐私-效用权衡。

Conclusion: 提出的三种机制能够在边差分隐私保护下有效进行谱图聚类，实现了隐私保护与聚类性能的良好平衡。

Abstract: We study the problem of spectral graph clustering under edge differential
privacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation
via randomized edge flipping combined with adjacency matrix shuffling, which
enforces edge privacy while preserving key spectral properties of the graph.
Importantly, shuffling considerably amplifies the guarantees: whereas flipping
edges with a fixed probability alone provides only a constant epsilon edge DP
guarantee as the number of nodes grows, the shuffled mechanism achieves
(epsilon, delta) edge DP with parameters that tend to zero as the number of
nodes increase; (ii) private graph projection with additive Gaussian noise in a
lower-dimensional space to reduce dimensionality and computational complexity;
and (iii) a noisy power iteration method that distributes Gaussian noise across
iterations to ensure edge DP while maintaining convergence. Our analysis
provides rigorous privacy guarantees and a precise characterization of the
misclassification error rate. Experiments on synthetic and real-world networks
validate our theoretical analysis and illustrate the practical privacy-utility
trade-offs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [17] [Distributed Detection and Bandwidth Allocation with Hybrid Quantized and Full-Precision Observations over Multiplicative Fading Channels](https://arxiv.org/abs/2510.06429)
*Linlin Mao,Zeping Sui,Michail Matthaiou,Hongbin Li*

Main category: eess.SP

TL;DR: 提出了一种融合量化和全精度观测的混合检测器，用于高斯加性和乘性噪声下的弱信号检测，并优化了量化阈值和带宽分配策略。


<details>
  <summary>Details</summary>
Motivation: 在存在加性和乘性高斯噪声的挑战性环境中，需要开发有效的弱信号检测方法，同时考虑传输带宽约束和易错信道条件。

Method: 基于复合观测的概率分布推导局部最优检验混合检测器，优化传感器级量化阈值，并提出混合整数线性规划方法解决带宽分配问题。

Result: 仿真结果表明，所提出的混合检测器和带宽分配策略在性能上优于现有方法，特别是在易错信道条件下表现优越。

Conclusion: 该混合检测器结合量化阈值优化和带宽分配策略，在弱信号检测中实现了接近最优的性能，特别适用于带宽受限和信道条件恶劣的场景。

Abstract: A hybrid detector that fuses both quantized and full-precision observations
is proposed for weak signal detection under additive and multiplicative
Gaussian noise. We first derive a locally most powerful test (LMPT)--based
hybrid detector from the composite probability distribution of the compound
observations received by the fusion center, and then analyze its asymptotic
detection performance. Subsequently, we optimize the sensor-wise quantization
thresholds to achieve near-optimal asymptotic performance at the local sensor
level. Moreover, we propose a mixed-integer linear programming approach to
solve the optimization problem of transmission bandwidth allocation accounting
for bandwidth constraints and error-prone channels. Finally, simulation results
demonstrate the superiority of the proposed hybrid detector and the bandwidth
allocation strategy, especially in challenging error-prone channel conditions.

</details>


### [18] [Optimized SVR Framework for Electric Load Forecasting](https://arxiv.org/abs/2510.06476)
*Nishant Gadde,Yoshua Alexander,Sarvesh Parthasarthy,Arman Allidina*

Main category: eess.SP

TL;DR: 提出基于支持向量回归(SVR)的电力负荷预测框架，相比行业标准方法在各项评估指标上表现更优，特别是在MSE和MAE方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于电力系统日益复杂、极端天气增多以及用户能源需求变化，传统负荷预测方法有时会失效，需要更准确的预测工具来支持电网运营。

Method: 采用支持向量回归(SVR)框架进行电力负荷预测，该方法在电力系统运行的重要评估指标上表现优于行业标准方法。

Result: SVR模型在所有评估指标上都表现出更好的准确性：MSE降低54.2%(31.91 vs 69.63)，MAE提升33.5%，其他指标也有性能优势。

Conclusion: 该SVR方法为电力系统规划和资源分配提供了额外的准确性检查工具，在与电力预测工具集成时显示出显著效益。

Abstract: Load forecasting has always been a challenge for grid operators due to the
growing complexity of power systems. The increase in extreme weather and the
need for energy from customers has led to load forecasting sometimes failing.
This research presents a Support Vector Regression (SVR) framework for electric
load forecasting that outperforms the industry standard. The SVR model
demonstrates better accuracy across all evaluation metrics that are important
for power system operations. The model has a 54.2\% reduction in Mean Squared
Error (31.91 vs. 69.63), a 33.5\% improvement in Mean Absolute Error, and
performance benefits across other metrics. These improvements show significant
benefits when integrated with power forecasting tools and show that the
approach provides an additional tool for accuracy checking for system planning
and resource allocation in times of need for resource allocation in electric
power systems.

</details>


### [19] [Cooperative Multi-Static ISAC Networks: A Unified Design Framework for Active and Passive Sensing](https://arxiv.org/abs/2510.06654)
*Yan Yang,Zhendong Li,Jianwei Zhao,Qingqing Wu,Zhiqing Wei,Wen Chen,Weimin Jia*

Main category: eess.SP

TL;DR: 提出了一种联合主动和被动感知(JAPS)的统一设计框架，用于多静态协作感知的集成感知与通信(ISAC)系统，通过交替优化算法解决非凸优化问题，提升通信速率同时保证感知需求。


<details>
  <summary>Details</summary>
Motivation: 多静态协作感知是推进集成感知与通信(ISAC)的有前景技术，能够提高感知精度和范围。本文旨在开发一个统一的JAPS设计框架来处理共存的下行和上行通信场景。

Method: 采用交替优化(AO)算法架构，分别处理波束成形子问题（使用SCA和基于惩罚的算法）以及接收滤波器和上行通信功率分配子问题（使用基于分数规划(FP)的算法）。

Result: 广泛的数值结果验证了所提出的JAPS方案在性能上的改进，并证明了所提出算法的有效性。

Conclusion: 提出的JAPS框架和算法能够有效解决多静态ISAC系统中的联合优化问题，在保证感知要求的同时显著提升通信性能。

Abstract: Multi-static cooperative sensing emerges as a promising technology for
advancing integrated sensing and communication (ISAC), enhancing sensing
accuracy and range. In this paper, we develop a unified design framework for
joint active and passive sensing (JAPS). In particular, we consider a JAPSbased
cooperative multi-static ISAC system for coexisting downlink (DL) and uplink
(UL) communications. An optimization problem is formulated for maximizing the
sum rate of both the DL and UL transmissions via jointly optimizing
beamforming, receive filters and power allocation, while guaranteeing the
sensing requirements and transmission power constraints. However, the
formulated problem is a non-convex optimization problem that is challenging to
solve directly due to the tight coupling among optimization variables. To
tackle this complicated issue, we employ an efficient algorithm architecture
leveraging alternating optimization (AO). Specifically, with the given receive
filters and transmission power for UL communication, the transmit beamforming
subproblem is addressed by successive convex approximation (SCA)-based and
penalty-based algorithms. A fractional programming (FP)-based algorithm is
developed to tackle the receive filters and transmission power for UL
communication optimization subproblem. Extensive numerical results validate the
performance improvement of our proposed JAPS scheme and demonstrate the
effectiveness of our proposed algorithms.

</details>


### [20] [Personalized Federated Learning-Driven Beamforming Optimization for Integrated Sensing and Communication Systems](https://arxiv.org/abs/2510.06709)
*Zhou Ni,Sravan Reddy Chintareddy,Peiyuan Guan,Morteza Hashemi*

Main category: eess.SP

TL;DR: 提出基于期望最大化算法的个性化联邦学习框架，用于集成感知与通信系统中的多目标优化，通过自适应确定基站聚合权重来提升性能


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习方法对所有客户端统一处理，无法适应集成感知与通信系统中通信与感知目标的竞争性需求，需要动态适应应用特定的权衡

Method: 使用期望最大化算法，在每个基站计算EM后验概率来量化全局模型与局部模型的相对适用性，基于模型在各自数据集上的损失

Result: 在目标同质和异质条件下进行仿真研究，结果显示该方法优于FedPer和pFedMe等现有PFL基线，实现更快收敛和更好的多目标性能

Conclusion: 提出的EM-based PFL框架能有效处理ISAC系统中的多目标优化问题，使基站能够动态适应竞争性通信与感知目标的应用特定权衡

Abstract: In this paper, we propose an Expectation-Maximization-based (EM) Personalized
Federated Learning (PFL) framework for multi-objective optimization (MOO) in
Integrated Sensing and Communication (ISAC) systems. In contrast to standard
federated learning (FL) methods that handle all clients uniformly, the proposed
approach enables each base station (BS) to adaptively determine its aggregation
weight with the EM algorithm. Specifically, an EM posterior is computed at each
BS to quantify the relative suitability between the global and each local
model, based on the losses of models on their respective datasets. The proposed
method is especially valuable in scenarios with competing communication and
sensing objectives, as it enables BSs to dynamically adapt to
application-specific trade-offs. To assess the effectiveness of the proposed
approach, we conduct simulation studies under both objective-wise homogeneous
and heterogeneous conditions. The results demonstrate that our approach
outperforms existing PFL baselines, such as FedPer and pFedMe, achieving faster
convergence and better multi-objective performance.

</details>


### [21] [Low Complexity Weight Flexible Decoding Schemes of Linear Block Code for 6G xURLLC](https://arxiv.org/abs/2510.06768)
*Di Zhang,Yinglei Yang,Zhilong Liu,Shaobo Jia,Kyungchun Lee,Zhirong Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于对偶码字特性的线性分组码解码方案，利用灵活权重的对偶码字提供错误位置和幅度的解码信息，提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 低复杂度纠错码是6G超可靠低延迟通信的关键使能技术，需要开发高效解码方案。

Method: 利用对偶码字的特性，提出了两种解码方案：一种直接使用固有信息进行迭代解码，另一种将先验信道信息与固有信息结合进行解码。两种方案都使用向量乘法和实数比较实现，便于硬件实现。

Result: 仿真结果验证了所提方案的有效性。

Conclusion: 基于对偶码字特性的解码方案能够提供更高的可靠性性能，且易于硬件实现，适用于6G通信系统。

Abstract: Low complexity error correction code is a key enabler for next generation
ultra-reliable low-latency communications (xURLLC) in six generation (6G).
Against this background, this paper proposes a decoding scheme for linear block
code by leveraging certain interesting properties of dual codewords. It is
found that dual codewords with flexible weights can provide useful decoding
information for the locations and magnitudes of error bits, which yielding
higher reliability performance. In addition, two decoding schemes are proposed,
in which one directly utilizes intrinsic information for iterative decoding,
and the other combines prior channel information with intrinsic information for
decoding. Both schemes are implemented using vector multiplication and
real-number comparisons, making them easy to implement in hardware. Simulation
results demonstrate the validness of our study.

</details>


### [22] [Mobility-Aware Localization in mmWave Channel: Adaptive Hybrid Filtering Approach](https://arxiv.org/abs/2510.06861)
*Abidemi Orimogunje,Kyeong-Ju Cha,Hyunwoo Park,Abdulahi A. Badrudeen,Sunwoo Kim,Dejan Vukobratovic*

Main category: eess.SP

TL;DR: 提出了一种混合移动感知自适应定位框架，根据用户速度在扩展卡尔曼滤波和无迹卡尔曼滤波之间自适应切换，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，精确的用户定位和追踪对于能效和超可靠低延迟应用至关重要。传统卡尔曼滤波定位技术存在计算复杂度高、数据关联问题，且随着用户移动速度增加，估计误差会增大。

Method: 利用毫米波信号进行联合感知和通信，无需额外传感器。提出混合移动感知自适应框架：在行人速度时使用扩展卡尔曼滤波，在车辆速度时使用无迹卡尔曼滤波。通过自适应噪声缩放、卡方门控、Rauch-Tung-Striebel平滑等技术缓解数据关联问题和估计误差。

Result: 使用绝对轨迹误差、相对位姿误差、归一化估计误差平方和均方根误差等指标评估，在各自场景下显示出30-60%的改进，明显优于现有的室内或静态场景定制方法。

Conclusion: 该混合自适应定位框架能够有效应对不同移动速度下的定位挑战，在保持高分辨率空间线索的同时显著提升了定位精度，为下一代无线网络的定位应用提供了有效解决方案。

Abstract: Precise user localization and tracking enhances energy-efficient and
ultra-reliable low latency applications in the next generation wireless
networks. In addition to computational complexity and data association
challenges with Kalman-filter localization techniques, estimation errors tend
to grow as the user's trajectory speed increases. By exploiting mmWave signals
for joint sensing and communication, our approach dispenses with additional
sensors adopted in most techniques while retaining high resolution spatial
cues. We present a hybrid mobility-aware adaptive framework that selects the
Extended Kalman filter at pedestrian speed and the Unscented Kalman filter at
vehicular speed. The scheme mitigates data-association problem and estimation
errors through adaptive noise scaling, chi-square gating, Rauch-Tung-Striebel
smoothing. Evaluations using Absolute Trajectory Error, Relative Pose Error,
Normalized Estimated Error Squared and Root Mean Square Error metrics
demonstrate roughly 30-60% improvement in their respective regimes indicating a
clear advantage over existing approaches tailored to either indoor or static
settings.

</details>


### [23] [Sensing Management for Pilot-Free Predictive Beamforming in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2510.06936)
*Eren Berk Kama,Murat Babek Salman,Isaac Skog,Emil Björnson*

Main category: eess.SP

TL;DR: 提出了一种用于无蜂窝大规模MIMO系统中集成感知与通信的感知管理方法，通过状态跟踪和预测波束成形减少信道估计开销


<details>
  <summary>Details</summary>
Motivation: 传统通信系统的信道估计过程在数据传输期间产生显著开销，消耗了本可用于数据的资源

Method: 采用基于状态的方法，利用感知能力在没有通信请求时跟踪用户位置；当收到通信请求时，基于跟踪的用户位置使用预测波束成形；结合扩展卡尔曼滤波跟踪算法和自适应感知管理

Result: 仿真结果表明，所提出的感知管理方法通过实现无开销的预测波束成形，提供了比现有方法更高的均匀下行链路通信速率

Conclusion: 该框架能够有效减少信道估计开销，提高系统性能，为集成感知与通信系统提供了一种高效的解决方案

Abstract: This paper introduces a sensing management method for integrated sensing and
communications (ISAC) in cell-free massive multiple-input multiple-output
(MIMO) systems. Conventional communication systems employ channel estimation
procedures that impose significant overhead during data transmission, consuming
resources that could otherwise be utilized for data. To address this challenge,
we propose a state-based approach that leverages sensing capabilities to track
the user when there is no communication request. Upon receiving a communication
request, predictive beamforming is employed based on the tracked user position,
thereby reducing the need for channel estimation. Our framework incorporates an
extended Kalman filter (EKF) based tracking algorithm with adaptive sensing
management to perform sensing operations only when necessary to maintain high
tracking accuracy. The simulation results demonstrate that our proposed sensing
management approach provides uniform downlink communication rates that are
higher than with existing methods by achieving overhead-free predictive
beamforming.

</details>


### [24] [Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments](https://arxiv.org/abs/2510.06884)
*Rahul Gulia,Amlan Ganguly,Michael E. Kuhl,Ehsan Rashedi,Clark Hochgraf*

Main category: eess.SP

TL;DR: Evo-WISVA是一种用于工业4.0环境下无线信道预测的深度学习架构，通过结合变分自编码器和卷积LSTM网络，实现了对SINR的高精度实时预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统物理或统计模型无法应对智能仓库中移动障碍物和瞬态干扰带来的时空复杂性，而URLLC需要准确实时的无线信道条件预测。

Method: 提出Evo-WISVA架构，集成带有注意力机制潜在记忆模块的VAE用于空间特征提取，以及ConvLSTM网络用于时间预测和序列优化，通过端到端联合损失函数优化。

Result: 在高保真工业仓库数据集上的实验表明，Evo-WISVA显著超越现有基线方法，平均重建误差降低47.6%，在复杂动态环境中表现出优异的泛化能力。

Conclusion: Evo-WISVA为主动无线资源管理建立了基础技术，推动了工业通信网络中预测性数字孪生的实现。

Abstract: Accurate and real-time prediction of wireless channel conditions,
particularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a
foundational requirement for enabling Ultra-Reliable Low-Latency Communication
(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based
or statistical models fail to cope with the spatio-temporal complexities
introduced by mobile obstacles and transient interference inherent to smart
warehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless
Infrastructure for Smart Warehouse using VAE), a novel synergistic deep
learning architecture that functions as a lightweight 2D predictive digital
twin of the radio environment. Evo-WISVA integrates a memory-augmented
Variational Autoencoder (VAE) featuring an Attention-driven Latent Memory
Module (LMM) for robust, context-aware spatial feature extraction, with a
Convolutional Long Short-Term Memory (ConvLSTM) network for precise temporal
forecasting and sequential refinement. The entire pipeline is optimized
end-to-end via a joint loss function, ensuring optimal feature alignment
between the generative and predictive components. Rigorous experimental
evaluation conducted on a high-fidelity ns-3-generated industrial warehouse
dataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art
baselines, achieving up to a 47.6\% reduction in average reconstruction error.
Crucially, the model exhibits exceptional generalization capacity to unseen
environments with vastly increased dynamic complexity (up to ten simultaneously
moving obstacles) while maintaining amortized computational efficiency
essential for real-time deployment. Evo-WISVA establishes a foundational
technology for proactive wireless resource management, enabling autonomous
optimization and advancing the realization of predictive digital twins in
industrial communication networks.

</details>


### [25] [Optimal Real-time Communication in 6G Ultra-Massive V2X Mobile Networks](https://arxiv.org/abs/2510.06937)
*He Huang,Zilong Liu,Zeping Sui,Wei Huang,Md. Noor-A-Rahim,Haishi Wang,Zhiheng Hu*

Main category: eess.SP

TL;DR: 提出一种面向6G超大规模V2X网络的协同车辆通信算法，通过低复杂度中继选择启发式算法提升信道容量


<details>
  <summary>Details</summary>
Motivation: 解决快速移动车辆间的实时信息交换挑战，利用空天地一体化通信系统

Method: 建立固定中继数量下的信道容量上界，提出低复杂度中继选择启发式算法

Result: 仿真结果表明所提算法相比现有协同车辆通信方法获得更优的信道容量

Conclusion: 该算法能有效提升6G V2X网络的通信性能

Abstract: This paper introduces a novel cooperative vehicular communication algorithm
tailored for future 6G ultra-massive vehicle-to-everything (V2X) networks
leveraging integrated space-air-ground communication systems. Specifically, we
address the challenge of real-time information exchange among rapidly moving
vehicles. We demonstrate the existence of an upper bound on channel capacity
given a fixed number of relays, and propose a low-complexity relay selection
heuristic algorithm. Simulation results verify that our proposed algorithm
achieves superior channel capacities compared to existing cooperative vehicular
communication approaches.

</details>


### [26] [Maritime Communication in Evaporation Duct Environment with Ship Trajectory Optimization](https://arxiv.org/abs/2510.06946)
*Ruifeng Gao,Hao Zhang,Jue Wang,Ye Li,Yingdong Hu,Qiuming Zhu,Shu Sun,Meixia Tao*

Main category: eess.SP

TL;DR: 提出了一种利用蒸发波导信道增益图先验信息的新型框架，通过优化船舶轨迹来最小化数据传输时间和航行时间，采用动态种群PSO集成的NSGA-II算法求解多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 在海上无线网络中，蒸发波导效应被认为是远距离传输的有利条件，但如何有效利用该效应进行高效通信设计仍有待研究。

Method: 考虑船对岸数据传输场景，提出利用蒸发波导信道增益图先验信息的框架，通过动态种群PSO集成的NSGA-II算法优化船舶轨迹。

Result: 仿真表明，与忽略蒸发波导有用信息的基准方案相比，所提方案能有效减少数据传输时间和航行时间。

Conclusion: 利用蒸发波导信道信息可以显著提升海上通信效率，通过轨迹优化实现时间和能耗的双重优化。

Abstract: In maritime wireless networks, the evaporation duct effect has been known as
a preferable condition for long-range transmissions. However, how to
effectively utilize the duct effect for efficient communication design is still
open for investigation. In this paper, we consider a typical scenario of
ship-to-shore data transmission, where a ship collects data from multiple
oceanographic buoys, sails from one to another, and transmits the collected
data back to a terrestrial base station during its voyage. A novel framework,
which exploits priori information of the channel gain map in the presence of
evaporation duct, is proposed to minimize the data transmission time and the
sailing time by optimizing the ship's trajectory. To this end, a
multi-objective optimization problem is formulated and is further solved by a
dynamic population PSO-integrated NSGA-II algorithm. Through simulations, it is
demonstrated that, compared to the benchmark scheme which ignores useful
information of the evaporation duct, the proposed scheme can effectively reduce
both the data transmission time and the sailing time.

</details>


### [27] [Towards Reliable Emergency Wireless Communications over SAGINs: A Composite Fading and QoS-Centric Perspective](https://arxiv.org/abs/2510.07120)
*Yinong Chen,Wenchi Cheng,Jingqing Wang,Xiao Zheng,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 本文针对卫星-空中-地面综合网络(SAGIN)在紧急无线通信场景中的性能建模问题，提出了基于Fisher-Snedecor F复合衰落模型的性能分析框架，准确描述了恶劣地面环境下的多径衰落和阴影效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽略了复杂地形变化导致的信道特性，或在缺乏QoS约束下分析性能，导致理论分析与实际性能不匹配。为解决这一问题，需要建立更准确的性能建模框架。

Method: 采用Fisher-Snedecor F复合衰落模型来表征空地链路，开发了端到端信噪比统计量的精确分布，分析了固定增益放大转发和解码转发中继协议下的级联信道，并提供了渐近表达式。

Result: 推导了具有QoS保障的有效容量、中断概率和ε中断容量的闭式解和渐近表达式，并通过现场测量和蒙特卡洛仿真验证了有效性。

Conclusion: 所提出的性能建模框架能够准确描述SAGIN在复杂信道条件下的性能，理论预测与高信噪比区域的实际表现高度一致，为紧急无线通信提供了可靠的理论基础。

Abstract: In emergency wireless communications (EWC) scenarios, ensuring reliable,
flexible, and high-rate transmission while simultaneously maintaining seamless
coverage and rapid response capabilities presents a critical technical
challenge. To this end, satellite-aerial-ground integrated network (SAGIN) has
emerged as a promising solution due to its comprehensive three-dimensional
coverage and capability to meet stringent, multi-faceted quality-of-service
(QoS) requirements. Nevertheless, most existing studies either neglected the
inherent characteristics of the complex channel conditions due to the terrain
changes or analyzed the performance in the absence of QoS constraints,
resulting in a mismatch between theoretical analysis and practical performance.
To remedy such deficiencies, in this paper we establish a performance modeling
framework for SAGIN employing the Fisher-Snedecor $\mathcal{F}$ composite
fading model to characterize the air-ground link. In specific, the proposed
$\mathcal{F}$ composite fading channel is adopted to accurately describe both
multipath fading and shadowing in harsh ground environments. The exact
distribution of end-to-end signal-to-noise (SNR) statistics for space-air and
air-ground links is developed, enabling theoretical analysis of cascaded
channels with fixed-gain amplify-and-forward (AF) and decode-and-forward (DF)
relaying protocols, respectively. Furthermore, asymptotic expressions of the
derived results are provided to offer concise representations and demonstrate
close alignment with theoretical predictions in the high-SNR regime. Finally,
the insightful closed-form and asymptotic expressions of effective capacity
with QoS provisioning, outage probability, and $\epsilon$-outage capacity are
investigated, respectively, followed by both field measurements and Monte Carlo
simulations to verify the effectiveness.

</details>


### [28] [Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks](https://arxiv.org/abs/2510.07199)
*Shirin Shoushtari,Edward P. Chandler,Ulugbek S. Kamilov*

Main category: eess.SP

TL;DR: 提出了一种基于对数网络的新策略用于泊松去噪，通过预测对数期望值而非后验均值，能够恢复高阶后验矩并支持后验分布近似。


<details>
  <summary>Details</summary>
Motivation: 传统使用MSE损失训练的深度学习方法只能获得后验均值，无法捕捉泊松去噪中的后验不确定性，而Tweedie公式在泊松噪声下不再适用。

Method: 训练对数网络来学习条件对数期望值E[log x|y]，利用对数作为泊松分布的便捷参数化方式，从而恢复高阶后验矩。

Result: 在模拟数据上的实验表明，该方法在去噪性能上与标准MMSE模型相当，同时能够提供后验分布信息。

Conclusion: 对数网络方法为泊松去噪提供了一种既能保持去噪性能又能获得后验不确定性的有效解决方案。

Abstract: Poisson denoising plays a central role in photon-limited imaging applications
such as microscopy, astronomy, and medical imaging. It is common to train deep
learning models for denoising using the mean-squared error (MSE) loss, which
corresponds to computing the posterior mean $\mathbb{E}[x \mid y]$. When the
noise is Gaussian, Tweedie's formula enables approximation of the posterior
distribution through its higher-order moments. However, this connection no
longer holds for Poisson denoising: while $ \mathbb{E}[x \mid y] $ still
minimizes MSE, it fails to capture posterior uncertainty. We propose a new
strategy for Poisson denoising based on training a log-network. Instead of
predicting the posterior mean $ \mathbb{E}[x \mid y] $, the log-network is
trained to learn $\mathbb{E}[\log x \mid y]$, leveraging the logarithm as a
convenient parameterization for the Poisson distribution. We provide a
theoretical proof that the proposed log-network enables recovery of
higher-order posterior moments and thus supports posterior approximation.
Experiments on simulated data show that our method matches the denoising
performance of standard MMSE models while providing access to the posterior.

</details>
