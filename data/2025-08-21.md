<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 12]
- [cs.IT](#cs.IT) [Total: 4]
- [eess.SP](#eess.SP) [Total: 8]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Hallucinations in medical devices](https://arxiv.org/abs/2508.14118)
*Jason Granstedt,Prabhat Kc,Rucha Deshpande,Victor Garcia,Aldo Badano*

Main category: eess.IV

TL;DR: 本文提出了医疗设备中幻觉错误的实用定义，将其定义为看似合理但可能对任务产生影响的错误类型，旨在促进跨产品领域的医疗设备评估。


<details>
  <summary>Details</summary>
Motivation: 医疗设备中的计算机方法经常不完美并产生错误，当深度学习等方法产生错误输出时，常被称为"幻觉"。需要统一的定义来促进评估。

Method: 基于理论发展和多个医疗设备领域的实证研究，提出了一个实用且通用的幻觉定义，并通过成像和非成像应用示例进行验证。

Result: 建立了幻觉作为看似合理错误的明确定义，讨论了现有评估方法学，并探索了减少幻觉发生率的现有方法。

Conclusion: 提出的定义有助于标准化医疗设备中幻觉错误的评估，为不同产品领域的设备性能比较提供了基础框架。

Abstract: Computer methods in medical devices are frequently imperfect and are known to
produce errors in clinical or diagnostic tasks. However, when deep learning and
data-based approaches yield output that exhibit errors, the devices are
frequently said to hallucinate. Drawing from theoretical developments and
empirical studies in multiple medical device areas, we introduce a practical
and universal definition that denotes hallucinations as a type of error that is
plausible and can be either impactful or benign to the task at hand. The
definition aims at facilitating the evaluation of medical devices that suffer
from hallucinations across product areas. Using examples from imaging and
non-imaging applications, we explore how the proposed definition relates to
evaluation methodologies and discuss existing approaches for minimizing the
prevalence of hallucinations.

</details>


### [2] [3D Cardiac Anatomy Generation Using Mesh Latent Diffusion Models](https://arxiv.org/abs/2508.14122)
*Jolanta Mozyrska,Marcel Beetz,Luke Melas-Kyriazi,Vicente Grau,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: eess.IV

TL;DR: 提出MeshLDM模型，首次将潜在扩散模型应用于生成3D心脏解剖网格，在急性心肌梗死患者数据上验证了生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在3D医学影像特别是心脏病学中的应用仍较少，生成多样化真实心脏解剖结构对于计算机模拟试验、电机械模拟和机器学习数据增强至关重要

Method: 提出新颖的MeshLDM架构，基于潜在扩散模型生成3D心脏网格，在急性心肌梗死患者的左心室3D网格数据集上进行训练和评估

Result: 模型成功捕捉舒张期和收缩期心脏形状特征，生成网格与金标准相比群体均值差异仅为2.4%，在临床和3D重建指标上表现优异

Conclusion: MeshLDM是首个成功应用于3D心脏解剖网格生成的扩散模型，为心脏病学领域的计算机模拟和数据增强提供了有效工具

Abstract: Diffusion models have recently gained immense interest for their generative
capabilities, specifically the high quality and diversity of the synthesized
data. However, examples of their applications in 3D medical imaging are still
scarce, especially in cardiology. Generating diverse realistic cardiac
anatomies is crucial for applications such as in silico trials,
electromechanical computer simulations, or data augmentations for machine
learning models. In this work, we investigate the application of Latent
Diffusion Models (LDMs) for generating 3D meshes of human cardiac anatomies. To
this end, we propose a novel LDM architecture -- MeshLDM. We apply the proposed
model on a dataset of 3D meshes of left ventricular cardiac anatomies from
patients with acute myocardial infarction and evaluate its performance in terms
of both qualitative and quantitative clinical and 3D mesh reconstruction
metrics. The proposed MeshLDM successfully captures characteristics of the
cardiac shapes at end-diastolic (relaxation) and end-systolic (contraction)
cardiac phases, generating meshes with a 2.4% difference in population mean
compared to the gold standard.

</details>


### [3] [Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants](https://arxiv.org/abs/2508.14129)
*Aditya Bagri,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Kalyan Sivasailam,Bargava Subramanian,VarshiniPriya,Meenakumari K S,Abi M,Renita S*

Main category: eess.IV

TL;DR: \u8fd9\u7bc7\u8bba\u6587\u5e94\u7528Co-DETR\u53d8\u6362\u5668\u6a21\u578b\u8fdb\u884c\u624b\u8150\u548c\u624b\u90e8\u9aa8\u6298X\u5149\u7247\u68c0\u6d4b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u5ea6\u7684\u9aa8\u6298\u5b9a\u4f4d\u548c\u5206\u7c7b\uff0c\u9002\u5408\u4e34\u5e8a\u90e8\u7f72\u3002


<details>
  <summary>Details</summary>
Motivation: \u624b\u8150\u548c\u624b\u90e8\u9aa8\u6298\u7684\u51c6\u786e\u8bca\u65ad\u5bf9\u6025\u8bca\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u89e3\u91ca\u6162\u901f\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u53d8\u6362\u5668\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5728\u673a\u6784\u9aa8\u6298\u68c0\u6d4b\u4e2d\u5e94\u7528\u6709\u9650\u3002

Method: \u4f7f\u7528\u8fc7\u26,000\u5f20\u5e26\u6807\u6ce8\u7684X\u5149\u7247\u5bf9RT-DETR\u548cCo-DETR\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u5408ResNet-50\u5206\u7c7b\u5668\u8fdb\u884c\u533b\u7597\u533a\u57df\u5206\u6790\uff0c\u91c7\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u5d4c\u5165\u8d28\u91cf\u3002

Result: Co-DETR\u8868\u73b0\u4f18\u5f02(AP@50=0.615)\uff0c\u96c6\u6210\u7ba1\u7ebf\u5728\u5b9e\u9645X\u5149\u7247\u4e0a\u8fbe\u523083.1%\u51c6\u786e\u5ea6\u300185.1%\u7cbe\u786e\u5ea6\u548c96.4%\u56de\u753b\u7387\uff0c\u80fd\u591f\u51c6\u786e\u5b9a\u4f4d13\u79cd\u9aa8\u6298\u7c7b\u578b\u3002

Conclusion: Co-DETR\u57fa\u4e8e\u7684\u65b9\u6848\u5728\u624b\u8150\u624b\u90e8\u9aa8\u6298\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u5ea6\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\uff0c\u9002\u5408\u5728\u533b\u9662\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b9e\u65f6\u90e8\u7f72\uff0c\u63d0\u5347\u9aa8\u9abc\u79d1\u5f71\u50cf\u8bca\u65ad\u7684\u901f\u5ea6\u548c\u53ef\u9760\u6027\u3002

Abstract: Background: Accurate diagnosis of wrist and hand fractures using radiographs
is essential in emergency care, but manual interpretation is slow and prone to
errors. Transformer-based models show promise in improving medical image
analysis, but their application to extremity fractures is limited. This study
addresses this gap by applying object detection transformers to wrist and hand
X-rays.
  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO,
using over 26,000 annotated X-rays from a proprietary clinical dataset. Each
image was labeled for fracture presence with bounding boxes. A ResNet-50
classifier was trained on cropped regions to refine abnormality classification.
Supervised contrastive learning was used to enhance embedding quality.
Performance was evaluated using AP@50, precision, and recall metrics, with
additional testing on real-world X-rays.
  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR
outperformed it with an AP@50 of 0.615 and faster convergence. The integrated
pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on
real-world X-rays, demonstrating strong generalization across 13 fracture
types. Visual inspection confirmed accurate localization.
  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and
clinical relevance in wrist and hand fracture detection, offering reliable
localization and differentiation of fracture types. It is scalable, efficient,
and suitable for real-time deployment in hospital workflows, improving
diagnostic speed and reliability in musculoskeletal radiology.

</details>


### [4] [Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI](https://arxiv.org/abs/2508.14133)
*Karin A. Olthof,Matteo Fusagli,Bianca Güttner,Tiziano Natali,Bram Westerink,Stefanie Speidel,Theo J. M. Ruers,Koert F. D. Kuhlmann,Andrey Zhylka*

Main category: eess.IV

TL;DR: 开发基于nnU-Net的深度学习自动分割方法，用于钆塞酸增强MRI肝胆期的肝脏解剖结构分割，包括肝实质、肿瘤、门静脉、肝静脉和胆道树，以简化术前规划临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 简化肝脏手术术前规划的临床工作流程，通过自动化分割方法提高效率，使3D规划能够作为标准护理应用于每位肝脏手术患者。

Method: 使用90例患者的肝胆期MRI扫描进行手动分割，其中72例用于训练nnU-Net v1网络，特别关注薄结构和地形保持。在18例测试集上评估性能，使用Dice相似系数比较自动和手动分割。

Result: 测试集DSC结果：肝实质0.97±0.01，肝静脉0.80±0.04，胆道树0.79±0.07，肿瘤0.77±0.17，门静脉0.74±0.06。肿瘤检测率76.6±24.1%，每例患者中位假阳性1个。临床评估显示3D模型仅需微小调整即可使用。

Conclusion: 基于nnU-Net的分割方法能够准确自动描绘肝脏解剖结构，使3D规划能够高效地作为肝脏手术患者的标准护理应用，在临床使用中还发现了放射科医生最初遗漏的3个额外肿瘤。

Abstract: Background: The aim of this study was to develop and evaluate a deep
learning-based automated segmentation method for hepatic anatomy (i.e.,
parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the
hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the
clinical workflow of preoperative planning.
  Methods: Manual segmentation was performed on hepatobiliary phase MRI scans
from 90 consecutive patients who underwent liver surgery between January 2020
and October 2023. A deep learning network (nnU-Net v1) was trained on 72
patients with an extra focus on thin structures and topography preservation.
Performance was evaluated on an 18-patient test set by comparing automated and
manual segmentations using Dice similarity coefficient (DSC). Following
clinical integration, 10 segmentations (assessment dataset) were generated
using the network and manually refined for clinical use to quantify required
adjustments using DSC.
  Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma,
0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for
tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was
76.6+/-24.1%, with a median of one false-positive per patient. The assessment
dataset showed minor adjustments were required for clinical use of the 3D
models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01)
and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater
variability (DSC 0.80+/-0.27). During prospective clinical use, the model
detected three additional tumors initially missed by radiologists.
  Conclusions: The proposed nnU-Net-based segmentation method enables accurate
and automated delineation of hepatic anatomy. This enables 3D planning to be
applied efficiently as a standard-of-care for every patient undergoing liver
surgery.

</details>


### [5] [A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans](https://arxiv.org/abs/2508.14151)
*Justin Yiu,Kushank Arora,Daniel Steinberg,Rohit Ghiya*

Main category: eess.IV

TL;DR: 这篇论文系统性评估了多种深度学习模型在膝盖MRI自动区域检测中的性能，发现ResNet50在分类和ROI识别中表现最优，超越了变换器模型，并通过Grad-CAM提供了最临床意义的解释。


<details>
  <summary>Details</summary>
Motivation: MRI手工解释耗时且存在观察者间差异，需要自动化的区域检测方法来提高诊断效率和准确性。

Method: 研究测试了监督和自监督学习方法，包括ResNet50、InceptionV3、Vision Transformers和多个U-Net变体，并集成Grad-CAM和Saliency Maps等xAI技术增强可解释性。

Result: ResNet50在分类和ROI识别中表现最优（AUC评价），超越了变换器模型。U-Net + MLP混合方法在重建质量和可解释性方面显示潜力，但分类性能较低。Grad-CAM提供了最临床意义的解释。

Conclusion: CNN基于过的转移学习是在MRNet数据集上最有效的方法，未来需要更大规模的预训练才能充分发挥变换器模型的潜力。

Abstract: Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for
assessing knee injuries. However, manual interpretation of MRI slices remains
time-consuming and prone to inter-observer variability. This study presents a
systematic evaluation of various deep learning architectures combined with
explainable AI (xAI) techniques for automated region of interest (ROI)
detection in knee MRI scans. We investigate both supervised and self-supervised
approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and
multiple U-Net variants augmented with multi-layer perceptron (MLP)
classifiers. To enhance interpretability and clinical relevance, we integrate
xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed
using AUC for classification and PSNR/SSIM for reconstruction quality, along
with qualitative ROI visualizations. Our results demonstrate that ResNet50
consistently excels in classification and ROI identification, outperforming
transformer-based models under the constraints of the MRNet dataset. While
hybrid U-Net + MLP approaches show potential for leveraging spatial features in
reconstruction and interpretability, their classification performance remains
lower. Grad-CAM consistently provided the most clinically meaningful
explanations across architectures. Overall, CNN-based transfer learning emerges
as the most effective approach for this dataset, while future work with
larger-scale pretraining may better unlock the potential of transformer models.

</details>


### [6] [Fine-grained Image Quality Assessment for Perceptual Image Restoration](https://arxiv.org/abs/2508.14475)
*Xiangfei Sheng,Xiaofeng Pan,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: 该论文提出了首个针对图像恢复任务的细粒度图像质量评估数据集FGRestore，并基于此开发了新的IQA模型FGResQ，该模型在粗粒度评分回归和细粒度质量排序方面表现出色，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估(IQA)指标在图像恢复(IR)任务中存在固有弱点，特别是在区分恢复图像之间的细粒度质量差异方面表现不佳，这阻碍了性能比较和算法优化。

Method: 1) 构建包含18,408张恢复图像的FGRestore数据集，涵盖6种常见IR任务；2) 除了传统标量质量分数外，还标注了30,886个细粒度成对偏好；3) 提出FGResQ模型，同时进行粗粒度评分回归和细粒度质量排序。

Result: 实验表明，FGResQ在图像恢复质量评估方面显著优于现有的最先进IQA指标，能够更准确地反映细粒度恢复质量差异。

Conclusion: 该研究填补了图像恢复领域细粒度质量评估的空白，提出的FGResQ模型为解决IR任务中的IQA挑战提供了有效解决方案，代码和模型权重已开源。

Abstract: Recent years have witnessed remarkable achievements in perceptual image
restoration (IR), creating an urgent demand for accurate image quality
assessment (IQA), which is essential for both performance comparison and
algorithm optimization. Unfortunately, the existing IQA metrics exhibit
inherent weakness for IR task, particularly when distinguishing fine-grained
quality differences among restored images. To address this dilemma, we
contribute the first-of-its-kind fine-grained image quality assessment dataset
for image restoration, termed FGRestore, comprising 18,408 restored images
across six common IR tasks. Beyond conventional scalar quality scores,
FGRestore was also annotated with 30,886 fine-grained pairwise preferences.
Based on FGRestore, a comprehensive benchmark was conducted on the existing IQA
metrics, which reveal significant inconsistencies between score-based IQA
evaluations and the fine-grained restoration quality. Motivated by these
findings, we further propose FGResQ, a new IQA model specifically designed for
image restoration, which features both coarse-grained score regression and
fine-grained quality ranking. Extensive experiments and comparisons demonstrate
that FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and
model weights have been released in https://pxf0429.github.io/FGResQ/

</details>


### [7] [Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis](https://arxiv.org/abs/2508.14509)
*Xin Wang,Xiaopei Zhang,Xingang Wang*

Main category: eess.IV

TL;DR: 提出基于改进TransUNet的高精度皮肤病变语义分割方法，通过集成Transformer模块和卷积分支，结合边界引导注意力机制和多尺度上采样，有效解决病变结构复杂、边界模糊和尺度变化大的问题。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变图像存在复杂病变结构、模糊边界和显著尺度变化等挑战，传统方法难以准确分割，需要开发能够同时建模全局语义信息和保留局部纹理特征的新方法。

Method: 在编码器-解码器框架中集成Transformer模块建模全局语义信息，保留卷积分支保持局部纹理和边缘特征；设计边界引导注意力机制和多尺度上采样路径，提升边界定位和分割一致性。

Result: 实验表明该方法在mIoU、mDice和mAcc指标上优于现有代表性方法，具有更强的病变识别准确性和鲁棒性，在复杂场景下实现更好的边界重建和结构恢复。

Conclusion: 该方法适用于皮肤病变分析中的自动化分割任务关键需求，能够有效处理复杂病变结构的分割问题，为医学图像分析提供了有效的解决方案。

Abstract: This paper proposes a high-precision semantic segmentation method based on an
improved TransUNet architecture to address the challenges of complex lesion
structures, blurred boundaries, and significant scale variations in skin lesion
images. The method integrates a transformer module into the traditional
encoder-decoder framework to model global semantic information, while retaining
a convolutional branch to preserve local texture and edge features. This
enhances the model's ability to perceive fine-grained structures. A
boundary-guided attention mechanism and multi-scale upsampling path are also
designed to improve lesion boundary localization and segmentation consistency.
To verify the effectiveness of the approach, a series of experiments were
conducted, including comparative studies, hyperparameter sensitivity analysis,
data augmentation effects, input resolution variation, and training data split
ratio tests. Experimental results show that the proposed model outperforms
existing representative methods in mIoU, mDice, and mAcc, demonstrating
stronger lesion recognition accuracy and robustness. In particular, the model
achieves better boundary reconstruction and structural recovery in complex
scenarios, making it well-suited for the key demands of automated segmentation
tasks in skin lesion analysis.

</details>


### [8] [From Slices to Structures: Unsupervised 3D Reconstruction of Female Pelvic Anatomy from Freehand Transvaginal Ultrasound](https://arxiv.org/abs/2508.14552)
*Max Krähenmann,Sergio Tascon-Morales,Fabian Laumer,Julia E. Vogt,Ece Ozkan*

Main category: eess.IV

TL;DR: 基于高斯拖洗的无监督方法，从自由手2D冗道超声扫描重建3D解剖结构，无需外部跟踪或位置估计


<details>
  <summary>Details</summary>
Motivation: 解决体积超声依赖专门硬件和严格采集协议的限制，推广体积超声在诊断中的应用

Method: 将高斯拖洗原理调适到超声领域，使用切片感知的可微光栅化器，建立各向异性3D高斯模型，通过图像级监督优化参数

Result: 实现了紧凑、灵活、内存高效的体积表示，高保真度捐捕解剖细节

Conclusion: 证明仅通过计算方法即可从2D超声图像完成准3D重建，为AI辅助诊断开启新可能

Abstract: Volumetric ultrasound has the potential to significantly improve diagnostic
accuracy and clinical decision-making, yet its widespread adoption remains
limited by dependence on specialized hardware and restrictive acquisition
protocols. In this work, we present a novel unsupervised framework for
reconstructing 3D anatomical structures from freehand 2D transvaginal
ultrasound (TVS) sweeps, without requiring external tracking or learned pose
estimators. Our method adapts the principles of Gaussian Splatting to the
domain of ultrasound, introducing a slice-aware, differentiable rasterizer
tailored to the unique physics and geometry of ultrasound imaging. We model
anatomy as a collection of anisotropic 3D Gaussians and optimize their
parameters directly from image-level supervision, leveraging sensorless probe
motion estimation and domain-specific geometric priors. The result is a
compact, flexible, and memory-efficient volumetric representation that captures
anatomical detail with high spatial fidelity. This work demonstrates that
accurate 3D reconstruction from 2D ultrasound images can be achieved through
purely computational means, offering a scalable alternative to conventional 3D
systems and enabling new opportunities for AI-assisted analysis and diagnosis.

</details>


### [9] [Broadband Near-Infrared Compressive Spectral Imaging System with Reflective Structure](https://arxiv.org/abs/2508.14573)
*Yutong Li,Zhenming Yu,Liming Cheng,Jiayu Di,Liang Lin,Jingyue Ma,Tongshuo Zhang,Yue Zhou,Haiying Zhao,Kun Xu*

Main category: eess.IV

TL;DR: 提出了一种宽带近红外压缩光谱成像系统，能够捕获700-1600nm的宽光谱数据，解决了传统系统成本高、体积大和数据采集效率低的问题


<details>
  <summary>Details</summary>
Motivation: 传统近红外高光谱成像系统面临成本高、仪器笨重和数据采集效率低的挑战，需要开发更紧凑高效的解决方案

Method: 通过波长分割和设计专用光学元件来克服硬件光谱限制，采用反射式光学结构使系统更加紧凑

Result: 成功实现了覆盖700-1600nm宽光谱带宽的高光谱数据捕获能力

Conclusion: 该方法为近红外高光谱成像提供了一种新颖的技术解决方案，具有重要的应用价值

Abstract: Near-infrared (NIR) hyperspectral imaging has become a critical tool in
modern analytical science. However, conventional NIR hyperspectral imaging
systems face challenges including high cost, bulky instrumentation, and
inefficient data collection. In this work, we demonstrate a broadband NIR
compressive spectral imaging system that is capable of capturing hyperspectral
data covering a broad spectral bandwidth ranging from 700 to 1600 nm. By
segmenting wavelengths and designing specialized optical components, our design
overcomes hardware spectral limitations to capture broadband data, while the
reflective optical structure makes the system compact. This approach provides a
novel technical solution for NIR hyperspectral imaging.

</details>


### [10] [Integrated Snapshot Near-infrared Hypersepctral Imaging Framework with Diffractive Optics](https://arxiv.org/abs/2508.14585)
*Jingyue Ma,Zhenming Yu,Zhengyang Li,Liang Lin,Liming Cheng,Kun Xu*

Main category: eess.IV

TL;DR: 提出一种集成快照近红外高光谱成像框架，结合设计的行流光学元件和NIRSA-Net网络，在700-1000nm波长范围内实现10nm分辨率的高性能谱像成像


<details>
  <summary>Details</summary>
Motivation: 解决近红外高光谱成像中的技术挑战，提高成像质量和分辨率

Method: 采用设计的行流光学元件(DOE)与NIRSA-Net深度学习网络相结合的集成快照方案

Result: 在700-1000nm波长范围实现了10nm分辨率的谱像成像，PSNR提高1.47dB，SSIM提高0.006

Conclusion: 该集成框架能够在近红外区域实现高分辨率高性能的快照高光谱成像

Abstract: We propose an integrated snapshot near-infrared hyperspectral imaging
framework that combines designed DOE with NIRSA-Net. The results demonstrate
near-infrared spectral imaging at 700-1000nm with 10nm resolution while
achieving improvement of PSNR 1.47dB and SSIM 0.006.

</details>


### [11] [Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model](https://arxiv.org/abs/2508.14681)
*Hyun-Jic Oh,Junsik Kim,Zhiyi Shi,Yichen Wu,Yu-An Chen,Peter K. Sorger,Hanspeter Pfister,Won-Ki Jeong*

Main category: eess.IV

TL;DR: 本文提出了一种基于潜在扩散模型的虚拟多重染色框架，能够从H&E图像生成多达18种不同标记的多重成像，显著提升了生成准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 多重成像技术虽然能提供分子水平的组织样本信息，但由于数据获取复杂且成本高昂，限制了其广泛应用。同时，现有的大量H&E图像缺乏对应的多重图像，制约了多模态分析的可能性。

Method: 利用预训练的潜在扩散模型参数，构建条件扩散模型从H&E图像生成多重图像。采用逐标记生成策略，通过单步采样微调模型，使用像素级损失函数提升颜色对比度保真度和推理效率。

Result: 在两个公开数据集上验证了框架有效性，能够生成多达18种不同标记类型，准确率显著提升，远超之前方法只能生成2-3种标记的能力。

Conclusion: 该框架成功弥合了H&E成像与多重成像之间的鸿沟，为回顾性研究和现有H&E图像库的大规模分析提供了可能，开创了虚拟多重染色的新途径。

Abstract: Multiplex imaging is revolutionizing pathology by enabling the simultaneous
visualization of multiple biomarkers within tissue samples, providing
molecular-level insights that traditional hematoxylin and eosin (H&E) staining
cannot provide. However, the complexity and cost of multiplex data acquisition
have hindered its widespread adoption. Additionally, most existing large
repositories of H&E images lack corresponding multiplex images, limiting
opportunities for multimodal analysis. To address these challenges, we leverage
recent advances in latent diffusion models (LDMs), which excel at modeling
complex data distributions utilizing their powerful priors for fine-tuning to a
target domain. In this paper, we introduce a novel framework for virtual
multiplex staining that utilizes pretrained LDM parameters to generate
multiplex images from H&E images using a conditional diffusion model. Our
approach enables marker-by-marker generation by conditioning the diffusion
model on each marker, while sharing the same architecture across all markers.
To tackle the challenge of varying pixel value distributions across different
marker stains and to improve inference speed, we fine-tune the model for
single-step sampling, enhancing both color contrast fidelity and inference
efficiency through pixel-level loss functions. We validate our framework on two
publicly available datasets, notably demonstrating its effectiveness in
generating up to 18 different marker types with improved accuracy, a
substantial increase over the 2-3 marker types achieved in previous approaches.
This validation highlights the potential of our framework, pioneering virtual
multiplex staining. Finally, this paper bridges the gap between H&E and
multiplex imaging, potentially enabling retrospective studies and large-scale
analyses of existing H&E image repositories.

</details>


### [12] [Rule-based Key-Point Extraction for MR-Guided Biomechanical Digital Twins of the Spine](https://arxiv.org/abs/2508.14708)
*Robert Graf,Tanja Lerchl,Kati Nispel,Hendrik Möller,Matan Atad,Julian McGinnis,Julius Maria Watrinet,Johannes Paetzold,Daniel Rueckert,Jan S. Kirschke*

Main category: eess.IV

TL;DR: 提出了一种基于规则的MRI亚像素精度关键点提取方法，用于构建个性化的脊柱生物力学数字孪生模型，支持无辐射的临床诊断和治疗规划。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要准确的个性化解剖建模，但现有方法往往依赖CT成像。本研究旨在开发基于MRI的无辐射方法，为大规模研究和特殊人群提供更好的解决方案。

Method: 采用基于规则的方法，从先前的CT方法改进而来，包含鲁棒的图像配准和椎骨特异性方向估计，生成解剖学上有意义的关键点作为生物力学模型的边界条件和力作用点。

Result: 成功实现了从MRI中提取亚像素精度的关键点，能够构建考虑个体解剖结构的脊柱力学仿真模型，支持个性化临床方法开发。

Conclusion: 该方法填补了精确医学图像分析与生物力学仿真之间的空白，为医疗保健个性化建模提供了无辐射的解决方案，适合大规模应用和特殊人群使用。

Abstract: Digital twins offer a powerful framework for subject-specific simulation and
clinical decision support, yet their development often hinges on accurate,
individualized anatomical modeling. In this work, we present a rule-based
approach for subpixel-accurate key-point extraction from MRI, adapted from
prior CT-based methods. Our approach incorporates robust image alignment and
vertebra-specific orientation estimation to generate anatomically meaningful
landmarks that serve as boundary conditions and force application points, like
muscle and ligament insertions in biomechanical models. These models enable the
simulation of spinal mechanics considering the subject's individual anatomy,
and thus support the development of tailored approaches in clinical diagnostics
and treatment planning. By leveraging MR imaging, our method is radiation-free
and well-suited for large-scale studies and use in underrepresented
populations. This work contributes to the digital twin ecosystem by bridging
the gap between precise medical image analysis with biomechanical simulation,
and aligns with key themes in personalized modeling for healthcare.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [13] [Multi-Source Peak Age of Information Optimization in Mobile Edge Computing Systems](https://arxiv.org/abs/2508.14328)
*Jianhang Zhu,Jie Gong*

Main category: cs.IT

TL;DR: 本文研究了多源单服务器系统中计算密集型状态数据的AoI优化问题，提出了随机调度器的最优性和基于交替优化的联合调度与采样算法


<details>
  <summary>Details</summary>
Motivation: 在实时监控系统中，计算密集型状态数据需要传输和处理后才能揭示信息，传统AoI度量无法充分反映这种场景的信息新鲜度，需要联合优化源调度和状态采样

Method: 考虑具有随机传输时间和处理时间的生成即发源，分析非抢占式和抢占式服务器设置下的最优调度器和采样器结构，提出交替优化算法联合优化调度频率和采样阈值/函数

Result: 证明随机调度器在两种服务器设置下都是最优的，最优采样器结构保持与单源系统一致，提出的交替优化算法在广泛设置下能达到最优性能

Conclusion: 多源系统中的AoI优化可以通过随机调度和结构一致的采样策略实现，交替优化算法能有效解决非凸的联合优化问题

Abstract: Age of Information (AoI) is emerging as a novel metric for measuring
information freshness in real-time monitoring systems. For
computation-intensive status data, the information is not revealed until being
processed. We consider a status update problem in a multi-source single-server
system where the sources are scheduled to generate and transmit status data
which are received and processed at the edge server. Generate-at-will sources
with both random transmission time and process time are considered, introducing
the joint optimization of source scheduling and status sampling on the basis of
transmission-computation balancing. We show that a random scheduler is optimal
for both non-preemptive and preemptive server settings, and the optimal sampler
depends on the scheduling result and its structure remains consistent with the
single-source system, i.e., threshold-based sampler for non-preemptive case and
transmission-aware deterministic sampler for preemptive case. Then, the problem
can be transformed to jointly optimizing the scheduling frequencies and the
sampling thresholds/functions, which is non-convex. We proposed an alternation
optimization algorithm to solve it. Numerical experiments show that the
proposed algorithm can achieve the optimal in a wide range of settings.

</details>


### [14] [Reconstruction Codes for Deletions and Insertions: Connection, Distinction, and Construction](https://arxiv.org/abs/2508.14386)
*Yubo Sun,Gennian Ge*

Main category: cs.IT

TL;DR: 本文研究了删除和插入错误下的重建码，建立了删除与插入重建码之间的基本联系，证明了ρ(n,q,N;D_t) ≤ ρ(n,q,N;I_t)，并发现了当N=O(n^{t-1})且t≥2时删除与插入重建码的重要区别。


<details>
  <summary>Details</summary>
Motivation: 研究重建码的主要目标是确定或建立(n,q,N;B)-重建码的最小冗余度ρ(n,q,N;B)的界限，其中错误球可以是t-删除球D_t(·)或t-插入球I_t(·)。

Method: 首先建立删除和插入重建码之间的基本联系，然后分别研究删除和插入情况下的冗余度界限，最后构造具体的重建码并建立相应的上界。

Result: 证明了删除重建码的冗余度为O(1)，插入重建码的冗余度为log log n + O(1)，并构造了N∈{2,3,4,5}时的重建码，分别得到了3log n+O(log log n)、3log n+O(1)、2log n+O(log log n)和log n+O(log log n)的上界。

Conclusion: 本文揭示了删除和插入重建码之间的重要差异，推翻了之前的猜想，扩展了已有结果，并为特定参数下的重建码构造提供了新的上界。

Abstract: Let $\mathcal{B}(\cdot)$ be an error ball function. A set of $q$-ary
sequences of length $n$ is referred to as an
\emph{$(n,q,N;\mathcal{B})$-reconstruction code} if each sequence
$\boldsymbol{x}$ within this set can be uniquely reconstructed from any $N$
distinct elements within its error ball $\mathcal{B}(\boldsymbol{x})$. The main
objective in this area is to determine or establish bounds for the minimum
redundancy of $(n,q,N;\mathcal{B})$-reconstruction codes, denoted by
$\rho(n,q,N;\mathcal{B})$. In this paper, we investigate reconstruction codes
where the error ball is either the \emph{$t$-deletion ball}
$\mathcal{D}_t(\cdot)$ or the \emph{$t$-insertion ball} $\mathcal{I}_t(\cdot)$.
Firstly, we establish a fundamental connection between reconstruction codes for
deletions and insertions. For any positive integers $n,t,q,N$, any
$(n,q,N;\mathcal{I}_t)$-reconstruction code is also an
$(n,q,N;\mathcal{D}_t)$-reconstruction code. This leads to the inequality
$\rho(n,q,N;\mathcal{D}_t)\leq \rho(n,q,N;\mathcal{I}_t)$. Then, we identify a
significant distinction between reconstruction codes for deletions and
insertions when $N=O(n^{t-1})$ and $t\geq 2$. For deletions, we prove that
$\rho(n,q,\tfrac{2(q-1)^{t-1}}{q^{t-1}(t-1)!}n^{t-1}+O(n^{t-2});\mathcal{D}_t)=O(1)$,
which disproves a conjecture posed in \cite{Chrisnata-22-IT}. For insertions,
we show that
$\rho(n,q,\tfrac{(q-1)^{t-1}}{(t-1)!}n^{t-1}+O(n^{t-2});\mathcal{I}_t)=\log\log
n + O(1)$, which extends a key result from \cite{Ye-23-IT}. Finally, we
construct $(n,q,N;\mathcal{B})$-reconstruction codes, where $\mathcal{B}\in
\{\mathcal{D}_2,\mathcal{I}_2\}$, for $N \in \{2,3, 4, 5\}$ and establish
respective upper bounds of $3\log n+O(\log\log n)$, $3\log n+O(1)$, $2\log
n+O(\log\log n)$ and $\log n+O(\log\log n)$ on the minimum redundancy
$\rho(n,q,N;\mathcal{B})$. This generalizes results previously established in
\cite{Sun-23-IT}.

</details>


### [15] [DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications](https://arxiv.org/abs/2508.14507)
*Bohao Wang,Zehua Jiang,Zhenyu Yang,Chongwen Huang,Yongliang Shen,Siming Jiang,Chen Zhu,Zhaohui Yang,Richeng Jin,Zhaoyang Zhang,Sami Muhaidat,Merouane Debbah*

Main category: cs.IT

TL;DR: DeepTelecom是一个3D数字孪生信道数据集，通过LLM辅助构建高细节场景，利用GPU加速的射线追踪技术生成大规模、高保真、多模态的无线信道数据，为无线AI研究提供统一基准和训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有无线AI数据集生成缓慢、建模保真度有限、场景覆盖狭窄，无法满足AI驱动的无线创新需求。

Method: 使用大语言模型辅助构建LoD3级室内外场景，基于Sionna射线追踪引擎模拟全无线电波传播效应，利用GPU加速生成射线路径轨迹、实时信号强度热图、多视图图像、信道张量和多尺度衰落轨迹等多模态数据。

Result: 创建了能够高效流式传输大规模、高保真、多模态信道数据的DeepTelecom数据集，支持实时视频流和同步多视图输出。

Conclusion: DeepTelecom为无线AI研究提供了统一基准，并为基础模型将大模型智能与未来通信系统紧密融合提供了丰富的领域训练数据。

Abstract: Domain-specific datasets are the foundation for unleashing artificial
intelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora
are slow to produce, offer limited modeling fidelity, and cover only narrow
scenario types. To address the challenges, we create DeepTelecom, a
three-dimension (3D) digital-twin channel dataset. Specifically, a large
language model (LLM)-assisted pipeline first builds the third level of details
(LoD3) outdoor and indoor scenes with segmentable material-parameterizable
surfaces. Then, DeepTelecom simulates full radio-wave propagation effects based
on Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom
streams ray-path trajectories and real-time signal-strength heat maps, compiles
them into high-frame-rate videos, and simultaneously outputs synchronized
multi-view images, channel tensors, and multi-scale fading traces. By
efficiently streaming large-scale, high-fidelity, and multimodal channel data,
DeepTelecom not only furnishes a unified benchmark for wireless AI research but
also supplies the domain-rich training substrate that enables foundation models
to tightly fuse large model intelligence with future communication systems.

</details>


### [16] [Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification](https://arxiv.org/abs/2508.14575)
*Shuying Gan,Xijun Wang,Chao Xu,Xiang Chen*

Main category: cs.IT

TL;DR: 提出了任务导向信息年龄(TAoI)新指标来衡量信息内容与系统任务的相关性，应用于无线监控系统，通过SMDP建模和阈值策略优化传输性能


<details>
  <summary>Details</summary>
Motivation: 新兴智能应用需要面向任务的通信范式，但缺乏全面、通用且实用的度量标准来充分释放该范式的潜力

Method: 引入TAoI指标，将动态传输问题建模为半马尔可夫决策过程(SMDP)并转化为等效MDP，提出基于阈值结构的低复杂度相对值迭代算法

Result: 分析表明最优策略具有TAoI阈值特性，提出的最优传输策略在综合实验中优于两种基线方法

Conclusion: TAoI是有效的任务导向通信度量标准，阈值策略能够显著提升系统性能，同时单阈值策略在性能和收敛速度间提供了良好权衡

Abstract: The emergence of new intelligent applications has fostered the development of
a task-oriented communication paradigm, where a comprehensive, universal, and
practical metric is crucial for unleashing the potential of this paradigm. To
this end, we introduce an innovative metric, the Task-oriented Age of
Information (TAoI), to measure whether the content of information is relevant
to the system task, thereby assisting the system in efficiently completing
designated tasks. We apply TAoI to a wireless monitoring system tasked with
identifying targets and transmitting their images for subsequent analysis. To
minimize TAoI and determine the optimal transmission policy, we formulate the
dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and
transform it into an equivalent Markov Decision Process (MDP). Our analysis
demonstrates that the optimal policy is threshold-based with respect to TAoI.
Building on this, we propose a low-complexity relative value iteration
algorithm tailored to this threshold structure to derive the optimal
transmission policy. Additionally, we introduce a simpler single-threshold
policy, which, despite a slight performance degradation, offers faster
convergence. Comprehensive experiments and simulations validate the superior
performance of our optimal transmission policy compared to two established
baseline approaches.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [17] [InverTwin: Solving Inverse Problems via Differentiable Radio Frequency Digital Twin](https://arxiv.org/abs/2508.14204)
*Xingyu Chen,Jianrong Ding,Kai Zheng,Xinmin Fang,Xinyu Zhang,Chris Xiaoxuan Lu,Zhengxiong Li*

Main category: eess.SP

TL;DR: InverTwin是一个优化驱动的框架，通过虚拟和物理领域的双向交互创建RF数字孪生，解决了RF优化的可微性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统RF模拟器的单向性限制了数字孪生在射频传感应用中的潜力，需要开发能够实现虚拟与物理领域双向交互的解决方案。

Method: 采用路径空间微分处理复杂模拟函数的不连续性，使用雷达替代模型缓解RF信号周期性导致的局部非凸性，实现平滑梯度传播和鲁棒优化。

Result: 实验证明InverTwin在增强数据驱动和模型驱动的RF传感系统进行数字孪生重建方面具有多功能性和有效性。

Conclusion: InverTwin框架成功克服了RF优化问题的可微性挑战，为RF数字孪生的创建提供了有效的双向交互解决方案。

Abstract: Digital twins (DTs), virtual simulated replicas of physical scenes, are
transforming various industries. However, their potential in radio frequency
(RF) sensing applications has been limited by the unidirectional nature of
conventional RF simulators. In this paper, we present InverTwin, an
optimization-driven framework that creates RF digital twins by enabling
bidirectional interaction between virtual and physical realms. InverTwin
overcomes the fundamental differentiability challenges of RF optimization
problems through novel design components, including path-space differentiation
to address discontinuity in complex simulation functions, and a radar surrogate
model to mitigate local non-convexity caused by RF signal periodicity. These
techniques enable smooth gradient propagation and robust optimization of the DT
model. Our implementation and experiments demonstrate InverTwin's versatility
and effectiveness in augmenting both data-driven and model-driven RF sensing
systems for DT reconstruction.

</details>


### [18] [Weakly-Convex Regularization for Magnetic Resonance Image Denoising](https://arxiv.org/abs/2508.14438)
*Akash Prabakar,Abhishek Shreekant Bhandiwad,Abijith Jagannath Kamath,Chandra Sekhar Seelamantula*

Main category: eess.SP

TL;DR: 该论文提出了一种构建弱凸正则化函数的方法用于MRI去噪，在保持与最先进去噪器相当性能的同时，提供了更好的可解释性、稳定性和更少的去噪伪影。


<details>
  <summary>Details</summary>
Motivation: 传统MRI去噪使用凸正则化函数，而深度学习虽然性能优越但缺乏可解释性、稳定性和可解释性，这些对MRI应用至关重要。

Method: 采用构造性方法设计弱凸正则化函数，可以构建具有原型激活函数的弱凸卷积神经网络，这些网络具有可解释性且可证明收敛。

Result: 该方法在扩散加权MR图像去噪方面与最先进的去噪器性能相当，同时表现出更少的去噪伪影，并在脑微结构建模中验证了其效果。

Conclusion: 提出的弱凸正则化方法为MRI去噪提供了一种既保持高性能又具备可解释性和稳定性的解决方案，特别适用于需要可靠解释的医学成像应用。

Abstract: Regularization for denoising in magnetic resonance imaging (MRI) is typically
achieved using convex regularization functions. Recently, deep learning
techniques have been shown to provide superior denoising performance. However,
this comes at the price of lack of explainability, interpretability and
stability, which are all crucial to MRI. In this work, we present a
constructive approach for designing weakly-convex regularization functions for
MR image denoising. We show that our technique performs on par with
state-of-the-art denoisers for diffusion-weighted MR image denoising. Our
technique can be applied to design weakly-convex convolutional neural networks
with prototype activation functions that impart interpretability and are
provably convergent. We also show that our technique exhibits fewer denoising
artifacts by demonstrating its effect on brain microstructure modelling.

</details>


### [19] [Pinching-Antenna Systems-Enabled Multi-User Communications: Transmission Structures and Beamforming Optimization](https://arxiv.org/abs/2508.14458)
*Jingjing Zhao,Haowen Song,Xidong Mu,Kaiquan Cai,Yanbo Zhu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文提出了三种基于捏合天线系统(PASS)的传输结构(WM、WD、WS)，研究了多组多播通信中的联合基带信号处理和波束成形设计，并提出了相应的优化算法。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线系统在无线通信中存在局限性，需要创新的柔性天线技术来确保可靠的视距连接和动态天线阵列重构，以提高多用户通信性能。

Method: 提出了波导复用(WM)、波导分割(WD)和波导切换(WS)三种传输结构；针对最大最小公平性问题，提出了基于惩罚对偶分解(PDD)的算法；为单播WS结构设计了低复杂度算法。

Result: 数值结果表明：1)PASS相比传统固定天线系统显著改善了MMF性能；2)WS适用于单播通信，WM适用于多播通信；3)当用户地理隔离时，WD和WM之间的性能差距显著减小。

Conclusion: PASS技术通过动态天线重构显著提升了无线通信性能，不同的传输结构适用于不同的通信场景，为多用户通信系统提供了有效的解决方案。

Abstract: Pinching-antenna systems (PASS) represent an innovative advancement in
flexible-antenna technologies, aimed at significantly improving wireless
communications by ensuring reliable line-of-sight connections and dynamic
antenna array reconfigurations. To employ multi-waveguide PASS in multi-user
communications, three practical transmission structures are proposed, namely
waveguide multiplexing (WM), waveguide division (WD), and waveguide switching
(WS). Based on the proposed structures, the joint baseband signal processing
and pinching beamforming design is studied for a general multi-group multicast
communication system, with the unicast communication encompassed as a special
case. A max-min fairness problem is formulated for each proposed transmission
structure, subject to the maximum transmit power constraint. For WM, to solve
the highly-coupled and non-convex MMF problem with complex exponential and
fractional expressions, a penalty dual decomposition (PDD)-based algorithm is
invoked for obtaining locally optimal solutions. Specifically, the augmented
Lagrangian relaxation is first applied to alleviate the stringent coupling
constraints, which is followed by the block decomposition over the resulting
augmented Lagrangian function. Then, the proposed PDD-based algorithm is
extended to solve the MMF problem for both WD and WS. Furthermore, a
low-complexity algorithm is proposed for the unicast case employing the WS
structure, by simultaneously aligning the signal phases and minimizing the
large-scale path loss at each user. Finally, numerical results reveal that: 1)
the MMF performance is significantly improved by employing the PASS compared to
conventional fixed-position antenna systems; 2) WS and WM are suitable for
unicast and multicast communications, respectively; 3) the performance gap
between WD and WM can be significantly alleviated when the users are
geographically isolated.

</details>


### [20] [FPGA Design and Implementation of Fixed-Point Fast Divider Using Goldschmidt Division Algorithm and Mitchell Multiplication Algorithm](https://arxiv.org/abs/2508.14611)
*Jinkun Yang*

Main category: eess.SP

TL;DR: 提出了一种基于Goldschmidt除法算法和Mitchell乘法算法的可变位宽定点快速除法器，在FPGA上实现99%以上的计算精度，延迟降低31.7ns，资源使用显著减少


<details>
  <summary>Details</summary>
Motivation: 针对FPGA系统中需要高性能除法运算但资源受限的问题，寻求在计算速度和资源利用率之间更好的平衡

Method: 采用Goldschmidt除法算法结合Mitchell乘法算法，使用Verilog HDL设计，在Xilinx XC7Z020-2CLG400I FPGA上实现

Result: 达到99%以上的计算精度，最小延迟99.1ns（比现有单精度除法器快31.7ns），相比使用Vedic乘法器的Goldschmidt除法器，Slice寄存器减少46.68%，Slice LUTs减少4.93%，Slices减少11.85%，精度损失小于1%，仅增加24.1ns延迟

Conclusion: 该设计在计算速度和资源利用率之间实现了更好的平衡，非常适合资源受限的高性能FPGA系统

Abstract: This paper presents a variable bit-width fixed-point fast divider using
Goldschmidt division algorithm and Mitchell multiplication algorithm. Described
using Verilog HDL and implemented on a Xilinx XC7Z020-2CLG400I FPGA, the
proposed divider achieves over 99% computational accuracy with a minimum
latency of 99.1 ns, which is 31.7 ns faster than existing single-precision
dividers. Compared with a Goldschmidt divider using a Vedic multiplier, the
proposed design reduces Slice Registers by 46.68%, Slice LUTs by 4.93%, and
Slices by 11.85%, with less than 1% accuracy loss and only 24.1 ns additional
delay. These results demonstrate an improved balance between computational
speed and resource utilization, making the divider well-suited for
high-performance FPGA-based systems with strict resource constraints.

</details>


### [21] [Design of a Gm-C Dynamic Amplifier with High Linearity and High Temperature and Power Supply Voltage Stability](https://arxiv.org/abs/2508.14637)
*Jinkun Yang,Pengbin Xu*

Main category: eess.SP

TL;DR: 提出了一种高线性度、高温度稳定性和电源电压稳定性的Gm-C动态放大器，采用非对称差分对结构提升线性度，在±40mV输入范围内保持恒定增益，THD达70.5dB，并通过恒定gm偏置电路实现在温度和电源电压变化下的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 传统放大器在温度变化和电源电压波动时性能不稳定，需要设计一种能够在宽温度范围(-40°C至120°C)和电源电压波动(±10%)下保持稳定增益和高线性度的动态放大器。

Method: 采用两个非对称差分对结构作为放大器核心，提升跨导线性度；使用恒定gm偏置电路来稳定跨导和增益，确保在温度和电源电压变化时的性能稳定性。

Result: 在±40mV差分输入范围内保持近乎恒定增益，THD达到70.5dB；在温度-40°C至120°C、电源电压±10%波动条件下，增益标准差为262m，增益分布范围为15.1至16.3，表现出优异的稳定性。

Conclusion: 所提出的Gm-C动态放大器通过创新的非对称差分对结构和恒定gm偏置技术，成功实现了高线性度和优异的温度/电源电压稳定性，适用于对性能稳定性要求严格的模拟电路应用。

Abstract: This paper presents a Gm-C dynamic amplifier with high linearity and high
temperature and power supply voltage stability. The main part of the amplifier
employs two asymmetric differential pairs to enhance transconductance
linearity. The amplifier maintains a nearly constant gain within a differential
input range of -40 mV to 40 mV, and achieves a total harmonic distortion (THD)
of 70.5 dB. The bias part of the amplifier adopts a constant-gm bias circuit,
which improves the temperature and supply voltage stability of the amplifier's
transconductance and gain. When the differential input is 1 mV, the power
supply voltage fluctuates by $\pm$10%, and the temperature varies between
-40$\mathrm{^\circ C}$ and 120$\mathrm{^\circ C}$, the standard deviation of
the gain distribution is 262m, and the distribution range is from 15.1 to 16.3.

</details>


### [22] [Failure Tolerant Phase-Only Indoor Positioning via Deep Learning](https://arxiv.org/abs/2508.14739)
*Fatih Ayten,Mehmet C. Ilter,Akshay Jain,Ossi Kaltiokallio,Jukka Talvitie,Elena Simona Lohan,Henk Wymeersch,Mikko Valkama*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于深度学习的号角位置定位方法，利用双曲线交点原理实现高精度定位，并具有天线故障强锐性。


<details>
  <summary>Details</summary>
Motivation: 高精度定位是下一代无线系统的关键价值，但实际应用中天线故障会导致性能显著下降。需要开发能够在天线元件故障下仍保持高精度的号位定位方法。

Method: 提出一种新的基于深度学习的定位方法，利用双曲线交点原理，并设计了具有天线元件故障强锐性的处理和学习机制。

Result: 所提方法在天线故障情况下仍能实现稳健和准确的定位，对比现有技术在定位精度上有显著提升。

Conclusion: 证明了数据驱动的、具有故障容忍性的号位定位机制的可行性，为5G-Advanced标准化提供了重要技术支撑。

Abstract: High-precision localization turns into a crucial added value and asset for
next-generation wireless systems. Carrier phase positioning (CPP) enables
sub-meter to centimeter-level accuracy and is gaining interest in 5G-Advanced
standardization. While CPP typically complements time-of-arrival (ToA)
measurements, recent literature has introduced a phase-only positioning
approach in a distributed antenna/MIMO system context with minimal bandwidth
requirements, using deep learning (DL) when operating under ideal hardware
assumptions. In more practical scenarios, however, antenna failures can largely
degrade the performance. In this paper, we address the challenging phase-only
positioning task, and propose a new DL-based localization approach harnessing
the so-called hyperbola intersection principle, clearly outperforming the
previous methods. Additionally, we consider and propose a processing and
learning mechanism that is robust to antenna element failures. Our results show
that the proposed DL model achieves robust and accurate positioning despite
antenna impairments, demonstrating the viability of data-driven,
impairment-tolerant phase-only positioning mechanisms. Comprehensive set of
numerical results demonstrates large improvements in localization accuracy
against the prior art methods.

</details>


### [23] [Full-Duplex Beamforming Optimization for Near-Field ISAC](https://arxiv.org/abs/2508.14753)
*Ahsan Nazar,Zhambyl Shaikhanov,Sennur Ulukus*

Main category: eess.SP

TL;DR: 这篇论文研究了全双工运行在近场集成感知与通信系统中的性能，通过聚焦特性提升功耗效率和感知能力。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)是未来无线网络的关键技术，而近场波传播的球面波特性为全双工系统提供了新的聚焦能力。需要研究如何在满足多用户通信和多目标感知要求的同时最小化发射功率。

Method: 提出了一种聚合优化框架，通过交替优化结合半正定松弛和Rayleigh商算法来处理非凸性优化问题，实现基站发射和接收波束形成的聚合设计。

Result: 模拟结果显示，全双工运行的近场ISAC系统在功耗效率方面显著优于半双工和远场对照组，能够在满足通信要求的同时有效检测相同角度的目标。

Conclusion: 全双工运行的近场ISAC系统通过波束聚焦特性实现了更高的功耗效率，为未来无线网络的集成感知与通信应用提供了有效的解决方案。

Abstract: Integrated Sensing and Communications (ISAC) is a promising technology for
future wireless networks, enabling simultaneous communication and sensing using
shared resources. This paper investigates the performance of full-duplex (FD)
communication in near-field ISAC systems, where spherical-wave propagation
introduces unique beam-focusing capabilities. We propose a joint optimization
framework for transmit and receive beamforming at the base station to minimize
transmit power while satisfying rate constraints for multi-user downlink
transmission, multi-user uplink reception, and multi-target sensing. Our
approach employs alternating optimization combined with semidefinite relaxation
and Rayleigh quotient techniques to address the non-convexity of the problem.
Simulation results demonstrate that FD-enabled near-field ISAC achieves
superior power efficiency compared to half-duplex and far-field benchmarks,
effectively detecting targets at identical angles while meeting communication
requirements.

</details>


### [24] [Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks](https://arxiv.org/abs/2508.14884)
*Brian Kim,Justin H. Kong,Terrence J. Moore,Fikadu T. Dagefu*

Main category: eess.SP

TL;DR: 提出基于深度Q网络(DQN)的路由框架，用于异构多跳无线网络，通过改进邻居节点选择策略来提升端到端速率和网络可扩展性


<details>
  <summary>Details</summary>
Motivation: 传统Q学习在异构网络中存在可扩展性差和泛化能力不足的问题，特别是在大规模或动态拓扑网络中管理Q表困难

Method: 使用深度神经网络估计Q值，联合选择下一跳中继和通信技术；提出基于信道增益和速率的邻居节点选择策略，而非简单距离基准

Result: 仿真结果显示，提出的邻居节点选择策略优于简单距离基准方法，DQN方法在各种基准方案中表现优异，性能接近最优方法

Conclusion: DQN-based路由框架能够有效解决异构多跳无线网络中的路由问题，具有更好的可扩展性、适应性和性能表现

Abstract: Routing in multi-hop wireless networks is a complex problem, especially in
heterogeneous networks where multiple wireless communication technologies
coexist. Reinforcement learning (RL) methods, such as Q-learning, have been
introduced for decentralized routing by allowing nodes to make decisions based
on local observations. However, Q-learning suffers from scalability issues and
poor generalization due to the difficulty in managing the Q-table in large or
dynamic network topologies, especially in heterogeneous networks (HetNets) with
diverse channel characteristics. Thus, in this paper, we propose a novel deep
Q-network (DQN)-based routing framework for heterogeneous multi-hop wireless
networks to maximize the end-to-end rate of the route by improving scalability
and adaptability, where each node uses a deep neural network (DNN) to estimate
the Q-values and jointly select the next-hop relay and a communication
technology for transmission. To achieve better performance with the DNN,
selecting which nodes to exchange information is critical, as it not only
defines the state and action spaces but also determines the input to the DNN.
To this end, we propose neighbor node selection strategies based on channel
gain and rate between nodes rather than a simple distance-based approach for an
improved set of states and actions for DQN-based routing. During training, the
model experiences diverse network topologies to ensure generalization and
robustness, and simulation results show that the proposed neighbor node
selection outperforms simple distance-based selection. Further, we observe that
the DQN-based approach outperforms various benchmark schemes and performs
comparably to the optimal approach.

</details>
