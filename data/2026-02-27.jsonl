{"id": "2602.22295", "categories": ["cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.22295", "abs": "https://arxiv.org/abs/2602.22295", "authors": ["Ashish Verma", "Sourav Pradhan"], "title": "Queue occupancy and server size distribution of a queue length dependent vacation queue with an optional service", "comment": "38 pages, 17 figures", "summary": "The discrete time queueing system is highly applicable to modern telecommunication systems, where it provides adaptive packet handling, congestion controlled security/inspection, energy efficient operation, and supports bursty traffic common in 5G, Internet of Things (IoT), and edge computing environments. In this article, we analyze an infinite-buffer discrete-time batch-arrival queue with single and multiple vacation policy where customers are served in batches, in two phases, namely first essential service (FES) and second optional service (SOS). In such systems, the FES corresponds to basic data processing or packet routing, while SOS represents secondary tasks such as encryption, error checking, data compression, or deep packet inspection that may not be necessary for every packet. Here, we derive the bivariate probability generating functions for the joint distribution of the number of packets waiting for transmission and the number are being processed immediately after the completion of both the FES and SOS. Furthermore, the complete joint distribution at arbitrary time slots, including vacation completion states, is established. Numerical illustrations demonstrate the applicability of the proposed framework, including an example with discrete phase type service time distribution. Finally, the sensitivity analysis of the key parameters on marginal system's probabilities and different performance measures have been investigated through several graphical representations."}
{"id": "2602.22482", "categories": ["cs.IT", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.22482", "abs": "https://arxiv.org/abs/2602.22482", "authors": ["Yufeng Zhou", "Hua Sun"], "title": "On the Computation Rate of All-Reduce", "comment": null, "summary": "In the All-Reduce problem, each one of the K nodes holds an input and wishes to compute the sum of all K inputs through a communication network where each pair of nodes is connected by a parallel link with arbitrary bandwidth. The computation rate of All-Reduce is defined as the number of sum instances that can be computed over each network use. For the computation rate, we provide a cut-set upper bound and a linear programming lower bound based on time (bandwidth) sharing over all schemes that first perform Reduce (aggregating all inputs at one node) and then perform Broadcast (sending the sum from that node to all other nodes). Specializing the two general bounds gives us the optimal computation rate for a class of communication networks and the best-known rate bounds (where the upper bound is no more than twice of the lower bound) for cyclic, complete, and hypercube networks."}
{"id": "2602.22605", "categories": ["cs.IT", "math.ST", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.22605", "abs": "https://arxiv.org/abs/2602.22605", "authors": ["Willy Wong"], "title": "A Thermodynamic Structure of Asymptotic Inference", "comment": "29 pages, 1 figure", "summary": "A thermodynamic framework for asymptotic inference is developed in which sample size and parameter variance define a state space. Within this description, Shannon information plays the role of entropy, and an integrating factor organizes its variation into a first-law-type balance equation. The framework supports a cyclic inequality analogous to a reversed second law, derived for the estimation of the mean. A non-trivial third-law-type result emerges as a lower bound on entropy set by representation noise. Optimal inference paths, global bounds on information gain, and a natural Carnot-like information efficiency follow from this structure, with efficiency fundamentally limited by a noise floor. Finally, de Bruijn's identity and the I-MMSE relation in the Gaussian-limit case appear as coordinate projections of the same underlying thermodynamic structure. This framework suggests that ensemble physics and inferential physics constitute shadow processes evolving in opposite directions within a unified thermodynamic description."}
{"id": "2602.22796", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.22796", "abs": "https://arxiv.org/abs/2602.22796", "authors": ["Yijie Bian", "Wei Guo", "Jie Yang", "Shenghui Song", "Jun Zhang", "Shi Jin", "Khaled B. Letaief"], "title": "Multi-modal Data Driven Virtual Base Station Construction for Massive MIMO Beam Alignment", "comment": null, "summary": "Massive multiple-input multiple-output (MIMO) is a key enabler for the high data rates required by the sixth-generation networks, yet its performance hinges on effective beam management with low training overhead. This paper proposes an interpretable framework to tackle beam alignment in mixed line-of-sight (LoS) and non-line-of-sight (NLoS) propagation environments. Our approach utilizes multi-modal data to construct virtual base stations (VBSs), which are geometrically defined as mirror images of the base station across reflecting surfaces reconstructed from 3D LiDAR points. These VBSs provide a sparse and spatial representation of the dominant features of the wireless environment. Based on the constructed VBSs, we develop a VBS-assisted beam alignment scheme comprising coarse channel reconstruction followed by partial beam training. Numerical results demonstrate that the proposed method achieves near-optimal performance in terms of spectral efficiency."}
{"id": "2602.22275", "categories": ["eess.IV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22275", "abs": "https://arxiv.org/abs/2602.22275", "authors": ["Saar Huberman", "Amit Bracha", "Ron Kimmel"], "title": "Deep Accurate Solver for the Geodesic Problem", "comment": "Extended version of Deep Accurate Solver for the Geodesic Problem originally published in Scale Space and Variational Methods in Computer Vision (SSVM 2023), Lecture Notes in Computer Science, Springer. This version includes additional experiments and detailed analysis", "summary": "A common approach to compute distances on continuous surfaces is by considering a discretized polygonal mesh approximating the surface and estimating distances on the polygon. We show that exact geodesic distances restricted to the polygon are at most second-order accurate with respect to the distances on the corresponding continuous surface. By order of accuracy we refer to the convergence rate as a function of the average distance between sampled points. Next, a higher-order accurate deep learning method for computing geodesic distances on surfaces is introduced. Traditionally, one considers two main components when computing distances on surfaces: a numerical solver that locally approximates the distance function, and an efficient causal ordering scheme by which surface points are updated. Classical minimal path methods often exploit a dynamic programming principle with quasi-linear computational complexity in the number of sampled points. The quality of the distance approximation is determined by the local solver that is revisited in this paper. To improve state of the art accuracy, we consider a neural network-based local solver which implicitly approximates the structure of the continuous surface. We supply numerical evidence that the proposed learned update scheme provides better accuracy compared to the best possible polyhedral approximations and previous learning-based methods. The result is a third-order accurate solver with a bootstrapping-recipe for further improvement."}
{"id": "2602.22231", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22231", "abs": "https://arxiv.org/abs/2602.22231", "authors": ["Dong Yang", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "FM-RME: Foundation Model Empowered Radio Map Estimation", "comment": "7 pages, 5 figures, conference", "summary": "Traditional radio map estimation (RME) techniques fail to capture multi-dimensional and dynamic characteristics of complex spectrum environments. Recent data-driven methods achieve accurate RME in spatial domain, but ignore physical prior knowledge of radio propagation, limiting data efficiency especially in multi-dimensional scenarios. To overcome such limitations, we propose a new foundation model, characterized by self-supervised pre-training on diverse data for zero-shot generalization, enabling multi-dimensional radio map estimation (FM-RME). Specifically, FM-RME builds an effective synergy of two core components: a geometry-aware feature extraction module that encodes physical propagation symmetries, i.e., translation and rotation invariance, as inductive bias, and an attention-based neural network that learns long-range correlations across the spatial-temporal-spectral domains. A masked self-supervised multi-dimensional pre-training strategy is further developed to learn generalizable spectrum representations across diverse wireless environments. Once pre-trained, FM-RME supports zero-shot inference for multi-dimensional RME, including spatial, temporal, and spectral estimation, without scenario-specific retraining. Simulation results verify that FM-RME exhibits desired learning performance across diverse datasets and zero-shot generalization capabilities beyond existing RME methods."}
{"id": "2602.22934", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.22934", "abs": "https://arxiv.org/abs/2602.22934", "authors": ["Javad Gholipour", "Rafael F. Schaefer", "Gerhard P. Fettweis"], "title": "Semantic Communication Through the Lens of Context-Dependent Channel Modeling", "comment": null, "summary": "Semantic communication has emerged as a promising paradigm for next-generation networks, yet several fundamental challenges remain unresolved. Building on the probabilistic model of semantic communication and leveraging the concept of context, this paper examines a specific subclass of semantic communication problems, where semantic noise originates solely from the semantic channel, assuming an ideal physical channel. To model this system, we introduce a virtual state-dependent channel, where the state-representing context-plays a crucial role in shaping communication. We further analyze the representational capability of the semantic encoder and explore various semantic communication scenarios in the presence of semantic noise, deriving capacity results for some cases and achievable rates for others."}
{"id": "2602.22279", "categories": ["eess.IV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.22279", "abs": "https://arxiv.org/abs/2602.22279", "authors": ["Victor Sechaud", "Laurent Jacques", "Patrice Abry", "Julián Tachella"], "title": "Learning to reconstruct from saturated data: audio declipping and high-dynamic range imaging", "comment": null, "summary": "Learning based methods are now ubiquitous for solving inverse problems, but their deployment in real-world applications is often hindered by the lack of ground truth references for training. Recent self-supervised learning strategies offer a promising alternative, avoiding the need for ground truth. However, most existing methods are limited to linear inverse problems. This work extends self-supervised learning to the non-linear problem of recovering audio and images from clipped measurements, by assuming that the signal distribution is approximately invariant to changes in amplitude. We provide sufficient conditions for learning to reconstruct from saturated signals alone and a self-supervised loss that can be used to train reconstruction networks. Experiments on both audio and image data show that the proposed approach is almost as effective as fully supervised approaches, despite relying solely on clipped measurements for training."}
{"id": "2602.22378", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22378", "abs": "https://arxiv.org/abs/2602.22378", "authors": ["Daniel Rossato", "Thiago Alberto Rigo Passarin", "Gustavo Pinto Pires", "Daniel Rodrigues Pipa"], "title": "Full Waveform Inversion using the Wasserstein metric for ultrasound transducer array based NDT", "comment": null, "summary": "Ultrasonic imaging methods often assume linear direct models, while in reality, many nonlinear phenomena are present, e.g. multiple reflections. A family of imaging methods called Full Waveform Inversion (FWI), which has been developed in the field of seismic imaging, uses full acoustic wave simulations as direct models, taking into account virtually all nonlinearities, which can ultimately enhance the accuracy of ultrasonic imaging. However, the problem of cycle skipping -- the existence of many local minima of the Least Squares (L2) misfit function due to the oscillatory nature of the signals -- is worsened when FWI is applied to ultrasound data because of a lack of low-frequency components. In this paper, we explore the use of the squared Wasserstein (W2) Optimal Transport Distance as the metric for the misfit between the acquired and the synthetic data, applying the method to Nondestructive Evaluation with ultrasonic phased arrays. An analytical continuous time-domain derivation of the adjoint acoustic field related to the W2 misfit is presented and used for the computation of the gradients. To cope with the computational burden of FWI, we apply a low-memory strategy that allows for the computation of the gradients without the storage of the full simulated fields. The GPU implementation of the method (in CUDA language) is detailed, and the source code is made available. Six prototypical cases are presented, and the corresponding sound speed maps are reconstructed with FWI using both the L2 and the W2 misfit functionals. In five of the six cases, the pixel-wise sum of squared errors obtained with W2 was at least one order of magnitude lower than that obtained with W2, with an increase in the gradient computation time not exceeding 2\\%. The results highlight both the adequacy of the W2 misfit for ultrasonic FWI with phased arrays and its computational feasibility."}
{"id": "2602.22958", "categories": ["cs.IT", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22958", "abs": "https://arxiv.org/abs/2602.22958", "authors": ["Maximilian Kalcher"], "title": "Frequency-Ordered Tokenization for Better Text Compression", "comment": "5 pages, 4 figures, 9 tables", "summary": "We present frequency-ordered tokenization, a simple preprocessing technique that improves lossless text compression by exploiting the power-law frequency distribution of natural language tokens (Zipf's law). The method tokenizes text with Byte Pair Encoding (BPE), reorders the vocabulary so that frequent tokens receive small integer identifiers, and encodes the result with variable-length integers before passing it to any standard compressor. On enwik8 (100 MB Wikipedia), this yields improvements of 7.08 percentage points (pp) for zlib, 1.69 pp for LZMA, and 0.76 pp for zstd (all including vocabulary overhead), outperforming the classical Word Replacing Transform. Gains are consistent at 1 GB scale (enwik9) and across Chinese and Arabic text. We further show that preprocessing accelerates compression for computationally expensive algorithms: the total wall-clock time including preprocessing is 3.1x faster than raw zstd-22 and 2.4x faster than raw LZMA, because the preprocessed input is substantially smaller. The method can be implemented in under 50 lines of code."}
{"id": "2602.22544", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22544", "abs": "https://arxiv.org/abs/2602.22544", "authors": ["Khuram Naveed", "Ruben Pauwels"], "title": "HARU-Net: Hybrid Attention Residual U-Net for Edge-Preserving Denoising in Cone-Beam Computed Tomography", "comment": null, "summary": "Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT while preserving edges. Although deep learning-based approaches offer high-fidelity restoration, their use in CBCT denoising is limited by the scarcity of high-resolution CBCT data for supervised training. To address this research gap, we propose a novel Hybrid Attention Residual U-Net (HARU-Net) for high-quality denoising of CBCT data, trained on a cadaver dataset of human hemimandibles acquired using a high-resolution protocol of the 3D Accuitomo 170 (J. Morita, Kyoto, Japan) CBCT system. The novel contribution of this approach is the integration of three complementary architectural components: (i) a hybrid attention transformer block (HAB) embedded within each skip connection to selectively emphasize salient anatomical features, (ii) a residual hybrid attention transformer group (RHAG) at the bottleneck to strengthen global contextual modeling and long-range feature interactions, and (iii) residual learning convolutional blocks to facilitate deeper, more stable feature extraction throughout the network. HARU-Net consistently outperforms state-of-the-art (SOTA) methods including SwinIR and Uformer, achieving the highest PSNR (37.52 dB), highest SSIM (0.9557), and lowest GMSD (0.1084). This effective and clinically reliable CBCT denoising is achieved at a computational cost significantly lower than that of the SOTA methods, offering a practical advancement toward improving diagnostic quality in low-dose CBCT imaging."}
{"id": "2602.22738", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22738", "abs": "https://arxiv.org/abs/2602.22738", "authors": ["Ruiqi Kong", "He Chen"], "title": "CSI-RFF: Leveraging Micro-Signals on CSI for RF Fingerprinting of Commodity WiFi", "comment": "15 pages", "summary": "This paper introduces CSI-RFF, a new framework that leverages micro-signals embedded within Channel State Information (CSI) curves to realize Radio-Frequency Fingerprinting of commodity off-the-shelf (COTS) WiFi devices for open-set authentication. The micro-signals that serve as RF fingerprints are termed ``micro-CSI''. Through experimentation, we have found that the presence of micro-CSI can primarily be attributed to imperfections in the RF circuitry. Furthermore, this characteristic signal is detectable in WiFi 4/5/6 network interface cards (NICs). We have conducted further experiments to determine the most effective CSI collection configurations to stabilize micro-CSI. Yet, extracting micro-CSI for authentication purposes poses a significant challenge. This complexity arises from the fact that CSI measurements inherently include both micro-CSI and the distortions introduced by wireless channels. These two elements are intricately intertwined, making their separation non-trivial. To tackle this challenge, we have developed a signal space-based extraction technique for line-of-sight (LoS) scenarios, which can effectively separate the distortions caused by wireless channels and micro-CSI. Over the course of our comprehensive CSI data collection period extending beyond one year, we found that the extracted micro-CSI displays unique characteristics specific to each WiFi device and remains invariant over time. This establishes micro-CSI as a suitable candidate for device fingerprinting. Finally, we conduct a case study focusing on area access control for mobile robots. Our experimental results demonstrate that the micro-CSI-based authentication algorithm can achieve an average attack detection rate close to 99% with a false alarm rate of 0% in both static and mobile conditions when using 20 CSI measurements to construct one fingerprint."}
{"id": "2602.22964", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22964", "abs": "https://arxiv.org/abs/2602.22964", "authors": ["Merijn Floren", "Jan Swevers"], "title": "A guided residual search for nonlinear state-space identification", "comment": "Preprint submitted to IEEE Control Systems Letters (L-CSS)", "summary": "Parameter estimation of nonlinear state-space models from input-output data typically requires solving a highly non-convex optimization problem prone to slow convergence and suboptimal solutions. This work improves the reliability and efficiency of the estimation process by decomposing the overall optimization problem into a sequence of tractable subproblems. Based on an initial linear model, nonlinear residual dynamics are first estimated via a guided residual search and subsequently refined using multiple-shooting optimization. Experimental results on two benchmarks demonstrate competitive performance relative to state-of-the-art black-box methods and improved convergence compared to naive initialization."}
{"id": "2602.22991", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22991", "abs": "https://arxiv.org/abs/2602.22991", "authors": ["Alexander Bonora", "Anna V. Guglielmi", "Davide Scazzoli", "Marco Giordani", "Maurizio Magarini", "Vineeth Teeda", "Stefano Tomasin"], "title": "Digital Twin-Based Beamforming for Interference Mitigation in AF Relay MIMO Systems", "comment": "12 pages, 13 figures, 1 table. Submitted to IEEE Special Issue on \"Digital Twins for Wireless Networks: Enabling Application-Aware and Closed-Loop Optimization\"", "summary": "Beamforming in multiple-input multiple-output (MIMO) systems should take interference mitigation into account. However, for beamform design, accurate channel state information (CSI) is needed, which is often difficult to obtain due to channel variability, feedback overhead, or hardware constraints. For example, amplify-and-forward (AF) relays passively forward signals without measurement, precluding full CSI acquisition to and from the relay. To address these issues, this paper introduces a novel prediction-assisted optimization (PAO) framework for beamform design in AF relay-assisted multiuser MIMO systems. The proposed solution in the AF relay aims at maximizing the signal-plus-interference-to-noise ratio (SINR). Unlike other methods, PAO relies solely on received power measurements, making it suitable for scenarios where CSI is unreliable or unavailable. PAO consists of two stages: a supervised-learning-based neural network (NN) that predicts the positions of transmitters using signal observations, and an optimization algorithm, guided by a digital twin (DT), that iteratively refines the beam direction of the relay in a simulated radio environment. As a key contribution, we validate the proposed framework using realistic measurements collected on a custom-built experimental millimeter wave (mmWave) platform, which enables training of the NN model under practical wireless conditions. The estimated information is then used to update the digital twin with knowledge of the surrounding environment, enabling online optimization. Numerical results show the trade-off between localization accuracy and beamforming performance and confirm that PAO maintains robustness even in the presence of localization errors while reducing the need for real-world measurements."}
{"id": "2602.23003", "categories": ["eess.SP", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.23003", "abs": "https://arxiv.org/abs/2602.23003", "authors": ["René Pallenberg", "Fabrice Katzberg", "Alfred Mertins", "Marco Maass"], "title": "Scattering Transform for Auditory Attention Decoding", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The use of hearing aids will increase in the coming years due to demographic change. One open problem that remains to be solved by a new generation of hearing aids is the cocktail party problem. A possible solution is electroencephalography-based auditory attention decoding. This has been the subject of several studies in recent years, which have in common that they use the same preprocessing methods in most cases. In this work, in order to achieve an advantage, the use of a scattering transform is proposed as an alternative to these preprocessing methods. The two-layer scattering transform is compared with a regular filterbank, the synchrosqueezing short-time Fourier transform and the common preprocessing. To demonstrate the performance, the known and the proposed preprocessing methods are compared for different classification tasks on two widely used datasets, provided by the KU Leuven (KUL) and the Technical University of Denmark (DTU). Both established and new neural-network-based models, CNNs, LSTMs, and recent Transformer/graph-based models are used for classification. Various evaluation strategies were compared, with a focus on the task of classifying speakers who are unknown from the training. We show that the two-layer scattering transform can significantly improve the performance for subject-related conditions, especially on the KUL dataset. However, on the DTU dataset, this only applies to some of the models, or when larger amounts of training data are provided, as in 10-fold cross-validation. This suggests that the scattering transform is capable of extracting additional relevant information."}
{"id": "2602.23252", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.23252", "abs": "https://arxiv.org/abs/2602.23252", "authors": ["Maximilian Kalcher", "Tena Dubcek"], "title": "A Scaling Law for Bandwidth Under Quantization", "comment": "4 pages, 3 figures, submitted to IEEE Signal Processing Letters", "summary": "We derive a scaling law relating ADC bit depth to effective bandwidth for signals with $1/f^α$ power spectra. Quantization introduces a flat noise floor whose intersection with the declining signal spectrum defines an effective cutoff frequency $f_c$. We show that each additional bit extends this cutoff by a factor of $2^{2/α}$, approximately doubling bandwidth per bit for $α= 2$. The law requires that quantization noise be approximately white, a condition whose minimum bit depth $N_{\\min}$ we show to be $α$-dependent. Validation on synthetic $1/f^α$ signals for $α\\in \\{1.5, 2.0, 2.5\\}$ yields prediction errors below 3\\% using the theoretical noise floor $Δ^2/(6f_s)$, and approximately 14\\% when the noise floor is estimated empirically from the quantized signal's spectrum. We illustrate practical implications on real EEG data."}
{"id": "2602.23284", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.23284", "abs": "https://arxiv.org/abs/2602.23284", "authors": ["Juana M. Martínez-Heredia", "Alfredo P. Vega-Leal"], "title": "Analog Time Multiplexing for Digital-to-Analog Conversion", "comment": "11 pages, 7 figures", "summary": "The signal bandwidth of Digital to Analog Converters based on Sigma Delta Modulation is limited by speed constrains. Time-Interleaving allows coping with complexity vs. speed by replacing the original architecture by M parallel paths. These path are clocked at a frequency M times smaller and their digital outputs time multiplexed. This is then converted to analog by means of a Digital to Analog Converter clocked at the high rate. This preprint proposes that time multiplexing be performed in the analog domain. As a result robustness against dynamic effects is achieved."}
{"id": "2602.23338", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2602.23338", "abs": "https://arxiv.org/abs/2602.23338", "authors": ["Kyle D. Massingill", "Tyler M. Karasinski", "Sean Bryan", "Michael Baricuatro", "Daniel Bliss", "Delondrae Carter", "Walter Goodwin", "Jonathan Greenfield", "Christopher Groppi", "Jae Joiner", "Philip Mauskopf", "Philip Rybak", "Scott Smas", "Roshni Suresh", "Joesph Tinlin", "Bianca Wullen", "Peter Wullen"], "title": "CubeSounder: Low SWaP-C 180 GHz Radiometer for Atmospheric Sensing Tested on High Altitude Balloons", "comment": "7 Pages, 11 Figures, Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Microwave sounding is the leading driver of global numerical weather forecasting, but is limited by the scalability of such instruments. With modern machining and commercial microwave components, it is now possible to design low size, weight, power, and cost (SWaP-C) microwave spectrometers while maintaining wide bandwidth performance. Here we report on the status of CubeSounder, a spectrometer tailored for water vapor radiometry that utilizes passive wave guide filter banks. After developing a prototype and high altitude balloon payload, we demonstrated CubeSounder on commercial stratospheric balloon flights. We report on our design process, especially the simulation and fabrication of the custom millimeter-wave filter banks. We also report the initial results of the data collected from the balloon flights."}
{"id": "2602.22544", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22544", "abs": "https://arxiv.org/abs/2602.22544", "authors": ["Khuram Naveed", "Ruben Pauwels"], "title": "HARU-Net: Hybrid Attention Residual U-Net for Edge-Preserving Denoising in Cone-Beam Computed Tomography", "comment": null, "summary": "Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT while preserving edges. Although deep learning-based approaches offer high-fidelity restoration, their use in CBCT denoising is limited by the scarcity of high-resolution CBCT data for supervised training. To address this research gap, we propose a novel Hybrid Attention Residual U-Net (HARU-Net) for high-quality denoising of CBCT data, trained on a cadaver dataset of human hemimandibles acquired using a high-resolution protocol of the 3D Accuitomo 170 (J. Morita, Kyoto, Japan) CBCT system. The novel contribution of this approach is the integration of three complementary architectural components: (i) a hybrid attention transformer block (HAB) embedded within each skip connection to selectively emphasize salient anatomical features, (ii) a residual hybrid attention transformer group (RHAG) at the bottleneck to strengthen global contextual modeling and long-range feature interactions, and (iii) residual learning convolutional blocks to facilitate deeper, more stable feature extraction throughout the network. HARU-Net consistently outperforms state-of-the-art (SOTA) methods including SwinIR and Uformer, achieving the highest PSNR (37.52 dB), highest SSIM (0.9557), and lowest GMSD (0.1084). This effective and clinically reliable CBCT denoising is achieved at a computational cost significantly lower than that of the SOTA methods, offering a practical advancement toward improving diagnostic quality in low-dose CBCT imaging."}
