<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.IT](#cs.IT) [Total: 8]
- [eess.IV](#eess.IV) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Time and Frequency Synchronization for Multiuser OTFS in Uplink](https://arxiv.org/abs/2507.17966)
*Mohsen Bayat,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 提出了一种针对高移动性场景下上行多用户OTFS系统的时间和频率同步技术，包括定时偏移（TO）和载波频率偏移（CFO）的估计与校正方法。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景下，准确估计和校正TO和CFO对多用户OTFS系统的性能至关重要，尤其是TO估计用于定位用户导频，CFO估计用于提高信道估计精度。

Method: 1. 提出基于单用户启发的PCP（SU-PCP）导频结构，替代传统IMP导频，利用不同ZC序列实现导频分离；2. 提出基于相关性的TO估计技术；3. 提出共享导频区域的MU-PCP导频模式，利用滤波器组分离用户信号并估计TO；4. 提出基于Chebyshev多项式的CFO估计技术，将多维ML搜索简化为多个一维搜索。

Result: 提出的TO和CFO估计技术能够有效提高同步精度，尤其是在高移动性场景下。

Conclusion: 所提出的同步技术为高移动性场景下的多用户OTFS系统提供了高效的时间和频率同步解决方案。

Abstract: In this paper, we propose time and frequency synchronization techniques for
uplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work
focuses on accurately estimating and correcting timing offsets (TOs) and
carrier frequency offsets (CFOs). Specifically, TO estimation is essential for
locating users' pilots on the delay-time plane, while CFO estimation enhances
channel estimation accuracy. First, we propose a TO estimation technique for an
existing multiuser pilot structure in MU-OTFS. We replace the impulse pilot
(IMP) in this pilot structure with a more practical pilot with a cyclic prefix
(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs
different Zadoff-Chu (ZC) sequences, which enables pilot separation via
correlation at the receiver side. Consequently, we introduce a
correlation-based TO estimation technique for uplink MU-OTFS using this pilot
structure. Next, a spectrally efficient and practical pilot pattern is
proposed, where each user transmits a PCP within a shared pilot region on the
delay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO
estimation technique utilizes a bank of filters to separate different users'
signals and accurately estimate their TOs. Then, we derive a mathematical
threshold range to enhance TO estimation accuracy by finding the first major
peak in the correlation function rather than relying solely on the highest
peak. After locating the received users' pilot signals using one of the
proposed TO estimation techniques, our proposed CFO estimation technique
reduces the multi-dimensional maximum likelihood (ML) search problem into
multiple one-dimensional search problems. In this technique, we apply the
Chebyshev polynomials of the first kind basis expansion model (CPF-BEM) to
effectively handle the time-variations of the channel in obtaining the CFO
estimates for all the users.

</details>


### [2] [Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model](https://arxiv.org/abs/2507.17982)
*Pablo Ramírez-Espinosa,Cleofás Segura-Gómez,Ángel Palomares-Caballero,F. Javier López-Martínez,David Morales-Jiménez*

Main category: eess.SP

TL;DR: 论文提出了一种基于动态超表面天线（DMA）的流体天线系统（FAS）的完整分析模型，解决了电子可重构天线在理论建模上的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子可重构天线在流体天线系统（FAS）中面临理论建模困难，限制了系统设计的理论指导。

Method: 利用电路理论将FAS的信号模型改写为考虑超表面电磁效应的导纳矩阵形式，并通过全波仿真验证模型。

Result: 模型验证显示良好一致性，DMA-based FAS性能接近理想化的位置灵活天线。

Conclusion: 提出的分析模型为FAS设计提供了理论支持，DMA-based FAS具有实际应用潜力。

Abstract: Fluid antenna systems (FASs) have become a popular topic in the wireless
community as an effective yet simple means of exploiting spatial diversity. Due
to the limitations of physically moving radiating elements, electronically
reconfigurable antennas are emerging as practical implementations of FASs,
since changing the radiation pattern is functionally equivalent to physically
moving the device. However, electronically reconfigurable antennas pose a
challenge in terms of analytical modeling, often requiring full-wave
simulations or measurements for their characterization; this severely limits
the extraction of theoretical insights useful for system design. Motivated by
these difficulties and the growing interest in FASs, we propose in this paper a
complete analytical model for metasurface-based embodiments of FASs.
Specifically, we advocate for the implementation of the FAS concept through
dynamic metasurface antennas (DMAs), hitherto proposed as array replacements in
multiple-input multiple-output (MIMO) systems. We leverage circuit theory to
rewrite the conventional signal model of FASs in terms of admittance matrices
accounting for the electromagnetic effects inherent to metasurfaces. The model
is validated with full-wave simulations, showing good agreement. We further
illustrate how to apply the model for standard performance analysis, and
provide closed-form expressions for key metrics, including the resulting signal
covariance matrix. Results confirm that practical DMA-based FASs can achieve
similar performance to that of idealized implementations of position-flexible
antennas.

</details>


### [3] [Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming](https://arxiv.org/abs/2507.18035)
*Hyeonho Noh,Hyeonsu Lyu,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 本文研究了由多个主动同时传输和反射可重构智能表面（STAR-RIS）支持的集成感知与通信（ISAC）网络，通过联合优化基站波束成形和STAR-RIS反射/传输系数，最大化通信总速率，同时满足感知、安全和功率约束。


<details>
  <summary>Details</summary>
Motivation: 探索如何在集成感知与通信网络中利用STAR-RIS技术提升性能，同时满足多目标约束。

Method: 采用交替优化（AO）框架，将问题分解为子问题，分别通过KKT条件和SCA方法优化基站波束成形和STAR-RIS参数。

Result: 仿真表明，所提算法在通信总速率上显著优于被动RIS和单STAR-RIS基准，同时满足感知和安全约束。

Conclusion: 主动STAR-RIS和多目标优化框架为ISAC网络提供了高效解决方案。

Abstract: This paper explores an integrated sensing and communication (ISAC) network
empowered by multiple active simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes
downlink communication to multiple users while concurrently interrogating a
sensing target. We jointly optimize the BS transmit beamformer and the
reflection/transmission coefficients of every active STAR-RIS in order to
maximize the aggregate communication sum-rate, subject to (i) a stringent
sensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an
upper bound on the leakage of confidential information, and (iii) individual
hardware and total power constraints at both the BS and the STAR-RISs. The
resulting highly non-convex program is tackled with an efficient alternating
optimization (AO) framework. First, the original formulation is reformulated
into an equivalent yet more tractable representation and partitioned into
subproblems. The BS beamformer is updated in closed form via the
Karush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and
transmission vectors are refined through successive convex approximation (SCA),
yielding a semidefinite program that is then solved via semidefinite
relaxation. Comprehensive simulations demonstrate that the proposed algorithm
delivers substantial sum-rate gains over passive-RIS and single STAR-RIS
baselines, all the while rigorously meeting the prescribed sensing and security
constraints.

</details>


### [4] [Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation](https://arxiv.org/abs/2507.18096)
*Jihong Huang,Rong Yang,Wei Gao,Xingqun Zhan,Zheng Yao*

Main category: eess.SP

TL;DR: 论文提出了一种几何分析方法，量化了多径误差对直接位置估计（DPE）的影响，并建立了卫星圆形多径偏置（SCMB）模型，通过蒙特卡洛模拟和城市峡谷测试验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对DPE理论中多径误差的理论描述，研究旨在填补这一空白，提升GNSS接收器在复杂环境中的性能。

Method: 通过几何分析量化多径误差对CAF和PVT解的影响，建立SCMB模型，并通过模拟和实验验证。

Result: 研究发现最大PVT偏置取决于多径误差的最大值，且PVT偏置随卫星仰角增加而增加。

Conclusion: 研究为从几何角度选择DPE卫星提供了参考，强调了高低仰角卫星组合的重要性。

Abstract: Direct Position Estimation (DPE) is a method that directly estimate position,
velocity, and time (PVT) information from cross ambiguity function (CAF) of the
GNSS signals, significantly enhancing receiver robustness in urban
environments. However, there is still a lack of theoretical characterization on
multipath errors in the context of DPE theory. Geometric observations highlight
the unique characteristics of DPE errors stemming from multipath and thermal
noise as estimation bias and variance respectively. Expanding upon the
theoretical framework of DPE noise variance through geometric analysis, this
paper focuses on a geometric representation of multipath errors by quantifying
the deviations in CAF and PVT solutions caused by off-centering bias relative
to the azimuth and elevation angles. A satellite circular multipath bias (SCMB)
model is introduced, amalgamating CAF and PVT errors from multiple satellite
channels. The boundaries for maximum or minimum PVT bias are established
through discussions encompassing various multipath conditions. The correctness
of the multipath geometrical portrait is confirmed through both Monte Carlo
simulations and urban canyon tests. The findings indicate that the maximum PVT
bias depends on the largest multipath errors observed across various satellite
channels. Additionally, the PVT bias increases with satellite elevation angles,
influenced by the CAF multipath bias projection. This serves as a reference for
selecting DPE satellites from a geometric standpoint, underscoring the
importance of choosing a balanced combination of high and low elevation angles
to achieve an optimal satellite geometry configuration.

</details>


### [5] [Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems](https://arxiv.org/abs/2507.18149)
*Dongdong Zou,Wei Wang,Jiawen Yao,Zhongxing Tian,Zeyu Feng,Huan Huang,Fan Li,Gordon Ning Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 提出了一种针对峰值功率受限IM-DD系统的间接概率整形方案，通过动态选择性映射机制和改进的M-BCJR算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: IM-DD系统中概率整形的有效应用仍是一个开放性问题，尤其是在存在记忆效应的情况下。

Method: 采用动态选择性映射机制（DSLM）控制信号包络，结合改进的M-BCJR算法的turbo均衡器。

Result: 在56GBaud PAM8系统中，接收灵敏度提升了1dB，且兼容典型概率幅度整形架构。

Conclusion: 该方案为峰值功率受限IM-DD系统中概率整形的应用提供了新思路。

Abstract: Probabilistic shaping (PS) has attracted significant attention in
intensity-modulation and direct-detection (IM-DD) systems. However, due to the
unique system model and inherent constraints, the effective application of the
PS technique is still an open question in IM-DD systems, particularly in
systems with memory effects. In this paper, a novel indirect PS scheme tailored
for peak power constrained (PPC) IM-DD systems is proposed. The key idea lies
in strategically controlling the signal envelope to mitigate memory-induced
impairments, such as nonlinearity, overshoot, peak-to-average power ratio
enhancement, etc. The proposed scheme incorporates a dynamic selective mapping
(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol
mapping in which the current symbol is not only determined by the current bits
pattern but also by previously generated symbols within a specified memory
length. At the receiver side, a turbo equalizer with a modified M-BCJR
algorithm is proposed to achieve the recovery of ambiguous bits induced by
DSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the
proposed scheme exhibits 1dB receiver sensitivity improvement over 2km
single-mode fiber transmission. In addition, the proposed scheme has also been
demonstrated to be compatible with the typical probabilistic amplitude shaping
architecture, enabling a simple and fine-granularity rate adaptation
capability. To the best of our knowledge, this work opens a new sight for the
application of the PS technique in PPC IM-DD systems with memory effects.

</details>


### [6] [GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing](https://arxiv.org/abs/2507.18166)
*Jonas Elmiger,Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: SCHIEBER是一种多天线GNSS接收器方法，无需先验知识即可缓解干扰和欺骗攻击。通过自适应空间滤波技术抑制干扰，并通过比较信号到达方向和伪距估计识别欺骗。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）信号易受干扰和欺骗攻击，威胁定位准确性。需要一种无需先验知识的方法来应对这些攻击。

Method: 使用自适应空间滤波技术抑制干扰，通过比较信号到达方向（DoA）和伪距估计识别欺骗信号。

Result: 在GPS L1 C/A系统的模拟中，SCHIEBER有效缓解了干扰和欺骗攻击。

Conclusion: SCHIEBER是一种无需先验知识的有效方法，可同时应对GNSS干扰和欺骗攻击。

Abstract: Modern positioning relies on radio signals from global navigation satellite
systems (GNSS). Their low receive power renders these radio signals susceptible
to jamming attacks, in which malicious transmitters emit strong interference to
disrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,
in which malicious transmitters mimic legitimate satellites by transmitting
spurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna
GNSS receivers that mitigates jammers as well as spoofers without requiring any
prior knowledge of the receiver position or attack type: Jammers are mitigated
during signal acquisition using a recently developed adaptive spatial filtering
technique. Spoofers are identified and rejected after signal acquisition using
a novel approach that tests the consistency of acquired signals by comparing
their respective direction of arrival (DoA) and pseudorange estimates in a test
that is invariant with respect to the unknown receiver position. We demonstrate
the efficacy of our method using extensive simulations of a GPS L1 C/A system
under spoofing and jamming attacks.

</details>


### [7] [ICWLM: A Multi-Task Wireless Large Model via In-Context Learning](https://arxiv.org/abs/2507.18167)
*Yuxuan Wen,Xiaoming Chen,Maojun Zhang,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: 提出了一种新型无线原生基础模型ICWLM，用于物理层的多任务学习，解决了传统深度学习方法在数据稀缺和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 无线通信技术快速发展带来网络复杂性和计算需求增加，传统深度学习方法任务特定且难以泛化。

Method: ICWLM直接从大规模混合无线数据训练，利用上下文学习适应不同系统配置，采用动态权重平均算法平衡多任务损失。

Result: ICWLM在性能上与任务特定方法相当，并展现出对未见系统配置的强泛化能力。

Conclusion: ICWLM为未来无线网络提供了统一且自适应的AI模型范式，有望降低部署复杂性并提升资源管理智能性。

Abstract: The rapid evolution of wireless communication technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (LLMs), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.

</details>


### [8] [Quantized Signal Recovery with Interference via Parametrized Look-Up Tables](https://arxiv.org/abs/2507.18370)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 论文提出了一种基于查找表（LUT）的数字后校正方法，通过参数化模型优化性能，适用于低分辨率、非线性或宽带量化器。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率模数转换器（ADC）在实时信号处理中的性能问题，尤其是面对非线性量化器和宽带干扰时的挑战。

Method: 结合参数化模型（输入信号、噪声和干扰信号的预期模型），评估三种分析估计器，并提出近似方法以提高估计问题的可解性。

Result: 模拟结果显示，该方法能高精度实时恢复输入信号，显著优于传统线性滤波技术，并在谐波失真消除方面表现优异。

Conclusion: 该方法在低分辨率量化器和时变干扰源下具有鲁棒性，显著提升了均方误差和无杂散动态范围。

Abstract: Efficient all-digital post-correction of low-resolution analog-to-digital
converters can be achieved by using Look-Up Tables (LUTs). The performance of a
LUT can be optimized by incorporating a parametric model for the expected input
signal, noise level, and interference signals. We evaluate three analytical
estimators for integration with parametrized LUTs, especially with applications
to low-resolution, non-linear, or wideband quantizers. We also propose several
approximations to improve tractability of the estimation problem for
Phase-Shift Keyed input signals and Linear Frequency Modulated interference
signals. Simulated results validate the ability of our estimator to recover the
instantaneous value of the desired input signal in real-time with a high degree
of accuracy. This includes cancellation of harmonic distortion that aliases
into the desired signal bandwidth from front-end saturation due to high-power
out-of-band interference. Our estimators are shown to achieve a significant
gain over conventional linear-filtering techniques while also being robust to
changes in input parameters, non-linear quantizers, and time-variant
interference sources. For a tone input quantized to 3 bits and estimated with a
fixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error
and $>$20 dBc improvement in Spurious-Free Dynamic Range.

</details>


### [9] [A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff](https://arxiv.org/abs/2507.18587)
*Jérôme Emery,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau*

Main category: eess.SP

TL;DR: 提出了一种基于Transformer的基础模型，用于mMIMO预编码，旨在减少发射机能耗并动态适应用户速率需求，同时通过数据增强解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中深度学习预编码模型对高质量本地数据集的依赖问题，并降低训练复杂度。

Method: 采用Transformer基础模型，结合数据增强方法，通过预训练特征提取器找到与目标分布相似的训练样本。

Result: 在相同能耗下，零样本部署性能显著优于零强迫法，接近加权最小均方误差性能，且复杂度降低8倍。

Conclusion: 该工作通过解决数据可用性和训练复杂度问题，实现了DL解决方案的实际应用，并为资源分配和调度算法提供了动态配置能力。

Abstract: Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [10] [Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes](https://arxiv.org/abs/2507.17893)
*Milad Taghipour,Bane Vasic*

Main category: cs.IT

TL;DR: 该论文探讨了如何利用强化学习技术提升线性分组码的解码性能，通过翻转比特和寻找最优决策。提出了将迭代解码过程映射为马尔可夫决策过程（MDP）的方法，并设计了减少状态数的策略。实验结果表明，所提方法在低密度奇偶校验码（LDPC）上表现高效。


<details>
  <summary>Details</summary>
Motivation: 提升线性分组码的解码性能，减少计算复杂度，并探索强化学习在解码中的应用潜力。

Method: 将解码过程映射为MDP，提出截断MDP以减少状态数，设计基于深度Q网络的动作列表解码器，并利用码的自同构群优化性能。

Result: 实验证明所提方法在LDPC码上显著提升了解码性能，同时降低了计算复杂度。

Conclusion: 强化学习方法能有效提升解码性能，且通过状态数减少和反馈机制进一步优化了复杂度。

Abstract: This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.

</details>


### [11] [Minimax Data Sanitization with Distortion Constraint and Adversarial Inference](https://arxiv.org/abs/2507.17942)
*Amirarsalan Moatazedian,Yauhen Yakimenka,Rémi A. Chou,Jörg Kliewer*

Main category: cs.IT

TL;DR: 论文研究了一种隐私保护数据共享场景，通过优化私有化器在保证重构器失真阈值的同时最大化两个对手的最小损失。


<details>
  <summary>Details</summary>
Motivation: 解决在多个对手拥有相关侧信息时，如何保护数据隐私并允许授权重构的问题。

Method: 提出了一种基于数据驱动的极小极大优化方法，交替更新私有化器、重构器和对手。

Result: 在高斯和二进制场景下获得了理论最优解，并作为基准评估了提出的方法。

Conclusion: 该方法在保护隐私的同时，仅允许通过协作实现准确重构，类似于秘密共享原则。

Abstract: We study a privacy-preserving data-sharing setting where a privatizer
transforms private data into a sanitized version observed by an authorized
reconstructor and two unauthorized adversaries, each with access to side
information correlated with the private data.
  The reconstructor is evaluated under a distortion function, while each
adversary is evaluated using a separate loss function. The privatizer ensures
the reconstructor distortion remains below a fixed threshold while maximizing
the minimum loss across the two adversaries. This two-adversary setting models
cases where individual users cannot reconstruct the data accurately, but their
combined side information enables estimation within the distortion threshold.
The privatizer maximizes individual loss while permitting accurate
reconstruction only through collaboration. This echoes secret-sharing
principles, but with lossy rather than perfect recovery. We frame this as a
constrained data-driven minimax optimization problem and propose a data-driven
training procedure that alternately updates the privatizer, reconstructor, and
adversaries. We also analyze the Gaussian and binary cases as special scenarios
where optimal solutions can be obtained. These theoretical optimal results are
benchmarks for evaluating the proposed minimax training approach.

</details>


### [12] [Deep Learning-based Position-domain Channel Extrapolation for Cell-Free Massive MIMO](https://arxiv.org/abs/2507.17950)
*Jiajia Guo,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的位置域信道外推框架（PCEnet），用于减少无小区大规模MIMO系统中的信道获取开销。


<details>
  <summary>Details</summary>
Motivation: 为了减少信道获取的开销，利用用户位置信息作为不同信道之间的桥梁，提升信道获取效率。

Method: 通过神经网络从获取的信道推断用户位置，利用位置信息设计导频符号并重建其他信道。还提出了简化策略和无位置标签的方法。

Result: 仿真结果表明，PCEnet框架可将导频和反馈开销减少高达50%。

Conclusion: PCEnet通过位置信息显著提升了信道获取性能，同时减少了开销和延迟。

Abstract: To reduce channel acquisition overhead, spatial, time, and frequency-domain
channel extrapolation techniques have been widely studied. In this paper, we
propose a novel deep learning-based Position-domain Channel Extrapolation
framework (named PCEnet) for cell-free massive multiple-input multiple-output
(MIMO) systems. The user's position, which contains significant channel
characteristic information, can greatly enhance the efficiency of channel
acquisition. In cell-free massive MIMO, while the propagation environments
between different base stations and a specific user vary and their respective
channels are uncorrelated, the user's position remains constant and unique
across all channels. Building on this, the proposed PCEnet framework leverages
the position as a bridge between channels to establish a mapping between the
characteristics of different channels, thereby using one acquired channel to
assist in the estimation and feedback of others. Specifically, this approach
first utilizes neural networks (NNs) to infer the user's position from the
obtained channel. {The estimated position, shared among BSs through a central
processing unit (CPU)}, is then fed into an NN to design pilot symbols and
concatenated with the feedback information to the channel reconstruction NN to
reconstruct other channels, thereby significantly enhancing channel acquisition
performance. Additionally, we propose a simplified strategy where only the
estimated position is used in the reconstruction process without modifying the
pilot design, thereby reducing latency. Furthermore, we introduce a position
label-free approach that infers the relative user position instead of the
absolute position, eliminating the need for ground truth position labels during
the localization NN training. Simulation results demonstrate that the proposed
PCEnet framework reduces pilot and feedback overheads by up to 50%.

</details>


### [13] [A Novel Coded Computing Approach for Distributed Multi-Task Learning](https://arxiv.org/abs/2507.18025)
*Minquan Cheng,Yongkang Wang,Lingyu Zhang,Youlong Wu*

Main category: cs.IT

TL;DR: 提出了一种新型编码分布式多任务学习（DMTL）方案，显著降低通信成本，并在理论和实践中验证其最优性。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式多任务学习中，通信瓶颈严重限制系统性能，尤其是在异构数据放置场景下。

Method: 将通信过程建模为矩阵分解问题，提出一种编码方案，利用上行链路编码矩阵的结构特性和下行链路编码矩阵的MDS特性。

Result: 理论分析表明，该方案在温和条件下达到通信开销的理论下限，适用于传统同构和异构计算环境。

Conclusion: 该方案为分布式应用中异构数据放置问题提供了新的解决思路，并具有广泛的可扩展性。

Abstract: Distributed multi-task learning (DMTL) effectively improves model
generalization performance through the collaborative training of multiple
related models. However, in large-scale learning scenarios, communication
bottlenecks severely limit practical system performance. In this paper, we
investigate the communication bottleneck within a typical DMTL system that
employs non-linear global updates. This system involves distributed workers,
assisted by a central server, who collaboratively learn distinct models derived
from a non-linear aggregation of their local model parameters. We first
characterize the communication process as a matrix decomposition problem. It
transforms workers' data storage constraints into structural characteristics of
the uplink encoding matrix, and worker data retrieval demands into Maximum
Distance Separable (MDS) properties of the downlink encoding matrix. Building
on this, we propose a novel coded DTML scheme that can greatly reduce the
communication cost of the DTML with heterogeneous data placement. Theoretical
analysis demonstrates that the proposed scheme achieves the theoretical lower
bound for communication overhead under mild conditions. Remarkably, this
optimality holds for both traditional homogeneous computing environments and
various heterogeneous scenarios. Furthermore, our scheme is extensible to a
distributed linearly separable computation problem where the target function
involves multiple linear combinations of local update values. This indicates
that our scheme offers a new way of tackling heterogeneous data placement
challenges in various distributed applications.

</details>


### [14] [Covert Communications in MEC-Based Networked ISAC Systems Towards Low-Altitude Economy](https://arxiv.org/abs/2507.18194)
*Weihao Mao,Yang Lu,Bo Ai,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 本文研究了基于移动边缘计算（MEC）的低空经济（LAE）系统中的隐蔽传输设计，优化了通信、感知和计算资源以及无人机轨迹，以最小化总能耗。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）依赖集成感知与通信（ISAC）、MEC和隐蔽通信，但现有研究未充分解决隐蔽传输与资源优化的联合问题。

Method: 提出了一种交替优化算法，分解为资源联合优化和无人机轨迹优化两个子问题，分别采用逐次凸逼近和信任域算法求解。

Result: 仿真验证了算法的有效性，揭示了通信、感知和计算之间的权衡关系。

Conclusion: 该算法为LAE系统中的隐蔽传输和资源优化提供了高效解决方案。

Abstract: Low-altitude economy (LAE) is an emerging business model, which heavily
relies on integrated sensing and communications (ISAC), mobile edge computing
(MEC), and covert communications. This paper investigates the convert
transmission design in MEC-based networked ISAC systems towards LAE, where an
MEC server coordinates multiple access points to simultaneously receive
computation tasks from multiple unmanned aerial vehicles (UAVs), locate a
target in a sensing area, and maintain UAVs' covert transmission against
multiple wardens. We first derive closed-form expressions for the detection
error probability (DEP) at wardens. Then, we formulate a total energy
consumption minimization problem by optimizing communication, sensing, and
computation resources as well as UAV trajectories, subject to the requirements
on quality of MEC services, DEP, and radar signal-to-interference-and-noise
ratio, and the causality of UAV trajectories. An alternating optimization based
algorithm is proposed to handle the considered problem, which decomposes it
into two subproblems: joint optimization of communication, sensing, and
computation resources, and UAV trajectory optimization. The former is addressed
by a successive convex approximation based algorithm, while the latter is
solved via a trust-region based algorithm. Simulations validate the
effectiveness of the proposed algorithm compared with various benchmarks, and
reveal the trade-offs among communication, sensing, and computation in LAE
systems.

</details>


### [15] [Hermitian hull of some GRS codes and new EAQMDS codes](https://arxiv.org/abs/2507.18361)
*Oisin Campion,Rodrigo San-José*

Main category: cs.IT

TL;DR: 研究了广义Reed-Solomon码的Hermitian壳的维度问题，通过晶格计数方法得到显式公式，并应用于构造纠缠辅助量子MDS码。


<details>
  <summary>Details</summary>
Motivation: 探索广义Reed-Solomon码的Hermitian壳维度，以优化纠缠辅助量子纠错码的设计。

Method: 将壳维度计算转化为晶格计数问题，求解后得到显式公式。

Result: 提供了壳维度的显式公式，确定了纠缠辅助量子MDS码所需的最大纠缠对数。

Conclusion: 该方法灵活，可生成多种纠缠辅助量子MDS码及新参数。

Abstract: We study the Hermitian hull of a particular family of generalized
Reed-Solomon codes. The problem of computing the dimension of the hull is
translated to a counting problem in a lattice. By solving this problem, we
provide explicit formulas for the dimension of the hull, which determines the
minimum number required of maximally entangled pairs for the associated
entanglement-assisted quantum error-correcting codes. This flexible
construction allows to obtain a wide range of entanglement-assisted quantum MDS
codes, as well as new parameters.

</details>


### [16] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 本文研究了基于语义感知的有限状态马尔可夫链远程估计，利用MAP估计器和AoCE、AoI两种度量优化传输策略，提出了一种高效的结构感知算法Insec-SPI。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过优化传输策略提升有限状态马尔可夫链的远程估计性能，同时满足传输频率约束。

Method: 采用最大后验（MAP）估计器，结合AoCE和AoI度量，将问题建模为带无界成本的约束马尔可夫决策过程（CMDP），并提出一种混合策略。

Result: 证明了存在一种最优的简单混合策略，并开发了高效算法Insec-SPI，显著提升了估计质量。

Conclusion: 结合AoI和AoCE的传输策略优于单独使用任一度量，验证了语义感知方法的有效性。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


### [17] [AI/ML Life Cycle Management for Interoperable AI Native RAN](https://arxiv.org/abs/2507.18538)
*Chu-Hsiang Huang,Chao-Kai Wen,Geoffrey Ye Li*

Main category: cs.IT

TL;DR: 论文探讨了AI/ML在5G RAN中的应用及标准化生命周期管理框架的重要性，总结了3GPP Releases 16-20的演进和挑战。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型在5G RAN中大规模应用时面临的模型漂移、供应商锁定和透明度不足等问题。

Method: 通过3GPP Releases 16-20逐步引入标准化接口和管理功能，构建五块生命周期管理架构。

Result: 实现了模型传输、执行、性能监控和闭环控制的标准化，为6G的AI原生收发器奠定基础。

Conclusion: 尽管取得进展，仍需解决资源高效监控、环境漂移检测等开放挑战。

Abstract: Artificial intelligence (AI) and machine learning (ML) models are rapidly
permeating the 5G Radio Access Network (RAN), powering beam management, channel
state information (CSI) feedback, positioning, and mobility prediction.
However, without a standardized life-cycle management (LCM) framework,
challenges, such as model drift, vendor lock-in, and limited transparency,
hinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML
from experimental features to managed, interoperable network functions.
Beginning with the Network Data Analytics Function (NWDAF) in Rel-16,
subsequent releases introduced standardized interfaces for model transfer,
execution, performance monitoring, and closed-loop control, culminating in
Rel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.
This article reviews the resulting five-block LCM architecture, KPI-driven
monitoring mechanisms, and inter-vendor collaboration schemes, while
identifying open challenges in resource-efficient monitoring, environment drift
detection, intelligent decision-making, and flexible model training. These
developments lay the foundation for AI-native transceivers as a key enabler for
6G.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [18] [Improving Multislice Electron Ptychography with a Generative Prior](https://arxiv.org/abs/2507.17800)
*Christian K. Belardi,Chia-Hao Lee,Yingheng Wang,Justin Lovelace,Kilian Q. Weinberger,David A. Muller,Carla P. Gomes*

Main category: eess.IV

TL;DR: MEP-Diffusion是一种基于扩散模型的生成先验方法，显著提升了多切片电子衍射成像的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有迭代算法耗时且结果不理想，需要一种更高效的方法来改进多切片电子衍射成像的重建质量。

Method: 开发了MEP-Diffusion扩散模型，结合Diffusion Posterior Sampling（DPS）作为生成先验，增强现有迭代求解器。

Result: 该方法在SSIM指标上比现有方法提升了90.50%。

Conclusion: MEP-Diffusion是一种有效的混合方法，显著提升了原子晶体结构的高分辨率成像质量。

Abstract: Multislice electron ptychography (MEP) is an inverse imaging technique that
computationally reconstructs the highest-resolution images of atomic crystal
structures from diffraction patterns. Available algorithms often solve this
inverse problem iteratively but are both time consuming and produce suboptimal
solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion
model trained on a large database of crystal structures specifically for MEP to
augment existing iterative solvers. MEP-Diffusion is easily integrated as a
generative prior into existing reconstruction methods via Diffusion Posterior
Sampling (DPS). We find that this hybrid approach greatly enhances the quality
of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over
existing methods.

</details>


### [19] [Towards Robust Foundation Models for Digital Pathology](https://arxiv.org/abs/2507.17845)
*Jonah Kömen,Edwin D. de Jong,Julius Hense,Hannah Marienwald,Jonas Dippel,Philip Naumann,Eric Marcus,Lukas Ruff,Maximilian Alber,Jonas Teuwen,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 该论文研究了生物医学基础模型（FMs）对非生物技术特征的鲁棒性，提出了PathoROB基准和鲁棒性度量方法，并展示了鲁棒性不足对临床应用的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 生物医学FMs在临床验证中面临非生物技术特征（如手术/内窥镜技术、实验室程序和扫描仪硬件）的干扰风险，需要系统评估其鲁棒性。

Method: 引入PathoROB基准，包含三个新指标（如鲁棒性指数）和四个数据集，评估20种FMs的鲁棒性，并提出鲁棒化框架。

Result: 所有评估的FMs均存在鲁棒性缺陷，非鲁棒性可能导致重大诊断错误；鲁棒化方法显著降低了风险但未完全消除。

Conclusion: 鲁棒性评估是临床采用FMs前的关键步骤，未来FMs开发需将鲁棒性作为核心设计原则，PathoROB为跨生物医学领域的鲁棒性评估提供了蓝图。

Abstract: Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled
healthcare research and entering clinical validation. However, their
susceptibility to learning non-biological technical features -- including
variations in surgical/endoscopic techniques, laboratory procedures, and
scanner hardware -- poses risks for clinical deployment. We present the first
systematic investigation of pathology FM robustness to non-biological features.
Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates
the consequences of limited robustness, and (iii) proposes a framework for FM
robustification to mitigate these issues. Specifically, we developed PathoROB,
a robustness benchmark with three novel metrics, including the robustness
index, and four datasets covering 28 biological classes from 34 medical
centers. Our experiments reveal robustness deficits across all 20 evaluated
FMs, and substantial robustness differences between them. We found that
non-robust FM representations can cause major diagnostic downstream errors and
clinical blunders that prevent safe clinical adoption. Using more robust FMs
and post-hoc robustification considerably reduced (but did not yet eliminate)
the risk of such errors. This work establishes that robustness evaluation is
essential for validating pathology FMs before clinical adoption and
demonstrates that future FM development must integrate robustness as a core
design principle. PathoROB provides a blueprint for assessing robustness across
biomedical domains, guiding FM improvement efforts towards more robust,
representative, and clinically deployable AI systems that prioritize biological
information over technical artifacts.

</details>


### [20] [Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging](https://arxiv.org/abs/2507.17869)
*Atif Bilal Asad,Achyut Paudel,Safal Kshetri,Chenchen Kang,Salik Ram Khanal,Nataliya Shcherbatyuk,Pierre Davadant,R. Paul Schreiner,Santosh Kalauni,Manoj Karkee,Markus Keller*

Main category: eess.IV

TL;DR: 利用高光谱成像和机器学习技术预测葡萄园叶片和冠层的氮浓度，结果显示关键光谱区域对氮含量预测具有稳健性。


<details>
  <summary>Details</summary>
Motivation: 氮是葡萄园中关键的营养元素，但土壤氮的空间和时间变异性高，需要准确估计叶片氮浓度以实现精准施肥。

Method: 采集四种葡萄品种在两个生长季节的高光谱图像，通过特征选择方法确定关键光谱波段，并利用梯度提升和XGBoost模型预测氮浓度。

Result: 机器学习模型在冠层和叶片数据上的R平方分别为0.49和0.57，关键光谱区域（如500-525nm）对氮含量预测具有稳健性。

Conclusion: 研究表明，高光谱成像结合特征选择和机器学习技术可用于监测葡萄园的氮状态。

Abstract: Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting
plant growth and subsequent products such as wine and juice. Because soil N has
high spatial and temporal variability, it is desirable to accurately estimate
the N concentration of grapevine leaves and manage fertilization at the
individual plant level to optimally meet plant needs. In this study, we used
in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of
four different grapevine cultivars collected from distinct vineyards and over
two growth stages during two growing seasons to develop models for predicting N
concentration at the leaf-level and canopy-level. After image processing, two
feature selection methods were employed to identify the optimal set of spectral
bands that were responsive to leaf N concentrations. The selected spectral
bands were used to train and test two different Machine Learning (ML) models,
Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The
comparison of selected bands for both leaf-level and canopy-level datasets
showed that most of the spectral regions identified by the feature selection
methods were across both methods and the dataset types (leaf- and canopy-level
datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,
and 900-950nm. These findings indicated the robustness of these spectral
regions for predicting nitrogen content. The results for N prediction
demonstrated that the ML model achieved an R square of 0.49 for canopy-level
data and an R square of 0.57 for leaf-level data, despite using different sets
of selected spectral bands for each analysis level. The study demonstrated the
potential of using in-field hyperspectral imaging and the use of spectral data
in integrated feature selection and ML techniques to monitor N status in
vineyards.

</details>


### [21] [Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency](https://arxiv.org/abs/2507.17911)
*Dou Hoon Kwark,Shirui Luo,Xiyue Zhu,Yudu Li,Zhi-Pei Liang,Volodymyr Kindratenko*

Main category: eess.IV

TL;DR: 提出了一种分层扩散框架，通过两个垂直的2D阶段（轴向和冠状）实现伪健康MRI图像修复，平衡数据效率和体积一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有2D方法在体积连续性上的不足和3D方法对大量训练数据的需求问题。

Method: 采用轴向扩散模型生成粗略全局一致的修复，再用冠状扩散模型细化解剖细节。

Result: 实验表明，该方法在真实性和体积一致性上优于现有基线。

Conclusion: 该方法为伪健康MRI图像修复提供了高效且一致的解决方案。

Abstract: Pseudo-healthy image inpainting is an essential preprocessing step for
analyzing pathological brain MRI scans. Most current inpainting methods favor
slice-wise 2D models for their high in-plane fidelity, but their independence
across slices produces discontinuities in the volume. Fully 3D models alleviate
this issue, but their high model capacity demands extensive training data for
reliable, high-fidelity synthesis -- often impractical in medical settings. We
address these limitations with a hierarchical diffusion framework by replacing
direct 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial
diffusion model first yields a coarse, globally consistent inpainting; a
coronal diffusion model then refines anatomical details. By combining
perpendicular spatial views with adaptive resampling, our method balances data
efficiency and volumetric consistency. Our experiments show our approach
outperforms state-of-the-art baselines in both realism and volumetric
consistency, making it a promising solution for pseudo-healthy image
inpainting. Code is available at
https://github.com/dou0000/3dMRI-Consistent-Inpaint.

</details>


### [22] [Benchmarking of Deep Learning Methods for Generic MRI Multi-OrganAbdominal Segmentation](https://arxiv.org/abs/2507.17971)
*Deepa Krishnaswamy,Cosmin Ciausu,Steve Pieper,Ron Kikinis,Benjamin Billot,Andrey Fedorov*

Main category: eess.IV

TL;DR: 论文比较了三种MRI腹部分割工具的性能，并提出了基于CT数据的ABDSynth模型，评估了其准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: MRI分割因信号多变性和标注成本高而更具挑战性，现有方法泛化性有限，需全面评估和改进。

Method: 对三种开源模型（MRSegmentator、MRISegmentator-Abdomen、TotalSegmentator MRI）和ABDSynth进行基准测试，使用三个公共数据集评估。

Result: MRSegmentator表现最佳且泛化性强，ABDSynth准确性稍低但数据需求更宽松。

Conclusion: MRSegmentator是首选工具，ABDSynth在标注预算有限时是可行替代方案，代码和数据集已开源。

Abstract: Recent advances in deep learning have led to robust automated tools for
segmentation of abdominal computed tomography (CT). Meanwhile, segmentation of
magnetic resonance imaging (MRI) is substantially more challenging due to the
inherent signal variability and the increased effort required for annotating
training datasets. Hence, existing approaches are trained on limited sets of
MRI sequences, which might limit their generalizability. To characterize the
landscape of MRI abdominal segmentation tools, we present here a comprehensive
benchmarking of the three state-of-the-art and open-source models:
MRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these
models are trained using labor-intensive manual annotation cycles, we also
introduce and evaluate ABDSynth, a SynthSeg-based model purely trained on
widely available CT segmentations (no real images). More generally, we assess
accuracy and generalizability by leveraging three public datasets (not seen by
any of the evaluated methods during their training), which span all major
manufacturers, five MRI sequences, as well as a variety of subject conditions,
voxel resolutions, and fields-of-view. Our results reveal that MRSegmentator
achieves the best performance and is most generalizable. In contrast, ABDSynth
yields slightly less accurate results, but its relaxed requirements in training
data make it an alternative when the annotation budget is limited. The
evaluation code and datasets are given for future benchmarking at
https://github.com/deepakri201/AbdoBench, along with inference code and weights
for ABDSynth.

</details>


### [23] [Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model](https://arxiv.org/abs/2507.18012)
*Hang Xu,Alexandre Bousse,Alessandro Perelli*

Main category: eess.IV

TL;DR: 提出了一种名为DEcomp-MoD的深度学习方法，直接从DECT投影数据生成材料图像，解决了传统方法中光束硬化效应和次优结果的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DECT材料分解方法在图像域进行后处理，未考虑光束硬化效应，导致结果不理想。

Method: 结合光谱DECT模型知识到深度学习训练损失中，并在材料图像域引入基于分数的去噪扩散先验。

Result: 在低剂量AAPM数据集上，DEcomp-MoD在定量和定性评估中均优于现有方法。

Conclusion: DEcomp-MoD在临床诊断中具有潜在应用价值，优于现有无监督和监督学习方法。

Abstract: Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced
technology which enables automatic decomposition of materials in clinical
images without manual segmentation using the dependency of the X-ray linear
attenuation with energy. However, most methods perform material decomposition
in the image domain as a post-processing step after reconstruction but this
procedure does not account for the beam-hardening effect and it results in
sub-optimal results. In this work, we propose a deep learning procedure called
Dual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative
material decomposition which directly converts the DECT projection data into
material images. The algorithm is based on incorporating the knowledge of the
spectral DECT model into the deep learning training loss and combining a
score-based denoising diffusion learned prior in the material image domain.
Importantly the inference optimization loss takes as inputs directly the
sinogram and converts to material images through a model-based conditional
diffusion model which guarantees consistency of the results. We evaluate the
performance with both quantitative and qualitative estimation of the proposed
DEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset.
Finally, we show that DEcomp-MoD outperform state-of-the-art unsupervised
score-based model and supervised deep learning networks, with the potential to
be deployed for clinical diagnosis.

</details>


### [24] [Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks](https://arxiv.org/abs/2507.18112)
*Binghua Li,Ziqing Chang,Tong Liang,Chao Li,Toshihisa Tanaka,Shigeki Aoki,Qibin Zhao,Zhe Sun*

Main category: eess.IV

TL;DR: 论文提出了一种名为TenVOO的参数高效微调方法，用于3D U-Net基础的DDPM在MRI图像生成中的优化，显著减少了参数需求并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D卷积操作参数高效表示的研究不足问题，提升MRI图像生成的效率和质量。

Method: 提出TenVOO方法，利用张量网络建模将3D卷积核表示为低维张量，以少量参数捕捉复杂空间依赖。

Result: 在三个脑MRI数据集上验证，TenVOO在MS-SSIM指标上表现最优，仅需原模型0.3%的可训练参数。

Conclusion: TenVOO为3D卷积操作的参数高效微调提供了有效解决方案，显著提升了MRI图像生成的性能。

Abstract: We address the challenge of parameter-efficient fine-tuning (PEFT) for
three-dimensional (3D) U-Net-based denoising diffusion probabilistic models
(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its
practical significance, research on parameter-efficient representations of 3D
convolution operations remains limited. To bridge this gap, we propose Tensor
Volumetric Operator (TenVOO), a novel PEFT method specifically designed for
fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network
modeling, TenVOO represents 3D convolution kernels with lower-dimensional
tensors, effectively capturing complex spatial dependencies during fine-tuning
with few parameters. We evaluate TenVOO on three downstream brain MRI
datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830
T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that
TenVOO achieves state-of-the-art performance in multi-scale structural
similarity index measure (MS-SSIM), outperforming existing approaches in
capturing spatial dependencies while requiring only 0.3% of the trainable
parameters of the original model. Our code is available at:
https://github.com/xiaovhua/tenvoo

</details>


### [25] [U-Net Based Healthy 3D Brain Tissue Inpainting](https://arxiv.org/abs/2507.18126)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: eess.IV

TL;DR: 论文提出了一种基于U-Net架构的新方法，用于从掩码输入图像合成健康的3D脑组织，并在ASNR-MICCAI BraTS挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决脑MRI扫描中缺失或损坏区域的健康组织重建问题。

Method: 采用U-Net架构，结合数据增强策略（随机掩码健康图像）提升模型泛化能力。

Result: 在BraTS-Local-Inpainting验证集上，SSIM为0.841，PSNR为23.257，MSE为0.007，表现稳定且优异。

Conclusion: 该方法在挑战赛中排名第一，证明了其在脑组织合成任务中的可靠性和一致性。

Abstract: This paper introduces a novel approach to synthesize healthy 3D brain tissue
from masked input images, specifically focusing on the task of 'ASNR-MICCAI
BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a
U-Net-based architecture, which is designed to effectively reconstruct the
missing or corrupted regions of brain MRI scans. To enhance our model's
generalization capabilities and robustness, we implement a comprehensive data
augmentation strategy that involves randomly masking healthy images during
training. Our model is trained on the BraTS-Local-Inpainting dataset and
demonstrates the exceptional performance in recovering healthy brain tissue.
The evaluation metrics employed, including Structural Similarity Index (SSIM),
Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently
yields impressive results. On the BraTS-Local-Inpainting validation set, our
model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score
of 0.007. Notably, these evaluation metrics exhibit relatively low standard
deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE
score, which indicates that our model's reliability and consistency across
various input scenarios. Our method also secured first place in the challenge.

</details>


### [26] [Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution](https://arxiv.org/abs/2507.18133)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: eess.IV

TL;DR: 该论文提出了一种基于预训练模型的方法，用于改进胶质母细胞瘤的诊断，但在BraTS-Path验证集上表现不佳，尽管在测试阶段获得了第二名。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤的异质性使其诊断具有挑战性，传统方法依赖组织样本特征，而深度学习提供了改进诊断的潜力。

Method: 利用预训练模型并在BraTS-Path训练数据集上进行微调。

Result: 模型在验证集上表现较差（准确率、召回率和F1分数均为0.392229），但特异性高达0.898704，测试阶段排名第二。

Conclusion: 模型在识别阴性病例方面表现优异，但整体预测能力有限，未来需进一步优化。

Abstract: Glioblastoma, a highly aggressive brain tumor with diverse molecular and
pathological features, poses a diagnostic challenge due to its heterogeneity.
Accurate diagnosis and assessment of this heterogeneity are essential for
choosing the right treatment and improving patient outcomes. Traditional
methods rely on identifying specific features in tissue samples, but deep
learning offers a promising approach for improved glioblastoma diagnosis. In
this paper, we present our approach to the BraTS-Path Challenge 2024. We
leverage a pre-trained model and fine-tune it on the BraTS-Path training
dataset. Our model demonstrates poor performance on the challenging BraTS-Path
validation set, as rigorously assessed by the Synapse online platform. The
model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of
0.392229, indicating a consistent ability to correctly identify instances under
the target condition. Notably, our model exhibits perfect specificity of
0.898704, showing an exceptional capacity to correctly classify negative cases.
Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,
to signify a limited positive correlation between predicted and actual values
and highlight our model's overall predictive power. Our solution also achieves
the second place during the testing phase.

</details>


### [27] [TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis](https://arxiv.org/abs/2507.18288)
*Xuebo Jin,Longfei Gao,Anshuo Tong,Zhengyang Chen,Jianlei Kong,Ning Sun,Huijun Ma,Qiang Wang,Yuting Bai,Tingli Su*

Main category: eess.IV

TL;DR: 该论文提出了首个专门用于AI驱动中医舌诊的数据集，包含6,719张高质量图像，标准化标注20种病理症状类别，并通过深度学习模型验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 中医舌诊因主观性和缺乏标准化数据集而面临挑战，阻碍了AI在该领域的发展。

Method: 构建了一个包含6,719张标准化舌诊图像的数据集，标注20种病理症状，并验证了九种深度学习模型的性能。

Result: 数据集支持多种标注格式，并通过模型验证展示了其在AI开发中的实用性。

Conclusion: 该数据集为中医舌诊的AI研究提供了关键基础，有助于推动标准化和临床应用。

Abstract: Traditional Chinese medicine (TCM) tongue diagnosis, while clinically
valuable, faces standardization challenges due to subjective interpretation and
inconsistent imaging protocols, compounded by the lack of large-scale,
annotated datasets for AI development. To address this gap, we present the
first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719
high-quality images captured under standardized conditions and annotated with
20 pathological symptom categories (averaging 2.54 clinically validated labels
per image, all verified by licensed TCM practitioners). The dataset supports
multiple annotation formats (COCO, TXT, XML) for broad usability and has been
benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and
MobileNetV2) to demonstrate its utility for AI development. This resource
provides a critical foundation for advancing reliable computational tools in
TCM, bridging the data shortage that has hindered progress in the field, and
facilitating the integration of AI into both research and clinical practice
through standardized, high-quality diagnostic data.

</details>


### [28] [UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model](https://arxiv.org/abs/2507.18362)
*Yilong Hu,Shijie Chang,Lihe Zhang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: eess.IV

TL;DR: UniSegDiff是一种新型扩散概率模型框架，通过分阶段训练和动态调整预测目标，解决了传统扩散模型在病灶分割中的注意力分布不均问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型在病灶分割中表现优异，但传统训练和推理策略导致注意力分布不均，影响效果和效率。

Method: 提出UniSegDiff框架，采用分阶段训练和动态调整预测目标的方法，预训练特征提取网络以实现统一的病灶分割。

Result: 在多种成像模态和器官上的实验表明，UniSegDiff显著优于现有最佳方法。

Conclusion: UniSegDiff通过优化注意力分布，实现了高效且统一的病灶分割，为多模态和多器官任务提供了新解决方案。

Abstract: The Diffusion Probabilistic Model (DPM) has demonstrated remarkable
performance across a variety of generative tasks. The inherent randomness in
diffusion models helps address issues such as blurring at the edges of medical
images and labels, positioning Diffusion Probabilistic Models (DPMs) as a
promising approach for lesion segmentation. However, we find that the current
training and inference strategies of diffusion models result in an uneven
distribution of attention across different timesteps, leading to longer
training times and suboptimal solutions. To this end, we propose UniSegDiff, a
novel diffusion model framework designed to address lesion segmentation in a
unified manner across multiple modalities and organs. This framework introduces
a staged training and inference approach, dynamically adjusting the prediction
targets at different stages, forcing the model to maintain high attention
across all timesteps, and achieves unified lesion segmentation through
pre-training the feature extraction network for segmentation. We evaluate
performance on six different organs across various imaging modalities.
Comprehensive experimental results demonstrate that UniSegDiff significantly
outperforms previous state-of-the-art (SOTA) approaches. The code is available
at https://github.com/HUYILONG-Z/UniSegDiff.

</details>


### [29] [DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis](https://arxiv.org/abs/2507.18433)
*Minxi Ouyang,Lianghui Zhu,Yaqing Bao,Qiang Huang,Jingli Ouyang,Tian Guan,Xitong Ling,Jiawen Li,Song Duan,Wenbin Dai,Li Zheng,Xuemei Zhang,Yonghong He*

Main category: eess.IV

TL;DR: 论文提出了一种针对胃肠道病理图像的多模态模型改进方法，通过构建高质量数据集和引入提示论证策略，显著提升了诊断文本生成的准确性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 当前胃肠道病理的多模态模型受限于数据质量和推理透明度，导致诊断文本生成中易出现事实性错误，且缺乏可审计的中间推理链。

Method: 构建大规模胃肠道病理数据集，结合病变分类和解剖部位信息的提示论证策略，并采用监督微调与GRPO的后训练流程。

Result: 实验显示，该方法在生成质量、结构完整性和临床相关性上显著优于现有模型，临床相关性提升18.7%，结构完整性提高32.4%，诊断错误减少41.2%。

Conclusion: 该方法通过改进数据质量和推理透明度，显著提升了胃肠道病理诊断的准确性和临床实用性。

Abstract: Multimodal large models have shown great potential in automating pathology
image analysis. However, current multimodal models for gastrointestinal
pathology are constrained by both data quality and reasoning transparency:
pervasive noise and incomplete annotations in public datasets predispose vision
language models to factual hallucinations when generating diagnostic text,
while the absence of explicit intermediate reasoning chains renders the outputs
difficult to audit and thus less trustworthy in clinical practice. To address
these issues, we construct a large scale gastrointestinal pathology dataset
containing both microscopic descriptions and diagnostic conclusions, and
propose a prompt argumentation strategy that incorporates lesion classification
and anatomical site information. This design guides the model to better capture
image specific features and maintain semantic consistency in generation.
Furthermore, we employ a post training pipeline that combines supervised fine
tuning with Group Relative Policy Optimization (GRPO) to improve reasoning
quality and output structure. Experimental results on real world pathology
report generation tasks demonstrate that our approach significantly outperforms
state of the art open source and proprietary baselines in terms of generation
quality, structural completeness, and clinical relevance. Our solution
outperforms state of the art models with 18.7% higher clinical relevance, 32.4%
improved structural completeness, and 41.2% fewer diagnostic errors,
demonstrating superior accuracy and clinical utility compared to existing
solutions.

</details>
